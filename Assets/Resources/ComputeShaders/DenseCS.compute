// For stupid future me: if you ever have errors with the compute shader, always check your fuckin' getter functions motherfucker
// they can access even outside the allocated memory
// btw, if some operations from gpu doesn't equal perfectly with the cpu operations, that's not my fault, idk how the gpu approximates the value, maybe some floating point arithmetic goes brr...
RWStructuredBuffer<float> gamma; // weights
RWStructuredBuffer<float> gamma_grad;

RWStructuredBuffer<float> beta; // biases
RWStructuredBuffer<float> beta_grad;

RWStructuredBuffer<float> input;
RWStructuredBuffer<float> output;

RWStructuredBuffer<float> loss;

int batch_size;
int in_features;
int out_features;


float gamma_get(int h, int w)
{
    return gamma[h * in_features + w];
}
void gamma_grad_add(int h, int w, float val)
{
    gamma_grad[h * in_features + w] += val;
}
float beta_get(int w)
{
    return beta[w];
}
void beta_grad_add(int w, float val)
{
    beta_grad[w] += val;
}
float input_get(int h, int w)
{
    return input[h * in_features + w];
}
void output_set(int h, int w, float val)
{
    output[h * out_features + w] = val;
}
float loss_get(int h, int w)
{
    return loss[h * out_features + w];
}

#pragma kernel Forward
[numthreads(32, 32, 1)]
void Forward(int3 id : SV_DispatchThreadID)
{
    // id.x = out_features, id.y = batch_size
    // id.x = p , id.y = n
    
    // input [batch_size * in_features] * tranposed_gamma[in_features, out_features]
    // output [batch_size * out_features]
    
    // input [J x 1 x N x M] * other [K x M x P]
    // out [J x K x N x P]
    
    // make sure if id is inside the matrix
    if (id.x >= out_features || id.y >= batch_size)
        return;
    
    float sum = beta_get(id.x);
    for (int m = 0; m < in_features; m++)
    {
        sum += input_get(id.y, m) * gamma_get(id.x, m); // transposed_gamma_get(m, id.x);
    }
    output_set(id.y, id.x, sum);
    
}

#pragma kernel ComputeGradients
[numthreads(32, 32, 1)]
void ComputeGradients(int3 id : SV_DispatchThreadID)
{
    if (id.x < in_features && id.y < out_features)
    {
        float sum = 0.0f;
        for (int m = 0; m < batch_size; m++)
        {
            sum += input_get(m, id.x) * loss_get(m, id.y); // transposed_loss_get(id.y, m);
        }
        
        gamma_grad_add(id.y, id.x, sum / batch_size);
    }

    // Mean along the batch
    if (id.x == 0 && id.y < out_features)
    {
        float sum = 0.0f;
        for (int m = 0; m < batch_size; m++)
        {
            sum += loss_get(m, id.y); //transposed_loss_get(id.y, m);
        }
        beta_grad_add(id.y, sum / batch_size);
    }
}