{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c7f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1116 10:13:28.884000 2052 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3TextModel(\n",
       "  (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 768, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x Gemma3DecoderLayer(\n",
       "      (self_attn): Gemma3Attention(\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "      )\n",
       "      (mlp): Gemma3MLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=1152, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=1152, bias=False)\n",
       "        (down_proj): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma3RMSNorm((768,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma3RMSNorm((768,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma3RMSNorm((768,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma3RMSNorm((768,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma3RMSNorm((768,), eps=1e-06)\n",
       "  (rotary_emb): Gemma3RotaryEmbedding()\n",
       "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download from the ðŸ¤— Hub\n",
    "emb_model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "model = emb_model[0].auto_model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05772ee",
   "metadata": {},
   "source": [
    "### Save the norm and embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d311dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 0 with 12582912 weights\n",
      "Saved chunk 1 with 12582912 weights\n",
      "Saved chunk 2 with 12582912 weights\n",
      "Saved chunk 3 with 12582912 weights\n",
      "Saved chunk 4 with 12582912 weights\n",
      "Saved chunk 5 with 12582912 weights\n",
      "Saved chunk 6 with 12582912 weights\n",
      "Saved chunk 7 with 12582912 weights\n",
      "Saved chunk 8 with 12582912 weights\n",
      "Saved chunk 9 with 12582912 weights\n",
      "Saved chunk 10 with 12582912 weights\n",
      "Saved chunk 11 with 12582912 weights\n",
      "Saved chunk 12 with 12582912 weights\n",
      "Saved chunk 13 with 12582912 weights\n",
      "Saved chunk 14 with 12582912 weights\n",
      "Saved chunk 15 with 12582912 weights\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "norm = model._modules[\"norm\"]\n",
    "os.makedirs(\"params\", exist_ok=True)\n",
    "\n",
    "with open(f\"params/norm.bin\", \"wb\")as f:\n",
    "    f.write(norm.weight.detach().cpu().numpy().astype(np.float32).tobytes())\n",
    "\n",
    "\n",
    "embed_tokens = model._modules[\"embed_tokens\"].weight.detach()\n",
    "embed_tokens_flat = embed_tokens.flatten()\n",
    "\n",
    "os.makedirs(\"params/embed_tokens\", exist_ok=True)\n",
    "\n",
    "num_chunks = 16\n",
    "chunks = torch.chunk(embed_tokens_flat, num_chunks)\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    # Convert to float32 numpy array\n",
    "    np_chunk = chunk.cpu().numpy().astype('float32')\n",
    "    \n",
    "    # Write raw binary\n",
    "    with open(f\"params/embed_tokens/part_{idx}.bin\", \"wb\") as f:\n",
    "        f.write(np_chunk.tobytes())\n",
    "    \n",
    "    print(f\"Saved chunk {idx} with {np_chunk.size} weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ff526",
   "metadata": {},
   "source": [
    "### Save the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "for idx, layer in tqdm(enumerate(model._modules[\"layers\"])):\n",
    "    self_attn = layer.self_attn\n",
    "    mlp = layer.mlp\n",
    "    input_layernorm = layer.input_layernorm\n",
    "    post_attention_layernorm = layer.post_attention_layernorm\n",
    "    pre_feedforward_layernorm = layer.pre_feedforward_layernorm\n",
    "    post_feedforward_layernorm = layer.post_feedforward_layernorm\n",
    "    \n",
    "    os.makedirs(f\"params/layer_{idx}\", exist_ok = True)\n",
    "    \n",
    "    # ================================================================ GQA =====================================================\n",
    "    with open(f\"params/layer_{idx}/self_attn_q_proj.bin\", \"wb\")as f:\n",
    "        f.write(self_attn.q_proj.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())\n",
    "        \n",
    "    with open(f\"params/layer_{idx}/self_attn_k_proj.bin\", \"wb\")as f:\n",
    "        f.write(self_attn.k_proj.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())\n",
    "        \n",
    "    with open(f\"params/layer_{idx}/self_attn_v_proj.bin\", \"wb\")as f:\n",
    "        f.write(self_attn.v_proj.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())\n",
    "    with open(f\"params/layer_{idx}/self_attn_o_proj.bin\", \"wb\")as f:\n",
    "        f.write(self_attn.o_proj.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())\n",
    "        \n",
    "    with open(f\"params/layer_{idx}/self_attn_q_norm.bin\", \"wb\")as f:\n",
    "        f.write(self_attn.q_norm.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())\n",
    "    with open(f\"params/layer_{idx}/self_attn_k_norm.bin\", \"wb\")as f:\n",
    "        f.write(self_attn.k_norm.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())\n",
    "        \n",
    "    \n",
    "    # =============================================================== MLP ======================================================\n",
    "    with open(f\"params/layer_{idx}/mlp_gate_proj.bin\", \"wb\")as f:\n",
    "        f.write(mlp.gate_proj.weight.detach().cpu().flatten().numpy().astype(np.float32).tobytes())\n",
    "    with open(f\"params/layer_{idx}/mlp_up_proj.bin\", \"wb\")as f:\n",
    "        f.write(mlp.up_proj.weight.detach().cpu().flatten().numpy().astype(np.float32).tobytes())\n",
    "    with open(f\"params/layer_{idx}/mlp_down_proj.bin\", \"wb\")as f:\n",
    "        f.write(mlp.down_proj.weight.detach().cpu().flatten().numpy().astype(np.float32).tobytes())\n",
    "        \n",
    "    # ================================================================ RMS =====================================================\n",
    "    with open(f\"params/layer_{idx}/input_layernorm.bin\", \"wb\")as f:\n",
    "        f.write(input_layernorm.weight.detach().cpu().flatten().numpy().astype(np.float32).tobytes())\n",
    "    with open(f\"params/layer_{idx}/post_attention_layernorm.bin\", \"wb\")as f:\n",
    "        f.write(post_attention_layernorm.weight.detach().cpu().flatten().numpy().astype(np.float32).tobytes())\n",
    "    with open(f\"params/layer_{idx}/pre_feedforward_layernorm.bin\", \"wb\")as f:\n",
    "        f.write(pre_feedforward_layernorm.weight.detach().cpu().flatten().numpy().astype(np.float32).tobytes())  \n",
    "    with open(f\"params/layer_{idx}/post_feedforward_layernorm.bin\", \"wb\")as f:\n",
    "        f.write(post_feedforward_layernorm.weight.detach().cpu().flatten().numpy().astype(np.float32).tobytes())\n",
    "        \n",
    "    # print(vars(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc081a",
   "metadata": {},
   "source": [
    "### Write the weights of the linear modules (module doesn't contain Bias btw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafee35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = emb_model[2].linear\n",
    "dense2 = emb_model[3].linear\n",
    "\n",
    "with open(f\"params/dense_1.bin\", \"wb\")as f:\n",
    "    f.write(dense1.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())\n",
    "        \n",
    "with open(f\"params/dense_2.bin\", \"wb\")as f:\n",
    "    f.write(dense2.weight.detach().flatten().cpu().numpy().astype(np.float32).tobytes())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
