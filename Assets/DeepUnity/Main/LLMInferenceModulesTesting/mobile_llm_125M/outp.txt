
================================================================================
000_model.embed_tokens: Embedding (model.embed_tokens)
================================================================================

  → INPUT[0]: model.embed_tokens_input_0
     Shape: [1, 5]
     Dtype: torch.int64
     Mean: 3.000000, Std: 1.581139
     First 10: [1, 2, 3, 4, 5]
     Last 10:  [1, 2, 3, 4, 5]
     Zeros: 0, Total: 5

  → OUTPUT[0]: model.embed_tokens_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000728, Std: 0.049521
     First 10: [-0.105712890625, -0.023468017578125, 0.051483154296875, 0.0313720703125, 0.047332763671875, -0.0167999267578125, -0.0188751220703125, 0.01373291015625, -0.002361297607421875, 0.005970001220703125]
     Last 10:  [0.041229248046875, 0.034027099609375, 0.0277557373046875, -0.01296234130859375, 0.006580352783203125, -0.083251953125, 0.0091400146484375, 0.05645751953125, 0.05078125, 0.00970458984375]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [32000, 576]
     Mean: 0.001268
     First 10: [-0.03887939453125, 0.005893707275390625, 0.00559234619140625, 0.023040771484375, 0.040069580078125, -0.042144775390625, -0.044403076171875, -0.0538330078125, -0.0206146240234375, -0.034515380859375]
     Last 10:  [-0.00321197509765625, -0.06219482421875, 0.031341552734375, 0.08843994140625, -0.03399658203125, -0.0048980712890625, -0.031463623046875, -0.052520751953125, 0.09637451171875, -0.0178375244140625]

================================================================================
001_model.layers.0.input_layernorm: LlamaRMSNorm (model.layers.0.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.0.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000728, Std: 0.049521
     First 10: [-0.105712890625, -0.023468017578125, 0.051483154296875, 0.0313720703125, 0.047332763671875, -0.0167999267578125, -0.0188751220703125, 0.01373291015625, -0.002361297607421875, 0.005970001220703125]
     Last 10:  [0.041229248046875, 0.034027099609375, 0.0277557373046875, -0.01296234130859375, 0.006580352783203125, -0.083251953125, 0.0091400146484375, 0.05645751953125, 0.05078125, 0.00970458984375]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001793, Std: 0.255825
     First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
     Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.041261
     First 10: [0.2191162109375, 0.07183837890625, 0.049072265625, -0.098388671875, 0.1998291015625, 0.08514404296875, 0.0985107421875, 0.06976318359375, -0.08270263671875, 0.0753173828125]
     Last 10:  [0.07904052734375, 0.06915283203125, 0.056884765625, -0.05657958984375, 0.07000732421875, -0.050628662109375, 0.053741455078125, 0.06427001953125, -0.051544189453125, 0.0657958984375]

================================================================================
002_model.layers.0.self_attn.q_proj: Linear (model.layers.0.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001793, Std: 0.255825
     First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
     Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.112946, Std: 1.552935
     First 10: [0.5033838152885437, 0.5412465333938599, 0.14994597434997559, -0.008902221918106079, -0.1874096691608429, 0.176373690366745, -0.44474470615386963, 0.03787432238459587, -0.31698372960090637, -0.2557930052280426]
     Last 10:  [4.165809631347656, -0.12427692860364914, -0.25147971510887146, -0.6601132750511169, 4.253054141998291, 0.5314412117004395, -0.09199923276901245, 0.17124396562576294, -0.2839110493659973, -0.10712286829948425]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000126
     First 10: [0.00018262863159179688, -0.0248260498046875, -0.042633056640625, 0.038330078125, -0.0032100677490234375, 0.06719970703125, 0.055694580078125, 0.0028667449951171875, -0.041717529296875, -0.03900146484375]
     Last 10:  [-0.0212860107421875, 0.0002760887145996094, 0.07080078125, -0.07012939453125, -0.007236480712890625, -0.04266357421875, -0.03173828125, 0.008270263671875, -0.08782958984375, 0.0026378631591796875]

================================================================================
003_model.layers.0.self_attn.k_proj: Linear (model.layers.0.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001793, Std: 0.255825
     First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
     Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.133324, Std: 1.404734
     First 10: [0.7571072578430176, 1.4551655054092407, -0.005290508270263672, 0.7294079065322876, -1.4618819952011108, 0.385603129863739, -0.6978878378868103, 1.6107616424560547, -0.5896987318992615, 0.29151690006256104]
     Last 10:  [-0.04396691545844078, 3.99662709236145, 0.016898587346076965, 1.3481683731079102, -1.0014292001724243, 0.45284271240234375, -1.1064977645874023, -0.21121539175510406, 0.4838324785232544, 1.4160747528076172]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000144
     First 10: [0.2314453125, 0.1094970703125, 0.0265045166015625, -0.30078125, 0.0224609375, -0.25439453125, -0.04559326171875, -0.1668701171875, 0.01197052001953125, 0.031707763671875]
     Last 10:  [-0.053192138671875, -0.158203125, 0.1083984375, 0.07769775390625, -0.03326416015625, -0.017974853515625, 0.10772705078125, 0.2410888671875, -0.2083740234375, 0.1888427734375]

================================================================================
004_model.layers.0.self_attn.v_proj: Linear (model.layers.0.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001793, Std: 0.255825
     First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
     Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.003835, Std: 0.055827
     First 10: [0.15630188584327698, 0.018999285995960236, -0.004712186753749847, -0.02590961381793022, 0.023915179073810577, -0.00251717958599329, 0.014611541293561459, -0.015919966623187065, 0.0029330477118492126, 0.009663678705692291]
     Last 10:  [0.017394091933965683, -0.06894639879465103, 0.01122063398361206, 0.016725104302167892, 0.10629651695489883, 0.007913182489573956, -0.000908607617020607, -0.05060180649161339, -0.08660925179719925, -0.001465432345867157]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000019
     First 10: [-0.005916595458984375, 0.01409149169921875, -0.040252685546875, 0.0209197998046875, 0.004058837890625, 0.047821044921875, 0.031402587890625, -0.0082244873046875, 0.01462554931640625, -0.024688720703125]
     Last 10:  [-0.018218994140625, 0.02972412109375, 0.03265380859375, -0.0085296630859375, -0.0467529296875, -0.01187896728515625, -0.008331298828125, -0.022491455078125, -0.0036373138427734375, 0.0235137939453125]

================================================================================
005_model.layers.0.self_attn.o_proj: Linear (model.layers.0.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005900, Std: 0.060204
     First 10: [0.15630188584327698, 0.018999285995960236, -0.004712186753749847, -0.02590961381793022, 0.023915179073810577, -0.00251717958599329, 0.014611541293561459, -0.015919966623187065, 0.0029330477118492126, 0.009663678705692291]
     Last 10:  [-0.0021775299683213234, -0.06498672813177109, -0.002680807374417782, 0.01607847400009632, 0.07597081363201141, 0.023918841034173965, 0.00777264591306448, -0.01605452038347721, -0.06435714662075043, 0.007037031464278698]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001761, Std: 0.039185
     First 10: [-0.025306524708867073, -0.021873250603675842, 0.0016701433341950178, 0.03850219398736954, 0.2911089360713959, 0.0014683189801871777, -0.08217266201972961, 0.007354758679866791, -0.01887727901339531, 0.037398651242256165]
     Last 10:  [0.0035476339980959892, 0.03285942226648331, -0.005317067727446556, 0.004096785560250282, 0.029731327667832375, -0.014254536479711533, -0.0008613960817456245, 0.038928065448999405, -0.02528518997132778, 0.014403929002583027]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000013
     First 10: [0.00213623046875, 0.04205322265625, -0.037353515625, 0.0132904052734375, -0.00032806396484375, -0.0511474609375, 0.019622802734375, 0.0174560546875, 0.02606201171875, 0.0063934326171875]
     Last 10:  [0.029022216796875, -0.00335693359375, -0.007747650146484375, 0.01456451416015625, -0.035552978515625, 0.0283355712890625, 0.0230712890625, 0.0102996826171875, -0.0120391845703125, -0.021209716796875]

================================================================================
006_model.layers.0.self_attn: LlamaSdpaAttention (model.layers.0.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.0.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001761, Std: 0.039185
     First 10: [-0.025306524708867073, -0.021873250603675842, 0.0016701433341950178, 0.03850219398736954, 0.2911089360713959, 0.0014683189801871777, -0.08217266201972961, 0.007354758679866791, -0.01887727901339531, 0.037398651242256165]
     Last 10:  [0.0035476339980959892, 0.03285942226648331, -0.005317067727446556, 0.004096785560250282, 0.029731327667832375, -0.014254536479711533, -0.0008613960817456245, 0.038928065448999405, -0.02528518997132778, 0.014403929002583027]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.0.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.255701
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5031329393386841, 0.49686703085899353, 0.0, 0.0, 0.0]
     Last 10:  [0.046319182962179184, 0.6555679440498352, 0.13640841841697693, 0.161704421043396, 0.0, 0.015517447143793106, 0.09633219242095947, 0.13731218874454498, 0.5063778758049011, 0.2444603592157364]
     Zeros: 90, Total: 225

================================================================================
007_model.layers.0.post_attention_layernorm: LlamaRMSNorm (model.layers.0.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.0.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001033, Std: 0.058745
     First 10: [-0.13101941347122192, -0.04534126818180084, 0.053153298795223236, 0.06987426429986954, 0.3384416997432709, -0.015331607311964035, -0.10104778409004211, 0.02108766883611679, -0.021238576620817184, 0.04336865246295929]
     Last 10:  [0.044776882976293564, 0.0668865218758583, 0.022438669577240944, -0.008865555748343468, 0.03631167858839035, -0.09750649333000183, 0.008278618566691875, 0.0953855812549591, 0.02549606002867222, 0.024108517915010452]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003832, Std: 0.212775
     First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
     Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.211037
     First 10: [0.2493896484375, 0.21044921875, 0.217041015625, 0.2183837890625, 0.11480712890625, 0.215576171875, 0.2100830078125, 0.199462890625, 0.2054443359375, 0.1966552734375]
     Last 10:  [0.213623046875, 0.2054443359375, 0.2132568359375, 0.1988525390625, 0.206787109375, 0.1947021484375, 0.205078125, 0.21337890625, 0.208984375, 0.20751953125]

================================================================================
008_model.layers.0.mlp.gate_proj: Linear (model.layers.0.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.0.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003832, Std: 0.212775
     First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
     Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.249022, Std: 0.454324
     First 10: [0.774133563041687, -0.1935971975326538, 0.42558181285858154, 0.5865437388420105, -0.17026937007904053, 0.4110816717147827, 0.3665488362312317, 0.5453060865402222, 0.6536177396774292, 0.5717343091964722]
     Last 10:  [-0.1495017558336258, -0.1377575397491455, 0.7607302665710449, 0.44456207752227783, 0.2130720615386963, 0.32657790184020996, 0.05526885390281677, 0.6562900543212891, -0.2257138341665268, 0.2352910041809082]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000107
     First 10: [-0.05572509765625, 0.025604248046875, 0.0056915283203125, 0.04302978515625, 0.0069122314453125, 0.048980712890625, -0.0038967132568359375, 0.030426025390625, -0.042816162109375, -0.037017822265625]
     Last 10:  [-0.010650634765625, -0.0274505615234375, -0.0286712646484375, -0.0946044921875, -0.050018310546875, -0.0184173583984375, 0.056427001953125, -0.053619384765625, 0.06524658203125, 0.03155517578125]

================================================================================
009_model.layers.0.mlp.act_fn: SiLU (model.layers.0.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.0.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.249022, Std: 0.454324
     First 10: [0.774133563041687, -0.1935971975326538, 0.42558181285858154, 0.5865437388420105, -0.17026937007904053, 0.4110816717147827, 0.3665488362312317, 0.5453060865402222, 0.6536177396774292, 0.5717343091964722]
     Last 10:  [-0.1495017558336258, -0.1377575397491455, 0.7607302665710449, 0.44456207752227783, 0.2130720615386963, 0.32657790184020996, 0.05526885390281677, 0.6562900543212891, -0.2257138341665268, 0.2352910041809082]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.0.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.178735, Std: 0.201166
     First 10: [0.5298281311988831, -0.08745779097080231, 0.25739961862564087, 0.3768964111804962, -0.07790423184633255, 0.2472028285264969, 0.21649283170700073, 0.3452037572860718, 0.4299662411212921, 0.365431547164917]
     Last 10:  [-0.06917357444763184, -0.06414197385311127, 0.5184470415115356, 0.270891934633255, 0.11784321814775467, 0.18971776962280273, 0.028397895395755768, 0.4321187734603882, -0.10017403215169907, 0.13142246007919312]
     Zeros: 0, Total: 7680

================================================================================
010_model.layers.0.mlp.up_proj: Linear (model.layers.0.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.0.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003832, Std: 0.212775
     First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
     Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.007905, Std: 0.451230
     First 10: [0.02607899159193039, 0.9251092672348022, -0.015188708901405334, -0.04484395682811737, 0.8570455312728882, 0.41126400232315063, 0.20365285873413086, -0.04396922141313553, -0.08920896053314209, 0.14263316988945007]
     Last 10:  [0.6473357677459717, 0.11297062039375305, -0.011756278574466705, 0.08435392379760742, 0.147542342543602, -0.5049746036529541, -0.7381752729415894, 0.013063106685876846, -0.3355069160461426, 0.4289051294326782]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000033
     First 10: [0.0301971435546875, 0.040283203125, 0.0265045166015625, -0.025482177734375, -0.031585693359375, -0.0611572265625, -0.06378173828125, -0.0132293701171875, 0.0030040740966796875, 0.039703369140625]
     Last 10:  [0.061126708984375, -0.01157379150390625, -0.07257080078125, 0.0310211181640625, 0.00347900390625, 0.01418304443359375, 0.09033203125, -0.07025146484375, -0.052398681640625, 0.0232391357421875]

================================================================================
011_model.layers.0.mlp.down_proj: Linear (model.layers.0.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.0.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000639, Std: 0.067761
     First 10: [0.013817382976412773, -0.08090801537036896, -0.003909567836672068, -0.016901526600122452, -0.06676747649908066, 0.10166562348604202, 0.044089384377002716, -0.01517834048718214, -0.03835684061050415, 0.05212265998125076]
     Last 10:  [-0.044778529554605484, -0.007246158551424742, -0.006095007993280888, 0.022850798442959785, 0.017386864870786667, -0.0958026573061943, -0.02096262387931347, 0.005644813645631075, 0.03360908105969429, 0.05636776611208916]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.0.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006827, Std: 0.147617
     First 10: [0.02055155485868454, 0.2455417662858963, -0.007903367280960083, 0.18999998271465302, -0.3644137680530548, 0.06386332213878632, 0.05789285525679588, -0.12974077463150024, 0.007551177404820919, -0.20064906775951385]
     Last 10:  [-0.013377692550420761, 0.0752212405204773, 0.0833132192492485, -0.03686423599720001, 0.06534500420093536, 0.18399345874786377, -0.01678416132926941, -0.01757822558283806, -0.05646134167909622, -0.03271164372563362]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000001
     First 10: [-0.0274810791015625, -0.0228271484375, 0.012603759765625, -0.0145263671875, -0.0584716796875, -0.06744384765625, -0.0357666015625, 0.01226043701171875, -0.0066070556640625, 0.059967041015625]
     Last 10:  [0.035186767578125, 0.04180908203125, 0.005237579345703125, -0.006439208984375, -0.0044708251953125, 0.0024871826171875, -0.018310546875, 0.0911865234375, 0.010772705078125, -0.037567138671875]

================================================================================
012_model.layers.0.mlp: LlamaMLP (model.layers.0.mlp)
================================================================================

  → INPUT[0]: model.layers.0.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003832, Std: 0.212775
     First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
     Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.0.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006827, Std: 0.147617
     First 10: [0.02055155485868454, 0.2455417662858963, -0.007903367280960083, 0.18999998271465302, -0.3644137680530548, 0.06386332213878632, 0.05789285525679588, -0.12974077463150024, 0.007551177404820919, -0.20064906775951385]
     Last 10:  [-0.013377692550420761, 0.0752212405204773, 0.0833132192492485, -0.03686423599720001, 0.06534500420093536, 0.18399345874786377, -0.01678416132926941, -0.01757822558283806, -0.05646134167909622, -0.03271164372563362]
     Zeros: 0, Total: 2880

================================================================================
013_model.layers.1.input_layernorm: LlamaRMSNorm (model.layers.1.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.1.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007860, Std: 0.150465
     First 10: [-0.11046785861253738, 0.20020049810409546, 0.04524993151426315, 0.25987425446510315, -0.025972068309783936, 0.04853171482682228, -0.04315492883324623, -0.10865310579538345, -0.013687399215996265, -0.15728041529655457]
     Last 10:  [0.0313991904258728, 0.142107754945755, 0.1057518869638443, -0.04572979360818863, 0.10165668278932571, 0.08648696541786194, -0.008505542762577534, 0.07780735194683075, -0.030965281650424004, -0.008603125810623169]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016017, Std: 0.314043
     First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
     Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.284939
     First 10: [0.315673828125, 0.487548828125, 0.265380859375, 0.40087890625, 0.1324462890625, 0.31982421875, 0.307861328125, 0.27001953125, 0.25146484375, 0.29296875]
     Last 10:  [0.2388916015625, 0.275390625, 0.309326171875, 0.262451171875, 0.250244140625, 0.249755859375, 0.247314453125, 0.2296142578125, 0.259033203125, 0.236328125]

================================================================================
014_model.layers.1.self_attn.q_proj: Linear (model.layers.1.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016017, Std: 0.314043
     First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
     Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.033712, Std: 1.402174
     First 10: [5.6544270515441895, -2.2833328247070312, 1.322256088256836, 1.120871901512146, -0.2613333761692047, 0.3489985167980194, 0.3242741525173187, -0.08701068162918091, -2.2208964824676514, 0.5514967441558838]
     Last 10:  [-1.7431881427764893, -0.31974127888679504, 2.33636212348938, 0.10411253571510315, 0.5894709229469299, 0.8580677509307861, 0.32326075434684753, -1.1342636346817017, 1.266931176185608, -0.6298837661743164]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000128
     First 10: [-0.044158935546875, 0.30810546875, 0.013519287109375, 0.20263671875, 0.1439208984375, -0.128662109375, -0.30615234375, 0.1961669921875, 0.0147552490234375, 0.12255859375]
     Last 10:  [-0.05706787109375, -0.0234832763671875, -0.03948974609375, 0.0211029052734375, -0.0545654296875, 0.006481170654296875, -0.00868988037109375, -0.0504150390625, 0.01641845703125, -0.05706787109375]

================================================================================
015_model.layers.1.self_attn.k_proj: Linear (model.layers.1.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016017, Std: 0.314043
     First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
     Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.174347, Std: 1.890776
     First 10: [4.307790279388428, 0.24903547763824463, -0.38460683822631836, -0.048388659954071045, -0.9271780252456665, -0.0726182609796524, -1.7457337379455566, -2.346543788909912, -1.1731445789337158, 0.9714718461036682]
     Last 10:  [-1.2557222843170166, 5.824743270874023, -0.5365407466888428, 3.121462821960449, -0.6908756494522095, 2.3866610527038574, 2.171079158782959, 1.6431232690811157, -1.3889046907424927, 0.9824296832084656]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000185
     First 10: [-0.08428955078125, 0.11114501953125, -0.036895751953125, 0.1407470703125, 0.0517578125, 0.09515380859375, -0.1229248046875, -0.020843505859375, -0.04541015625, -0.0125579833984375]
     Last 10:  [0.10723876953125, -0.216064453125, 0.1014404296875, 0.1356201171875, 0.08258056640625, -0.10888671875, -0.1500244140625, 0.040924072265625, 0.297607421875, -0.032958984375]

================================================================================
016_model.layers.1.self_attn.v_proj: Linear (model.layers.1.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016017, Std: 0.314043
     First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
     Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.006574, Std: 0.150328
     First 10: [-0.013140656054019928, 0.05268750712275505, 0.08544564247131348, 0.08652074635028839, 0.016232751309871674, -0.07923436164855957, 0.09547887742519379, -0.022125933319330215, 0.1006070002913475, -0.021838292479515076]
     Last 10:  [-0.04853528365492821, -0.05762871354818344, -0.04027634114027023, 0.15780788660049438, -0.12782247364521027, 0.09446044266223907, 0.05632761865854263, -0.0265656765550375, -0.008354637771844864, -0.2001357078552246]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000014
     First 10: [-0.003269195556640625, 0.0056915283203125, -0.02081298828125, -0.0126190185546875, 0.02825927734375, 0.009063720703125, -0.02593994140625, 0.036834716796875, 0.006710052490234375, 0.0006375312805175781]
     Last 10:  [-0.0112762451171875, -0.002902984619140625, 0.005451202392578125, -0.0303497314453125, -0.017120361328125, 0.031707763671875, 0.035430908203125, -0.0014696121215820312, -0.059051513671875, -0.026641845703125]

================================================================================
017_model.layers.1.self_attn.o_proj: Linear (model.layers.1.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.009280, Std: 0.142116
     First 10: [-0.013140656054019928, 0.05268750712275505, 0.08544564247131348, 0.08652074635028839, 0.016232751309871674, -0.07923436164855957, 0.09547887742519379, -0.022125933319330215, 0.1006070002913475, -0.021838292479515076]
     Last 10:  [-0.06786902993917465, -0.011088712140917778, 0.023502111434936523, 0.07350620627403259, -0.14521102607250214, -0.06621630489826202, 0.16627566516399384, 0.08805088698863983, -0.06611942499876022, -0.10407154262065887]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003439, Std: 0.094421
     First 10: [0.14699488878250122, 0.16522417962551117, 0.018222704529762268, 0.039799533784389496, -0.36835792660713196, -0.45509135723114014, 0.6053880453109741, -0.1511288583278656, 0.01855621114373207, 0.06573210656642914]
     Last 10:  [-0.007342725992202759, -0.06285791099071503, -0.0969674289226532, 0.032375916838645935, -0.009292319416999817, -0.09914619475603104, 0.015424775891005993, -0.07605130970478058, -0.055568400770425797, 0.057337209582328796]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000006
     First 10: [-0.00313568115234375, 0.003841400146484375, 0.005802154541015625, -0.002532958984375, -0.0015935897827148438, -0.017822265625, 0.00891876220703125, 0.0024242401123046875, -0.01453399658203125, -0.00560760498046875]
     Last 10:  [0.02288818359375, 0.037811279296875, 0.004253387451171875, -0.006809234619140625, 0.007228851318359375, -0.050201416015625, -0.0039520263671875, 0.015655517578125, -0.003875732421875, -0.0057830810546875]

================================================================================
018_model.layers.1.self_attn: LlamaSdpaAttention (model.layers.1.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.1.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003439, Std: 0.094421
     First 10: [0.14699488878250122, 0.16522417962551117, 0.018222704529762268, 0.039799533784389496, -0.36835792660713196, -0.45509135723114014, 0.6053880453109741, -0.1511288583278656, 0.01855621114373207, 0.06573210656642914]
     Last 10:  [-0.007342725992202759, -0.06285791099071503, -0.0969674289226532, 0.032375916838645935, -0.009292319416999817, -0.09914619475603104, 0.015424775891005993, -0.07605130970478058, -0.055568400770425797, 0.057337209582328796]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.1.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.295431
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.9421323537826538, 0.057867635041475296, 0.0, 0.0, 0.0]
     Last 10:  [0.1998409777879715, 0.20282889902591705, 0.0888049304485321, 0.5085251927375793, 0.0, 0.2701634168624878, 0.26139888167381287, 0.040388479828834534, 0.2363702803850174, 0.19167892634868622]
     Zeros: 90, Total: 225

================================================================================
019_model.layers.1.post_attention_layernorm: LlamaRMSNorm (model.layers.1.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.1.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.011299, Std: 0.179623
     First 10: [0.03652703016996384, 0.3654246926307678, 0.06347263604402542, 0.29967379570007324, -0.3943299949169159, -0.40655964612960815, 0.5622330904006958, -0.25978195667266846, 0.0048688119277358055, -0.09154830873012543]
     Last 10:  [0.024056464433670044, 0.07924984395503998, 0.008784458041191101, -0.013353876769542694, 0.0923643633723259, -0.012659229338169098, 0.006919233128428459, 0.001756042242050171, -0.08653368055820465, 0.04873408377170563]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.009972, Std: 0.203663
     First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
     Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.266304
     First 10: [0.244873046875, 0.1514892578125, 0.29150390625, 0.272216796875, 0.10955810546875, 0.2056884765625, 0.1561279296875, 0.283203125, 0.292724609375, 0.1654052734375]
     Last 10:  [0.301025390625, 0.3291015625, 0.251953125, 0.271728515625, 0.309814453125, 0.27099609375, 0.30322265625, 0.296875, 0.301025390625, 0.297119140625]

================================================================================
020_model.layers.1.mlp.gate_proj: Linear (model.layers.1.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.1.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.009972, Std: 0.203663
     First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
     Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.030218, Std: 0.220124
     First 10: [-0.2788000702857971, -0.15132349729537964, -0.29731041193008423, 0.05133878067135811, 0.10717805474996567, 0.19758184254169464, 0.11098693311214447, 0.17757414281368256, 0.026973171159625053, -0.048807285726070404]
     Last 10:  [-0.099079929292202, 0.07228939235210419, -0.3826596140861511, -0.1305699199438095, 0.19183552265167236, 0.4133463501930237, 0.06703313440084457, 0.24891294538974762, -0.0895785391330719, -0.027612023055553436]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000043
     First 10: [0.049713134765625, -0.06365966796875, -0.06298828125, 0.006801605224609375, 0.0087890625, 0.01332855224609375, 0.05859375, 0.05126953125, -0.073486328125, -0.0011854171752929688]
     Last 10:  [0.01031494140625, -0.004909515380859375, 0.06378173828125, -0.0721435546875, -0.004344940185546875, -0.0496826171875, 0.0012035369873046875, -0.0192718505859375, -0.02105712890625, 0.0377197265625]

================================================================================
021_model.layers.1.mlp.act_fn: SiLU (model.layers.1.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.1.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.030218, Std: 0.220124
     First 10: [-0.2788000702857971, -0.15132349729537964, -0.29731041193008423, 0.05133878067135811, 0.10717805474996567, 0.19758184254169464, 0.11098693311214447, 0.17757414281368256, 0.026973171159625053, -0.048807285726070404]
     Last 10:  [-0.099079929292202, 0.07228939235210419, -0.3826596140861511, -0.1305699199438095, 0.19183552265167236, 0.4133463501930237, 0.06703313440084457, 0.24891294538974762, -0.0895785391330719, -0.027612023055553436]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.1.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.003073, Std: 0.116947
     First 10: [-0.12009256333112717, -0.06994794309139252, -0.1267181932926178, 0.026328163221478462, 0.056458063423633575, 0.10851893573999405, 0.05856983736157417, 0.09664956480264664, 0.013668462634086609, -0.023808224126696587]
     Last 10:  [-0.047087762504816055, 0.03745056688785553, -0.1551629602909088, -0.06102887541055679, 0.10508987307548523, 0.24878902733325958, 0.03463950753211975, 0.1398664116859436, -0.04278453066945076, -0.013615417294204235]
     Zeros: 0, Total: 7680

================================================================================
022_model.layers.1.mlp.up_proj: Linear (model.layers.1.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.1.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.009972, Std: 0.203663
     First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
     Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.001334, Std: 0.187504
     First 10: [0.04637036845088005, 0.03801455348730087, 0.24506424367427826, 0.0020537003874778748, -0.10567010194063187, -0.13597896695137024, -0.3662850260734558, -0.2959253787994385, -0.1478966772556305, -0.014155663549900055]
     Last 10:  [-0.08734194934368134, 0.13003328442573547, -0.1292160451412201, -0.014719143509864807, -0.06583994626998901, 0.0639127641916275, -0.25653275847435, 0.2551459074020386, -0.04634295403957367, -0.22183072566986084]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000013
     First 10: [-0.0195465087890625, 0.0102081298828125, 0.00489044189453125, -0.01049041748046875, 0.001766204833984375, 0.0015544891357421875, 0.0022830963134765625, -0.039581298828125, 0.021575927734375, 0.0285186767578125]
     Last 10:  [0.048858642578125, 0.06610107421875, -0.045379638671875, 0.03314208984375, -0.08197021484375, -0.04437255859375, -0.09375, -0.049591064453125, -0.0780029296875, -0.059356689453125]

================================================================================
023_model.layers.1.mlp.down_proj: Linear (model.layers.1.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.1.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000776, Std: 0.055321
     First 10: [-0.005568736232817173, -0.002659039804711938, -0.031054098159074783, 5.4070158512331545e-05, -0.005965929478406906, -0.014756293036043644, -0.02145325392484665, -0.028601059690117836, -0.0020215201657265425, 0.0003370212216395885]
     Last 10:  [0.004112736787647009, 0.004869820084422827, 0.02004954405128956, 0.0008982927538454533, -0.006919111590832472, 0.01590079441666603, -0.008886168710887432, 0.035686343908309937, 0.0019827615469694138, 0.003020317992195487]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.1.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001952, Std: 0.099599
     First 10: [-0.057210881263017654, 0.1283363699913025, -0.048121389001607895, 0.12318471074104309, -0.6774101257324219, -0.1365349292755127, 0.037335556000471115, -0.09794369339942932, 0.0053521692752838135, -0.059597574174404144]
     Last 10:  [-0.011542011983692646, -0.019285093992948532, -0.006983857601881027, -0.011420756578445435, 0.04144163429737091, 0.05083943158388138, -0.08604114502668381, 0.056537989526987076, -0.046440817415714264, 0.0858861654996872]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000023
     First 10: [-0.02484130859375, 0.03533935546875, -0.0389404296875, -0.01195526123046875, 0.00391387939453125, 0.0699462890625, -0.0085601806640625, 0.0308074951171875, 0.022064208984375, -0.043914794921875]
     Last 10:  [-0.09228515625, -0.003032684326171875, -0.06597900390625, -0.025054931640625, -0.006954193115234375, 0.0187225341796875, 0.026397705078125, 0.034149169921875, -0.0229644775390625, -0.035125732421875]

================================================================================
024_model.layers.1.mlp: LlamaMLP (model.layers.1.mlp)
================================================================================

  → INPUT[0]: model.layers.1.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.009972, Std: 0.203663
     First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
     Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.1.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001952, Std: 0.099599
     First 10: [-0.057210881263017654, 0.1283363699913025, -0.048121389001607895, 0.12318471074104309, -0.6774101257324219, -0.1365349292755127, 0.037335556000471115, -0.09794369339942932, 0.0053521692752838135, -0.059597574174404144]
     Last 10:  [-0.011542011983692646, -0.019285093992948532, -0.006983857601881027, -0.011420756578445435, 0.04144163429737091, 0.05083943158388138, -0.08604114502668381, 0.056537989526987076, -0.046440817415714264, 0.0858861654996872]
     Zeros: 0, Total: 2880

================================================================================
025_model.layers.2.input_layernorm: LlamaRMSNorm (model.layers.2.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.2.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013250, Std: 0.240525
     First 10: [-0.020683851093053818, 0.4937610626220703, 0.015351247042417526, 0.42285850644111633, -1.0717401504516602, -0.5430945754051208, 0.5995686650276184, -0.3577256500720978, 0.010220981203019619, -0.15114587545394897]
     Last 10:  [0.012514452449977398, 0.059964749962091446, 0.0018006004393100739, -0.02477463334798813, 0.1338059902191162, 0.03818020224571228, -0.0791219100356102, 0.05829403176903725, -0.1329745054244995, 0.13462024927139282]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.015760, Std: 0.335650
     First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
     Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.404170
     First 10: [0.45166015625, 0.470703125, 0.416748046875, 0.399169921875, 0.1986083984375, 0.58642578125, 0.474365234375, 0.43115234375, 0.37841796875, 0.293212890625]
     Last 10:  [0.401123046875, 0.412109375, 0.323486328125, 0.392822265625, 0.420166015625, 0.4375, 0.442626953125, 0.40380859375, 0.396728515625, 0.466552734375]

================================================================================
026_model.layers.2.self_attn.q_proj: Linear (model.layers.2.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.015760, Std: 0.335650
     First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
     Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000261, Std: 1.064702
     First 10: [0.7333023548126221, -0.9111893773078918, 0.19315338134765625, -0.022953733801841736, 0.21768642961978912, 0.5415355563163757, -0.07642477750778198, 0.15493005514144897, -0.011029332876205444, 0.14526233077049255]
     Last 10:  [-0.3527904748916626, -0.7480610609054565, 1.3232334852218628, 2.100210428237915, -0.5787128806114197, -0.020605504512786865, -0.4597489833831787, 0.5251026153564453, 0.24047750234603882, -0.43320098519325256]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000047
     First 10: [0.0269012451171875, -0.044158935546875, 0.03472900390625, -0.04656982421875, -0.01396942138671875, -0.04791259765625, -0.0787353515625, -0.0584716796875, 0.015838623046875, 0.05322265625]
     Last 10:  [-0.06280517578125, -0.173828125, -0.06414794921875, 0.03656005859375, -0.066650390625, 0.06988525390625, 0.03948974609375, 0.0002682209014892578, -0.035308837890625, -0.050933837890625]

================================================================================
027_model.layers.2.self_attn.k_proj: Linear (model.layers.2.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.015760, Std: 0.335650
     First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
     Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.030963, Std: 1.076693
     First 10: [0.3599778413772583, 0.8220376968383789, -0.18712687492370605, 0.24025574326515198, 0.18656666576862335, -0.38477277755737305, 0.32027801871299744, -0.38603395223617554, 0.8414723873138428, 1.2157974243164062]
     Last 10:  [-0.11390848457813263, 3.0013866424560547, 0.17063909769058228, 1.1882054805755615, 1.1870455741882324, -0.8459323048591614, -0.30034971237182617, 0.3669646680355072, -3.0457301139831543, -0.19939634203910828]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000021
     First 10: [-0.042266845703125, 0.06854248046875, -0.11273193359375, 0.032745361328125, -0.05078125, 0.06317138671875, 0.050537109375, -0.06317138671875, 0.056732177734375, 0.053955078125]
     Last 10:  [-0.0777587890625, 0.116943359375, 0.06817626953125, -0.09967041015625, 0.083251953125, 0.113037109375, -0.0216827392578125, 0.044708251953125, -0.1729736328125, -0.11029052734375]

================================================================================
028_model.layers.2.self_attn.v_proj: Linear (model.layers.2.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.015760, Std: 0.335650
     First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
     Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.005181, Std: 0.124867
     First 10: [0.10930074751377106, 0.01852099969983101, -0.03498408943414688, -0.016908537596464157, -0.10928560793399811, 0.12025588005781174, 0.06853493303060532, -0.0369487889111042, -0.033113833516836166, -0.01424737274646759]
     Last 10:  [-0.09038268029689789, 0.09866522252559662, -0.1454405039548874, 0.0017186123877763748, 0.09347393363714218, 0.02997349388897419, 0.40196937322616577, 0.03717218339443207, -0.009397335350513458, 0.07749859988689423]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000105
     First 10: [0.016387939453125, -0.00035071372985839844, -0.029815673828125, 0.0102081298828125, -0.0044708251953125, 0.001232147216796875, 0.0113525390625, 0.023773193359375, 0.02264404296875, -0.0174407958984375]
     Last 10:  [-0.0004165172576904297, -0.002288818359375, 0.04132080078125, -0.01485443115234375, -0.0328369140625, -0.00974273681640625, -0.0166778564453125, 0.01654052734375, -0.00557708740234375, -0.01788330078125]

================================================================================
029_model.layers.2.self_attn.o_proj: Linear (model.layers.2.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007934, Std: 0.094966
     First 10: [0.10930074751377106, 0.01852099969983101, -0.03498408943414688, -0.016908537596464157, -0.10928560793399811, 0.12025588005781174, 0.06853493303060532, -0.0369487889111042, -0.033113833516836166, -0.01424737274646759]
     Last 10:  [-0.07381939888000488, 0.05289873480796814, -0.07953393459320068, -0.024912891909480095, -0.0200420580804348, 0.03782053664326668, 0.039256174117326736, -0.08953704684972763, 0.03561319410800934, 0.020485728979110718]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000555, Std: 0.040133
     First 10: [0.03333117812871933, -0.04117128252983093, -0.045981816947460175, -0.014533319510519505, -0.01351021695882082, -0.047197747975587845, 0.043454091995954514, 0.04102688655257225, -0.031258583068847656, 0.05345965176820755]
     Last 10:  [-0.005331395659595728, -0.07464171200990677, 0.03104270249605179, -0.003783557564020157, 0.03477132320404053, 0.0362672433257103, 0.016830790787935257, 0.03133479878306389, 0.033532146364450455, -0.09051783382892609]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000005
     First 10: [-0.04229736328125, -0.061431884765625, 0.00699615478515625, -0.0751953125, -0.0267791748046875, -0.037017822265625, -0.0309600830078125, -0.0166473388671875, 0.01432037353515625, 0.0010833740234375]
     Last 10:  [-0.00713348388671875, 0.0251312255859375, -0.0087738037109375, -0.01776123046875, -0.00689697265625, -0.042877197265625, -0.03582763671875, 0.0257568359375, -0.0084686279296875, -0.0153961181640625]

================================================================================
030_model.layers.2.self_attn: LlamaSdpaAttention (model.layers.2.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.2.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000555, Std: 0.040133
     First 10: [0.03333117812871933, -0.04117128252983093, -0.045981816947460175, -0.014533319510519505, -0.01351021695882082, -0.047197747975587845, 0.043454091995954514, 0.04102688655257225, -0.031258583068847656, 0.05345965176820755]
     Last 10:  [-0.005331395659595728, -0.07464171200990677, 0.03104270249605179, -0.003783557564020157, 0.03477132320404053, 0.0362672433257103, 0.016830790787935257, 0.03133479878306389, 0.033532146364450455, -0.09051783382892609]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.2.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.253912
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4496649503707886, 0.5503350496292114, 0.0, 0.0, 0.0]
     Last 10:  [0.24087342619895935, 0.48508960008621216, 0.08232543617486954, 0.19171148538589478, 0.0, 0.16241221129894257, 0.3128097951412201, 0.1413467824459076, 0.29635104537010193, 0.08708019554615021]
     Zeros: 90, Total: 225

================================================================================
031_model.layers.2.post_attention_layernorm: LlamaRMSNorm (model.layers.2.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.2.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013805, Std: 0.241902
     First 10: [0.012647327035665512, 0.4525897800922394, -0.03063056990504265, 0.4083251953125, -1.0852503776550293, -0.5902923345565796, 0.6430227756500244, -0.31669875979423523, -0.021037600934505463, -0.09768622368574142]
     Last 10:  [0.00718305679038167, -0.014676962047815323, 0.03284330293536186, -0.028558190912008286, 0.16857731342315674, 0.07444744557142258, -0.06229111924767494, 0.08962883055210114, -0.09944236278533936, 0.044102415442466736]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007226, Std: 0.221753
     First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
     Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.304605
     First 10: [0.29248046875, 0.1905517578125, 0.33740234375, 0.309326171875, 0.12548828125, 0.2213134765625, 0.1697998046875, 0.3212890625, 0.353515625, 0.1990966796875]
     Last 10:  [0.341552734375, 0.345458984375, 0.301513671875, 0.30615234375, 0.348388671875, 0.31689453125, 0.345947265625, 0.3310546875, 0.3349609375, 0.343505859375]

================================================================================
032_model.layers.2.mlp.gate_proj: Linear (model.layers.2.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.2.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007226, Std: 0.221753
     First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
     Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.083423, Std: 0.222432
     First 10: [-0.14181184768676758, -0.041152454912662506, -0.07861782610416412, 0.14319467544555664, 0.05370566248893738, 0.1934967339038849, 0.15107406675815582, -0.14219583570957184, 0.30142146348953247, 0.1317731738090515]
     Last 10:  [0.2716795802116394, 0.07624541223049164, 0.10985670238733292, -0.1743449568748474, -0.145074725151062, -0.41468238830566406, -0.2947149872779846, 0.00977267324924469, 0.11093562841415405, 0.20685385167598724]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000018
     First 10: [0.045623779296875, -0.045013427734375, 0.052978515625, 0.0185546875, 0.019317626953125, 0.020477294921875, 0.043060302734375, 0.0032672882080078125, 0.0404052734375, 0.0165252685546875]
     Last 10:  [-0.0648193359375, 0.005886077880859375, 0.046173095703125, -0.09942626953125, -0.027557373046875, -0.0298004150390625, -0.05230712890625, 0.01419830322265625, -0.0243072509765625, -0.0171051025390625]

================================================================================
033_model.layers.2.mlp.act_fn: SiLU (model.layers.2.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.2.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.083423, Std: 0.222432
     First 10: [-0.14181184768676758, -0.041152454912662506, -0.07861782610416412, 0.14319467544555664, 0.05370566248893738, 0.1934967339038849, 0.15107406675815582, -0.14219583570957184, 0.30142146348953247, 0.1317731738090515]
     Last 10:  [0.2716795802116394, 0.07624541223049164, 0.10985670238733292, -0.1743449568748474, -0.145074725151062, -0.41468238830566406, -0.2947149872779846, 0.00977267324924469, 0.11093562841415405, 0.20685385167598724]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.2.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.027995, Std: 0.105524
     First 10: [-0.06588667631149292, -0.020152907818555832, -0.03776451572775841, 0.07671477645635605, 0.027573732659220695, 0.10607951134443283, 0.08123204857110977, -0.06605149805545807, 0.17325404286384583, 0.07022135704755783]
     Last 10:  [0.15417957305908203, 0.03957534208893776, 0.05794244632124901, -0.07959263026714325, -0.06728489696979523, -0.1649564653635025, -0.12579907476902008, 0.004910212941467762, 0.05854133889079094, 0.1140860766172409]
     Zeros: 0, Total: 7680

================================================================================
034_model.layers.2.mlp.up_proj: Linear (model.layers.2.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.2.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007226, Std: 0.221753
     First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
     Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.005740, Std: 0.218702
     First 10: [-0.0445304773747921, -0.06631763279438019, 0.09514583647251129, 0.01395963504910469, 0.2227368950843811, 0.1034187376499176, 0.21252581477165222, 0.08791664242744446, 0.28555843234062195, 0.11279655992984772]
     Last 10:  [-0.010365337133407593, 0.05311800539493561, -0.10396414995193481, 0.08182191848754883, 0.23828716576099396, -0.215040922164917, 0.29237526655197144, 0.09003258496522903, -0.16172713041305542, -0.43905413150787354]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000011
     First 10: [0.047027587890625, -0.043731689453125, -0.039520263671875, 0.043853759765625, 0.0007152557373046875, -0.00040531158447265625, -0.00745391845703125, -0.03729248046875, 0.0447998046875, -0.02215576171875]
     Last 10:  [0.0244140625, -0.0111846923828125, 0.0924072265625, 0.029876708984375, -0.0452880859375, -0.10052490234375, -0.01497650146484375, -0.01206207275390625, 0.0567626953125, -0.0428466796875]

================================================================================
035_model.layers.2.mlp.down_proj: Linear (model.layers.2.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.2.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000894, Std: 0.102264
     First 10: [0.002933965064585209, 0.0013364931801334023, -0.0035931363236159086, 0.0010709102498367429, 0.006141687743365765, 0.01097060926258564, 0.017263907939195633, -0.005807025823742151, 0.04947415366768837, 0.007920727133750916]
     Last 10:  [-0.0015981232281774282, 0.002102163154631853, -0.0060239373706281185, -0.006512421648949385, -0.016033127903938293, 0.03547238931059837, -0.03678053617477417, 0.0004420791519805789, -0.009467722848057747, -0.050089962780475616]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.2.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003222, Std: 0.148650
     First 10: [0.013843273743987083, 0.0741475448012352, 0.233220174908638, 0.08282329887151718, -0.094773069024086, -0.12372307479381561, -0.36185935139656067, -0.15723083913326263, 0.19096963107585907, -0.35864683985710144]
     Last 10:  [-0.027818966656923294, 0.026171445846557617, -0.03871838375926018, -0.07840397953987122, 0.04126948118209839, -0.06331732869148254, 0.05096636712551117, 0.02210124582052231, 0.01394903939217329, 0.01598244160413742]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000012
     First 10: [0.0009107589721679688, -0.02764892578125, -0.03216552734375, -0.005893707275390625, 0.0282745361328125, 0.035888671875, -0.0202789306640625, -0.015167236328125, 0.058135986328125, -0.0731201171875]
     Last 10:  [0.0164947509765625, 0.032806396484375, 0.00382232666015625, -0.0233154296875, 0.059539794921875, 0.065185546875, -0.036376953125, 0.0222625732421875, -0.0980224609375, -0.0093994140625]

================================================================================
036_model.layers.2.mlp: LlamaMLP (model.layers.2.mlp)
================================================================================

  → INPUT[0]: model.layers.2.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007226, Std: 0.221753
     First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
     Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.2.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003222, Std: 0.148650
     First 10: [0.013843273743987083, 0.0741475448012352, 0.233220174908638, 0.08282329887151718, -0.094773069024086, -0.12372307479381561, -0.36185935139656067, -0.15723083913326263, 0.19096963107585907, -0.35864683985710144]
     Last 10:  [-0.027818966656923294, 0.026171445846557617, -0.03871838375926018, -0.07840397953987122, 0.04126948118209839, -0.06331732869148254, 0.05096636712551117, 0.02210124582052231, 0.01394903939217329, 0.01598244160413742]
     Zeros: 0, Total: 2880

================================================================================
037_model.layers.3.input_layernorm: LlamaRMSNorm (model.layers.3.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.3.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010583, Std: 0.303761
     First 10: [0.026490600779652596, 0.5267373323440552, 0.20258960127830505, 0.4911485016345978, -1.180023431777954, -0.7140154242515564, 0.28116342425346375, -0.47392958402633667, 0.1699320375919342, -0.45633307099342346]
     Last 10:  [-0.02063591033220291, 0.011494483798742294, -0.005875080823898315, -0.1069621741771698, 0.20984679460525513, 0.011130116879940033, -0.011324752122163773, 0.11173007637262344, -0.08549332618713379, 0.060084857046604156]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.012797, Std: 0.347329
     First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
     Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.389791
     First 10: [0.389892578125, 0.4580078125, 0.38818359375, 0.382568359375, 0.203369140625, 0.55859375, 0.51416015625, 0.4130859375, 0.39697265625, 0.26708984375]
     Last 10:  [0.3623046875, 0.4189453125, 0.32861328125, 0.35107421875, 0.41552734375, 0.396728515625, 0.408203125, 0.383544921875, 0.36962890625, 0.437255859375]

================================================================================
038_model.layers.3.self_attn.q_proj: Linear (model.layers.3.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.012797, Std: 0.347329
     First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
     Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.034215, Std: 0.915691
     First 10: [-0.9603913426399231, 1.0111327171325684, 1.2033007144927979, -0.6479910612106323, -1.7761136293411255, -1.1252028942108154, -0.48106515407562256, 1.0533173084259033, 0.48715144395828247, -0.9295320510864258]
     Last 10:  [0.04749685525894165, -3.303799629211426, -1.9044828414916992, -1.8732447624206543, -0.6835434436798096, -1.5139360427856445, 0.14144492149353027, -0.5830146074295044, -1.2441668510437012, -1.078377604484558]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000111
     First 10: [-0.024993896484375, 0.00875091552734375, -0.057891845703125, 0.07867431640625, -0.06597900390625, 0.059906005859375, -0.051910400390625, -0.026031494140625, -0.0010280609130859375, -0.0156402587890625]
     Last 10:  [-0.01453399658203125, -0.038726806640625, -0.00402069091796875, -0.1287841796875, -0.10028076171875, 0.12109375, -0.0185394287109375, -0.14453125, -0.1358642578125, -0.07501220703125]

================================================================================
039_model.layers.3.self_attn.k_proj: Linear (model.layers.3.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.012797, Std: 0.347329
     First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
     Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.068470, Std: 1.258778
     First 10: [0.527774453163147, -0.252979576587677, -0.666265606880188, -0.08112695068120956, 0.5307960510253906, 0.1443297266960144, -0.0904371440410614, -0.4899388551712036, 1.4791536331176758, 0.4138545095920563]
     Last 10:  [1.6532325744628906, -0.17497801780700684, -1.088820219039917, 0.6564754247665405, -1.3117616176605225, 3.0680859088897705, -0.4599526524543762, 0.8695080280303955, 0.9346773624420166, 0.20680835843086243]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000202
     First 10: [0.0830078125, 0.15380859375, 0.04351806640625, -0.01096343994140625, -0.1748046875, 0.1524658203125, -0.183837890625, 0.05364990234375, 0.036041259765625, -0.07537841796875]
     Last 10:  [-0.06787109375, -0.025482177734375, 0.062164306640625, 0.1566162109375, 0.061737060546875, 0.0633544921875, -0.076416015625, -0.1502685546875, -0.058563232421875, 0.036376953125]

================================================================================
040_model.layers.3.self_attn.v_proj: Linear (model.layers.3.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.012797, Std: 0.347329
     First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
     Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.000508, Std: 0.134544
     First 10: [-0.06963096559047699, -0.0823480486869812, -0.16832402348518372, 0.035402826964855194, 0.04505624249577522, 0.030097216367721558, 0.015270955860614777, 0.08999179303646088, 0.07497339695692062, -0.03490070998668671]
     Last 10:  [-0.03366328775882721, 0.19623829424381256, -0.018137972801923752, 0.1117287129163742, -0.03150084614753723, 0.07688938081264496, -0.08506433665752411, 0.08511245250701904, 0.005982831120491028, -0.10437114536762238]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000110
     First 10: [-0.0059051513671875, 0.00667572021484375, 0.0202789306640625, -0.047882080078125, 0.00415802001953125, 0.01111602783203125, -0.000156402587890625, -0.01885986328125, 0.0184326171875, 0.0260467529296875]
     Last 10:  [-0.0027217864990234375, -0.01175689697265625, -0.03228759765625, 0.0155181884765625, 0.008880615234375, 0.00836181640625, 0.044281005859375, -0.016387939453125, 0.030731201171875, -0.01397705078125]

================================================================================
041_model.layers.3.self_attn.o_proj: Linear (model.layers.3.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001692, Std: 0.092180
     First 10: [-0.06963096559047699, -0.0823480486869812, -0.16832402348518372, 0.035402826964855194, 0.04505624249577522, 0.030097216367721558, 0.015270955860614777, 0.08999179303646088, 0.07497339695692062, -0.03490070998668671]
     Last 10:  [0.07921987771987915, 0.04688526317477226, 0.004165417514741421, 0.0407702811062336, -0.018677763640880585, -0.026585672050714493, -0.05888291075825691, 0.23680880665779114, 0.027741873636841774, 0.006870264653116465]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002011, Std: 0.075345
     First 10: [0.04790180176496506, 0.00790725089609623, 0.017801187932491302, -0.029068030416965485, 0.044126514345407486, -0.07165082544088364, 0.22726190090179443, 0.0024346911814063787, 0.017036156728863716, 0.0885390043258667]
     Last 10:  [-0.0018596882000565529, -0.058536283671855927, 0.0740852802991867, -0.04166227951645851, -0.030921295285224915, 0.0032780999317765236, 0.051121316850185394, 0.0303204283118248, 0.06873306632041931, -0.036461178213357925]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000009
     First 10: [-0.024688720703125, -0.0187225341796875, 0.03668212890625, -0.02685546875, 0.039398193359375, -0.046234130859375, -0.01116943359375, -0.019927978515625, -0.030914306640625, -0.003948211669921875]
     Last 10:  [-0.0121002197265625, 0.0262451171875, 0.0022335052490234375, -0.0421142578125, 0.02301025390625, -0.0775146484375, -0.03045654296875, -0.039337158203125, 0.01776123046875, 0.02398681640625]

================================================================================
042_model.layers.3.self_attn: LlamaSdpaAttention (model.layers.3.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.3.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002011, Std: 0.075345
     First 10: [0.04790180176496506, 0.00790725089609623, 0.017801187932491302, -0.029068030416965485, 0.044126514345407486, -0.07165082544088364, 0.22726190090179443, 0.0024346911814063787, 0.017036156728863716, 0.0885390043258667]
     Last 10:  [-0.0018596882000565529, -0.058536283671855927, 0.0740852802991867, -0.04166227951645851, -0.030921295285224915, 0.0032780999317765236, 0.051121316850185394, 0.0303204283118248, 0.06873306632041931, -0.036461178213357925]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.3.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.270087
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5563852787017822, 0.4436147212982178, 0.0, 0.0, 0.0]
     Last 10:  [0.3896559476852417, 0.42685142159461975, 0.054475583136081696, 0.12901698052883148, 0.0, 0.35147666931152344, 0.3832237422466278, 0.05836702510714531, 0.1851484179496765, 0.021784164011478424]
     Zeros: 90, Total: 225

================================================================================
043_model.layers.3.post_attention_layernorm: LlamaRMSNorm (model.layers.3.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.3.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008572, Std: 0.303909
     First 10: [0.0743924006819725, 0.534644603729248, 0.22039079666137695, 0.4620804786682129, -1.135896921157837, -0.7856662273406982, 0.5084253549575806, -0.4714948832988739, 0.18696819245815277, -0.36779406666755676]
     Last 10:  [-0.02249559760093689, -0.04704179987311363, 0.06821019947528839, -0.148624449968338, 0.1789254993200302, 0.014408216811716557, 0.03979656472802162, 0.14205050468444824, -0.016760259866714478, 0.02362367883324623]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001490, Std: 0.272908
     First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
     Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.329189
     First 10: [0.321044921875, 0.2454833984375, 0.345703125, 0.344482421875, 0.1580810546875, 0.239501953125, 0.21826171875, 0.3603515625, 0.3798828125, 0.2166748046875]
     Last 10:  [0.356689453125, 0.369384765625, 0.32177734375, 0.329345703125, 0.3798828125, 0.338134765625, 0.36572265625, 0.3642578125, 0.3662109375, 0.345458984375]

================================================================================
044_model.layers.3.mlp.gate_proj: Linear (model.layers.3.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.3.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001490, Std: 0.272908
     First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
     Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.140182, Std: 0.327649
     First 10: [-0.12652917206287384, -0.12910917401313782, -0.16115781664848328, -0.08507073670625687, -0.2554311454296112, -0.38588207960128784, -0.05802881345152855, -0.18510855734348297, -0.04753420501947403, -0.46895045042037964]
     Last 10:  [0.16312150657176971, 0.30525004863739014, -0.20691262185573578, 0.34166014194488525, -0.5112568140029907, -0.4452665150165558, -0.15119311213493347, -0.10216635465621948, -0.15853510797023773, 0.1006893664598465]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000112
     First 10: [0.010528564453125, 0.00592041015625, 0.0194549560546875, -0.0283355712890625, -0.04730224609375, -0.01995849609375, -0.021209716796875, -0.053680419921875, 0.082763671875, 0.00836944580078125]
     Last 10:  [0.0184783935546875, 0.004322052001953125, 0.01338958740234375, -0.0198516845703125, 0.033447265625, -0.025390625, -0.00516510009765625, 0.034637451171875, 0.06976318359375, 0.0012111663818359375]

================================================================================
045_model.layers.3.mlp.act_fn: SiLU (model.layers.3.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.3.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.140182, Std: 0.327649
     First 10: [-0.12652917206287384, -0.12910917401313782, -0.16115781664848328, -0.08507073670625687, -0.2554311454296112, -0.38588207960128784, -0.05802881345152855, -0.18510855734348297, -0.04753420501947403, -0.46895045042037964]
     Last 10:  [0.16312150657176971, 0.30525004863739014, -0.20691262185573578, 0.34166014194488525, -0.5112568140029907, -0.4452665150165558, -0.15119311213493347, -0.10216635465621948, -0.15853510797023773, 0.1006893664598465]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.3.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.042037, Std: 0.191050
     First 10: [-0.05926751345396042, -0.0603930726647377, -0.07409995794296265, -0.040727198123931885, -0.11149241775274277, -0.15616995096206665, -0.028172805905342102, -0.08401235938072205, -0.023202331736683846, -0.18048247694969177]
     Last 10:  [0.08819819241762161, 0.17574021220207214, -0.0927911251783371, 0.19973237812519073, -0.19166962802410126, -0.17387068271636963, -0.06989257782697678, -0.048475950956344604, -0.07299733906984329, 0.05287713184952736]
     Zeros: 0, Total: 7680

================================================================================
046_model.layers.3.mlp.up_proj: Linear (model.layers.3.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.3.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001490, Std: 0.272908
     First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
     Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.005932, Std: 0.272456
     First 10: [0.08344591408967972, 0.1461186408996582, -0.2568758428096771, -0.02086719125509262, 0.0530514121055603, 0.31716015934944153, 0.19432389736175537, 0.009800553321838379, -0.07615585625171661, 0.12208016216754913]
     Last 10:  [-0.49951571226119995, 0.0481812059879303, 0.12930969893932343, -0.15984658896923065, -0.11159844696521759, -0.0447094701230526, 0.15453165769577026, 0.06432764232158661, -0.49257153272628784, -0.48752832412719727]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000027
     First 10: [0.06524658203125, 0.0494384765625, -0.010986328125, -0.0284423828125, -0.0020313262939453125, -0.005771636962890625, 0.0286102294921875, 0.049285888671875, 0.02880859375, -0.0748291015625]
     Last 10:  [-0.08575439453125, 0.068359375, -0.040740966796875, -0.00508880615234375, -0.016021728515625, -0.01995849609375, -0.07769775390625, -0.0117340087890625, -0.05340576171875, -0.0487060546875]

================================================================================
047_model.layers.3.mlp.down_proj: Linear (model.layers.3.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.3.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000615, Std: 0.068785
     First 10: [-0.004945631604641676, -0.008824553340673447, 0.0190344899892807, 0.000849862233735621, -0.005914830137044191, -0.049530886113643646, -0.005474649369716644, -0.0008233676198869944, 0.0017669934313744307, -0.02203333005309105]
     Last 10:  [-0.04405638203024864, 0.008467375300824642, -0.011998792178928852, -0.03192653879523277, 0.021390032023191452, 0.007773666176944971, -0.010800615884363651, -0.0031183436512947083, 0.03595641255378723, -0.025779100134968758]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.3.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004070, Std: 0.096762
     First 10: [0.10987060517072678, -0.19733841717243195, 0.13236036896705627, 0.054239749908447266, 0.12869082391262054, 0.17019742727279663, 0.004337344318628311, 0.15330487489700317, 0.09178666770458221, 0.04711531847715378]
     Last 10:  [0.06788361072540283, 0.06107707694172859, 0.007321672514081001, 0.18564552068710327, 0.046938370913267136, -0.006687292829155922, 0.07814761251211166, 0.02487228810787201, -0.09398137778043747, 0.027552496641874313]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000017
     First 10: [0.003047943115234375, 0.01629638671875, -0.0231170654296875, 0.059906005859375, -0.004192352294921875, -0.007038116455078125, 0.07965087890625, -0.052947998046875, -0.03314208984375, 0.054107666015625]
     Last 10:  [0.0118255615234375, -0.042724609375, -0.054351806640625, -0.00949859619140625, -0.004657745361328125, -0.00214385986328125, 0.01812744140625, 0.00807952880859375, 0.0185699462890625, -0.019927978515625]

================================================================================
048_model.layers.3.mlp: LlamaMLP (model.layers.3.mlp)
================================================================================

  → INPUT[0]: model.layers.3.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001490, Std: 0.272908
     First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
     Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.3.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004070, Std: 0.096762
     First 10: [0.10987060517072678, -0.19733841717243195, 0.13236036896705627, 0.054239749908447266, 0.12869082391262054, 0.17019742727279663, 0.004337344318628311, 0.15330487489700317, 0.09178666770458221, 0.04711531847715378]
     Last 10:  [0.06788361072540283, 0.06107707694172859, 0.007321672514081001, 0.18564552068710327, 0.046938370913267136, -0.006687292829155922, 0.07814761251211166, 0.02487228810787201, -0.09398137778043747, 0.027552496641874313]
     Zeros: 0, Total: 2880

================================================================================
049_model.layers.4.input_layernorm: LlamaRMSNorm (model.layers.4.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.4.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004501, Std: 0.291158
     First 10: [0.18426300585269928, 0.3373062014579773, 0.3527511656284332, 0.5163202285766602, -1.0072060823440552, -0.6154688000679016, 0.512762725353241, -0.3181900084018707, 0.278754860162735, -0.3206787407398224]
     Last 10:  [0.04538801312446594, 0.01403527706861496, 0.07553187012672424, 0.03702107071876526, 0.22586387395858765, 0.007720923982560635, 0.11794418096542358, 0.16692279279232025, -0.11074163764715195, 0.051176175475120544]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008128, Std: 0.345495
     First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
     Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.368273
     First 10: [0.394287109375, 0.434814453125, 0.364501953125, 0.38623046875, 0.30908203125, 0.68505859375, 0.47216796875, 0.3798828125, 0.380126953125, 0.262939453125]
     Last 10:  [0.3291015625, 0.37890625, 0.331787109375, 0.34619140625, 0.405029296875, 0.37353515625, 0.434814453125, 0.37548828125, 0.349609375, 0.381103515625]

================================================================================
050_model.layers.4.self_attn.q_proj: Linear (model.layers.4.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008128, Std: 0.345495
     First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
     Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.033955, Std: 0.969834
     First 10: [0.3499539792537689, -0.6262015104293823, -0.07102936506271362, 0.6608253717422485, 0.7933931946754456, -0.07798846065998077, -0.5186876058578491, 0.0365929901599884, 0.49463796615600586, -0.29091185331344604]
     Last 10:  [0.015421181917190552, 1.6089082956314087, 0.7674106359481812, 0.9109204411506653, 0.12071073055267334, 1.076124906539917, -0.3998664617538452, 0.8778401017189026, 0.8417953252792358, 0.43057602643966675]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000168
     First 10: [-0.0196533203125, 0.0213623046875, 0.0374755859375, 0.032623291015625, -0.006656646728515625, 0.1104736328125, 0.0809326171875, 0.0196990966796875, 0.055023193359375, 0.120361328125]
     Last 10:  [0.024505615234375, -0.126953125, 0.09619140625, -0.0187835693359375, -0.02069091796875, -0.041717529296875, -0.10992431640625, 0.040924072265625, 0.073486328125, 0.006397247314453125]

================================================================================
051_model.layers.4.self_attn.k_proj: Linear (model.layers.4.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008128, Std: 0.345495
     First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
     Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.065226, Std: 1.312274
     First 10: [-0.45099467039108276, 0.48654043674468994, -0.03908967971801758, 0.3915303945541382, 0.140016108751297, 0.6874562501907349, 0.3279276490211487, 1.121958613395691, -0.2037087380886078, 0.30507588386535645]
     Last 10:  [4.374963760375977, 0.6331333518028259, 1.1623709201812744, 0.5452325940132141, -0.4822693169116974, -0.8844747543334961, -0.5815154910087585, 0.8653417229652405, -0.41775965690612793, 0.2902383506298065]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000452
     First 10: [0.06329345703125, -0.0869140625, 0.03753662109375, 0.053436279296875, 0.1336669921875, -0.0633544921875, 0.1566162109375, -0.00836181640625, 0.0236053466796875, -0.050018310546875]
     Last 10:  [-0.0208892822265625, 0.2744140625, 0.01090240478515625, 0.007656097412109375, -0.06744384765625, 0.08148193359375, -0.215087890625, -0.369873046875, 0.1519775390625, 0.0052337646484375]

================================================================================
052_model.layers.4.self_attn.v_proj: Linear (model.layers.4.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008128, Std: 0.345495
     First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
     Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.019253, Std: 0.255743
     First 10: [-0.013895686715841293, -0.07613816112279892, 0.04684597626328468, 0.043456338346004486, -0.03315308690071106, 0.0287503432482481, 0.08841660618782043, -0.021907487884163857, -0.004777014255523682, 0.028528928756713867]
     Last 10:  [-0.05991671606898308, -0.09439192712306976, -0.424213707447052, -0.222523495554924, -0.014070529490709305, 0.1818118393421173, -0.116051584482193, 0.061205532401800156, -0.3673054575920105, -0.23282015323638916]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000016
     First 10: [-0.0268402099609375, -0.0128631591796875, 0.006378173828125, 0.00952911376953125, -0.0015478134155273438, 0.0024814605712890625, -0.0034961700439453125, 0.0278472900390625, -0.0714111328125, 0.0313720703125]
     Last 10:  [-0.0037746429443359375, 0.027923583984375, 0.044464111328125, 0.0260467529296875, -0.01200103759765625, 0.01462554931640625, 0.0083465576171875, 0.0406494140625, 0.01100921630859375, -0.001239776611328125]

================================================================================
053_model.layers.4.self_attn.o_proj: Linear (model.layers.4.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.020297, Std: 0.239129
     First 10: [-0.013895686715841293, -0.07613816112279892, 0.04684597626328468, 0.043456338346004486, -0.03315308690071106, 0.0287503432482481, 0.08841660618782043, -0.021907487884163857, -0.004777014255523682, 0.028528928756713867]
     Last 10:  [-0.07677724957466125, 0.02002410963177681, -0.11201566457748413, -0.0919637605547905, -0.04795454069972038, 0.24534744024276733, -0.05155344679951668, 0.06982751190662384, -0.22088414430618286, -0.17089876532554626]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004233, Std: 0.207638
     First 10: [0.23437198996543884, -0.652759313583374, 0.295864999294281, 0.18938946723937988, 0.3568170964717865, 0.18249580264091492, 0.1303810179233551, -0.09144800901412964, 0.1684950441122055, -0.14709945023059845]
     Last 10:  [-0.093539297580719, -0.04752352088689804, -0.002180315088480711, -0.009834744036197662, -0.021878289058804512, -0.032222360372543335, -0.01297493651509285, -0.027456313371658325, -0.00022863643243908882, -0.061685558408498764]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000018
     First 10: [-0.00435638427734375, -0.016021728515625, 0.027099609375, -0.018157958984375, 0.028839111328125, 0.059356689453125, 0.0180816650390625, -0.045867919921875, 0.05908203125, 0.0243377685546875]
     Last 10:  [0.007511138916015625, -0.0207672119140625, -0.0296478271484375, -0.018218994140625, -0.0224761962890625, -0.04327392578125, 0.044403076171875, -0.035491943359375, 0.00040030479431152344, 0.0020885467529296875]

================================================================================
054_model.layers.4.self_attn: LlamaSdpaAttention (model.layers.4.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.4.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004233, Std: 0.207638
     First 10: [0.23437198996543884, -0.652759313583374, 0.295864999294281, 0.18938946723937988, 0.3568170964717865, 0.18249580264091492, 0.1303810179233551, -0.09144800901412964, 0.1684950441122055, -0.14709945023059845]
     Last 10:  [-0.093539297580719, -0.04752352088689804, -0.002180315088480711, -0.009834744036197662, -0.021878289058804512, -0.032222360372543335, -0.01297493651509285, -0.027456313371658325, -0.00022863643243908882, -0.061685558408498764]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.4.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.259131
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5383939146995544, 0.46160608530044556, 0.0, 0.0, 0.0]
     Last 10:  [0.09336786717176437, 0.08206779509782791, 0.16843755543231964, 0.6561267375946045, 0.0, 0.10221681743860245, 0.1113375723361969, 0.13872957229614258, 0.3980114161968231, 0.24970459938049316]
     Zeros: 90, Total: 225

================================================================================
055_model.layers.4.post_attention_layernorm: LlamaRMSNorm (model.layers.4.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.4.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000269, Std: 0.367799
     First 10: [0.4186350107192993, -0.31545311212539673, 0.6486161947250366, 0.70570969581604, -0.6503889560699463, -0.4329729974269867, 0.6431437730789185, -0.40963801741600037, 0.4472498893737793, -0.46777820587158203]
     Last 10:  [-0.04815128445625305, -0.03348824381828308, 0.07335155457258224, 0.027186326682567596, 0.20398558676242828, -0.024501435458660126, 0.10496924817562103, 0.13946647942066193, -0.11097027361392975, -0.01050938293337822]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000176, Std: 0.321012
     First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
     Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.340310
     First 10: [0.31982421875, 0.203857421875, 0.35498046875, 0.346435546875, 0.176025390625, 0.2198486328125, 0.236083984375, 0.354736328125, 0.36279296875, 0.252197265625]
     Last 10:  [0.380126953125, 0.3623046875, 0.313720703125, 0.33251953125, 0.4033203125, 0.333251953125, 0.36181640625, 0.37158203125, 0.356201171875, 0.364013671875]

================================================================================
056_model.layers.4.mlp.gate_proj: Linear (model.layers.4.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.4.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000176, Std: 0.321012
     First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
     Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.213641, Std: 0.595861
     First 10: [-0.26438701152801514, -0.09240249544382095, -0.17170540988445282, -0.1680271178483963, -0.3767663538455963, -0.6068867444992065, -0.4031659960746765, -0.36431801319122314, -0.4601881206035614, -0.5172584652900696]
     Last 10:  [-0.8473682403564453, -0.6399716138839722, -0.803383469581604, -0.775547444820404, 0.1463160514831543, 0.038333386182785034, -0.49034029245376587, -0.3351455330848694, -0.5544290542602539, 0.31126660108566284]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000102
     First 10: [-0.0097198486328125, 0.03155517578125, -0.0094757080078125, -0.06854248046875, 0.0209197998046875, 0.01190185546875, 0.0924072265625, -0.0117645263671875, -0.0235748291015625, 0.044403076171875]
     Last 10:  [0.10589599609375, -0.058380126953125, 0.024444580078125, -0.0628662109375, 0.0224609375, -0.01273345947265625, 0.036590576171875, 0.07568359375, 0.1168212890625, 0.067626953125]

================================================================================
057_model.layers.4.mlp.act_fn: SiLU (model.layers.4.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.4.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.213641, Std: 0.595861
     First 10: [-0.26438701152801514, -0.09240249544382095, -0.17170540988445282, -0.1680271178483963, -0.3767663538455963, -0.6068867444992065, -0.4031659960746765, -0.36431801319122314, -0.4601881206035614, -0.5172584652900696]
     Last 10:  [-0.8473682403564453, -0.6399716138839722, -0.803383469581604, -0.775547444820404, 0.1463160514831543, 0.038333386182785034, -0.49034029245376587, -0.3351455330848694, -0.5544290542602539, 0.31126660108566284]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.4.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.055302, Std: 0.486111
     First 10: [-0.1148194670677185, -0.04406821355223656, -0.07850007712841034, -0.07697184383869171, -0.15330888330936432, -0.2140912562608719, -0.16148890554904938, -0.14933931827545166, -0.17806574702262878, -0.19319269061088562]
     Last 10:  [-0.25419795513153076, -0.2209520787000656, -0.24848829209804535, -0.2445148229598999, 0.07850059121847153, 0.01953401044011116, -0.18623782694339752, -0.13975206017494202, -0.20227648317813873, 0.1796613335609436]
     Zeros: 0, Total: 7680

================================================================================
058_model.layers.4.mlp.up_proj: Linear (model.layers.4.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.4.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000176, Std: 0.321012
     First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
     Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.005057, Std: 0.558208
     First 10: [0.3182176649570465, 0.23535531759262085, 0.22958579659461975, -0.05688841640949249, 0.1990031599998474, 0.22303371131420135, -0.1264333724975586, -0.22615531086921692, -0.05546984821557999, -0.2018885463476181]
     Last 10:  [0.19127242267131805, -0.0013832151889801025, -0.2616879343986511, 0.11673107743263245, -0.25986170768737793, -0.2821997404098511, -0.0696256011724472, -0.49264949560165405, -0.19463461637496948, 0.3575946092605591]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000052
     First 10: [0.0031681060791015625, -0.0232086181640625, -0.0164642333984375, -0.0003142356872558594, -0.039581298828125, -0.017059326171875, -0.03741455078125, 0.00304412841796875, 0.04339599609375, 0.057373046875]
     Last 10:  [0.038482666015625, 0.1038818359375, -0.038421630859375, -0.0487060546875, -0.06396484375, -0.01221466064453125, 0.0079193115234375, -0.0265045166015625, -0.04833984375, -0.051239013671875]

================================================================================
059_model.layers.4.mlp.down_proj: Linear (model.layers.4.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.4.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.038350, Std: 6.269567
     First 10: [-0.03653758391737938, -0.01037168875336647, -0.018022501841187477, 0.004378806333988905, -0.03050895221531391, -0.04774956777691841, 0.0204175878316164, 0.03377388045191765, 0.009877280332148075, 0.039003390818834305]
     Last 10:  [-0.04862105846405029, 0.00030562427127733827, 0.06502638757228851, -0.028542479500174522, -0.020399298518896103, -0.00551249273121357, 0.012966920621693134, 0.06884878128767014, 0.03937000408768654, 0.06424592435359955]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.4.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.071432, Std: 11.620458
     First 10: [-1.4895509481430054, -2.5261735916137695, -0.7691031098365784, -0.06825529783964157, -11.86688232421875, -7.811278820037842, 5.723982334136963, 0.520793080329895, -0.7017213106155396, -1.7934167385101318]
     Last 10:  [-0.02422521449625492, -0.01246379129588604, -0.1001502275466919, 0.12633441388607025, -0.049669299274683, 0.04701988399028778, -0.07920810580253601, -0.044595733284950256, 0.07090699672698975, 0.07987726479768753]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000007
     First 10: [0.0826416015625, -0.038330078125, 0.06524658203125, 0.02362060546875, -0.0068359375, 0.0006055831909179688, -0.0259246826171875, 0.0149688720703125, -0.049285888671875, 0.038116455078125]
     Last 10:  [0.06640625, -0.0025882720947265625, -0.01204681396484375, -0.02752685546875, 0.0130157470703125, -0.0187835693359375, -0.045654296875, 0.047119140625, 0.03424072265625, -0.0293731689453125]

================================================================================
060_model.layers.4.mlp: LlamaMLP (model.layers.4.mlp)
================================================================================

  → INPUT[0]: model.layers.4.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000176, Std: 0.321012
     First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
     Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.4.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.071432, Std: 11.620458
     First 10: [-1.4895509481430054, -2.5261735916137695, -0.7691031098365784, -0.06825529783964157, -11.86688232421875, -7.811278820037842, 5.723982334136963, 0.520793080329895, -0.7017213106155396, -1.7934167385101318]
     Last 10:  [-0.02422521449625492, -0.01246379129588604, -0.1001502275466919, 0.12633441388607025, -0.049669299274683, 0.04701988399028778, -0.07920810580253601, -0.044595733284950256, 0.07090699672698975, 0.07987726479768753]
     Zeros: 0, Total: 2880

================================================================================
061_model.layers.5.input_layernorm: LlamaRMSNorm (model.layers.5.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.5.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.071701, Std: 11.735382
     First 10: [-1.070915937423706, -2.8416266441345215, -0.12048691511154175, 0.6374543905258179, -12.517271041870117, -8.24425220489502, 6.367125988006592, 0.11115506291389465, -0.25447142124176025, -2.261194944381714]
     Last 10:  [-0.07237649708986282, -0.04595203697681427, -0.02679867297410965, 0.15352073311805725, 0.15431629121303558, 0.022518448531627655, 0.025761142373085022, 0.09487074613571167, -0.04006327688694, 0.06936788558959961]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006907, Std: 0.476014
     First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
     Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.552186
     First 10: [0.471435546875, 0.71337890625, 0.466064453125, 0.509765625, 0.460693359375, 0.5791015625, 0.67333984375, 0.69482421875, 0.46435546875, 0.78759765625]
     Last 10:  [0.5224609375, 0.51953125, 0.5322265625, 0.59716796875, 0.52587890625, 0.56591796875, 0.529296875, 0.5625, 0.60888671875, 0.521484375]

================================================================================
062_model.layers.5.self_attn.q_proj: Linear (model.layers.5.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006907, Std: 0.476014
     First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
     Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.006911, Std: 1.039437
     First 10: [0.011148429475724697, -0.14922311902046204, -0.09255900979042053, -0.2372174710035324, -0.05077295005321503, -0.31918737292289734, 0.11206366866827011, -0.06453379988670349, 0.08821548521518707, -0.40594813227653503]
     Last 10:  [0.28330445289611816, -4.590604782104492, 0.18568271398544312, -0.2808797359466553, 0.06834368407726288, 0.14365559816360474, -1.5202274322509766, 0.14717307686805725, -0.35367539525032043, -2.315927743911743]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000023
     First 10: [-0.013916015625, -0.00457000732421875, -0.03411865234375, -0.023834228515625, 0.007694244384765625, -0.016448974609375, -0.01282501220703125, 0.003993988037109375, 0.0087432861328125, 0.0021762847900390625]
     Last 10:  [0.0167999267578125, -0.040557861328125, -0.00882720947265625, 0.00278472900390625, 0.0017480850219726562, 0.00238800048828125, 0.0186920166015625, -0.0478515625, 0.02392578125, -0.05792236328125]

================================================================================
063_model.layers.5.self_attn.k_proj: Linear (model.layers.5.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006907, Std: 0.476014
     First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
     Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.123410, Std: 1.410842
     First 10: [0.004205349832773209, -0.015669070184230804, -0.0032307193614542484, 0.02122553251683712, -0.006750196218490601, 0.010638093575835228, 0.0006491690874099731, 0.006316151469945908, 0.0018900725990533829, -0.00372314453125]
     Last 10:  [-0.24470752477645874, 0.31668946146965027, 0.07764565944671631, -0.749000072479248, 0.7401829957962036, 0.43620842695236206, 0.49503323435783386, -1.5563727617263794, 1.2032849788665771, 1.6344879865646362]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000112
     First 10: [-0.00975799560546875, 0.00955963134765625, 0.03509521484375, 0.08831787109375, -0.0017900466918945312, 0.030181884765625, 0.01229095458984375, -0.11981201171875, 0.0298004150390625, 0.03289794921875]
     Last 10:  [-0.01617431640625, 0.0165863037109375, 0.0009298324584960938, 0.0261993408203125, 0.00910186767578125, 0.006481170654296875, 0.046234130859375, -0.005123138427734375, 0.0150146484375, -0.005184173583984375]

================================================================================
064_model.layers.5.self_attn.v_proj: Linear (model.layers.5.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006907, Std: 0.476014
     First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
     Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.009668, Std: 0.243731
     First 10: [0.0011690286919474602, -0.005026388913393021, 0.00967811606824398, -0.010617650113999844, 0.0019956007599830627, 0.005627055186778307, -0.0008182357996702194, 0.0007400680333375931, -0.08969124406576157, -0.007815008983016014]
     Last 10:  [0.05988962948322296, 0.029111593961715698, 0.07845437526702881, 0.17910826206207275, -0.12014491111040115, -0.7699041962623596, -0.7329123616218567, -0.098568394780159, -0.10866663604974747, 0.12320106476545334]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000007
     First 10: [0.01085662841796875, 0.007411956787109375, -0.0007085800170898438, 0.01123046875, 0.003955841064453125, -0.0225982666015625, 0.0267486572265625, -0.0014820098876953125, -0.0071563720703125, 0.0007700920104980469]
     Last 10:  [0.0555419921875, -0.028167724609375, 0.047943115234375, -5.614757537841797e-05, -0.0080108642578125, -0.017547607421875, 0.051055908203125, 0.005100250244140625, 0.005191802978515625, -0.07305908203125]

================================================================================
065_model.layers.5.self_attn.o_proj: Linear (model.layers.5.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001668, Std: 0.046043
     First 10: [0.0011690286919474602, -0.005026388913393021, 0.00967811606824398, -0.010617650113999844, 0.0019956007599830627, 0.005627055186778307, -0.0008182357996702194, 0.0007400680333375931, -0.08969124406576157, -0.007815008983016014]
     Last 10:  [-0.012782054953277111, 0.06759936362504959, 0.05926797166466713, 0.016518346965312958, -0.0037660959642380476, -0.16632206737995148, -0.20830589532852173, 0.019624952226877213, -0.002291879616677761, -0.07409363240003586]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002110, Std: 0.030210
     First 10: [0.02999141626060009, -0.10225832462310791, 0.002578926272690296, 0.003558077849447727, -0.04219193011522293, 0.060872748494148254, -0.05349038541316986, -0.034452736377716064, -0.02760591171681881, -0.00025088293477892876]
     Last 10:  [-0.031333230435848236, -0.03280074894428253, 0.02215174399316311, -0.005409642122685909, 0.03900017589330673, 0.036158882081508636, 0.014893000945448875, -0.04701649770140648, -0.007887379266321659, -0.059257686138153076]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000016
     First 10: [0.050140380859375, 0.0157623291015625, -0.032806396484375, 0.005367279052734375, 0.00351715087890625, -0.0142364501953125, 0.0264739990234375, -0.0131683349609375, 0.01824951171875, 0.01509857177734375]
     Last 10:  [0.003246307373046875, 0.018646240234375, -0.035858154296875, 0.01371002197265625, -0.0301361083984375, 0.007740020751953125, -0.060028076171875, -0.0183258056640625, 0.03369140625, -0.04046630859375]

================================================================================
066_model.layers.5.self_attn: LlamaSdpaAttention (model.layers.5.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.5.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002110, Std: 0.030210
     First 10: [0.02999141626060009, -0.10225832462310791, 0.002578926272690296, 0.003558077849447727, -0.04219193011522293, 0.060872748494148254, -0.05349038541316986, -0.034452736377716064, -0.02760591171681881, -0.00025088293477892876]
     Last 10:  [-0.031333230435848236, -0.03280074894428253, 0.02215174399316311, -0.005409642122685909, 0.03900017589330673, 0.036158882081508636, 0.014893000945448875, -0.04701649770140648, -0.007887379266321659, -0.059257686138153076]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.5.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.271301
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000327825546265, 0.49996721744537354, 0.0, 0.0, 0.0]
     Last 10:  [0.37404295802116394, 0.37462079524993896, 0.10715237259864807, 0.14418385922908783, 0.0, 0.2872178256511688, 0.2875540256500244, 0.13794679939746857, 0.20291583240032196, 0.08436549454927444]
     Zeros: 90, Total: 225

================================================================================
067_model.layers.5.post_attention_layernorm: LlamaRMSNorm (model.layers.5.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.5.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.073811, Std: 11.739354
     First 10: [-1.0409245491027832, -2.94388484954834, -0.11790798604488373, 0.6410124897956848, -12.559462547302246, -8.183379173278809, 6.31363582611084, 0.07670232653617859, -0.2820773422718048, -2.2614457607269287]
     Last 10:  [-0.10370972752571106, -0.0787527859210968, -0.004646928980946541, 0.14811109006404877, 0.19331645965576172, 0.05867733061313629, 0.04065414518117905, 0.04785424843430519, -0.04795065522193909, 0.010110199451446533]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003503, Std: 0.317641
     First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
     Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.361149
     First 10: [0.353759765625, 0.26611328125, 0.37841796875, 0.365234375, 0.212158203125, 0.24462890625, 0.25341796875, 0.36962890625, 0.397705078125, 0.2294921875]
     Last 10:  [0.41796875, 0.391357421875, 0.34033203125, 0.355712890625, 0.43505859375, 0.365234375, 0.3828125, 0.38720703125, 0.37451171875, 0.388671875]

================================================================================
068_model.layers.5.mlp.gate_proj: Linear (model.layers.5.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.5.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003503, Std: 0.317641
     First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
     Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.154025, Std: 0.345780
     First 10: [-0.12451913207769394, -0.22349907457828522, -0.09223880618810654, 0.11339700222015381, -0.1403258740901947, 0.046081554144620895, -0.07643620669841766, -0.17636334896087646, -0.23387077450752258, -0.03838684782385826]
     Last 10:  [-0.33730441331863403, -0.015362799167633057, 0.15328581631183624, -0.013188585638999939, -0.45498764514923096, 0.009647637605667114, -0.20168349146842957, -0.586155891418457, -0.17486010491847992, -0.1503814160823822]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000246
     First 10: [-0.00936126708984375, 0.01477813720703125, -0.035064697265625, -0.04345703125, -0.058349609375, -0.01503753662109375, -0.0240325927734375, -0.030792236328125, -0.079833984375, -0.09979248046875]
     Last 10:  [0.0230712890625, 0.062042236328125, 0.00748443603515625, -0.056884765625, 0.05133056640625, 0.00765228271484375, 0.0130767822265625, -0.06414794921875, -0.048736572265625, -0.0853271484375]

================================================================================
069_model.layers.5.mlp.act_fn: SiLU (model.layers.5.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.5.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.154025, Std: 0.345780
     First 10: [-0.12451913207769394, -0.22349907457828522, -0.09223880618810654, 0.11339700222015381, -0.1403258740901947, 0.046081554144620895, -0.07643620669841766, -0.17636334896087646, -0.23387077450752258, -0.03838684782385826]
     Last 10:  [-0.33730441331863403, -0.015362799167633057, 0.15328581631183624, -0.013188585638999939, -0.45498764514923096, 0.009647637605667114, -0.20168349146842957, -0.586155891418457, -0.17486010491847992, -0.1503814160823822]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.5.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.042803, Std: 0.145955
     First 10: [-0.05838831514120102, -0.09931330382823944, -0.043993908911943436, 0.05990977957844734, -0.06524816155433655, 0.0235715601593256, -0.0367581881582737, -0.08042575418949127, -0.10332348942756653, -0.01882508210837841]
     Last 10:  [-0.1404752880334854, -0.007622396573424339, 0.08250556886196136, -0.006550808437168598, -0.17661510407924652, 0.004847087897360325, -0.09070701897144318, -0.20956088602542877, -0.07980545610189438, -0.06954769790172577]
     Zeros: 0, Total: 7680

================================================================================
070_model.layers.5.mlp.up_proj: Linear (model.layers.5.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.5.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003503, Std: 0.317641
     First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
     Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000849, Std: 0.251089
     First 10: [-0.4126914143562317, 0.06960908323526382, -0.1612505167722702, 0.16509202122688293, -0.37451666593551636, 0.37716835737228394, -0.19824187457561493, -0.02358924224972725, 0.029068293049931526, -0.5193350911140442]
     Last 10:  [0.043286487460136414, 0.06982308626174927, -0.06090734899044037, -0.19634106755256653, 0.3223060369491577, 0.1708851456642151, -0.12156863510608673, -0.07584212720394135, 0.6584208607673645, 0.09324852377176285]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000092
     First 10: [-0.032928466796875, 0.0166778564453125, -0.003429412841796875, 0.02374267578125, 0.00466156005859375, -0.00881195068359375, 0.064697265625, 0.029083251953125, -0.00106048583984375, -0.0210418701171875]
     Last 10:  [0.0026454925537109375, -0.1082763671875, -0.0400390625, 0.01617431640625, 0.053955078125, -0.051300048828125, 0.038116455078125, -0.04962158203125, -0.01019287109375, 0.0556640625]

================================================================================
071_model.layers.5.mlp.down_proj: Linear (model.layers.5.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.5.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000780, Std: 0.040417
     First 10: [0.024096356704831123, -0.006913107819855213, 0.007094040513038635, 0.009890626184642315, 0.024436524137854576, 0.008890446275472641, 0.007287011947482824, 0.001897182548418641, -0.003003437537699938, 0.009776526130735874]
     Last 10:  [-0.006080681923776865, -0.00053221924463287, -0.005025195423513651, 0.001286192680709064, -0.05692411586642265, 0.0008282953058369458, 0.01102712843567133, 0.015893543139100075, -0.05254557728767395, -0.006485220044851303]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.5.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001338, Std: 0.067386
     First 10: [0.04179523140192032, 0.03470918536186218, 0.037108033895492554, -0.010444579645991325, 0.021473418921232224, 0.021371955052018166, -0.07702352851629257, 0.07774808257818222, 0.05501452460885048, 0.04908186197280884]
     Last 10:  [0.06648523360490799, -0.11129830777645111, -0.003178226761519909, 0.035789813846349716, -0.061983391642570496, -0.04516693949699402, -0.07562772929668427, 0.047824934124946594, -0.082781121134758, 0.036904413253068924]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000002
     First 10: [0.05230712890625, -0.0364990234375, -0.017608642578125, -0.0252838134765625, -0.050140380859375, 0.019073486328125, 0.00173187255859375, 0.0172119140625, 0.0301971435546875, 0.0263214111328125]
     Last 10:  [0.0255126953125, 0.050262451171875, 0.044586181640625, -0.0270538330078125, 0.0190277099609375, 0.05462646484375, -0.0303192138671875, 0.055755615234375, 0.027587890625, 0.0841064453125]

================================================================================
072_model.layers.5.mlp: LlamaMLP (model.layers.5.mlp)
================================================================================

  → INPUT[0]: model.layers.5.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003503, Std: 0.317641
     First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
     Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.5.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001338, Std: 0.067386
     First 10: [0.04179523140192032, 0.03470918536186218, 0.037108033895492554, -0.010444579645991325, 0.021473418921232224, 0.021371955052018166, -0.07702352851629257, 0.07774808257818222, 0.05501452460885048, 0.04908186197280884]
     Last 10:  [0.06648523360490799, -0.11129830777645111, -0.003178226761519909, 0.035789813846349716, -0.061983391642570496, -0.04516693949699402, -0.07562772929668427, 0.047824934124946594, -0.082781121134758, 0.036904413253068924]
     Zeros: 0, Total: 2880

================================================================================
073_model.layers.6.input_layernorm: LlamaRMSNorm (model.layers.6.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.6.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.075150, Std: 11.757419
     First 10: [-0.9991292953491211, -2.9091756343841553, -0.08079995214939117, 0.6305679082870483, -12.537988662719727, -8.162007331848145, 6.236612319946289, 0.1544504165649414, -0.22706282138824463, -2.2123639583587646]
     Last 10:  [-0.03722449392080307, -0.1900510936975479, -0.00782515574246645, 0.18390090763568878, 0.13133306801319122, 0.013510391116142273, -0.03497358411550522, 0.09567917883396149, -0.13073177635669708, 0.04701461270451546]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.014552, Std: 0.359396
     First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
     Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.450693
     First 10: [0.5400390625, 0.59130859375, 0.42431640625, 0.459716796875, 0.386962890625, 0.6943359375, 0.58837890625, 0.50048828125, 0.533203125, 0.330322265625]
     Last 10:  [0.457275390625, 0.447021484375, 0.40576171875, 0.421875, 0.541015625, 0.44482421875, 0.490478515625, 0.501953125, 0.466552734375, 0.521484375]

================================================================================
074_model.layers.6.self_attn.q_proj: Linear (model.layers.6.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.014552, Std: 0.359396
     First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
     Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001164, Std: 0.995748
     First 10: [0.16988146305084229, -0.18693286180496216, -0.31996291875839233, 0.10492569208145142, -0.006624385714530945, -0.35189545154571533, -0.09926792234182358, 0.060010507702827454, 0.20669510960578918, -0.01383022591471672]
     Last 10:  [0.2204882800579071, 0.38752418756484985, 0.890075147151947, 0.30162888765335083, 0.15894903242588043, -0.7649171948432922, 0.2146669626235962, 1.6500415802001953, 1.4152417182922363, 1.4316480159759521]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000046
     First 10: [-0.11993408203125, -0.09381103515625, 0.017181396484375, 0.051788330078125, -0.0081939697265625, -0.10833740234375, 0.057891845703125, -0.005931854248046875, 0.006999969482421875, -0.05804443359375]
     Last 10:  [0.07568359375, 0.002597808837890625, 0.062225341796875, 0.059326171875, 0.00868988037109375, -0.0182342529296875, -0.1181640625, 0.01922607421875, -0.045379638671875, 0.0064697265625]

================================================================================
075_model.layers.6.self_attn.k_proj: Linear (model.layers.6.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.014552, Std: 0.359396
     First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
     Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.024756, Std: 1.308856
     First 10: [-0.010584324598312378, 0.002955809235572815, -0.00781702995300293, -0.006952986121177673, 0.007539138197898865, -0.0057596489787101746, -0.007018059492111206, -0.02408599853515625, 0.006329894065856934, -0.009182603098452091]
     Last 10:  [0.6661744713783264, -4.13813591003418, -7.789874076843262, -0.23498621582984924, 0.7908744812011719, -0.5723912715911865, -2.9560964107513428, -0.3319821357727051, -0.17184317111968994, 0.6571153998374939]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000379
     First 10: [-0.07611083984375, -0.14697265625, -0.032135009765625, 0.02264404296875, -0.08319091796875, -0.1175537109375, 0.10699462890625, -0.060821533203125, -0.1456298828125, -0.07342529296875]
     Last 10:  [-0.026092529296875, 0.0021820068359375, -0.01554107666015625, -0.05352783203125, 0.03228759765625, -0.060546875, -0.1248779296875, -0.032257080078125, -0.0264892578125, 0.0537109375]

================================================================================
076_model.layers.6.self_attn.v_proj: Linear (model.layers.6.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.014552, Std: 0.359396
     First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
     Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.006922, Std: 0.188651
     First 10: [0.0011938447132706642, -0.00031461846083402634, -0.0017507178708910942, -0.006220131181180477, 0.004878289997577667, -0.0014283442869782448, 0.0026737451553344727, -0.004144820384681225, 0.00033484864979982376, -0.01543469913303852]
     Last 10:  [0.42791229486465454, -0.1636705994606018, 0.04263182356953621, 0.44120025634765625, -0.11101143062114716, -0.5337561964988708, 0.2755693197250366, -0.3248146176338196, 0.17731505632400513, -0.08723072707653046]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000029
     First 10: [-0.0097198486328125, 0.01100921630859375, -0.035552978515625, 0.023773193359375, 0.01678466796875, 0.01207733154296875, 0.001766204833984375, 0.041717529296875, 0.02655029296875, -0.0169830322265625]
     Last 10:  [0.0487060546875, 0.01336669921875, 0.01023101806640625, -0.004436492919921875, 0.00726318359375, -0.01557159423828125, 0.060638427734375, 0.033905029296875, 0.0151519775390625, 0.0168914794921875]

================================================================================
077_model.layers.6.self_attn.o_proj: Linear (model.layers.6.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000718, Std: 0.045032
     First 10: [0.0011938447132706642, -0.00031461846083402634, -0.0017507178708910942, -0.006220131181180477, 0.004878289997577667, -0.0014283442869782448, 0.0026737451553344727, -0.004144820384681225, 0.00033484864979982376, -0.01543469913303852]
     Last 10:  [0.03290938958525658, -0.01516047865152359, 0.0003158330509904772, 0.030431047081947327, -0.012317821383476257, -0.036268800497055054, 0.02889242395758629, -0.07076777517795563, 0.02717437967658043, 0.007521362509578466]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000639, Std: 0.030342
     First 10: [0.010518603026866913, -0.0713873952627182, 0.030366849154233932, 0.0012832274660468102, -0.10351624339818954, 0.035623472183942795, -0.03942456096410751, 0.00708397850394249, -0.03945886716246605, 0.012444126419723034]
     Last 10:  [0.0036127367056906223, 0.018163451924920082, -0.013205692172050476, -0.01990918442606926, 0.03495420143008232, 0.00119050033390522, -0.01550910621881485, -0.08906139433383942, 0.012268740683794022, -0.026396457105875015]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000030
     First 10: [-0.005084991455078125, -0.0224609375, 0.002330780029296875, 0.017578125, -0.033905029296875, -0.00859832763671875, -0.03070068359375, -0.00013899803161621094, -0.0411376953125, 0.05419921875]
     Last 10:  [0.05084228515625, 0.09375, -0.08135986328125, -0.0232086181640625, 0.050506591796875, 0.040557861328125, -0.11273193359375, -0.0256500244140625, 0.0016641616821289062, -0.0416259765625]

================================================================================
078_model.layers.6.self_attn: LlamaSdpaAttention (model.layers.6.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.6.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000639, Std: 0.030342
     First 10: [0.010518603026866913, -0.0713873952627182, 0.030366849154233932, 0.0012832274660468102, -0.10351624339818954, 0.035623472183942795, -0.03942456096410751, 0.00708397850394249, -0.03945886716246605, 0.012444126419723034]
     Last 10:  [0.0036127367056906223, 0.018163451924920082, -0.013205692172050476, -0.01990918442606926, 0.03495420143008232, 0.00119050033390522, -0.01550910621881485, -0.08906139433383942, 0.012268740683794022, -0.026396457105875015]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.6.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.268045
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.499860554933548, 0.5001394152641296, 0.0, 0.0, 0.0]
     Last 10:  [0.4635840952396393, 0.46304911375045776, 0.024189665913581848, 0.04917712137103081, 0.0, 0.42398008704185486, 0.42348578572273254, 0.01828223094344139, 0.05518507957458496, 0.07906688004732132]
     Zeros: 90, Total: 225

================================================================================
079_model.layers.6.post_attention_layernorm: LlamaRMSNorm (model.layers.6.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.6.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.075789, Std: 11.759266
     First 10: [-0.9886106848716736, -2.980562925338745, -0.05043310299515724, 0.6318511366844177, -12.641505241394043, -8.126383781433105, 6.197187900543213, 0.1615343987941742, -0.266521692276001, -2.1999199390411377]
     Last 10:  [-0.033611755818128586, -0.17188763618469238, -0.02103084698319435, 0.16399171948432922, 0.16628727316856384, 0.014700891450047493, -0.05048269033432007, 0.00661778450012207, -0.11846303939819336, 0.020618155598640442]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005965, Std: 0.321562
     First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
     Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.372873
     First 10: [0.3515625, 0.295166015625, 0.402099609375, 0.39111328125, 0.2308349609375, 0.274169921875, 0.285888671875, 0.416748046875, 0.4365234375, 0.224609375]
     Last 10:  [0.4453125, 0.423828125, 0.345458984375, 0.3515625, 0.476318359375, 0.380859375, 0.419189453125, 0.424560546875, 0.39306640625, 0.419189453125]

================================================================================
080_model.layers.6.mlp.gate_proj: Linear (model.layers.6.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.6.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005965, Std: 0.321562
     First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
     Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.152544, Std: 0.342788
     First 10: [-0.040437571704387665, -0.18226295709609985, -0.022938815876841545, 0.13532845675945282, 0.346408873796463, -0.4484005272388458, -0.17972970008850098, -0.33383217453956604, -0.5342963337898254, -0.03976327180862427]
     Last 10:  [0.6838191747665405, -0.3304096758365631, -0.226284459233284, 0.2253202497959137, -0.3593934178352356, 0.4935992658138275, -0.08089885860681534, -0.11824265867471695, -0.6270008683204651, 0.1258198618888855]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000057
     First 10: [0.07135009765625, -0.007282257080078125, 0.0276641845703125, -0.0291900634765625, -0.040771484375, -0.011871337890625, 0.0176544189453125, 0.042572021484375, -0.00042724609375, -0.04449462890625]
     Last 10:  [0.01776123046875, 0.10394287109375, -0.07244873046875, -0.00635528564453125, 0.007617950439453125, -0.00795745849609375, -0.0262603759765625, -0.005786895751953125, -0.0584716796875, 0.0360107421875]

================================================================================
081_model.layers.6.mlp.act_fn: SiLU (model.layers.6.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.6.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.152544, Std: 0.342788
     First 10: [-0.040437571704387665, -0.18226295709609985, -0.022938815876841545, 0.13532845675945282, 0.346408873796463, -0.4484005272388458, -0.17972970008850098, -0.33383217453956604, -0.5342963337898254, -0.03976327180862427]
     Last 10:  [0.6838191747665405, -0.3304096758365631, -0.226284459233284, 0.2253202497959137, -0.3593934178352356, 0.4935992658138275, -0.08089885860681534, -0.11824265867471695, -0.6270008683204651, 0.1258198618888855]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.6.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.042578, Std: 0.147085
     First 10: [-0.019810041412711143, -0.08284944295883179, -0.011337867006659508, 0.07223569601774216, 0.2029077708721161, -0.1747601330280304, -0.0818108320236206, -0.13931100070476532, -0.19743071496486664, -0.019486406818032265]
     Last 10:  [0.4544597566127777, -0.13815781474113464, -0.10039541125297546, 0.12529900670051575, -0.1477489322423935, 0.30650243163108826, -0.03881416469812393, -0.05563006550073624, -0.218315988779068, 0.06686237454414368]
     Zeros: 0, Total: 7680

================================================================================
082_model.layers.6.mlp.up_proj: Linear (model.layers.6.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.6.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005965, Std: 0.321562
     First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
     Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.001380, Std: 0.254316
     First 10: [0.01806211844086647, 0.04115426540374756, -0.27983665466308594, -0.040245961397886276, -0.3854528069496155, -0.026170141994953156, 0.0709039717912674, 0.08914915472269058, -0.014766678214073181, -0.05836687237024307]
     Last 10:  [0.19518424570560455, 0.5093384981155396, -0.06118357926607132, 0.21455642580986023, -0.3395906388759613, -0.17478854954242706, -0.01667008548974991, -0.262503981590271, 0.5017397999763489, -0.07819198817014694]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000066
     First 10: [-0.08660888671875, -0.0091552734375, 0.0048675537109375, 0.034759521484375, -0.0297393798828125, -0.01497650146484375, 0.0001385211944580078, -0.0003154277801513672, 0.0197906494140625, -0.03240966796875]
     Last 10:  [0.0226898193359375, 0.09930419921875, -0.029022216796875, 0.0018901824951171875, -0.00786590576171875, -0.02435302734375, -0.07537841796875, 0.045989990234375, -0.06268310546875, -0.0265350341796875]

================================================================================
083_model.layers.6.mlp.down_proj: Linear (model.layers.6.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.6.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000177, Std: 0.044493
     First 10: [-0.00035781131009571254, -0.003409608034417033, 0.0031727508176118135, -0.0029071951285004616, -0.07821136713027954, 0.0045734974555671215, -0.00580071285367012, -0.012419457547366619, 0.002915395889431238, 0.0011373605811968446]
     Last 10:  [0.08870338648557663, -0.07036909461021423, 0.006142550613731146, 0.02688370645046234, 0.050174154341220856, -0.0535731166601181, 0.0006470354273915291, 0.014603113755583763, -0.10953781753778458, -0.005228102207183838]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.6.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001956, Std: 0.074355
     First 10: [0.09686530381441116, 0.06077098473906517, -0.002336648292839527, 0.01037447340786457, 0.10881470888853073, 0.038441069424152374, -0.14172066748142242, -0.04874054342508316, 0.02985478565096855, 0.024338584393262863]
     Last 10:  [-0.10427617281675339, -0.03805746138095856, 0.20600228011608124, -0.04282350838184357, -0.09786099940538406, 0.023553866893053055, -0.09372316300868988, -0.05960901081562042, 0.05307437852025032, 0.03354959562420845]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000010
     First 10: [-0.09259033203125, -0.00011652708053588867, 0.011505126953125, -0.00396728515625, -0.03057861328125, 0.0748291015625, -0.0265655517578125, 0.040069580078125, 0.0108184814453125, -0.0723876953125]
     Last 10:  [0.0174713134765625, 0.01374053955078125, 0.0181121826171875, -0.00699615478515625, 0.047882080078125, -0.0239105224609375, -0.0136260986328125, -0.005191802978515625, 0.0128173828125, -0.027862548828125]

================================================================================
084_model.layers.6.mlp: LlamaMLP (model.layers.6.mlp)
================================================================================

  → INPUT[0]: model.layers.6.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005965, Std: 0.321562
     First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
     Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.6.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001956, Std: 0.074355
     First 10: [0.09686530381441116, 0.06077098473906517, -0.002336648292839527, 0.01037447340786457, 0.10881470888853073, 0.038441069424152374, -0.14172066748142242, -0.04874054342508316, 0.02985478565096855, 0.024338584393262863]
     Last 10:  [-0.10427617281675339, -0.03805746138095856, 0.20600228011608124, -0.04282350838184357, -0.09786099940538406, 0.023553866893053055, -0.09372316300868988, -0.05960901081562042, 0.05307437852025032, 0.03354959562420845]
     Zeros: 0, Total: 2880

================================================================================
085_model.layers.7.input_layernorm: LlamaRMSNorm (model.layers.7.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.7.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.073834, Std: 11.786163
     First 10: [-0.891745388507843, -2.9197919368743896, -0.052769750356674194, 0.6422256231307983, -12.53269100189209, -8.087943077087402, 6.055467128753662, 0.11279385536909103, -0.23666690289974213, -2.175581455230713]
     Last 10:  [-0.13788792490959167, -0.20994509756565094, 0.18497143685817719, 0.12116821110248566, 0.06842627376317978, 0.0382547602057457, -0.14420585334300995, -0.05299122631549835, -0.06538866460323334, 0.05416775122284889]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013281, Std: 0.389365
     First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
     Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.466474
     First 10: [0.6298828125, 0.61376953125, 0.45068359375, 0.4443359375, 0.47119140625, 0.8212890625, 0.66552734375, 0.61962890625, 0.5703125, 0.374755859375]
     Last 10:  [0.494140625, 0.434814453125, 0.41015625, 0.471923828125, 0.685546875, 0.457763671875, 0.43896484375, 0.50537109375, 0.40869140625, 0.4931640625]

================================================================================
086_model.layers.7.self_attn.q_proj: Linear (model.layers.7.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013281, Std: 0.389365
     First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
     Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.028081, Std: 1.073232
     First 10: [-0.163141667842865, 0.14344964921474457, 0.3623453974723816, -0.20442359149456024, -0.017802029848098755, 0.0024100691080093384, -0.21329712867736816, 0.06796086579561234, 0.17820033431053162, -0.02573048323392868]
     Last 10:  [-3.3963112831115723, 0.44770246744155884, -0.7187802791595459, 0.783698558807373, -3.9939937591552734, -0.22990316152572632, 0.08183640241622925, 3.3939690589904785, 0.3571989834308624, -0.08588109910488129]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000023
     First 10: [-0.034759521484375, -0.035247802734375, -0.06072998046875, 0.03167724609375, 0.035491943359375, -0.0056610107421875, 0.02215576171875, -0.0206451416015625, 0.031524658203125, -0.00321197509765625]
     Last 10:  [0.0007658004760742188, 0.1148681640625, -0.07354736328125, -0.06866455078125, -0.00780487060546875, 0.0192718505859375, 0.028594970703125, -0.0076904296875, 0.01085662841796875, -0.0060882568359375]

================================================================================
087_model.layers.7.self_attn.k_proj: Linear (model.layers.7.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013281, Std: 0.389365
     First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
     Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.072052, Std: 1.327435
     First 10: [-0.005859445780515671, 0.013216912746429443, -0.005568936467170715, 0.011307068169116974, -0.010726341977715492, 0.0036410391330718994, -0.003086097538471222, -0.0008528083562850952, -0.0008198175928555429, -0.002969607710838318]
     Last 10:  [-0.906575083732605, -0.5878944396972656, 1.990466833114624, -3.1886727809906006, 1.2782139778137207, -1.192990779876709, -0.2717127799987793, -2.788429021835327, -0.34668365120887756, 0.39768052101135254]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000041
     First 10: [-0.01396942138671875, -0.0276947021484375, -0.027008056640625, 0.0797119140625, -0.051300048828125, -0.06939697265625, -0.023345947265625, -0.01483154296875, 0.0462646484375, -0.06365966796875]
     Last 10:  [0.056732177734375, -0.003688812255859375, -0.0975341796875, 0.058135986328125, -0.054107666015625, 0.05218505859375, 0.148193359375, -0.0777587890625, 0.0253753662109375, 0.033538818359375]

================================================================================
088_model.layers.7.self_attn.v_proj: Linear (model.layers.7.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013281, Std: 0.389365
     First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
     Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.021394, Std: 0.186402
     First 10: [0.003178112208843231, -0.009626634418964386, 0.0038685426115989685, 0.001936180517077446, 0.0014038663357496262, 0.002399721648544073, -0.011415351182222366, 0.003929476719349623, 0.005665038712322712, -0.0008842595852911472]
     Last 10:  [-0.08630238473415375, 0.11324853450059891, 0.23456799983978271, -0.44920745491981506, 0.07723478972911835, 0.032636433839797974, 0.1038743257522583, -0.0991995707154274, 0.08751662075519562, 0.05932270362973213]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000019
     First 10: [-0.007305145263671875, -0.01457977294921875, 0.0244598388671875, 0.01311492919921875, -0.0254364013671875, 0.00750732421875, 0.014404296875, 0.025604248046875, 0.0232391357421875, 0.024169921875]
     Last 10:  [0.0100250244140625, 0.018310546875, -0.0014963150024414062, 0.0046539306640625, 0.00203704833984375, 0.050262451171875, 0.00450897216796875, -0.0048980712890625, 0.00022780895233154297, -0.02978515625]

================================================================================
089_model.layers.7.self_attn.o_proj: Linear (model.layers.7.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002824, Std: 0.059541
     First 10: [0.003178112208843231, -0.009626634418964386, 0.0038685426115989685, 0.001936180517077446, 0.0014038663357496262, 0.002399721648544073, -0.011415351182222366, 0.003929476719349623, 0.005665038712322712, -0.0008842595852911472]
     Last 10:  [0.009981338866055012, 0.02078321948647499, 0.017580674961209297, -0.03162253275513649, 0.016499441117048264, 0.005469663068652153, 5.365723336581141e-05, 0.017046190798282623, -0.015902500599622726, -0.003825886407867074]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000842, Std: 0.034819
     First 10: [0.011671651154756546, -0.10711812227964401, 0.0315721333026886, -0.04139361530542374, -0.1153922826051712, 0.14532168209552765, -0.03303227946162224, 0.002347170375287533, -0.05380215495824814, 0.02514120563864708]
     Last 10:  [-0.0012159813195466995, -0.0018751071766018867, -0.03404180705547333, 0.005855739116668701, 0.0419999398291111, 0.012451693415641785, -0.011214371770620346, -0.09693868458271027, 0.00971376895904541, 0.010746289044618607]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000025
     First 10: [0.01568603515625, -0.007572174072265625, -0.06231689453125, 0.029327392578125, -0.0010585784912109375, 0.00937652587890625, -0.0235595703125, -0.0127410888671875, -0.01346588134765625, -0.0025920867919921875]
     Last 10:  [0.0748291015625, 0.01514434814453125, 0.01395416259765625, 0.0299072265625, 0.01171112060546875, 0.0109405517578125, -0.0100555419921875, 0.0277862548828125, -0.05224609375, 0.01226043701171875]

================================================================================
090_model.layers.7.self_attn: LlamaSdpaAttention (model.layers.7.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.7.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000842, Std: 0.034819
     First 10: [0.011671651154756546, -0.10711812227964401, 0.0315721333026886, -0.04139361530542374, -0.1153922826051712, 0.14532168209552765, -0.03303227946162224, 0.002347170375287533, -0.05380215495824814, 0.02514120563864708]
     Last 10:  [-0.0012159813195466995, -0.0018751071766018867, -0.03404180705547333, 0.005855739116668701, 0.0419999398291111, 0.012451693415641785, -0.011214371770620346, -0.09693868458271027, 0.00971376895904541, 0.010746289044618607]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.7.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.263115
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4999862313270569, 0.5000138282775879, 0.0, 0.0, 0.0]
     Last 10:  [0.47835108637809753, 0.4797568619251251, 0.023529866710305214, 0.018362192437052727, 0.0, 0.43640467524528503, 0.4358574151992798, 0.04408802464604378, 0.05303559452295303, 0.030614394694566727]
     Zeros: 90, Total: 225

================================================================================
091_model.layers.7.post_attention_layernorm: LlamaRMSNorm (model.layers.7.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.7.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.074675, Std: 11.786484
     First 10: [-0.8800737261772156, -3.0269100666046143, -0.021197617053985596, 0.6008319854736328, -12.648083686828613, -7.942621231079102, 6.022434711456299, 0.11514102667570114, -0.29046905040740967, -2.150440216064453]
     Last 10:  [-0.13910390436649323, -0.21182020008563995, 0.15092962980270386, 0.12702395021915436, 0.11042621731758118, 0.05070645362138748, -0.1554202288389206, -0.14992991089820862, -0.05567489564418793, 0.0649140402674675]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003303, Std: 0.312913
     First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
     Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.370987
     First 10: [0.35009765625, 0.273193359375, 0.39697265625, 0.40869140625, 0.2276611328125, 0.2646484375, 0.260498046875, 0.42236328125, 0.4345703125, 0.2181396484375]
     Last 10:  [0.4345703125, 0.4189453125, 0.325927734375, 0.349853515625, 0.432373046875, 0.39990234375, 0.42431640625, 0.422607421875, 0.413818359375, 0.409423828125]

================================================================================
092_model.layers.7.mlp.gate_proj: Linear (model.layers.7.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.7.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003303, Std: 0.312913
     First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
     Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.143692, Std: 0.335674
     First 10: [-0.44644343852996826, -0.6296501755714417, -0.3028048872947693, -0.3355351686477661, 0.16969412565231323, -0.060166798532009125, -0.9633184671401978, 0.21841850876808167, -0.1178133562207222, 0.006205204874277115]
     Last 10:  [0.15233208239078522, -0.20627819001674652, 0.11936020851135254, -0.6294994354248047, -0.012607485055923462, -0.25003352761268616, -0.3742865025997162, -0.11403638124465942, -0.22093722224235535, -0.4997594356536865]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000115
     First 10: [-0.09625244140625, 0.0195770263671875, -0.0701904296875, -0.0697021484375, -0.00298309326171875, -0.042388916015625, 0.07769775390625, 0.046417236328125, -0.013336181640625, -0.0224151611328125]
     Last 10:  [0.05841064453125, 0.08319091796875, 0.07275390625, 0.0014638900756835938, 0.0931396484375, -0.09991455078125, 0.1026611328125, -0.05596923828125, -0.046722412109375, -0.002574920654296875]

================================================================================
093_model.layers.7.mlp.act_fn: SiLU (model.layers.7.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.7.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.143692, Std: 0.335674
     First 10: [-0.44644343852996826, -0.6296501755714417, -0.3028048872947693, -0.3355351686477661, 0.16969412565231323, -0.060166798532009125, -0.9633184671401978, 0.21841850876808167, -0.1178133562207222, 0.006205204874277115]
     Last 10:  [0.15233208239078522, -0.20627819001674652, 0.11936020851135254, -0.6294994354248047, -0.012607485055923462, -0.25003352761268616, -0.3742865025997162, -0.11403638124465942, -0.22093722224235535, -0.4997594356536865]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.7.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.039899, Std: 0.145827
     First 10: [-0.1742052286863327, -0.21886001527309418, -0.12865330278873444, -0.13988274335861206, 0.09202886372804642, -0.029178662225604057, -0.2660823166370392, 0.12108872830867767, -0.055440690368413925, 0.003112228587269783]
     Last 10:  [0.08195611089468002, -0.092538982629776, 0.06323759257793427, -0.21882915496826172, -0.0062640062533319, -0.1094684824347496, -0.15252386033535004, -0.05377063527703285, -0.09831469506025314, -0.18870776891708374]
     Zeros: 0, Total: 7680

================================================================================
094_model.layers.7.mlp.up_proj: Linear (model.layers.7.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.7.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003303, Std: 0.312913
     First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
     Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000920, Std: 0.246985
     First 10: [0.1448216438293457, 0.07980825006961823, -0.12147991359233856, 0.08897227048873901, 0.036001063883304596, 0.007466818671673536, 0.07499468326568604, 0.21740218997001648, 0.0670018270611763, 0.0916796624660492]
     Last 10:  [0.22887952625751495, 0.7798824310302734, 0.36847352981567383, 0.5763276219367981, 0.267422080039978, 0.16564378142356873, 0.11106894910335541, -0.26560235023498535, 0.5022395253181458, -0.2580797076225281]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000039
     First 10: [0.01467132568359375, -0.017059326171875, -0.0011072158813476562, 0.0055084228515625, -0.0254364013671875, 0.09576416015625, -0.016876220703125, 0.0270538330078125, 0.082763671875, -0.0011730194091796875]
     Last 10:  [0.032989501953125, -0.01445770263671875, -0.06536865234375, 0.0543212890625, 0.024566650390625, 0.052581787109375, 0.039642333984375, -0.005298614501953125, 0.036865234375, -0.043914794921875]

================================================================================
095_model.layers.7.mlp.down_proj: Linear (model.layers.7.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.7.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000164, Std: 0.036808
     First 10: [-0.02522868849337101, -0.01746683567762375, 0.015628792345523834, -0.01244568545371294, 0.003313136985525489, -0.0002178717841161415, -0.019954759627580643, 0.026324953883886337, -0.0037146275863051414, 0.00028532807482406497]
     Last 10:  [0.018758075311779976, -0.07216952741146088, 0.02330137975513935, -0.1261172890663147, -0.0016751335933804512, -0.018132774159312248, -0.016940664499998093, 0.01428160723298788, -0.049377527087926865, 0.04870164766907692]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.7.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002309, Std: 0.065699
     First 10: [0.06277474761009216, 0.07845433056354523, 0.011200192384421825, 0.019179776310920715, 0.15895119309425354, -0.009320810437202454, -0.05352012440562248, -0.08226831257343292, -0.01105063408613205, 0.032855160534381866]
     Last 10:  [-0.0831308662891388, -0.017722953110933304, -0.07431754469871521, 0.023814208805561066, -0.011128347367048264, -0.058775145560503006, -0.032205622643232346, 0.07332844287157059, -0.034068942070007324, 0.09669757634401321]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000055
     First 10: [-0.03680419921875, -0.005893707275390625, 0.01142120361328125, -0.02349853515625, 0.0295867919921875, 0.0306243896484375, -0.0193939208984375, -0.004306793212890625, -0.0114288330078125, 0.041229248046875]
     Last 10:  [-0.078125, -0.0001468658447265625, -0.0406494140625, -0.002689361572265625, 0.050445556640625, -0.003231048583984375, -0.00836944580078125, 0.0243988037109375, 0.038970947265625, -0.020477294921875]

================================================================================
096_model.layers.7.mlp: LlamaMLP (model.layers.7.mlp)
================================================================================

  → INPUT[0]: model.layers.7.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003303, Std: 0.312913
     First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
     Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.7.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002309, Std: 0.065699
     First 10: [0.06277474761009216, 0.07845433056354523, 0.011200192384421825, 0.019179776310920715, 0.15895119309425354, -0.009320810437202454, -0.05352012440562248, -0.08226831257343292, -0.01105063408613205, 0.032855160534381866]
     Last 10:  [-0.0831308662891388, -0.017722953110933304, -0.07431754469871521, 0.023814208805561066, -0.011128347367048264, -0.058775145560503006, -0.032205622643232346, 0.07332844287157059, -0.034068942070007324, 0.09669757634401321]
     Zeros: 0, Total: 2880

================================================================================
097_model.layers.8.input_layernorm: LlamaRMSNorm (model.layers.8.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.8.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.076984, Std: 11.809260
     First 10: [-0.8172990083694458, -2.948455810546875, -0.00999742466956377, 0.6200117468833923, -12.48913288116455, -7.951941967010498, 5.96891450881958, 0.03287271410226822, -0.3015196919441223, -2.1175849437713623]
     Last 10:  [-0.22223477065563202, -0.22954314947128296, 0.07661208510398865, 0.15083816647529602, 0.09929786622524261, -0.008068691939115524, -0.18762585520744324, -0.07660146802663803, -0.08974383771419525, 0.1616116166114807]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013028, Std: 0.361611
     First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
     Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.480348
     First 10: [0.57568359375, 0.544921875, 0.5087890625, 0.5029296875, 0.42822265625, 0.74072265625, 0.5, 0.6083984375, 0.51904296875, 0.320068359375]
     Last 10:  [0.56005859375, 0.51513671875, 0.483154296875, 0.4814453125, 0.58642578125, 0.49169921875, 0.53515625, 0.587890625, 0.466552734375, 0.55078125]

================================================================================
098_model.layers.8.self_attn.q_proj: Linear (model.layers.8.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013028, Std: 0.361611
     First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
     Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007449, Std: 0.930965
     First 10: [0.5700347423553467, -0.03055143542587757, 0.17181363701820374, 0.4668569564819336, -0.19051846861839294, -0.13547587394714355, -0.10261942446231842, 0.05653965473175049, 0.15724077820777893, 0.1988602578639984]
     Last 10:  [0.32177528738975525, 1.141357660293579, -4.713472366333008, -0.6389011144638062, 0.15223488211631775, -1.4306690692901611, 0.8448787927627563, 1.2303187847137451, -1.3698142766952515, -1.591043472290039]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000201
     First 10: [-0.0120849609375, 0.0148162841796875, 0.06982421875, -0.001659393310546875, 0.0303497314453125, -0.0171661376953125, 0.044342041015625, 0.095458984375, 0.1002197265625, -0.004634857177734375]
     Last 10:  [-0.00025844573974609375, -0.0287628173828125, 0.046478271484375, 3.2961368560791016e-05, 0.07525634765625, 0.030242919921875, 0.0716552734375, -0.05999755859375, 0.0250091552734375, -0.0161895751953125]

================================================================================
099_model.layers.8.self_attn.k_proj: Linear (model.layers.8.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013028, Std: 0.361611
     First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
     Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.036599, Std: 1.187968
     First 10: [-0.02051757276058197, -0.009375354275107384, -0.0025253184139728546, 0.0029267892241477966, 0.0033593736588954926, -0.007810264825820923, 0.009312622249126434, -0.004092715680599213, 0.004238307476043701, 0.0013017654418945312]
     Last 10:  [0.7285547852516174, 2.060030460357666, 0.6305367350578308, -2.146085262298584, -1.142793893814087, -1.1884491443634033, -1.6811960935592651, -1.4684768915176392, 0.4585420489311218, 2.4857797622680664]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000376
     First 10: [-0.05438232421875, -0.0892333984375, 0.11346435546875, 0.00998687744140625, -0.056488037109375, -0.07958984375, 0.1383056640625, 0.03350830078125, -0.01385498046875, -0.1116943359375]
     Last 10:  [-0.0596923828125, -0.08404541015625, 0.00408935546875, 0.1767578125, 0.03851318359375, 0.297119140625, -0.05499267578125, -0.0273284912109375, 0.0516357421875, 0.0714111328125]

================================================================================
100_model.layers.8.self_attn.v_proj: Linear (model.layers.8.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.013028, Std: 0.361611
     First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
     Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.004660, Std: 0.185265
     First 10: [-0.006926373578608036, 4.363991320133209e-05, 0.001378399319946766, -0.0014966316521167755, -0.005546521861106157, 0.0017741546034812927, 0.007012592628598213, 0.001315896981395781, -0.009965930134057999, -0.0034552570432424545]
     Last 10:  [-0.9632606506347656, -0.0759386271238327, 0.18810619413852692, -0.38354724645614624, -0.13642236590385437, 0.018882527947425842, -0.140649676322937, 0.2591759264469147, -0.07496477663516998, 0.5457326769828796]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000050
     First 10: [-0.0003771781921386719, -0.0005693435668945312, 0.002719879150390625, 0.0277099609375, 0.004291534423828125, -0.004055023193359375, -0.0005393028259277344, -0.0474853515625, -0.0163116455078125, -0.01136016845703125]
     Last 10:  [0.01451873779296875, 0.0019254684448242188, -0.01090240478515625, 0.01139068603515625, -0.016326904296875, -0.031005859375, 0.015777587890625, -0.050079345703125, -0.007717132568359375, 0.001861572265625]

================================================================================
101_model.layers.8.self_attn.o_proj: Linear (model.layers.8.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000838, Std: 0.045013
     First 10: [-0.006926373578608036, 4.363991320133209e-05, 0.001378399319946766, -0.0014966316521167755, -0.005546521861106157, 0.0017741546034812927, 0.007012592628598213, 0.001315896981395781, -0.009965930134057999, -0.0034552570432424545]
     Last 10:  [-0.13573822379112244, -0.0035587006714195013, 0.013239198364317417, -0.015732279047369957, -0.00821719877421856, 0.04775655269622803, -0.021149393171072006, -8.777123730396852e-05, 0.00849158875644207, 0.07303531467914581]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000537, Std: 0.028551
     First 10: [-0.013519397005438805, -0.014633879996836185, 0.0028122453950345516, -0.011699226684868336, -0.06821851432323456, 0.024635016918182373, 0.047196321189403534, 0.00959493312984705, -0.027519261464476585, 0.011014912277460098]
     Last 10:  [0.009924521669745445, -0.02715279534459114, 0.0016385400667786598, -0.03373562544584274, 0.0574520118534565, -0.009041397832334042, -0.00010344292968511581, -0.0816773921251297, -0.004547027871012688, -0.027889255434274673]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000031
     First 10: [-0.004093170166015625, 0.00449371337890625, -0.018829345703125, 0.01568603515625, 0.06182861328125, 0.047760009765625, -0.025634765625, 0.0214996337890625, -0.032928466796875, 0.031494140625]
     Last 10:  [0.0244903564453125, -0.00897979736328125, 0.003753662109375, -0.03924560546875, 0.0190277099609375, 0.042572021484375, 0.061737060546875, -0.0014858245849609375, 0.0003871917724609375, 0.020111083984375]

================================================================================
102_model.layers.8.self_attn: LlamaSdpaAttention (model.layers.8.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.8.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000537, Std: 0.028551
     First 10: [-0.013519397005438805, -0.014633879996836185, 0.0028122453950345516, -0.011699226684868336, -0.06821851432323456, 0.024635016918182373, 0.047196321189403534, 0.00959493312984705, -0.027519261464476585, 0.011014912277460098]
     Last 10:  [0.009924521669745445, -0.02715279534459114, 0.0016385400667786598, -0.03373562544584274, 0.0574520118534565, -0.009041397832334042, -0.00010344292968511581, -0.0816773921251297, -0.004547027871012688, -0.027889255434274673]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.8.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.265354
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5004750490188599, 0.4995250403881073, 0.0, 0.0, 0.0]
     Last 10:  [0.4536062180995941, 0.4518175423145294, 0.070783831179142, 0.023792341351509094, 0.0, 0.4170399606227875, 0.4159311056137085, 0.07248721271753311, 0.06135969236493111, 0.033182017505168915]
     Zeros: 90, Total: 225

================================================================================
103_model.layers.8.post_attention_layernorm: LlamaRMSNorm (model.layers.8.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.8.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.077521, Std: 11.808899
     First 10: [-0.8308184146881104, -2.96308970451355, -0.007185179274529219, 0.6083125472068787, -12.557351112365723, -7.92730712890625, 6.016110897064209, 0.042467646300792694, -0.32903894782066345, -2.10657000541687]
     Last 10:  [-0.21231025457382202, -0.256695955991745, 0.07825062423944473, 0.11710254102945328, 0.1567498743534088, -0.01711009070277214, -0.18772929906845093, -0.15827885270118713, -0.09429086744785309, 0.13372236490249634]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005885, Std: 0.315244
     First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
     Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.377048
     First 10: [0.36572265625, 0.296630859375, 0.422607421875, 0.41845703125, 0.246337890625, 0.26220703125, 0.255126953125, 0.427978515625, 0.4384765625, 0.230224609375]
     Last 10:  [0.44580078125, 0.43994140625, 0.32568359375, 0.345703125, 0.440673828125, 0.3818359375, 0.436767578125, 0.435791015625, 0.41455078125, 0.4169921875]

================================================================================
104_model.layers.8.mlp.gate_proj: Linear (model.layers.8.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.8.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005885, Std: 0.315244
     First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
     Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.126363, Std: 0.331795
     First 10: [0.11922789365053177, 0.07598148286342621, -0.26246118545532227, -0.244480162858963, -0.009143909439444542, -0.18244251608848572, 0.13987009227275848, 0.07401307672262192, -0.2145039439201355, -0.7887895107269287]
     Last 10:  [-0.46190840005874634, -0.3206123113632202, -0.22908928990364075, 0.29855889081954956, -0.24688836932182312, 0.49990299344062805, -0.6661768555641174, 0.11332497000694275, -0.8373229503631592, 0.1334666609764099]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000332
     First 10: [-0.008392333984375, 0.0247344970703125, -0.02716064453125, 0.05316162109375, -0.00316619873046875, -0.0760498046875, 0.01061248779296875, 0.0031414031982421875, -0.032806396484375, -0.0260009765625]
     Last 10:  [0.11212158203125, -0.0227813720703125, -0.00926971435546875, -0.00730133056640625, 0.131103515625, -0.06903076171875, 0.00859832763671875, -0.000652313232421875, 0.057952880859375, -0.0163116455078125]

================================================================================
105_model.layers.8.mlp.act_fn: SiLU (model.layers.8.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.8.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.126363, Std: 0.331795
     First 10: [0.11922789365053177, 0.07598148286342621, -0.26246118545532227, -0.244480162858963, -0.009143909439444542, -0.18244251608848572, 0.13987009227275848, 0.07401307672262192, -0.2145039439201355, -0.7887895107269287]
     Last 10:  [-0.46190840005874634, -0.3206123113632202, -0.22908928990364075, 0.29855889081954956, -0.24688836932182312, 0.49990299344062805, -0.6661768555641174, 0.11332497000694275, -0.8373229503631592, 0.1334666609764099]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.8.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.032932, Std: 0.145950
     First 10: [0.06316357105970383, 0.03943334519863129, -0.11410730332136154, -0.10737143456935883, -0.004551052115857601, -0.08292294293642044, 0.07481800019741058, 0.03837539628148079, -0.09579289704561234, -0.24644044041633606]
     Last 10:  [-0.17854292690753937, -0.13482598960399628, -0.10148125141859055, 0.17139972746372223, -0.1082826554775238, 0.31115788221359253, -0.2260694056749344, 0.05986969172954559, -0.2529543340206146, 0.0711800679564476]
     Zeros: 0, Total: 7680

================================================================================
106_model.layers.8.mlp.up_proj: Linear (model.layers.8.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.8.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005885, Std: 0.315244
     First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
     Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.002450, Std: 0.247449
     First 10: [0.03304416313767433, 0.14383885264396667, 0.40053126215934753, -0.13585264980793, 0.11536655575037003, -0.040009111166000366, -0.12442360073328018, 0.3450770378112793, 0.17298424243927002, -0.04370095580816269]
     Last 10:  [0.3887243866920471, 0.21747443079948425, -0.313065767288208, -0.39275580644607544, -0.3752979040145874, 0.058640554547309875, 0.019689293578267097, -0.34404757618904114, -0.08564433455467224, -0.35616177320480347]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000031
     First 10: [-0.0007352828979492188, -0.008575439453125, -0.047637939453125, -0.01100921630859375, 0.0340576171875, -0.03924560546875, 0.0005335807800292969, -0.0653076171875, -0.05267333984375, -0.0169677734375]
     Last 10:  [0.00927734375, 0.1024169921875, 0.041778564453125, -0.0472412109375, -0.004608154296875, 0.0208282470703125, -0.11566162109375, -0.016204833984375, 0.01093292236328125, 0.041351318359375]

================================================================================
107_model.layers.8.mlp.down_proj: Linear (model.layers.8.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.8.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000049, Std: 0.045002
     First 10: [0.0020871872548013926, 0.005672046914696693, -0.04570354148745537, 0.014586693607270718, -0.0005250392132438719, 0.003317673224955797, -0.009309125132858753, 0.013242468237876892, -0.01657066121697426, 0.010769682936370373]
     Last 10:  [-0.0694039911031723, -0.029321204870939255, 0.03177030757069588, -0.06731823831796646, 0.04063825309276581, 0.018246470019221306, -0.004451146814972162, -0.020598022267222404, 0.021664105355739594, -0.025351619347929955]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.8.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002781, Std: 0.076271
     First 10: [0.043941691517829895, 0.07703574001789093, -0.02891324646770954, -0.0001335269771516323, 0.2610950171947479, -0.06291749328374863, -0.04276524484157562, -0.03892497345805168, -0.09670408070087433, 0.009521453641355038]
     Last 10:  [0.12537935376167297, -0.028978396207094193, 0.03350488096475601, -0.04899672046303749, -0.012718352489173412, -0.012230819091200829, 0.04065439850091934, 0.006573688238859177, -0.006543746683746576, 0.0736502930521965]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000032
     First 10: [0.036102294921875, -0.0188751220703125, -0.0211944580078125, -0.091796875, -0.033355712890625, -0.024688720703125, 0.04180908203125, -0.0019893646240234375, 0.0870361328125, -0.01171875]
     Last 10:  [0.004665374755859375, 0.01885986328125, -0.01971435546875, -0.0458984375, -0.0020599365234375, -0.0188446044921875, 0.0087432861328125, -0.024383544921875, -0.01529693603515625, -0.007114410400390625]

================================================================================
108_model.layers.8.mlp: LlamaMLP (model.layers.8.mlp)
================================================================================

  → INPUT[0]: model.layers.8.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.005885, Std: 0.315244
     First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
     Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.8.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002781, Std: 0.076271
     First 10: [0.043941691517829895, 0.07703574001789093, -0.02891324646770954, -0.0001335269771516323, 0.2610950171947479, -0.06291749328374863, -0.04276524484157562, -0.03892497345805168, -0.09670408070087433, 0.009521453641355038]
     Last 10:  [0.12537935376167297, -0.028978396207094193, 0.03350488096475601, -0.04899672046303749, -0.012718352489173412, -0.012230819091200829, 0.04065439850091934, 0.006573688238859177, -0.006543746683746576, 0.0736502930521965]
     Zeros: 0, Total: 2880

================================================================================
109_model.layers.9.input_layernorm: LlamaRMSNorm (model.layers.9.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.9.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080302, Std: 11.837265
     First 10: [-0.7868767380714417, -2.886054039001465, -0.0360984243452549, 0.6081790328025818, -12.296256065368652, -7.990224838256836, 5.973345756530762, 0.0035426728427410126, -0.425743043422699, -2.09704852104187]
     Last 10:  [-0.08693090081214905, -0.2856743633747101, 0.11175550520420074, 0.06810581684112549, 0.14403152465820312, -0.02934090979397297, -0.14707490801811218, -0.15170516073703766, -0.10083461552858353, 0.20737266540527344]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016054, Std: 0.355860
     First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
     Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.473485
     First 10: [0.56396484375, 0.45361328125, 0.50537109375, 0.517578125, 0.5126953125, 0.7255859375, 0.5517578125, 0.58984375, 0.56103515625, 0.325439453125]
     Last 10:  [0.525390625, 0.509765625, 0.368408203125, 0.404541015625, 0.54541015625, 0.482421875, 0.548828125, 0.61279296875, 0.496826171875, 0.55029296875]

================================================================================
110_model.layers.9.self_attn.q_proj: Linear (model.layers.9.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016054, Std: 0.355860
     First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
     Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.027000, Std: 0.963388
     First 10: [0.04387283697724342, -0.09526120871305466, 0.25692039728164673, -0.13929833471775055, 0.15903016924858093, 0.011951208114624023, -0.4834483563899994, -0.30207350850105286, -0.11317446827888489, -0.05286040157079697]
     Last 10:  [-0.3126358985900879, -0.02996121346950531, -0.6781793832778931, -0.05468013882637024, -0.8086473345756531, 0.9408209323883057, -1.4899678230285645, 0.4838234484195709, 2.1043386459350586, 2.251519203186035]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000177
     First 10: [0.04205322265625, 0.006923675537109375, -0.04461669921875, -0.0107269287109375, 0.037628173828125, -0.07635498046875, -0.04278564453125, 0.0227508544921875, -0.00841522216796875, 0.0305938720703125]
     Last 10:  [0.0177764892578125, 0.044952392578125, 0.0263214111328125, 0.029052734375, -0.07489013671875, -0.034881591796875, -0.07489013671875, 0.05523681640625, -0.0201263427734375, -0.06304931640625]

================================================================================
111_model.layers.9.self_attn.k_proj: Linear (model.layers.9.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016054, Std: 0.355860
     First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
     Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.081130, Std: 1.248149
     First 10: [-0.003329940140247345, 0.007019057869911194, -0.0015396997332572937, 0.0020980872213840485, 0.015500739216804504, 0.0009484663605690002, 0.004050256684422493, -0.010980360209941864, -0.005229022353887558, -0.003665059804916382]
     Last 10:  [-0.6097809672355652, 5.3869171142578125, 0.06584972143173218, -0.5834517478942871, -1.6502655744552612, -0.766019880771637, 0.7581527233123779, -0.1505623459815979, -1.8711820840835571, -2.11316180229187]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000276
     First 10: [0.00328826904296875, 0.006336212158203125, 0.01788330078125, 0.033203125, -0.0182342529296875, 0.0181121826171875, -0.045501708984375, 0.050140380859375, -0.0205230712890625, 0.0241241455078125]
     Last 10:  [-0.1611328125, 0.0008192062377929688, -0.1417236328125, -0.032318115234375, 0.07171630859375, 0.03472900390625, 0.0849609375, -0.00396728515625, 0.1312255859375, 0.07708740234375]

================================================================================
112_model.layers.9.self_attn.v_proj: Linear (model.layers.9.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016054, Std: 0.355860
     First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
     Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.012590, Std: 0.214816
     First 10: [0.0038800013717263937, 0.010776727460324764, 0.004395492374897003, 0.020588960498571396, 0.006884204223752022, -0.012203143909573555, -0.002978650387376547, -0.007464400492608547, -0.00919080525636673, -0.0036137141287326813]
     Last 10:  [-0.020976930856704712, -0.2573348879814148, 0.29238787293434143, 0.22658833861351013, -0.15203285217285156, 0.1117548793554306, 0.19532284140586853, -0.2886280417442322, 0.1203177273273468, 0.3968658447265625]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000011
     First 10: [0.006870269775390625, -0.0012788772583007812, -0.09820556640625, 0.005359649658203125, -0.0023517608642578125, 0.018280029296875, 0.004596710205078125, 0.006649017333984375, -0.021942138671875, 0.0269927978515625]
     Last 10:  [-0.01251983642578125, -0.01180267333984375, 0.0240478515625, 0.0286102294921875, 0.032470703125, -0.03240966796875, -0.0357666015625, 0.0093536376953125, -0.0341796875, 0.03662109375]

================================================================================
113_model.layers.9.self_attn.o_proj: Linear (model.layers.9.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002527, Std: 0.056748
     First 10: [0.0038800013717263937, 0.010776727460324764, 0.004395492374897003, 0.020588960498571396, 0.006884204223752022, -0.012203143909573555, -0.002978650387376547, -0.007464400492608547, -0.00919080525636673, -0.0036137141287326813]
     Last 10:  [-0.0032318830490112305, -0.023941343650221825, -0.0016776962438598275, 0.017985839396715164, 0.011797924526035786, -0.00509921507909894, 0.0051344591192901134, -0.026174673810601234, 0.002657231641933322, 0.006500010844320059]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000683, Std: 0.035899
     First 10: [-0.00839797593653202, -0.035551298409700394, -0.019028956070542336, -0.02648957632482052, -0.11190793663263321, 0.08060210198163986, -0.06207432225346565, -0.05581294745206833, -0.023520302027463913, 0.026259351521730423]
     Last 10:  [-0.007665117736905813, -0.06332117319107056, -0.009396101348102093, 0.00953938253223896, 0.02488362230360508, 0.04025761038064957, 0.026513539254665375, -0.08103131502866745, 0.007841799408197403, 0.04468400031328201]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000034
     First 10: [0.0236663818359375, 0.047760009765625, -0.0281982421875, 0.005496978759765625, 0.02276611328125, 0.0013427734375, 0.033447265625, 0.00830078125, -0.0269775390625, 0.0255279541015625]
     Last 10:  [0.01016998291015625, -0.0279998779296875, 0.01837158203125, -0.038848876953125, -0.0013456344604492188, 0.0225067138671875, -0.0014133453369140625, -0.01873779296875, -0.064208984375, -0.00710296630859375]

================================================================================
114_model.layers.9.self_attn: LlamaSdpaAttention (model.layers.9.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.9.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000683, Std: 0.035899
     First 10: [-0.00839797593653202, -0.035551298409700394, -0.019028956070542336, -0.02648957632482052, -0.11190793663263321, 0.08060210198163986, -0.06207432225346565, -0.05581294745206833, -0.023520302027463913, 0.026259351521730423]
     Last 10:  [-0.007665117736905813, -0.06332117319107056, -0.009396101348102093, 0.00953938253223896, 0.02488362230360508, 0.04025761038064957, 0.026513539254665375, -0.08103131502866745, 0.007841799408197403, 0.04468400031328201]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.9.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.266120
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4998621642589569, 0.5001378059387207, 0.0, 0.0, 0.0]
     Last 10:  [0.4769057035446167, 0.47668829560279846, 0.02589985355734825, 0.02050611935555935, 0.0, 0.4565238356590271, 0.4567650556564331, 0.033541686832904816, 0.03426787629723549, 0.018901566043496132]
     Zeros: 90, Total: 225

================================================================================
115_model.layers.9.post_attention_layernorm: LlamaRMSNorm (model.layers.9.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.9.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.079619, Std: 11.838077
     First 10: [-0.7952747344970703, -2.921605348587036, -0.05512738227844238, 0.5816894769668579, -12.408164024353027, -7.909622669219971, 5.911271572113037, -0.052270274609327316, -0.449263334274292, -2.070789098739624]
     Last 10:  [-0.0945960208773613, -0.34899553656578064, 0.10235940665006638, 0.0776451975107193, 0.16891515254974365, 0.010916700586676598, -0.12056136876344681, -0.2327364683151245, -0.09299281239509583, 0.25205665826797485]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007798, Std: 0.313722
     First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
     Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.371665
     First 10: [0.353271484375, 0.266845703125, 0.4140625, 0.40234375, 0.2958984375, 0.260009765625, 0.26171875, 0.426025390625, 0.439697265625, 0.2293701171875]
     Last 10:  [0.432373046875, 0.447021484375, 0.313720703125, 0.33642578125, 0.41259765625, 0.36962890625, 0.4501953125, 0.45556640625, 0.41064453125, 0.40673828125]

================================================================================
116_model.layers.9.mlp.gate_proj: Linear (model.layers.9.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.9.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007798, Std: 0.313722
     First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
     Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.143020, Std: 0.338150
     First 10: [-0.11589538305997849, 0.13814741373062134, -0.07678347826004028, -0.0069809406995773315, -0.15250791609287262, -0.09538917243480682, 0.267318457365036, 0.2933540344238281, -0.01690390706062317, -0.02467486262321472]
     Last 10:  [-0.3502469062805176, -0.16496935486793518, 0.11747929453849792, -0.2565889358520508, -0.20227660238742828, 0.04262363910675049, -0.6897583603858948, -0.4268530607223511, 0.3874856233596802, -0.486378014087677]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000324
     First 10: [0.11077880859375, -0.0435791015625, 0.12066650390625, 0.08734130859375, 0.016021728515625, -0.08441162109375, 0.09063720703125, 0.018524169921875, -0.05718994140625, 0.0555419921875]
     Last 10:  [0.08795166015625, -0.052215576171875, 0.060546875, 0.08062744140625, 0.06732177734375, -0.0352783203125, 0.16162109375, -0.0140228271484375, -0.1912841796875, 0.15673828125]

================================================================================
117_model.layers.9.mlp.act_fn: SiLU (model.layers.9.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.9.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.143020, Std: 0.338150
     First 10: [-0.11589538305997849, 0.13814741373062134, -0.07678347826004028, -0.0069809406995773315, -0.15250791609287262, -0.09538917243480682, 0.267318457365036, 0.2933540344238281, -0.01690390706062317, -0.02467486262321472]
     Last 10:  [-0.3502469062805176, -0.16496935486793518, 0.11747929453849792, -0.2565889358520508, -0.20227660238742828, 0.04262363910675049, -0.6897583603858948, -0.4268530607223511, 0.3874856233596802, -0.486378014087677]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.9.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.039259, Std: 0.143964
     First 10: [-0.05459350720047951, 0.07383731007575989, -0.03691853582859039, -0.0034782872535288334, -0.0704505443572998, -0.0454215370118618, 0.15141838788986206, 0.16803818941116333, -0.008380519226193428, -0.012185226194560528]
     Last 10:  [-0.1447649449110031, -0.07569634169340134, 0.06218602880835533, -0.11192470788955688, -0.0909440889954567, 0.021765943616628647, -0.2304392009973526, -0.16855491697788239, 0.23081637918949127, -0.18518705666065216]
     Zeros: 0, Total: 7680

================================================================================
118_model.layers.9.mlp.up_proj: Linear (model.layers.9.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.9.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007798, Std: 0.313722
     First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
     Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000083, Std: 0.252712
     First 10: [-0.3458192050457001, -0.21221843361854553, 0.1545608639717102, -0.3760257363319397, 0.12089995294809341, -0.13314272463321686, -0.05786123871803284, -0.1671293079853058, -0.19515642523765564, -0.15980640053749084]
     Last 10:  [0.019852448254823685, -0.20478665828704834, -0.02680964767932892, -0.10130223631858826, 0.515162467956543, 0.22985829412937164, 0.06860505044460297, 0.502947211265564, -0.2903328537940979, -0.08740289509296417]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000006
     First 10: [-0.004650115966796875, 0.0160369873046875, 0.087158203125, -0.0036411285400390625, 0.0141143798828125, 0.003284454345703125, -0.00919342041015625, 0.023956298828125, -0.0139007568359375, 0.00608062744140625]
     Last 10:  [-0.0084075927734375, 0.019256591796875, -0.0022220611572265625, -0.08416748046875, 0.057708740234375, 0.03277587890625, -0.0153045654296875, 0.0660400390625, 0.0272064208984375, -0.0106353759765625]

================================================================================
119_model.layers.9.mlp.down_proj: Linear (model.layers.9.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.9.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000276, Std: 0.039275
     First 10: [0.01887948252260685, -0.01566963829100132, -0.005706160794943571, 0.0013079255586490035, -0.00851746741682291, 0.0060475473292171955, -0.008761255070567131, -0.028084106743335724, 0.001635512220673263, 0.0019472771091386676]
     Last 10:  [-0.0028739385306835175, 0.015501600690186024, -0.0016671855701133609, 0.01133822277188301, -0.046850983053445816, 0.005003082565963268, -0.01580929383635521, -0.08477422595024109, -0.06701357662677765, 0.016185885295271873]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.9.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000030, Std: 0.070793
     First 10: [0.026561398059129715, 0.07255397737026215, -0.011190158315002918, 0.0182608962059021, 0.12309234589338303, 0.11817166209220886, -0.040946267545223236, -0.0007919203490018845, -0.07040541619062424, 0.03770525008440018]
     Last 10:  [0.13253718614578247, 0.08999583125114441, -5.2306801080703735e-05, 0.0688304528594017, -0.007980171591043472, 0.02235664241015911, 0.09866877645254135, 0.0845712423324585, -0.15372058749198914, -0.15473106503486633]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000047
     First 10: [0.0035152435302734375, -0.0116119384765625, 0.037445068359375, 0.028228759765625, 0.0521240234375, -0.01071929931640625, 0.0172882080078125, -0.08380126953125, -0.020477294921875, 0.003963470458984375]
     Last 10:  [-0.007503509521484375, 0.01291656494140625, -0.063720703125, 0.0540771484375, 0.04132080078125, 0.00966644287109375, -0.023651123046875, 0.0197906494140625, 0.004276275634765625, -0.032684326171875]

================================================================================
120_model.layers.9.mlp: LlamaMLP (model.layers.9.mlp)
================================================================================

  → INPUT[0]: model.layers.9.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007798, Std: 0.313722
     First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
     Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.9.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000030, Std: 0.070793
     First 10: [0.026561398059129715, 0.07255397737026215, -0.011190158315002918, 0.0182608962059021, 0.12309234589338303, 0.11817166209220886, -0.040946267545223236, -0.0007919203490018845, -0.07040541619062424, 0.03770525008440018]
     Last 10:  [0.13253718614578247, 0.08999583125114441, -5.2306801080703735e-05, 0.0688304528594017, -0.007980171591043472, 0.02235664241015911, 0.09866877645254135, 0.0845712423324585, -0.15372058749198914, -0.15473106503486633]
     Zeros: 0, Total: 2880

================================================================================
121_model.layers.10.input_layernorm: LlamaRMSNorm (model.layers.10.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.10.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.079589, Std: 11.856653
     First 10: [-0.7687133550643921, -2.8490514755249023, -0.06631754338741302, 0.59995037317276, -12.28507137298584, -7.7914509773254395, 5.870325088500977, -0.05306219309568405, -0.5196687579154968, -2.033083915710449]
     Last 10:  [0.03794116526842117, -0.25899970531463623, 0.10230709612369537, 0.1464756429195404, 0.16093498468399048, 0.03327334299683571, -0.021892592310905457, -0.14816522598266602, -0.24671339988708496, 0.09732559323310852]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016196, Std: 0.380332
     First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
     Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.467250
     First 10: [0.6572265625, 0.49853515625, 0.494140625, 0.4169921875, 0.51953125, 0.70947265625, 0.58056640625, 0.54345703125, 0.5, 0.4755859375]
     Last 10:  [0.51806640625, 0.45068359375, 0.371337890625, 0.479248046875, 0.5341796875, 0.43896484375, 0.51025390625, 0.57666015625, 0.4580078125, 0.54541015625]

================================================================================
122_model.layers.10.self_attn.q_proj: Linear (model.layers.10.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016196, Std: 0.380332
     First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
     Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.061932, Std: 0.978175
     First 10: [0.14946657419204712, 0.4237329065799713, 0.016238979995250702, 0.10533840209245682, 0.3687155842781067, -0.1618715077638626, -0.3842354714870453, 0.03658205270767212, -0.04985567554831505, -0.2313602864742279]
     Last 10:  [-0.022825270891189575, -5.202563285827637, -0.22738832235336304, 1.337790846824646, -0.27868229150772095, -1.1344449520111084, -0.23994173109531403, -1.1602038145065308, 0.3008744716644287, -0.7587815523147583]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000124
     First 10: [-0.0162353515625, 0.006862640380859375, 0.01593017578125, 0.0091705322265625, -0.022003173828125, -0.0096588134765625, 0.0155792236328125, 0.01556396484375, -0.015960693359375, 0.00371551513671875]
     Last 10:  [-0.06146240234375, -0.031707763671875, -0.0207672119140625, -0.03619384765625, -0.08209228515625, 0.07171630859375, 0.0579833984375, -0.021575927734375, -0.0706787109375, 0.061309814453125]

================================================================================
123_model.layers.10.self_attn.k_proj: Linear (model.layers.10.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016196, Std: 0.380332
     First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
     Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.047379, Std: 1.453040
     First 10: [-0.006445109844207764, 2.396106719970703e-05, 0.0011231075040996075, 0.01953183300793171, 0.013939488679170609, -0.013579700142145157, 0.009705308824777603, -0.011269599199295044, -0.004460237920284271, -0.0007031559944152832]
     Last 10:  [0.10211500525474548, 1.3010547161102295, -0.8465775847434998, -4.474518299102783, 0.7756940126419067, 0.29951944947242737, 0.3242015838623047, 0.0409388542175293, -0.08240259438753128, -0.6977173089981079]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000026
     First 10: [0.0390625, 0.0902099609375, -0.027374267578125, 0.052276611328125, -0.0440673828125, 0.169921875, -0.0271148681640625, -0.024627685546875, 0.0838623046875, 0.01525115966796875]
     Last 10:  [-0.02093505859375, -0.03387451171875, -0.06842041015625, 0.033355712890625, -0.048492431640625, 0.034271240234375, -0.032012939453125, 0.039398193359375, -0.032135009765625, 0.02886962890625]

================================================================================
124_model.layers.10.self_attn.v_proj: Linear (model.layers.10.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016196, Std: 0.380332
     First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
     Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.002164, Std: 0.229439
     First 10: [-0.01095241867005825, 0.01454925537109375, 0.0038912040181457996, -0.012795633636415005, -0.008487294428050518, 0.010548923164606094, -0.002227945253252983, -0.01663871482014656, -0.007873760536313057, 0.04521648585796356]
     Last 10:  [-0.12089092284440994, -0.152170792222023, 0.1492007076740265, -0.14464828372001648, -0.013770520687103271, 0.12011227756738663, 0.01828405261039734, 0.31870919466018677, 0.3168059289455414, 0.1735941767692566]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000071
     First 10: [0.016357421875, 0.01953125, -0.039886474609375, 0.06231689453125, 0.0029296875, 0.01541900634765625, 0.002666473388671875, -0.039794921875, 0.038604736328125, 0.0009431838989257812]
     Last 10:  [0.0260009765625, -0.053619384765625, 0.00330352783203125, 0.00928497314453125, -0.0034160614013671875, 0.0215911865234375, 0.0228271484375, -0.005062103271484375, -0.03912353515625, 0.045074462890625]

================================================================================
125_model.layers.10.self_attn.o_proj: Linear (model.layers.10.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004843, Std: 0.075206
     First 10: [-0.01095241867005825, 0.01454925537109375, 0.0038912040181457996, -0.012795633636415005, -0.008487294428050518, 0.010548923164606094, -0.002227945253252983, -0.01663871482014656, -0.007873760536313057, 0.04521648585796356]
     Last 10:  [0.041268475353717804, -0.028927715495228767, 0.07998748868703842, 0.013751420192420483, -0.07310836762189865, 0.04812069609761238, -0.06191546469926834, 0.06021188572049141, 0.0625753328204155, 0.026494214311242104]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000155, Std: 0.037771
     First 10: [0.011354760266840458, -0.018301716074347496, -0.023903844878077507, 0.0033173873089253902, -0.08320734649896622, 0.051929011940956116, -0.017452513799071312, -0.02396094799041748, -0.017407327890396118, 0.004157497081905603]
     Last 10:  [0.016488492488861084, 0.00819007121026516, -0.03847689554095268, -0.007706239819526672, 0.018173955380916595, -0.021537160500884056, 0.0006799343973398209, 0.011152059771120548, 0.01965831033885479, -0.04831507056951523]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000067
     First 10: [0.01305389404296875, 0.023773193359375, -0.064453125, 0.020355224609375, 0.0036602020263671875, 0.06585693359375, 0.0178070068359375, -0.0298004150390625, -0.07476806640625, -0.01190948486328125]
     Last 10:  [-0.055267333984375, -0.0231781005859375, -0.0025043487548828125, -0.021392822265625, -0.01544952392578125, -0.0207061767578125, 0.0280914306640625, -0.0155181884765625, 0.06292724609375, 0.0235443115234375]

================================================================================
126_model.layers.10.self_attn: LlamaSdpaAttention (model.layers.10.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.10.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000155, Std: 0.037771
     First 10: [0.011354760266840458, -0.018301716074347496, -0.023903844878077507, 0.0033173873089253902, -0.08320734649896622, 0.051929011940956116, -0.017452513799071312, -0.02396094799041748, -0.017407327890396118, 0.004157497081905603]
     Last 10:  [0.016488492488861084, 0.00819007121026516, -0.03847689554095268, -0.007706239819526672, 0.018173955380916595, -0.021537160500884056, 0.0006799343973398209, 0.011152059771120548, 0.01965831033885479, -0.04831507056951523]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.10.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.263318
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001978278160095, 0.4998021423816681, 0.0, 0.0, 0.0]
     Last 10:  [0.330867737531662, 0.33217498660087585, 0.24370139837265015, 0.09325594455003738, 0.0, 0.3100683093070984, 0.3114705979824066, 0.12488590180873871, 0.19453102350234985, 0.05904412269592285]
     Zeros: 90, Total: 225

================================================================================
127_model.layers.10.post_attention_layernorm: LlamaRMSNorm (model.layers.10.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.10.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.079744, Std: 11.857547
     First 10: [-0.7573586106300354, -2.8673532009124756, -0.09022139012813568, 0.6032677888870239, -12.368278503417969, -7.7395219802856445, 5.852872371673584, -0.07702314108610153, -0.5370761156082153, -2.028926372528076]
     Last 10:  [0.05442965775728226, -0.2508096396923065, 0.06383019685745239, 0.13876940310001373, 0.17910894751548767, 0.011736182495951653, -0.021212657913565636, -0.13701316714286804, -0.22705508768558502, 0.04901052266359329]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007058, Std: 0.316369
     First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
     Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.374137
     First 10: [0.351318359375, 0.28125, 0.42236328125, 0.4189453125, 0.296630859375, 0.27783203125, 0.29345703125, 0.417724609375, 0.44775390625, 0.214111328125]
     Last 10:  [0.436767578125, 0.459716796875, 0.3154296875, 0.3369140625, 0.40673828125, 0.381103515625, 0.44482421875, 0.422607421875, 0.420654296875, 0.41259765625]

================================================================================
128_model.layers.10.mlp.gate_proj: Linear (model.layers.10.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.10.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007058, Std: 0.316369
     First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
     Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.150160, Std: 0.365669
     First 10: [0.12533587217330933, 0.539575457572937, 0.5159253478050232, 0.23019398748874664, 0.0697816014289856, 0.05280643701553345, 0.8693276047706604, 0.36892059445381165, 0.02790701389312744, -0.42387571930885315]
     Last 10:  [-0.3378849923610687, -0.09051961451768875, -0.26919126510620117, -0.14221619069576263, -0.34570538997650146, -0.5091465711593628, 0.621299147605896, -0.11430764943361282, -0.15690205991268158, 0.21985876560211182]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000269
     First 10: [0.01512908935546875, -0.03961181640625, 0.08251953125, 0.1553955078125, -0.07861328125, -0.037872314453125, -0.0035991668701171875, 0.0306854248046875, -0.0662841796875, -0.03668212890625]
     Last 10:  [0.0097198486328125, -0.00949859619140625, -0.00971221923828125, -0.003818511962890625, -0.043975830078125, 0.0181427001953125, -0.016571044921875, -0.020538330078125, -0.0732421875, -0.059967041015625]

================================================================================
129_model.layers.10.mlp.act_fn: SiLU (model.layers.10.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.10.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.150160, Std: 0.365669
     First 10: [0.12533587217330933, 0.539575457572937, 0.5159253478050232, 0.23019398748874664, 0.0697816014289856, 0.05280643701553345, 0.8693276047706604, 0.36892059445381165, 0.02790701389312744, -0.42387571930885315]
     Last 10:  [-0.3378849923610687, -0.09051961451768875, -0.26919126510620117, -0.14221619069576263, -0.34570538997650146, -0.5091465711593628, 0.621299147605896, -0.11430764943361282, -0.15690205991268158, 0.21985876560211182]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.10.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.038094, Std: 0.156071
     First 10: [0.0665900781750679, 0.34085720777511597, 0.32306960225105286, 0.12828612327575684, 0.03610767796635628, 0.02710018679499626, 0.6125332713127136, 0.21810515224933624, 0.014148194342851639, -0.16768087446689606]
     Last 10:  [-0.14066940546035767, -0.04321275278925896, -0.11658824980258942, -0.06606023758649826, -0.14326868951320648, -0.1911303699016571, 0.40416377782821655, -0.053890813142061234, -0.07230906933546066, 0.12196540832519531]
     Zeros: 0, Total: 7680

================================================================================
130_model.layers.10.mlp.up_proj: Linear (model.layers.10.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.10.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007058, Std: 0.316369
     First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
     Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.003681, Std: 0.265156
     First 10: [0.3073301911354065, 0.023201871663331985, 0.010693086311221123, -0.671901285648346, -0.035838931798934937, -0.060866743326187134, 0.06572648137807846, 0.14378011226654053, 0.007574064657092094, 0.15290313959121704]
     Last 10:  [0.0033807307481765747, 0.06142779067158699, 0.056572698056697845, -0.027657799422740936, 0.334308385848999, -0.23611915111541748, 0.3624194264411926, -0.5461869835853577, -0.5727048516273499, -0.1320411115884781]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000047
     First 10: [-0.00328826904296875, 0.010345458984375, -0.048919677734375, -0.044708251953125, 0.00783538818359375, 0.037994384765625, -0.0182647705078125, 0.040802001953125, -0.03741455078125, -0.0251312255859375]
     Last 10:  [-0.01497650146484375, 0.003292083740234375, -0.0006985664367675781, 0.027374267578125, -0.0882568359375, 0.04296875, -0.033599853515625, 0.058563232421875, 0.081787109375, 0.08514404296875]

================================================================================
131_model.layers.10.mlp.down_proj: Linear (model.layers.10.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.10.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000311, Std: 0.056046
     First 10: [0.020465141162276268, 0.007908524945378304, 0.0034546111710369587, -0.08619561046361923, -0.0012940606102347374, -0.0016495001036673784, 0.04025965556502342, 0.03135918453335762, 0.00010715933603933081, -0.02563893236219883]
     Last 10:  [-0.00047556537901982665, -0.0026544639840722084, -0.006595711689442396, 0.0018270808504894376, -0.047895923256874084, 0.04512954130768776, 0.14647680521011353, 0.029434461146593094, 0.04141175374388695, -0.016104448586702347]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.10.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000507, Std: 0.092877
     First 10: [0.006723128259181976, 0.12636102735996246, -0.07643591612577438, -0.06954610347747803, 0.1770104169845581, -0.010447674430906773, -0.1546364426612854, -0.09481354802846909, 0.03530086949467659, 0.011641738936305046]
     Last 10:  [-0.14670315384864807, 0.13639436662197113, -0.005013647023588419, 0.15865878760814667, 0.016409333795309067, -0.014368413016200066, 0.02435031160712242, -0.017596062272787094, 0.06264224648475647, -0.11068691313266754]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000031
     First 10: [0.006725311279296875, 0.059539794921875, -0.033294677734375, 0.030303955078125, -0.026092529296875, 0.005199432373046875, 0.04693603515625, 0.0288238525390625, -0.0970458984375, -0.005168914794921875]
     Last 10:  [0.02325439453125, 0.009552001953125, -0.01128387451171875, 0.09051513671875, -0.03863525390625, 0.015472412109375, 0.007083892822265625, -0.01416778564453125, -0.06475830078125, 0.0307159423828125]

================================================================================
132_model.layers.10.mlp: LlamaMLP (model.layers.10.mlp)
================================================================================

  → INPUT[0]: model.layers.10.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007058, Std: 0.316369
     First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
     Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.10.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000507, Std: 0.092877
     First 10: [0.006723128259181976, 0.12636102735996246, -0.07643591612577438, -0.06954610347747803, 0.1770104169845581, -0.010447674430906773, -0.1546364426612854, -0.09481354802846909, 0.03530086949467659, 0.011641738936305046]
     Last 10:  [-0.14670315384864807, 0.13639436662197113, -0.005013647023588419, 0.15865878760814667, 0.016409333795309067, -0.014368413016200066, 0.02435031160712242, -0.017596062272787094, 0.06264224648475647, -0.11068691313266754]
     Zeros: 0, Total: 2880

================================================================================
133_model.layers.11.input_layernorm: LlamaRMSNorm (model.layers.11.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.11.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080251, Std: 11.887290
     First 10: [-0.7506355047225952, -2.7409920692443848, -0.16665729880332947, 0.5337216854095459, -12.191267967224121, -7.749969482421875, 5.698235988616943, -0.17183668911457062, -0.5017752647399902, -2.017284631729126]
     Last 10:  [-0.09227349609136581, -0.11441527307033539, 0.05881654843688011, 0.2974281907081604, 0.19551828503608704, -0.002632230520248413, 0.0031376536935567856, -0.15460923314094543, -0.16441284120082855, -0.06167639046907425]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016261, Std: 0.419355
     First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
     Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.538685
     First 10: [0.70263671875, 0.4951171875, 0.56298828125, 0.5546875, 0.65771484375, 0.83447265625, 0.708984375, 0.70703125, 0.6435546875, 0.451904296875]
     Last 10:  [0.64697265625, 0.55419921875, 0.416015625, 0.50732421875, 0.591796875, 0.5048828125, 0.5751953125, 0.68115234375, 0.56494140625, 0.6240234375]

================================================================================
134_model.layers.11.self_attn.q_proj: Linear (model.layers.11.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016261, Std: 0.419355
     First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
     Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.013588, Std: 0.869937
     First 10: [-0.09833730012178421, -0.3380144536495209, -0.10580829530954361, 0.37589961290359497, 0.39564716815948486, -0.3084254264831543, -0.4078513979911804, 0.5017313361167908, -0.5694472789764404, -0.05693710595369339]
     Last 10:  [3.198655128479004, -0.7434597611427307, 1.5761477947235107, 0.6453534960746765, 0.005273308604955673, 1.7134997844696045, -0.44339093565940857, -2.44767427444458, -1.5307910442352295, 2.3896775245666504]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000011
     First 10: [0.01522064208984375, 0.022979736328125, -0.00015413761138916016, 0.01544189453125, 0.01049041748046875, -0.018798828125, 0.0094451904296875, -0.0325927734375, 0.0286865234375, 0.0016117095947265625]
     Last 10:  [-0.06549072265625, 0.0743408203125, 0.004955291748046875, -0.0222320556640625, -0.035491943359375, -0.0286712646484375, 0.01416778564453125, 0.03662109375, 0.0677490234375, -0.016876220703125]

================================================================================
135_model.layers.11.self_attn.k_proj: Linear (model.layers.11.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016261, Std: 0.419355
     First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
     Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.055707, Std: 1.275205
     First 10: [-0.0022728517651557922, -0.0015389621257781982, -0.0026853345334529877, 0.004023056477308273, 0.007059872150421143, -0.011105760931968689, -0.012318063527345657, -0.006543412804603577, -0.012114412151277065, 0.012987462803721428]
     Last 10:  [0.8687489628791809, -0.9701200723648071, 0.5929964780807495, -0.859692394733429, 1.3973126411437988, 0.6041626930236816, 4.163776397705078, -1.3620789051055908, 0.347576379776001, -1.899458408355713]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000033
     First 10: [-0.0367431640625, -0.01439666748046875, -0.014007568359375, -0.022491455078125, -0.021881103515625, -0.0204925537109375, 0.04791259765625, -0.0178680419921875, 0.01123046875, 0.0166168212890625]
     Last 10:  [0.07257080078125, -0.063720703125, -0.004047393798828125, -0.07269287109375, -0.0692138671875, -0.04156494140625, -0.032989501953125, -0.1187744140625, 0.01029205322265625, -0.08343505859375]

================================================================================
136_model.layers.11.self_attn.v_proj: Linear (model.layers.11.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016261, Std: 0.419355
     First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
     Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.017497, Std: 0.223269
     First 10: [-0.02910691127181053, -0.04331636428833008, 0.05339854583144188, -0.5225426554679871, -0.04208200424909592, 0.002118171425536275, 0.047071706503629684, -0.014669853262603283, 0.0578739270567894, -0.05996791273355484]
     Last 10:  [-0.06093572825193405, -0.01800355315208435, -0.08862394094467163, -0.3583317995071411, 0.09321369230747223, 0.014956891536712646, -0.4446098804473877, -0.05460396409034729, -0.23791399598121643, -0.051828183233737946]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000143
     First 10: [0.0022830963134765625, 0.006320953369140625, 0.0116424560546875, 0.0102691650390625, 0.0034618377685546875, 0.0015392303466796875, 0.0027313232421875, -0.01364898681640625, -0.022247314453125, -0.006778717041015625]
     Last 10:  [-0.0290985107421875, -0.057708740234375, -0.00757598876953125, 0.01320648193359375, -0.0465087890625, 0.03826904296875, 0.058441162109375, -0.041351318359375, -0.04461669921875, 0.056060791015625]

================================================================================
137_model.layers.11.self_attn.o_proj: Linear (model.layers.11.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006694, Std: 0.099394
     First 10: [-0.02910691127181053, -0.04331636428833008, 0.05339854583144188, -0.5225426554679871, -0.04208200424909592, 0.002118171425536275, 0.047071706503629684, -0.014669853262603283, 0.0578739270567894, -0.05996791273355484]
     Last 10:  [0.01429824810475111, -0.011123277246952057, -0.008670673705637455, -0.018727988004684448, 0.01208626851439476, -0.010177751071751118, -0.022329753264784813, 0.0027228668332099915, 0.00968257151544094, 0.01294622290879488]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000955, Std: 0.036818
     First 10: [0.03644127398729324, -0.060190871357917786, -0.07995402067899704, -0.05023517459630966, -0.07148309797048569, 0.029158759862184525, -0.02042042277753353, 0.04403755068778992, -0.00829369854182005, 0.006124582607299089]
     Last 10:  [0.09288059175014496, -0.06529207527637482, -0.01697033829987049, 0.02100716531276703, -0.00387251703068614, 0.03574519231915474, -0.03610561415553093, -0.015736475586891174, -0.06841997802257538, 0.01371303852647543]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000046
     First 10: [-0.00449371337890625, -0.022308349609375, -0.07293701171875, 0.01043701171875, -0.01226806640625, -0.049591064453125, -0.002330780029296875, -0.023773193359375, -0.00750732421875, -0.0142364501953125]
     Last 10:  [0.0165557861328125, 0.000690460205078125, 0.07073974609375, 0.002788543701171875, 0.007312774658203125, 0.01381683349609375, -0.04351806640625, 0.01611328125, 0.08294677734375, -0.0136566162109375]

================================================================================
138_model.layers.11.self_attn: LlamaSdpaAttention (model.layers.11.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.11.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000955, Std: 0.036818
     First 10: [0.03644127398729324, -0.060190871357917786, -0.07995402067899704, -0.05023517459630966, -0.07148309797048569, 0.029158759862184525, -0.02042042277753353, 0.04403755068778992, -0.00829369854182005, 0.006124582607299089]
     Last 10:  [0.09288059175014496, -0.06529207527637482, -0.01697033829987049, 0.02100716531276703, -0.00387251703068614, 0.03574519231915474, -0.03610561415553093, -0.015736475586891174, -0.06841997802257538, 0.01371303852647543]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.11.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.267236
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000377893447876, 0.4999622404575348, 0.0, 0.0, 0.0]
     Last 10:  [0.48177382349967957, 0.48069870471954346, 0.021917590871453285, 0.015609879978001118, 0.0, 0.45844319462776184, 0.45742303133010864, 0.03090348094701767, 0.02063487283885479, 0.032595373690128326]
     Zeros: 90, Total: 225

================================================================================
139_model.layers.11.post_attention_layernorm: LlamaRMSNorm (model.layers.11.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.11.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.079296, Std: 11.887081
     First 10: [-0.7141942381858826, -2.801182985305786, -0.2466113269329071, 0.48348650336265564, -12.262750625610352, -7.720810890197754, 5.6778154373168945, -0.1277991384267807, -0.510068953037262, -2.011160135269165]
     Last 10:  [0.0006070956587791443, -0.1797073483467102, 0.04184620827436447, 0.3184353709220886, 0.1916457712650299, 0.033112961798906326, -0.03296796232461929, -0.1703457087278366, -0.23283281922340393, -0.047963351011276245]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006562, Std: 0.312966
     First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
     Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.378323
     First 10: [0.341064453125, 0.31884765625, 0.403564453125, 0.420166015625, 0.37109375, 0.3115234375, 0.28955078125, 0.424072265625, 0.440673828125, 0.2646484375]
     Last 10:  [0.4892578125, 0.4697265625, 0.30419921875, 0.325927734375, 0.392578125, 0.365966796875, 0.462890625, 0.437744140625, 0.4140625, 0.406982421875]

================================================================================
140_model.layers.11.mlp.gate_proj: Linear (model.layers.11.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.11.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006562, Std: 0.312966
     First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
     Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.129474, Std: 0.367356
     First 10: [0.7379658818244934, -0.02564872056245804, 0.5850489735603333, -0.18778732419013977, 0.14048448204994202, 0.3175482451915741, -0.09952732175588608, 0.12970271706581116, 0.008425537496805191, -0.4662023186683655]
     Last 10:  [-0.4869888722896576, -0.48205459117889404, -0.8015052080154419, -0.6381601691246033, -0.911293625831604, 0.009540095925331116, -0.006061370950192213, -0.20772233605384827, 0.05674118921160698, -0.19846075773239136]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000218
     First 10: [-0.04962158203125, -0.1478271484375, -0.01410675048828125, 0.012115478515625, 0.045318603515625, -0.1407470703125, 0.12890625, 0.0186309814453125, 0.0309295654296875, 0.0107574462890625]
     Last 10:  [-0.01451873779296875, -0.07830810546875, -0.007129669189453125, 0.0011796951293945312, -0.0875244140625, 0.09234619140625, -0.09893798828125, 0.0196685791015625, 0.0689697265625, 0.10418701171875]

================================================================================
141_model.layers.11.mlp.act_fn: SiLU (model.layers.11.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.11.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.129474, Std: 0.367356
     First 10: [0.7379658818244934, -0.02564872056245804, 0.5850489735603333, -0.18778732419013977, 0.14048448204994202, 0.3175482451915741, -0.09952732175588608, 0.12970271706581116, 0.008425537496805191, -0.4662023186683655]
     Last 10:  [-0.4869888722896576, -0.48205459117889404, -0.8015052080154419, -0.6381601691246033, -0.911293625831604, 0.009540095925331116, -0.006061370950192213, -0.20772233605384827, 0.05674118921160698, -0.19846075773239136]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.11.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.028836, Std: 0.158227
     First 10: [0.4992714822292328, -0.012659905478358269, 0.37573501467704773, -0.08510345965623856, 0.07516811788082123, 0.18377363681793213, -0.04728928208351135, 0.06905117630958557, 0.004230516031384468, -0.17972822487354279]
     Last 10:  [-0.1853495091199875, -0.18403257429599762, -0.24822908639907837, -0.2205880731344223, -0.2612999677658081, 0.004792801104485989, -0.003021500539034605, -0.09311265498399734, 0.029175270348787308, -0.08941590040922165]
     Zeros: 0, Total: 7680

================================================================================
142_model.layers.11.mlp.up_proj: Linear (model.layers.11.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.11.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006562, Std: 0.312966
     First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
     Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.004999, Std: 0.299657
     First 10: [-0.366992324590683, 0.3556661009788513, 0.11972010880708694, -0.28342050313949585, 0.006526939570903778, 0.4807150363922119, -0.028140781447291374, -0.046711307018995285, 0.09118714183568954, 0.2354607880115509]
     Last 10:  [0.03171835094690323, 0.046058982610702515, 0.2318703532218933, -0.07702048122882843, -0.06078110262751579, 0.20739692449569702, -0.0663781613111496, 0.42122286558151245, -0.119338758289814, 0.027966633439064026]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000009
     First 10: [-0.04132080078125, -0.0240020751953125, -0.044647216796875, 0.0308685302734375, 0.01146697998046875, 0.006015777587890625, 0.0133819580078125, 0.01311492919921875, 0.0316162109375, -0.017333984375]
     Last 10:  [0.0233917236328125, 0.036590576171875, -0.058197021484375, -0.0179595947265625, -0.00656890869140625, -0.0115966796875, 0.0001964569091796875, -0.03631591796875, -0.01245880126953125, -0.00974273681640625]

================================================================================
143_model.layers.11.mlp.down_proj: Linear (model.layers.11.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.11.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.002258, Std: 0.050446
     First 10: [-0.18322880566120148, -0.0045026992447674274, 0.04498303681612015, 0.024120064452290535, 0.0004906177637167275, 0.08834274858236313, 0.0013307573972269893, -0.0032254706602543592, 0.00038576865335926414, -0.04231894761323929]
     Last 10:  [-0.005878980737179518, -0.008476353250443935, -0.057556964457035065, 0.016989799216389656, 0.01588210090994835, 0.000994012225419283, 0.0002005616552196443, -0.039221178740262985, -0.0034817405976355076, -0.00250066164880991]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.11.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001173, Std: 0.084820
     First 10: [0.120505690574646, 0.0981190949678421, 0.06592634320259094, -0.013438425958156586, 0.2713161110877991, 0.08287996053695679, -0.16010701656341553, -0.012011315673589706, -0.018759645521640778, 0.0857534408569336]
     Last 10:  [-0.02987774834036827, 0.13103070855140686, 0.022475168108940125, -0.03254164755344391, -0.01837688684463501, 0.011171840131282806, 0.01572682335972786, -0.0866255909204483, -0.015201804228127003, -0.12918591499328613]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000036
     First 10: [-0.0968017578125, -0.04638671875, 0.01666259765625, 0.051422119140625, -0.033660888671875, 0.035919189453125, -0.01143646240234375, 0.004421234130859375, -0.0198516845703125, -0.033660888671875]
     Last 10:  [0.002349853515625, 0.050445556640625, -0.0338134765625, 0.042999267578125, -0.0264434814453125, 0.055328369140625, -0.005764007568359375, -0.061767578125, 0.0653076171875, -0.0241851806640625]

================================================================================
144_model.layers.11.mlp: LlamaMLP (model.layers.11.mlp)
================================================================================

  → INPUT[0]: model.layers.11.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006562, Std: 0.312966
     First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
     Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.11.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001173, Std: 0.084820
     First 10: [0.120505690574646, 0.0981190949678421, 0.06592634320259094, -0.013438425958156586, 0.2713161110877991, 0.08287996053695679, -0.16010701656341553, -0.012011315673589706, -0.018759645521640778, 0.0857534408569336]
     Last 10:  [-0.02987774834036827, 0.13103070855140686, 0.022475168108940125, -0.03254164755344391, -0.01837688684463501, 0.011171840131282806, 0.01572682335972786, -0.0866255909204483, -0.015201804228127003, -0.12918591499328613]
     Zeros: 0, Total: 2880

================================================================================
145_model.layers.12.input_layernorm: LlamaRMSNorm (model.layers.12.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.12.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.078123, Std: 11.912354
     First 10: [-0.5936885476112366, -2.70306396484375, -0.18068498373031616, 0.47004806995391846, -11.991434097290039, -7.637930870056152, 5.5177083015441895, -0.1398104578256607, -0.5288286209106445, -1.9254066944122314]
     Last 10:  [-0.029270652681589127, -0.048676639795303345, 0.0643213763833046, 0.2858937382698059, 0.1732688844203949, 0.04428480193018913, -0.017241138964891434, -0.2569712996482849, -0.24803462624549866, -0.17714926600456238]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008111, Std: 0.309859
     First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
     Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.398369
     First 10: [0.39599609375, 0.40283203125, 0.4462890625, 0.439208984375, 0.4091796875, 0.5751953125, 0.469482421875, 0.52490234375, 0.47802734375, 0.31103515625]
     Last 10:  [0.493408203125, 0.47900390625, 0.2822265625, 0.379638671875, 0.412109375, 0.3720703125, 0.50244140625, 0.460693359375, 0.429443359375, 0.46435546875]

================================================================================
146_model.layers.12.self_attn.q_proj: Linear (model.layers.12.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008111, Std: 0.309859
     First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
     Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.099025, Std: 1.247658
     First 10: [-0.10300732403993607, -0.12910595536231995, 0.3526054918766022, 0.516670286655426, 0.06717849522829056, 0.31428688764572144, -0.5736387372016907, -0.06254216283559799, 0.020749956369400024, -0.5572965741157532]
     Last 10:  [-8.515111923217773, 0.3810027241706848, -5.5177531242370605, -0.641026496887207, -0.21589386463165283, 1.0270981788635254, -1.1573641300201416, -0.3658088445663452, -0.40399372577667236, -0.11495378613471985]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000376
     First 10: [0.0103912353515625, 0.01702880859375, 0.0122833251953125, 0.01873779296875, -0.0165557861328125, 0.010955810546875, -0.00887298583984375, 0.010009765625, -0.005985260009765625, 0.0012359619140625]
     Last 10:  [-0.0167999267578125, 0.047210693359375, -0.0587158203125, -0.0007781982421875, -0.11322021484375, 0.0469970703125, 0.07489013671875, -0.01134490966796875, 0.057281494140625, -0.03765869140625]

================================================================================
147_model.layers.12.self_attn.k_proj: Linear (model.layers.12.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008111, Std: 0.309859
     First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
     Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.024225, Std: 1.566500
     First 10: [0.014684587717056274, -0.0053712427616119385, -0.009041905403137207, 0.016889702528715134, 0.008277103304862976, 0.010395310819149017, -0.012290023267269135, -0.020806701853871346, 0.0028519518673419952, -0.025799276307225227]
     Last 10:  [0.354488730430603, 1.8524433374404907, 0.8638663291931152, -1.8554483652114868, 1.3563103675842285, 0.01034882664680481, 3.426816940307617, -0.22806595265865326, 0.08597460389137268, 0.7460355162620544]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000167
     First 10: [0.154052734375, 0.1680908203125, -0.0711669921875, 0.020233154296875, 0.08441162109375, 0.1260986328125, -0.0648193359375, 0.07232666015625, 0.048553466796875, 0.10772705078125]
     Last 10:  [-0.053619384765625, -0.09539794921875, 0.039031982421875, 0.048126220703125, -0.1923828125, 0.1959228515625, 0.08087158203125, 0.1556396484375, 0.0284423828125, 0.036712646484375]

================================================================================
148_model.layers.12.self_attn.v_proj: Linear (model.layers.12.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008111, Std: 0.309859
     First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
     Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.007348, Std: 0.209788
     First 10: [0.007480494678020477, -0.003598351962864399, -0.012262886390089989, 0.00414435938000679, -0.012407689355313778, -0.0067106736823916435, 0.005282680969685316, -0.024111688137054443, 0.04743749648332596, 0.03158693015575409]
     Last 10:  [0.0037540867924690247, 0.3114430606365204, 0.02938469871878624, 0.08932285010814667, -0.17938753962516785, -0.16625963151454926, 0.2751380205154419, -0.017767369747161865, -0.1637609601020813, -0.16993016004562378]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000014
     First 10: [-0.000946044921875, 0.0166778564453125, -0.03558349609375, -0.04046630859375, 0.0153961181640625, 0.00972747802734375, -0.003452301025390625, 0.005596160888671875, -0.031890869140625, -0.00021898746490478516]
     Last 10:  [-0.01464080810546875, -0.0217437744140625, 0.00374603271484375, 0.0240325927734375, 0.063232421875, -0.031707763671875, 0.035888671875, 0.0240936279296875, 0.035614013671875, 0.002262115478515625]

================================================================================
149_model.layers.12.self_attn.o_proj: Linear (model.layers.12.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.005420, Std: 0.078160
     First 10: [0.007480494678020477, -0.003598351962864399, -0.012262886390089989, 0.00414435938000679, -0.012407689355313778, -0.0067106736823916435, 0.005282680969685316, -0.024111688137054443, 0.04743749648332596, 0.03158693015575409]
     Last 10:  [-0.0017163162119686604, 0.06303659826517105, 0.003836127230897546, 0.008229025639593601, 0.037445344030857086, 0.0185327660292387, 0.038581494241952896, -0.002250899560749531, -0.007847270928323269, -0.04468422755599022]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001117, Std: 0.076270
     First 10: [0.014697681181132793, -0.06356437504291534, -0.03209638223052025, -0.09019242227077484, -0.0296503733843565, 0.18400844931602478, -0.18186622858047485, 0.053008757531642914, -0.06632836163043976, -0.07376118004322052]
     Last 10:  [0.10826735198497772, -0.11999920755624771, -0.1100778728723526, 0.05835971608757973, 0.027590161189436913, 0.028105370700359344, 0.100612573325634, -0.062093883752822876, -0.056943461298942566, 0.13751718401908875]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000104
     First 10: [0.022613525390625, 0.005161285400390625, -0.0160980224609375, -0.02276611328125, -0.006847381591796875, 0.00017142295837402344, -0.01035308837890625, 0.0263671875, -0.005184173583984375, -0.018402099609375]
     Last 10:  [-0.023468017578125, -0.01168060302734375, 0.037261962890625, 0.0501708984375, -0.017059326171875, -0.005825042724609375, -0.0238494873046875, -0.01337432861328125, -0.0176849365234375, 0.014373779296875]

================================================================================
150_model.layers.12.self_attn: LlamaSdpaAttention (model.layers.12.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.12.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001117, Std: 0.076270
     First 10: [0.014697681181132793, -0.06356437504291534, -0.03209638223052025, -0.09019242227077484, -0.0296503733843565, 0.18400844931602478, -0.18186622858047485, 0.053008757531642914, -0.06632836163043976, -0.07376118004322052]
     Last 10:  [0.10826735198497772, -0.11999920755624771, -0.1100778728723526, 0.05835971608757973, 0.027590161189436913, 0.028105370700359344, 0.100612573325634, -0.062093883752822876, -0.056943461298942566, 0.13751718401908875]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.12.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.262263
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4999360740184784, 0.500063955783844, 0.0, 0.0, 0.0]
     Last 10:  [0.44887983798980713, 0.4486074447631836, 0.06463519483804703, 0.03787752613425255, 0.0, 0.4395343065261841, 0.43919113278388977, 0.050784882158041, 0.03833407536149025, 0.032155636698007584]
     Zeros: 90, Total: 225

================================================================================
151_model.layers.12.post_attention_layernorm: LlamaRMSNorm (model.layers.12.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.12.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.079240, Std: 11.918379
     First 10: [-0.5789908766746521, -2.7666282653808594, -0.2127813696861267, 0.3798556327819824, -12.021084785461426, -7.453922271728516, 5.335842132568359, -0.08680170029401779, -0.5951569676399231, -1.9991679191589355]
     Last 10:  [0.0789967030286789, -0.16867583990097046, -0.045756496489048004, 0.34425345063209534, 0.20085904002189636, 0.07239016890525818, 0.08337143063545227, -0.3190651834011078, -0.30497807264328003, -0.03963208198547363]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003346, Std: 0.278439
     First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
     Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.319480
     First 10: [0.30224609375, 0.255859375, 0.329833984375, 0.3623046875, 0.2705078125, 0.26220703125, 0.2374267578125, 0.33935546875, 0.381103515625, 0.232421875]
     Last 10:  [0.3828125, 0.39013671875, 0.267822265625, 0.3056640625, 0.34765625, 0.317626953125, 0.379150390625, 0.37451171875, 0.346923828125, 0.336669921875]

================================================================================
152_model.layers.12.mlp.gate_proj: Linear (model.layers.12.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.12.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003346, Std: 0.278439
     First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
     Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.120033, Std: 0.340734
     First 10: [-0.270221471786499, 0.12549084424972534, -0.1775200068950653, 0.06510748714208603, -0.3582253158092499, 0.03724752739071846, -0.5686708688735962, -0.12960727512836456, -0.06560668349266052, -0.13023902475833893]
     Last 10:  [-0.20723649859428406, 0.7631307244300842, -0.5525973439216614, -0.4322618544101715, -0.5279912948608398, -0.2163187563419342, 0.050342097878456116, -0.4461264908313751, -0.007415442727506161, -0.041411831974983215]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000056
     First 10: [-0.0601806640625, 0.0299835205078125, 0.01146697998046875, -0.03729248046875, 0.03857421875, 0.01306915283203125, -0.01165771484375, -0.1417236328125, 0.054229736328125, 0.016937255859375]
     Last 10:  [-0.0233306884765625, 0.01395416259765625, 0.03607177734375, 0.057281494140625, 0.00542449951171875, 0.06170654296875, -0.041717529296875, 0.01467132568359375, 0.0360107421875, 0.0892333984375]

================================================================================
153_model.layers.12.mlp.act_fn: SiLU (model.layers.12.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.12.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.120033, Std: 0.340734
     First 10: [-0.270221471786499, 0.12549084424972534, -0.1775200068950653, 0.06510748714208603, -0.3582253158092499, 0.03724752739071846, -0.5686708688735962, -0.12960727512836456, -0.06560668349266052, -0.13023902475833893]
     Last 10:  [-0.20723649859428406, 0.7631307244300842, -0.5525973439216614, -0.4322618544101715, -0.5279912948608398, -0.2163187563419342, 0.050342097878456116, -0.4461264908313751, -0.007415442727506161, -0.041411831974983215]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.12.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.028904, Std: 0.158054
     First 10: [-0.11696609854698181, 0.06667724996805191, -0.08090228587388992, 0.033613115549087524, -0.14737004041671753, 0.018970569595694542, -0.20559930801391602, -0.06060999259352684, -0.03172766789793968, -0.06088494881987572]
     Last 10:  [-0.09291976690292358, 0.5204803347587585, -0.20184281468391418, -0.1701323539018631, -0.19587711989879608, -0.09650633484125137, 0.025804495438933372, -0.17411518096923828, -0.003693974344059825, -0.020277243107557297]
     Zeros: 0, Total: 7680

================================================================================
154_model.layers.12.mlp.up_proj: Linear (model.layers.12.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.12.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003346, Std: 0.278439
     First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
     Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.001043, Std: 0.276864
     First 10: [-0.14680781960487366, 0.07992859929800034, -0.42700833082199097, 0.0538935549557209, 0.25258222222328186, -0.18725481629371643, 0.16362294554710388, 0.4287998080253601, -0.14947807788848877, -0.048913855105638504]
     Last 10:  [-0.21148604154586792, -0.42731842398643494, -0.25878578424453735, 0.45757076144218445, 0.0678255558013916, -0.13032351434230804, -0.1819869577884674, 0.0992664247751236, 0.14924320578575134, 0.29146289825439453]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000021
     First 10: [-0.00409698486328125, -0.02935791015625, 0.0281219482421875, 0.030029296875, -0.040283203125, -0.0162506103515625, 0.042266845703125, 0.01091766357421875, -0.01183319091796875, 0.047210693359375]
     Last 10:  [0.0258026123046875, 0.10406494140625, -0.00083160400390625, -0.0033855438232421875, -0.005687713623046875, 0.01340484619140625, 0.0175933837890625, 0.01447296142578125, -0.0833740234375, 0.06512451171875]

================================================================================
155_model.layers.12.mlp.down_proj: Linear (model.layers.12.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.12.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.001120, Std: 0.047342
     First 10: [0.01717153750360012, 0.005329419393092394, 0.03454595059156418, 0.0018115303246304393, -0.037223052233457565, -0.0035523304250091314, -0.03364076465368271, -0.025989552959799767, 0.004742590710520744, 0.002978117670863867]
     Last 10:  [0.01965123414993286, -0.22241084277629852, 0.05223404988646507, -0.07784759253263474, -0.013285474851727486, 0.012577044777572155, -0.004696081392467022, -0.017283791676163673, -0.0005513005889952183, -0.005910064093768597]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.12.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000092, Std: 0.088985
     First 10: [0.0234675370156765, 0.12398941814899445, -0.011473916471004486, -0.01738768070936203, 0.2233818620443344, 0.01835508458316326, -0.09885233640670776, -0.010740630328655243, 0.07286752760410309, 0.0022932225838303566]
     Last 10:  [0.10431592166423798, -0.11954665184020996, 0.1165568009018898, 0.010377686470746994, -0.028768960386514664, -0.031969573348760605, -0.10366721451282501, 0.033560849726200104, 0.18722179532051086, -0.06568539142608643]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000069
     First 10: [0.064453125, 0.0103607177734375, 0.02325439453125, 0.003444671630859375, -0.0059967041015625, 0.0020847320556640625, 0.05242919921875, -0.013916015625, 0.07025146484375, -0.0292816162109375]
     Last 10:  [0.056671142578125, 0.00414276123046875, 0.00534820556640625, -0.006244659423828125, 0.0306396484375, 0.002819061279296875, 0.00551605224609375, 0.01250457763671875, 0.051239013671875, 0.015045166015625]

================================================================================
156_model.layers.12.mlp: LlamaMLP (model.layers.12.mlp)
================================================================================

  → INPUT[0]: model.layers.12.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003346, Std: 0.278439
     First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
     Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.12.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000092, Std: 0.088985
     First 10: [0.0234675370156765, 0.12398941814899445, -0.011473916471004486, -0.01738768070936203, 0.2233818620443344, 0.01835508458316326, -0.09885233640670776, -0.010740630328655243, 0.07286752760410309, 0.0022932225838303566]
     Last 10:  [0.10431592166423798, -0.11954665184020996, 0.1165568009018898, 0.010377686470746994, -0.028768960386514664, -0.031969573348760605, -0.10366721451282501, 0.033560849726200104, 0.18722179532051086, -0.06568539142608643]
     Zeros: 0, Total: 2880

================================================================================
157_model.layers.13.input_layernorm: LlamaRMSNorm (model.layers.13.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.13.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.079332, Std: 11.952465
     First 10: [-0.5555233359336853, -2.642638921737671, -0.2242552936077118, 0.3624679446220398, -11.79770278930664, -7.435567378997803, 5.236989974975586, -0.09754233062267303, -0.5222894549369812, -1.9968746900558472]
     Last 10:  [0.18331262469291687, -0.2882224917411804, 0.0708003044128418, 0.35463112592697144, 0.172090083360672, 0.040420595556497574, -0.02029578387737274, -0.2855043411254883, -0.11775627732276917, -0.10531747341156006]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004712, Std: 0.460520
     First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
     Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.566995
     First 10: [0.54931640625, 0.607421875, 0.61669921875, 0.5390625, 0.4873046875, 0.701171875, 0.72998046875, 0.580078125, 0.79736328125, 0.837890625]
     Last 10:  [0.90087890625, 0.5458984375, 0.4921875, 0.96240234375, 0.45849609375, 0.440673828125, 0.5107421875, 0.923828125, 0.52392578125, 0.505859375]

================================================================================
158_model.layers.13.self_attn.q_proj: Linear (model.layers.13.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004712, Std: 0.460520
     First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
     Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.066818, Std: 1.190367
     First 10: [-0.42076098918914795, 0.6793600916862488, -0.3703729808330536, 0.435073584318161, -0.2199559509754181, -0.0200016051530838, 0.10038726031780243, -0.01716776192188263, 0.07641714066267014, -0.18805919587612152]
     Last 10:  [-1.7435721158981323, -0.004382014274597168, 1.4221065044403076, -0.6278623938560486, -1.5212430953979492, -1.202261209487915, 2.2697012424468994, -1.72633957862854, 3.7544283866882324, 1.6160914897918701]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000109
     First 10: [-0.006565093994140625, 0.026641845703125, -0.07061767578125, -0.09912109375, -0.00942230224609375, -0.0084686279296875, -0.0268707275390625, -0.0008087158203125, -0.04046630859375, 0.00567626953125]
     Last 10:  [-0.0177001953125, -0.056732177734375, -0.03228759765625, 0.011383056640625, 0.07672119140625, -0.054901123046875, 0.00748443603515625, 0.0189056396484375, 0.04296875, -0.0161590576171875]

================================================================================
159_model.layers.13.self_attn.k_proj: Linear (model.layers.13.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004712, Std: 0.460520
     First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
     Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.035674, Std: 1.445020
     First 10: [-0.008639417588710785, -0.010184798389673233, -0.002089519053697586, -0.018481580540537834, 0.006759904325008392, 0.010181590914726257, -0.00013541430234909058, 0.021402381360530853, -0.01400740072131157, -0.024360768496990204]
     Last 10:  [-1.4801174402236938, -2.876662015914917, 0.7733408808708191, -6.402985572814941, 1.1416953802108765, -0.3163928985595703, -3.121788263320923, 4.005972862243652, -1.4569640159606934, -5.875749111175537]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000291
     First 10: [0.0147705078125, 0.001453399658203125, 0.041717529296875, -0.033050537109375, -0.00888824462890625, 0.0153350830078125, -0.00748443603515625, 0.057586669921875, 0.0203857421875, 0.0015811920166015625]
     Last 10:  [0.0482177734375, 0.07666015625, -0.10723876953125, -0.03753662109375, -0.01493072509765625, -0.1629638671875, 0.0267333984375, 0.0128631591796875, -0.27490234375, -0.0224151611328125]

================================================================================
160_model.layers.13.self_attn.v_proj: Linear (model.layers.13.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004712, Std: 0.460520
     First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
     Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.005100, Std: 0.379252
     First 10: [-0.0030643390491604805, -0.003891207743436098, -0.0007160466630011797, 0.0005905274301767349, 0.005899776704609394, 0.01040727086365223, 0.004341306164860725, -0.005344033241271973, -0.0012756967917084694, 0.0021893689408898354]
     Last 10:  [-0.07414878159761429, -0.1413586139678955, -0.48202940821647644, 0.30420979857444763, -0.22484007477760315, 0.1498182713985443, -0.25613903999328613, -0.7514139413833618, 0.07537653297185898, -0.2919069528579712]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000046
     First 10: [-0.0002665519714355469, 0.0080108642578125, 0.0283660888671875, 0.047271728515625, 0.02703857421875, -0.0204315185546875, 0.018829345703125, -0.055145263671875, -0.005260467529296875, 0.02142333984375]
     Last 10:  [-0.0011911392211914062, -0.01116180419921875, -0.0208282470703125, 0.0025806427001953125, -0.01103973388671875, 0.0146331787109375, 0.0033740997314453125, -0.00717926025390625, -0.0421142578125, 0.04132080078125]

================================================================================
161_model.layers.13.self_attn.o_proj: Linear (model.layers.13.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.005242, Std: 0.048990
     First 10: [-0.0030643390491604805, -0.003891207743436098, -0.0007160466630011797, 0.0005905274301767349, 0.005899776704609394, 0.01040727086365223, 0.004341306164860725, -0.005344033241271973, -0.0012756967917084694, 0.0021893689408898354]
     Last 10:  [-0.0004892586730420589, -0.003795405151322484, -0.020822331309318542, -0.0026854330208152533, -0.004840119741857052, 0.006202497985213995, -0.004435309208929539, -0.016293026506900787, -9.417619730811566e-05, -0.0021265435498207808]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001198, Std: 0.033941
     First 10: [-0.008968332782387733, 0.018781838938593864, -0.05217666178941727, 0.026461176574230194, -0.0163239985704422, 0.027069933712482452, 0.005506984423846006, 0.010798115283250809, -0.02114054188132286, -0.037783071398735046]
     Last 10:  [0.09548792243003845, -0.00459502637386322, -0.019622422754764557, -0.08481042087078094, -0.01826043426990509, -0.03214899078011513, -0.05278797075152397, -0.04224478080868721, -0.009386582300066948, -0.0018432867946103215]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000024
     First 10: [0.007488250732421875, 0.0225982666015625, 0.002292633056640625, 0.002964019775390625, -0.0172271728515625, -0.0191802978515625, 0.04632568359375, 0.025665283203125, 0.032958984375, -0.00518798828125]
     Last 10:  [-0.030853271484375, -0.04803466796875, 0.0168609619140625, 0.0216217041015625, -0.0216064453125, -0.022308349609375, 0.044921875, 0.02679443359375, 0.041473388671875, -0.082275390625]

================================================================================
162_model.layers.13.self_attn: LlamaSdpaAttention (model.layers.13.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.13.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001198, Std: 0.033941
     First 10: [-0.008968332782387733, 0.018781838938593864, -0.05217666178941727, 0.026461176574230194, -0.0163239985704422, 0.027069933712482452, 0.005506984423846006, 0.010798115283250809, -0.02114054188132286, -0.037783071398735046]
     Last 10:  [0.09548792243003845, -0.00459502637386322, -0.019622422754764557, -0.08481042087078094, -0.01826043426990509, -0.03214899078011513, -0.05278797075152397, -0.04224478080868721, -0.009386582300066948, -0.0018432867946103215]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.13.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.280217
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000064969062805, 0.49999353289604187, 0.0, 0.0, 0.0]
     Last 10:  [0.4898017644882202, 0.49066632986068726, 0.00640150299295783, 0.01313041616231203, 0.0, 0.49137449264526367, 0.49358707666397095, 0.001001422991976142, 0.0017073185881599784, 0.012329678051173687]
     Zeros: 90, Total: 225

================================================================================
163_model.layers.13.post_attention_layernorm: LlamaRMSNorm (model.layers.13.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.13.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080530, Std: 11.956353
     First 10: [-0.5644916892051697, -2.623857021331787, -0.27643194794654846, 0.3889291286468506, -11.814026832580566, -7.408497333526611, 5.242496967315674, -0.08674421906471252, -0.543429970741272, -2.0346577167510986]
     Last 10:  [0.2788005471229553, -0.29281753301620483, 0.05117788165807724, 0.2698206901550293, 0.1538296490907669, 0.008271604776382446, -0.07308375835418701, -0.3277491331100464, -0.12714286148548126, -0.10716076195240021]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003862, Std: 0.323448
     First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
     Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.371248
     First 10: [0.344482421875, 0.300537109375, 0.3798828125, 0.403564453125, 0.3876953125, 0.296630859375, 0.298095703125, 0.40087890625, 0.416259765625, 0.257568359375]
     Last 10:  [0.4296875, 0.440185546875, 0.32275390625, 0.33642578125, 0.403076171875, 0.375, 0.456298828125, 0.379638671875, 0.41455078125, 0.39892578125]

================================================================================
164_model.layers.13.mlp.gate_proj: Linear (model.layers.13.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.13.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003862, Std: 0.323448
     First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
     Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.104733, Std: 0.385689
     First 10: [-0.13648712635040283, -0.02419028803706169, 0.1458909809589386, 0.0854378417134285, -0.3010406792163849, -0.11004713922739029, 0.4170633852481842, 0.01187189482152462, -0.10243837535381317, 0.35705187916755676]
     Last 10:  [-0.11883067339658737, 0.282428503036499, 0.5761182904243469, 0.07445305585861206, -0.20090416073799133, -1.3292427062988281, 0.23481516540050507, 0.17988209426403046, -0.01845243200659752, -0.5340774059295654]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000153
     First 10: [0.027313232421875, 0.01480865478515625, 0.048614501953125, 0.0015554428100585938, 0.0152740478515625, -0.07464599609375, -0.047943115234375, 0.01061248779296875, -0.0274200439453125, -0.0210723876953125]
     Last 10:  [0.0197601318359375, -0.0166778564453125, -0.0306396484375, 0.016876220703125, 0.003997802734375, 0.06060791015625, 0.0008997917175292969, -0.0037078857421875, 0.10821533203125, -0.0509033203125]

================================================================================
165_model.layers.13.mlp.act_fn: SiLU (model.layers.13.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.13.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.104733, Std: 0.385689
     First 10: [-0.13648712635040283, -0.02419028803706169, 0.1458909809589386, 0.0854378417134285, -0.3010406792163849, -0.11004713922739029, 0.4170633852481842, 0.01187189482152462, -0.10243837535381317, 0.35705187916755676]
     Last 10:  [-0.11883067339658737, 0.282428503036499, 0.5761182904243469, 0.07445305585861206, -0.20090416073799133, -1.3292427062988281, 0.23481516540050507, 0.17988209426403046, -0.01845243200659752, -0.5340774059295654]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.13.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.014712, Std: 0.182810
     First 10: [-0.06359358876943588, -0.011948859319090843, 0.07825712114572525, 0.044542718678712845, -0.12803353369235992, -0.05199902877211571, 0.25139760971069336, 0.005971182603389025, -0.048598069697618484, 0.2100631296634674]
     Last 10:  [-0.055889301002025604, 0.16102421283721924, 0.3688157796859741, 0.038611702620983124, -0.09039527177810669, -0.27819010615348816, 0.13112913072109222, 0.09800869971513748, -0.009141094982624054, -0.1973770558834076]
     Zeros: 0, Total: 7680

================================================================================
166_model.layers.13.mlp.up_proj: Linear (model.layers.13.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.13.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003862, Std: 0.323448
     First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
     Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.001981, Std: 0.276855
     First 10: [-0.26818615198135376, 0.1785079538822174, -0.10966960340738297, -0.14726987481117249, 0.345971018075943, -0.3213123381137848, 0.16391846537590027, 0.3449362516403198, -0.08196382224559784, 0.149034783244133]
     Last 10:  [-0.20740064978599548, 0.08973650634288788, 0.1117028072476387, 0.5778361558914185, -0.058040767908096313, 0.46338027715682983, 0.10986851155757904, 0.16729488968849182, -0.3580135703086853, 0.16964662075042725]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000065
     First 10: [0.0298309326171875, -0.06915283203125, -0.08941650390625, 0.01495361328125, -0.0212249755859375, 0.043426513671875, -0.01493072509765625, 0.042694091796875, 0.062408447265625, -0.00551605224609375]
     Last 10:  [0.034454345703125, -0.06378173828125, 0.03863525390625, 0.01861572265625, 0.018463134765625, -0.05181884765625, 0.017822265625, -0.04412841796875, -0.0262298583984375, -0.0240631103515625]

================================================================================
167_model.layers.13.mlp.down_proj: Linear (model.layers.13.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.13.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000329, Std: 0.053829
     First 10: [0.01705491915345192, -0.0021329664159566164, -0.00858242716640234, -0.006559800822287798, -0.044295892119407654, 0.016707928851246834, 0.04120871052145958, 0.0020596773829311132, 0.003983283415436745, 0.031306713819503784]
     Last 10:  [0.011591477319598198, 0.014449750073254108, 0.0411977581679821, 0.022311238572001457, 0.005246610846370459, -0.12890781462192535, 0.01440696232020855, 0.016396354883909225, 0.0032726360950618982, -0.03348435088992119]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.13.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000468, Std: 0.088488
     First 10: [-0.039452143013477325, 0.011190068908035755, -0.008586423471570015, 3.8825906813144684e-05, 0.07798518240451813, 0.010613531805574894, -0.08701080083847046, 0.023920992389321327, -0.04606546461582184, 0.0046095699071884155]
     Last 10:  [-0.22204355895519257, 0.18444284796714783, 0.11296658217906952, -0.2987004518508911, -0.061648618429899216, 0.011459406465291977, -0.005854015238583088, 0.24903547763824463, -0.016605587676167488, 0.054717931896448135]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000004
     First 10: [0.0114288330078125, 0.04083251953125, 0.0303497314453125, -0.041595458984375, 0.01067352294921875, 0.008209228515625, -0.039306640625, 0.029754638671875, 0.060943603515625, -0.0546875]
     Last 10:  [-0.007747650146484375, -0.01428985595703125, -0.01271820068359375, 0.038330078125, -0.08282470703125, 0.022857666015625, -0.024444580078125, -0.01427459716796875, 0.00885772705078125, -0.033477783203125]

================================================================================
168_model.layers.13.mlp: LlamaMLP (model.layers.13.mlp)
================================================================================

  → INPUT[0]: model.layers.13.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003862, Std: 0.323448
     First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
     Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.13.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000468, Std: 0.088488
     First 10: [-0.039452143013477325, 0.011190068908035755, -0.008586423471570015, 3.8825906813144684e-05, 0.07798518240451813, 0.010613531805574894, -0.08701080083847046, 0.023920992389321327, -0.04606546461582184, 0.0046095699071884155]
     Last 10:  [-0.22204355895519257, 0.18444284796714783, 0.11296658217906952, -0.2987004518508911, -0.061648618429899216, 0.011459406465291977, -0.005854015238583088, 0.24903547763824463, -0.016605587676167488, 0.054717931896448135]
     Zeros: 0, Total: 2880

================================================================================
169_model.layers.14.input_layernorm: LlamaRMSNorm (model.layers.14.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.14.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080062, Std: 11.965488
     First 10: [-0.6039438247680664, -2.6126668453216553, -0.2850183844566345, 0.38896796107292175, -11.736042022705078, -7.397883892059326, 5.155486106872559, -0.06282322853803635, -0.5894954204559326, -2.030048131942749]
     Last 10:  [0.056756988167762756, -0.108374685049057, 0.16414445638656616, -0.028879761695861816, 0.09218102693557739, 0.019731011241674423, -0.07893777638673782, -0.07871365547180176, -0.1437484472990036, -0.05244283005595207]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010466, Std: 0.422991
     First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
     Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.519671
     First 10: [0.475830078125, 0.58154296875, 0.580078125, 0.6162109375, 0.55029296875, 0.767578125, 0.67138671875, 0.66259765625, 0.64013671875, 0.43017578125]
     Last 10:  [0.59619140625, 0.5859375, 0.431884765625, 0.56201171875, 0.482666015625, 0.484619140625, 0.6767578125, 0.65283203125, 0.515625, 0.607421875]

================================================================================
170_model.layers.14.self_attn.q_proj: Linear (model.layers.14.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010466, Std: 0.422991
     First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
     Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.061669, Std: 1.189427
     First 10: [-0.3514802157878876, -0.05922148376703262, -0.36461183428764343, -0.022580616176128387, 0.13230937719345093, 0.3659815490245819, -0.20977497100830078, -0.34770911931991577, -0.328805536031723, -0.03519643843173981]
     Last 10:  [-0.2600557804107666, -1.1311225891113281, 0.4009439945220947, 0.9139565825462341, 0.7931382656097412, -1.0097589492797852, 0.8571145534515381, -1.6014360189437866, 0.28154757618904114, 2.5096559524536133]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000020
     First 10: [-0.09478759765625, -0.055450439453125, 0.0479736328125, 0.08575439453125, -0.08953857421875, 0.0794677734375, 0.11981201171875, -0.028350830078125, -0.014373779296875, 0.07061767578125]
     Last 10:  [-0.11798095703125, 0.002132415771484375, 0.1053466796875, 0.058685302734375, -0.03656005859375, -0.0290679931640625, -0.054168701171875, 0.0207977294921875, 0.1046142578125, -0.045501708984375]

================================================================================
171_model.layers.14.self_attn.k_proj: Linear (model.layers.14.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010466, Std: 0.422991
     First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
     Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.011048, Std: 1.598904
     First 10: [-0.00015250593423843384, 0.00789109617471695, 0.018104486167430878, -0.016701724380254745, 0.0014400612562894821, -0.00628583412617445, -0.006771024316549301, -0.012862557545304298, 0.0028364062309265137, 0.002975955605506897]
     Last 10:  [0.504362940788269, 10.621129035949707, 1.588103175163269, -0.2511930465698242, 1.3895225524902344, 2.1603503227233887, -1.7744572162628174, 0.7686845064163208, 2.139622926712036, -1.8115251064300537]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000041
     First 10: [0.02410888671875, -0.033355712890625, 0.1328125, 0.032379150390625, 0.043243408203125, 0.054901123046875, 0.01971435546875, -0.11083984375, -0.022064208984375, 0.01218414306640625]
     Last 10:  [-0.0709228515625, 0.07427978515625, -0.01409912109375, -0.01222991943359375, -0.0787353515625, -0.041259765625, -0.0849609375, -0.1356201171875, 0.016387939453125, -0.0115966796875]

================================================================================
172_model.layers.14.self_attn.v_proj: Linear (model.layers.14.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010466, Std: 0.422991
     First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
     Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.003793, Std: 0.227292
     First 10: [-0.004392341244965792, -0.003490538103505969, 0.008717425167560577, 0.0021784938871860504, -0.3298429846763611, 0.00830900575965643, -0.0037184394896030426, 0.004065722227096558, -0.0016832845285534859, 0.0011702044866979122]
     Last 10:  [-0.6568197011947632, -0.2794167101383209, -0.07752219587564468, 0.009186781942844391, 0.5729794502258301, -0.1233617514371872, 0.005425295792520046, -0.13127289712429047, -0.280194491147995, 0.003177165985107422]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000045
     First 10: [-0.014190673828125, 0.006191253662109375, 0.00014531612396240234, 0.003856658935546875, 0.0062713623046875, -0.005767822265625, 0.0010023117065429688, -0.02801513671875, -0.025299072265625, 0.0300140380859375]
     Last 10:  [-0.021942138671875, 0.0014801025390625, 0.019073486328125, 0.0211029052734375, -0.06756591796875, 0.0321044921875, -0.01360321044921875, 0.005077362060546875, 0.0114593505859375, -0.0814208984375]

================================================================================
173_model.layers.14.self_attn.o_proj: Linear (model.layers.14.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003562, Std: 0.074555
     First 10: [-0.004392341244965792, -0.003490538103505969, 0.008717425167560577, 0.0021784938871860504, -0.3298429846763611, 0.00830900575965643, -0.0037184394896030426, 0.004065722227096558, -0.0016832845285534859, 0.0011702044866979122]
     Last 10:  [-0.03878210112452507, -0.0024176547303795815, 0.014912809245288372, 0.03185832500457764, 0.06820628046989441, -0.013219917193055153, 0.04151254892349243, 0.01744043454527855, -0.018768716603517532, -0.00031905024661682546]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000517, Std: 0.043271
     First 10: [-0.004497063346207142, -0.020656470209360123, -0.008182350546121597, 0.029331877827644348, -0.06959445774555206, 0.011004443280398846, -0.021061347797513008, 0.0316961295902729, 0.013342833146452904, -0.028646090999245644]
     Last 10:  [0.06155891716480255, -0.03895111009478569, -0.01922137476503849, -0.022138241678476334, -0.01845867559313774, 0.010098550468683243, -0.0738924890756607, -0.03796697035431862, -0.035423748195171356, 0.021282747387886047]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000034
     First 10: [-0.021209716796875, 0.020721435546875, -0.026641845703125, 0.026092529296875, 0.0024585723876953125, 0.026123046875, -0.02899169921875, -0.017578125, -0.0134124755859375, -0.08404541015625]
     Last 10:  [0.0038166046142578125, 0.0255584716796875, -0.055450439453125, 0.043975830078125, 0.03076171875, -0.01132965087890625, 0.0186309814453125, -0.0027675628662109375, 0.005954742431640625, -0.08837890625]

================================================================================
174_model.layers.14.self_attn: LlamaSdpaAttention (model.layers.14.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.14.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000517, Std: 0.043271
     First 10: [-0.004497063346207142, -0.020656470209360123, -0.008182350546121597, 0.029331877827644348, -0.06959445774555206, 0.011004443280398846, -0.021061347797513008, 0.0316961295902729, 0.013342833146452904, -0.028646090999245644]
     Last 10:  [0.06155891716480255, -0.03895111009478569, -0.01922137476503849, -0.022138241678476334, -0.01845867559313774, 0.010098550468683243, -0.0738924890756607, -0.03796697035431862, -0.035423748195171356, 0.021282747387886047]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.14.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.271420
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000494718551636, 0.4999505877494812, 0.0, 0.0, 0.0]
     Last 10:  [0.42812231183052063, 0.42677605152130127, 0.0884450227022171, 0.05665652081370354, 0.0, 0.45246925950050354, 0.45050230622291565, 0.02913064695894718, 0.02884392999112606, 0.0390537865459919]
     Zeros: 90, Total: 225

================================================================================
175_model.layers.14.post_attention_layernorm: LlamaRMSNorm (model.layers.14.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.14.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080579, Std: 11.967939
     First 10: [-0.6084408760070801, -2.6333234310150146, -0.2932007312774658, 0.4182998538017273, -11.805636405944824, -7.3868794441223145, 5.134424686431885, -0.031127098947763443, -0.5761525630950928, -2.0586941242218018]
     Last 10:  [0.11831590533256531, -0.147325798869133, 0.14492307603359222, -0.05101800337433815, 0.07372234761714935, 0.029829561710357666, -0.15283027291297913, -0.11668062210083008, -0.17917218804359436, -0.031160082668066025]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003208, Std: 0.323494
     First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
     Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.377509
     First 10: [0.354736328125, 0.315673828125, 0.380859375, 0.409423828125, 0.3583984375, 0.277587890625, 0.31103515625, 0.400390625, 0.415771484375, 0.295166015625]
     Last 10:  [0.427001953125, 0.445068359375, 0.3369140625, 0.337158203125, 0.40283203125, 0.380859375, 0.433349609375, 0.386962890625, 0.406005859375, 0.398681640625]

================================================================================
176_model.layers.14.mlp.gate_proj: Linear (model.layers.14.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.14.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003208, Std: 0.323494
     First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
     Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.106218, Std: 0.367682
     First 10: [-0.07407622039318085, -0.31577643752098083, -0.6433165073394775, -0.1772184520959854, 0.1140700951218605, -0.2813716232776642, 0.1797490268945694, -0.7669975161552429, -0.12558990716934204, -0.31941285729408264]
     Last 10:  [-0.6567513346672058, -0.16795724630355835, 0.4937131404876709, 0.1826907992362976, 0.31825947761535645, -0.04405509680509567, 0.6012969613075256, -0.43153178691864014, -0.3819727897644043, -0.3574410080909729]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000200
     First 10: [0.0304412841796875, 0.0110626220703125, -0.0250396728515625, -0.0249176025390625, 0.0117340087890625, -0.05279541015625, 0.038970947265625, 0.005008697509765625, 0.03631591796875, -0.01515960693359375]
     Last 10:  [0.0230712890625, 0.025787353515625, -0.003780364990234375, 0.07171630859375, -0.027801513671875, 0.05792236328125, 0.046295166015625, 0.00212860107421875, 0.0323486328125, -0.042449951171875]

================================================================================
177_model.layers.14.mlp.act_fn: SiLU (model.layers.14.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.14.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.106218, Std: 0.367682
     First 10: [-0.07407622039318085, -0.31577643752098083, -0.6433165073394775, -0.1772184520959854, 0.1140700951218605, -0.2813716232776642, 0.1797490268945694, -0.7669975161552429, -0.12558990716934204, -0.31941285729408264]
     Last 10:  [-0.6567513346672058, -0.16795724630355835, 0.4937131404876709, 0.1826907992362976, 0.31825947761535645, -0.04405509680509567, 0.6012969613075256, -0.43153178691864014, -0.3819727897644043, -0.3574410080909729]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.14.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.018214, Std: 0.172543
     First 10: [-0.03566691279411316, -0.13316462934017181, -0.22162075340747833, -0.08077811449766159, 0.06028452143073082, -0.12102286517620087, 0.09793026745319366, -0.24323713779449463, -0.05885691940784454, -0.13441495597362518]
     Last 10:  [-0.22426071763038635, -0.07694274187088013, 0.306586354970932, 0.09966625273227692, 0.1842404156923294, -0.021542415022850037, 0.3884095847606659, -0.1699202060699463, -0.1549476981163025, -0.14711527526378632]
     Zeros: 0, Total: 7680

================================================================================
178_model.layers.14.mlp.up_proj: Linear (model.layers.14.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.14.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003208, Std: 0.323494
     First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
     Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.004692, Std: 0.284830
     First 10: [0.11860073357820511, 0.09151222556829453, -0.017519276589155197, -0.12478967010974884, 0.19767773151397705, -0.14508064091205597, -0.12148916721343994, -0.2326991707086563, -0.32709673047065735, -0.022750381380319595]
     Last 10:  [0.0010524839162826538, 0.32037317752838135, -0.09161709994077682, -0.8842049241065979, 0.7040984630584717, 0.1478009968996048, 0.39878538250923157, 0.17709651589393616, 0.09404692053794861, -0.10393446683883667]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000011
     First 10: [-0.0439453125, -0.0116729736328125, 0.01336669921875, 0.0228424072265625, -0.0150146484375, -0.04656982421875, -0.0043487548828125, -0.0321044921875, 0.01690673828125, 0.019287109375]
     Last 10:  [0.02764892578125, 0.019866943359375, 0.01776123046875, -0.01123046875, 0.007564544677734375, -0.00972747802734375, -0.0276336669921875, 0.00507354736328125, 0.01273345947265625, 0.0255126953125]

================================================================================
179_model.layers.14.mlp.down_proj: Linear (model.layers.14.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.14.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000682, Std: 0.057558
     First 10: [-0.004230122081935406, -0.012186191976070404, 0.00388263538479805, 0.010080274194478989, 0.011916907504200935, 0.017558075487613678, -0.011897467076778412, 0.05660108104348183, 0.019251905381679535, 0.0030579916201531887]
     Last 10:  [-0.00023603079898748547, -0.024650391191244125, -0.028088552877306938, -0.08812539279460907, 0.12972339987754822, -0.0031839904841035604, 0.15489207208156586, -0.030092276632785797, -0.014572354033589363, 0.01529034785926342]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.14.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000176, Std: 0.084054
     First 10: [-0.03280874341726303, -0.04123706370592117, -0.014504763297736645, 0.0506117045879364, 0.016550302505493164, -0.04607955366373062, -0.08920478820800781, -0.020554790273308754, 0.03946232795715332, 0.054748889058828354]
     Last 10:  [0.12243931740522385, -0.044186852872371674, 0.0958390012383461, -0.03137553855776787, -0.00720810703933239, -0.15006276965141296, 0.16627170145511627, -0.02194659411907196, -0.032278984785079956, 0.024131760001182556]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000080
     First 10: [0.012969970703125, 0.0254364013671875, 0.047607421875, -0.02392578125, -0.020294189453125, 0.0265350341796875, -0.00447845458984375, -0.020843505859375, -0.044830322265625, 0.022705078125]
     Last 10:  [0.0014209747314453125, 0.029937744140625, 0.0230865478515625, 0.004108428955078125, -0.0264129638671875, 0.043731689453125, 0.02789306640625, 0.0217132568359375, -0.00920867919921875, -0.0024967193603515625]

================================================================================
180_model.layers.14.mlp: LlamaMLP (model.layers.14.mlp)
================================================================================

  → INPUT[0]: model.layers.14.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003208, Std: 0.323494
     First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
     Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.14.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000176, Std: 0.084054
     First 10: [-0.03280874341726303, -0.04123706370592117, -0.014504763297736645, 0.0506117045879364, 0.016550302505493164, -0.04607955366373062, -0.08920478820800781, -0.020554790273308754, 0.03946232795715332, 0.054748889058828354]
     Last 10:  [0.12243931740522385, -0.044186852872371674, 0.0958390012383461, -0.03137553855776787, -0.00720810703933239, -0.15006276965141296, 0.16627170145511627, -0.02194659411907196, -0.032278984785079956, 0.024131760001182556]
     Zeros: 0, Total: 2880

================================================================================
181_model.layers.15.input_layernorm: LlamaRMSNorm (model.layers.15.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.15.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080755, Std: 11.976205
     First 10: [-0.6412495970726013, -2.674560546875, -0.30770549178123474, 0.4689115583896637, -11.78908634185791, -7.432959079742432, 5.045219898223877, -0.051681891083717346, -0.5366902351379395, -2.0039453506469727]
     Last 10:  [0.24075523018836975, -0.19151264429092407, 0.24076208472251892, -0.08239354193210602, 0.06651423871517181, -0.1202332079410553, 0.013441428542137146, -0.13862721621990204, -0.21145117282867432, -0.007028322666883469]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008878, Std: 0.435554
     First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
     Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.557016
     First 10: [0.52685546875, 0.56982421875, 0.5595703125, 0.6240234375, 0.487548828125, 0.78076171875, 0.677734375, 0.685546875, 0.59619140625, 0.59228515625]
     Last 10:  [0.580078125, 0.62451171875, 0.48046875, 0.5263671875, 0.55419921875, 0.56103515625, 0.65234375, 0.63623046875, 0.5498046875, 0.64697265625]

================================================================================
182_model.layers.15.self_attn.q_proj: Linear (model.layers.15.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008878, Std: 0.435554
     First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
     Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.092776, Std: 0.982665
     First 10: [0.07802552729845047, -0.16831021010875702, -0.15376557409763336, 0.29059451818466187, 0.22482773661613464, 0.2570437788963318, 0.31175166368484497, -0.26605933904647827, -0.13152524828910828, -0.06264762580394745]
     Last 10:  [0.06408193707466125, -4.008188724517822, 0.5420756936073303, -0.7783502340316772, 1.8474944829940796, -0.6108828186988831, 1.1639776229858398, -0.6746225357055664, -0.5543580055236816, 2.634235382080078]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000159
     First 10: [0.030792236328125, 0.039306640625, -0.01800537109375, -0.0252685546875, -0.01392364501953125, -0.001071929931640625, -0.033782958984375, 0.0438232421875, -0.039337158203125, 0.044219970703125]
     Last 10:  [0.027618408203125, 0.0019855499267578125, 0.058197021484375, -0.05804443359375, 0.017791748046875, -0.1358642578125, 0.1822509765625, -0.0982666015625, -0.059967041015625, 0.00969696044921875]

================================================================================
183_model.layers.15.self_attn.k_proj: Linear (model.layers.15.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008878, Std: 0.435554
     First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
     Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.087334, Std: 1.462452
     First 10: [-0.01848674565553665, 0.008560292422771454, -0.0014318674802780151, 0.018284618854522705, 0.002789027988910675, -0.005718842148780823, 0.005018316209316254, 0.012630641460418701, 0.018589362502098083, 0.011986792087554932]
     Last 10:  [0.6881583333015442, 1.3895552158355713, -0.35941582918167114, 0.21085986495018005, 2.3370471000671387, 0.31648313999176025, -3.3314404487609863, 0.5890377163887024, 1.4098063707351685, -1.9833316802978516]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000036
     First 10: [0.0237884521484375, 0.0367431640625, -0.0655517578125, 0.030609130859375, -0.06256103515625, 0.0467529296875, -0.0762939453125, 0.0032672882080078125, -0.01837158203125, -0.019439697265625]
     Last 10:  [0.130859375, -0.1265869140625, -0.2083740234375, -0.1524658203125, 0.10919189453125, 0.09368896484375, 0.131591796875, -0.10089111328125, 0.03509521484375, 0.09149169921875]

================================================================================
184_model.layers.15.self_attn.v_proj: Linear (model.layers.15.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008878, Std: 0.435554
     First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
     Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.014041, Std: 0.245201
     First 10: [-7.308647036552429e-05, -0.0019907504320144653, -0.002632011193782091, 0.0036939848214387894, 0.0016977153718471527, 0.000722990371286869, -0.007085083052515984, -0.008940479718148708, -0.0008091479539871216, 0.0054744696244597435]
     Last 10:  [-0.22604970633983612, 0.37279489636421204, -0.1945694535970688, 0.1265314817428589, -0.13461855053901672, -0.031081005930900574, -0.2440500557422638, 0.21388238668441772, -0.11362393200397491, 0.4031302332878113]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000067
     First 10: [-0.0020198822021484375, 0.0364990234375, -0.023590087890625, 0.0105438232421875, -0.006313323974609375, -0.004604339599609375, -0.03021240234375, -0.0296173095703125, 0.0175628662109375, -0.017059326171875]
     Last 10:  [0.0012845993041992188, -0.046539306640625, -0.0036067962646484375, 0.041259765625, 0.0001957416534423828, 0.047088623046875, 0.0091705322265625, 0.0168609619140625, 0.04583740234375, 0.007843017578125]

================================================================================
185_model.layers.15.self_attn.o_proj: Linear (model.layers.15.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.007818, Std: 0.065691
     First 10: [-7.308647036552429e-05, -0.0019907504320144653, -0.002632011193782091, 0.0036939848214387894, 0.0016977153718471527, 0.000722990371286869, -0.007085083052515984, -0.008940479718148708, -0.0008091479539871216, 0.0054744696244597435]
     Last 10:  [-0.011285901069641113, 0.00859407801181078, 0.004827586002647877, 0.021838994696736336, -0.007263955194503069, -0.0479145310819149, 0.010600698180496693, -0.004944524262100458, -0.0014514672802761197, 0.017791738733649254]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000480, Std: 0.028471
     First 10: [0.01998063176870346, 0.013053908944129944, 0.05504915118217468, -0.00795756559818983, -0.038467392325401306, -0.011210080236196518, 0.04191960394382477, -0.01814422383904457, -0.010399280115962029, -0.002248473232612014]
     Last 10:  [0.016002409160137177, -0.033594053238630295, -0.005251521244645119, -0.02002592757344246, 0.008774877525866032, -0.001293916255235672, -0.012526373378932476, 0.021626941859722137, -0.021112563088536263, 0.012510336935520172]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000059
     First 10: [0.00699615478515625, -0.03485107421875, -0.005931854248046875, -0.0023975372314453125, 0.0249481201171875, 0.00038933753967285156, -0.00766754150390625, 0.01068115234375, -0.011566162109375, -0.10418701171875]
     Last 10:  [0.029144287109375, -0.0302886962890625, 0.0513916015625, -0.0181884765625, 0.031982421875, 0.0239105224609375, -0.0184478759765625, 0.02435302734375, 0.0657958984375, -0.005504608154296875]

================================================================================
186_model.layers.15.self_attn: LlamaSdpaAttention (model.layers.15.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.15.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000480, Std: 0.028471
     First 10: [0.01998063176870346, 0.013053908944129944, 0.05504915118217468, -0.00795756559818983, -0.038467392325401306, -0.011210080236196518, 0.04191960394382477, -0.01814422383904457, -0.010399280115962029, -0.002248473232612014]
     Last 10:  [0.016002409160137177, -0.033594053238630295, -0.005251521244645119, -0.02002592757344246, 0.008774877525866032, -0.001293916255235672, -0.012526373378932476, 0.021626941859722137, -0.021112563088536263, 0.012510336935520172]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.15.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.272957
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.499970406293869, 0.5000296235084534, 0.0, 0.0, 0.0]
     Last 10:  [0.4725435972213745, 0.4740278422832489, 0.028379594907164574, 0.025048935785889626, 0.0, 0.4666740298271179, 0.46783795952796936, 0.02236875705420971, 0.01639929786324501, 0.026719938963651657]
     Zeros: 90, Total: 225

================================================================================
187_model.layers.15.post_attention_layernorm: LlamaRMSNorm (model.layers.15.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.15.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.081235, Std: 11.975645
     First 10: [-0.6212689876556396, -2.6615066528320312, -0.25265634059906006, 0.4609539806842804, -11.827553749084473, -7.444169044494629, 5.08713960647583, -0.06982611119747162, -0.5470895171165466, -2.0061938762664795]
     Last 10:  [0.2567576467990875, -0.22510670125484467, 0.23551055788993835, -0.10241946578025818, 0.07528911530971527, -0.12152712047100067, 0.00091505516320467, -0.1170002743601799, -0.23256373405456543, 0.0054820142686367035]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001926, Std: 0.326463
     First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
     Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.386820
     First 10: [0.377197265625, 0.320556640625, 0.4052734375, 0.4091796875, 0.35205078125, 0.2646484375, 0.311767578125, 0.38916015625, 0.438232421875, 0.296142578125]
     Last 10:  [0.436279296875, 0.448486328125, 0.361572265625, 0.365478515625, 0.4208984375, 0.4033203125, 0.427490234375, 0.380126953125, 0.4384765625, 0.399658203125]

================================================================================
188_model.layers.15.mlp.gate_proj: Linear (model.layers.15.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.15.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001926, Std: 0.326463
     First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
     Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.097665, Std: 0.368647
     First 10: [-0.4651666581630707, -0.3400917649269104, 0.052383504807949066, -0.26150426268577576, 0.11982455849647522, -0.3308011591434479, -0.0024910876527428627, 0.3283015489578247, 0.4007241427898407, 0.04473571106791496]
     Last 10:  [0.04808475077152252, 0.5974282026290894, -0.6890300512313843, -0.17770467698574066, -0.27593955397605896, -0.19452960789203644, -0.5481966137886047, 0.6126914620399475, 0.23851102590560913, -0.0579935722053051]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000238
     First 10: [0.0165863037109375, -0.0008487701416015625, 0.0026302337646484375, -0.01056671142578125, 0.0100860595703125, -0.036346435546875, 0.031982421875, 0.0189361572265625, 0.018829345703125, 0.0229034423828125]
     Last 10:  [-0.068603515625, -0.0177001953125, 0.0216827392578125, 0.028167724609375, -0.06884765625, 0.0218505859375, -0.0123138427734375, -0.00743865966796875, -0.01323699951171875, 0.0235137939453125]

================================================================================
189_model.layers.15.mlp.act_fn: SiLU (model.layers.15.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.15.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.097665, Std: 0.368647
     First 10: [-0.4651666581630707, -0.3400917649269104, 0.052383504807949066, -0.26150426268577576, 0.11982455849647522, -0.3308011591434479, -0.0024910876527428627, 0.3283015489578247, 0.4007241427898407, 0.04473571106791496]
     Last 10:  [0.04808475077152252, 0.5974282026290894, -0.6890300512313843, -0.17770467698574066, -0.27593955397605896, -0.19452960789203644, -0.5481966137886047, 0.6126914620399475, 0.23851102590560913, -0.0579935722053051]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.15.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.014063, Std: 0.175482
     First 10: [-0.1794431060552597, -0.14140580594539642, 0.02687760442495346, -0.11375277489423752, 0.06349747627973557, -0.13829000294208527, -0.0012439923593774438, 0.19085681438446045, 0.23997831344604492, 0.022868093103170395]
     Last 10:  [0.0246203001588583, 0.38538163900375366, -0.23030751943588257, -0.08097831159830093, -0.11905399709939957, -0.08783408999443054, -0.2007950395345688, 0.3973638117313385, 0.13341034948825836, -0.028156209737062454]
     Zeros: 0, Total: 7680

================================================================================
190_model.layers.15.mlp.up_proj: Linear (model.layers.15.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.15.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001926, Std: 0.326463
     First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
     Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.002155, Std: 0.278305
     First 10: [0.29630032181739807, 0.24168069660663605, 0.0652238130569458, 0.14969247579574585, 0.4160180687904358, 0.02824719063937664, -0.09548992663621902, 0.039694029837846756, 0.5051534175872803, -0.4870710074901581]
     Last 10:  [0.25510597229003906, 0.20596373081207275, 0.47933492064476013, 0.47168609499931335, -0.005025304853916168, 0.09475453197956085, 0.24663090705871582, 0.31489357352256775, -0.3587854504585266, 0.09580744057893753]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000019
     First 10: [0.0243682861328125, -0.0248260498046875, -0.033721923828125, 0.066162109375, 0.025543212890625, -0.005695343017578125, -0.031829833984375, 0.005859375, -0.033538818359375, -0.004863739013671875]
     Last 10:  [0.01331329345703125, -0.0106964111328125, 0.012176513671875, -0.04937744140625, 0.0228118896484375, 0.0936279296875, 0.01039886474609375, -0.036865234375, 0.0216827392578125, -0.052154541015625]

================================================================================
191_model.layers.15.mlp.down_proj: Linear (model.layers.15.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.15.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000566, Std: 0.050496
     First 10: [-0.05316904932260513, -0.03417505323886871, 0.0017530597979202867, -0.01702793501317501, 0.026416096836328506, -0.003906304016709328, 0.00011878873920068145, 0.0075758760794997215, 0.12122586369514465, -0.01113838516175747]
     Last 10:  [0.006280785426497459, 0.07937464118003845, -0.1103944331407547, -0.038196343928575516, 0.0005982826114632189, -0.008322678506374359, -0.04952226206660271, 0.12512731552124023, -0.047865692526102066, -0.0026975744403898716]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.15.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001018, Std: 0.078609
     First 10: [-0.020521845668554306, -0.02138121984899044, -0.06537812203168869, 0.0317850336432457, 0.025577450171113014, -0.05370941385626793, -0.05586445704102516, -0.002750442363321781, -0.054448485374450684, 0.023503364995121956]
     Last 10:  [-0.07972723245620728, 0.07079939544200897, 0.145757257938385, -0.3011530935764313, -0.04218877851963043, -0.026874978095293045, 0.08341453969478607, 0.06711170822381973, -0.01482907310128212, -0.05398576334118843]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000065
     First 10: [-0.0125579833984375, 0.0158538818359375, -0.07501220703125, -0.0496826171875, -0.016082763671875, -0.02105712890625, -0.039459228515625, 0.0218963623046875, 0.033721923828125, 0.032867431640625]
     Last 10:  [-0.031158447265625, -0.003116607666015625, -0.0419921875, 0.0216522216796875, -0.031341552734375, -0.0447998046875, -0.023590087890625, 0.01500701904296875, -0.06182861328125, -0.0191497802734375]

================================================================================
192_model.layers.15.mlp: LlamaMLP (model.layers.15.mlp)
================================================================================

  → INPUT[0]: model.layers.15.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001926, Std: 0.326463
     First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
     Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.15.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001018, Std: 0.078609
     First 10: [-0.020521845668554306, -0.02138121984899044, -0.06537812203168869, 0.0317850336432457, 0.025577450171113014, -0.05370941385626793, -0.05586445704102516, -0.002750442363321781, -0.054448485374450684, 0.023503364995121956]
     Last 10:  [-0.07972723245620728, 0.07079939544200897, 0.145757257938385, -0.3011530935764313, -0.04218877851963043, -0.026874978095293045, 0.08341453969478607, 0.06711170822381973, -0.01482907310128212, -0.05398576334118843]
     Zeros: 0, Total: 2880

================================================================================
193_model.layers.16.input_layernorm: LlamaRMSNorm (model.layers.16.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.16.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080217, Std: 11.980221
     First 10: [-0.6417908072471619, -2.6828877925872803, -0.31803447008132935, 0.4927390217781067, -11.801976203918457, -7.497878551483154, 5.031275272369385, -0.07257655262947083, -0.6015380024909973, -1.982690453529358]
     Last 10:  [0.17703041434288025, -0.1543073058128357, 0.38126781582832336, -0.40357255935668945, 0.03310033679008484, -0.14840209484100342, 0.08432959765195847, -0.04988856613636017, -0.24739280343055725, -0.04850374907255173]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006325, Std: 0.362131
     First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
     Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.461956
     First 10: [0.41259765625, 0.572265625, 0.463134765625, 0.5341796875, 0.45361328125, 0.7734375, 0.529296875, 0.5703125, 0.517578125, 0.36328125]
     Last 10:  [0.50341796875, 0.525390625, 0.38525390625, 0.45361328125, 0.44091796875, 0.47705078125, 0.54443359375, 0.50927734375, 0.5048828125, 0.515625]

================================================================================
194_model.layers.16.self_attn.q_proj: Linear (model.layers.16.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006325, Std: 0.362131
     First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
     Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.074436, Std: 1.227069
     First 10: [-0.3350387215614319, 0.21581757068634033, 0.003996551036834717, 0.24866832792758942, -0.48115384578704834, -0.4574110507965088, -0.408858060836792, -0.1826893389225006, 0.8016403317451477, 0.014042600989341736]
     Last 10:  [0.8217407464981079, 0.1928538680076599, -0.6873186230659485, 0.4910053014755249, -0.9956344366073608, 0.15623699128627777, -0.7574701309204102, -2.625814914703369, 0.5928845405578613, 2.3561651706695557]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000035
     First 10: [-0.01456451416015625, -0.0205078125, 0.07867431640625, -0.0298004150390625, -0.026824951171875, 0.047576904296875, -0.0201568603515625, 0.006229400634765625, 0.085693359375, 0.05792236328125]
     Last 10:  [-0.09637451171875, -0.001499176025390625, 0.0279083251953125, 0.092529296875, 0.01195526123046875, -0.105224609375, 0.150634765625, 0.0162353515625, 0.0867919921875, 0.0163421630859375]

================================================================================
195_model.layers.16.self_attn.k_proj: Linear (model.layers.16.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006325, Std: 0.362131
     First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
     Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.009940, Std: 1.507380
     First 10: [-0.005198568105697632, 0.00022322498261928558, 0.015129856765270233, -0.0038931481540203094, 0.0008921083062887192, 0.015013113617897034, -0.013862546533346176, -0.0013660825788974762, -0.0019885972142219543, 0.002210184931755066]
     Last 10:  [-3.592010259628296, -1.5219674110412598, 9.552899360656738, 0.5880433917045593, 0.45306408405303955, -0.021950215101242065, -1.1067618131637573, 1.9469947814941406, 0.6273645162582397, -2.558842658996582]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000381
     First 10: [-0.00469207763671875, 0.0118865966796875, -0.00653839111328125, 0.09381103515625, -0.01413726806640625, -0.020904541015625, -0.055389404296875, 0.0635986328125, 0.01284027099609375, -0.04388427734375]
     Last 10:  [0.0006513595581054688, 0.0869140625, 0.0689697265625, -0.01430511474609375, 0.11907958984375, 0.03936767578125, -0.038909912109375, -0.0859375, 0.023529052734375, 0.0220184326171875]

================================================================================
196_model.layers.16.self_attn.v_proj: Linear (model.layers.16.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006325, Std: 0.362131
     First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
     Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.003748, Std: 0.188717
     First 10: [-0.07723812013864517, 0.014440563507378101, -0.011758588254451752, 0.016981225460767746, 0.002741817384958267, 0.0114127891138196, -0.0004495615139603615, 0.0037898439913988113, 0.005003102123737335, 0.0005356031470000744]
     Last 10:  [0.49229294061660767, -0.1582852005958557, 0.014253538101911545, -0.08009505271911621, 0.2842690646648407, -0.2741151452064514, -0.04489077627658844, 0.08477137237787247, 0.018200833350419998, 0.003392346203327179]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000088
     First 10: [-0.043121337890625, 0.010009765625, -0.050079345703125, 0.0016546249389648438, 0.0169525146484375, 0.004058837890625, 0.01445770263671875, -0.0394287109375, 0.0416259765625, 0.007843017578125]
     Last 10:  [-0.00959014892578125, 0.01410675048828125, 0.012542724609375, -0.0294189453125, 0.0175323486328125, -0.01552581787109375, -0.0149383544921875, 0.028289794921875, -0.040069580078125, 0.0158233642578125]

================================================================================
197_model.layers.16.self_attn.o_proj: Linear (model.layers.16.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002060, Std: 0.046003
     First 10: [-0.07723812013864517, 0.014440563507378101, -0.011758588254451752, 0.016981225460767746, 0.002741817384958267, 0.0114127891138196, -0.0004495615139603615, 0.0037898439913988113, 0.005003102123737335, 0.0005356031470000744]
     Last 10:  [-0.23123101890087128, -0.01605270244181156, -0.0009162032511085272, 0.0184547770768404, 0.0005499367835000157, -0.011951816268265247, -0.006968324538320303, -0.005955745466053486, -0.002505909651517868, -0.014335423707962036]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000202, Std: 0.036923
     First 10: [0.005855962168425322, -0.07277625054121017, 0.019377999007701874, 0.006385022774338722, -0.023855526000261307, 0.041514746844768524, -0.02654946781694889, 0.018934648483991623, -0.015052136033773422, -0.008229364641010761]
     Last 10:  [0.026641204953193665, -0.053711824119091034, 0.003905202727764845, 0.01146527100354433, 0.010883665643632412, 0.01851741038262844, -0.016230152919888496, 0.009730711579322815, 0.04558057337999344, 0.0014970204792916775]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000007
     First 10: [-0.044525146484375, 0.0236968994140625, -0.0262298583984375, -0.006290435791015625, 0.02294921875, 0.005565643310546875, 0.014312744140625, -0.06573486328125, 0.0059967041015625, -0.0384521484375]
     Last 10:  [-0.039031982421875, 0.035797119140625, -0.0085601806640625, 0.03851318359375, -0.01549530029296875, 0.026275634765625, 0.049102783203125, 0.1036376953125, 0.032257080078125, 0.06829833984375]

================================================================================
198_model.layers.16.self_attn: LlamaSdpaAttention (model.layers.16.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.16.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000202, Std: 0.036923
     First 10: [0.005855962168425322, -0.07277625054121017, 0.019377999007701874, 0.006385022774338722, -0.023855526000261307, 0.041514746844768524, -0.02654946781694889, 0.018934648483991623, -0.015052136033773422, -0.008229364641010761]
     Last 10:  [0.026641204953193665, -0.053711824119091034, 0.003905202727764845, 0.01146527100354433, 0.010883665643632412, 0.01851741038262844, -0.016230152919888496, 0.009730711579322815, 0.04558057337999344, 0.0014970204792916775]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.16.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.267624
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49966737627983093, 0.5003325939178467, 0.0, 0.0, 0.0]
     Last 10:  [0.46071377396583557, 0.46109503507614136, 0.03419998660683632, 0.04399118572473526, 0.0, 0.4800853431224823, 0.4804545044898987, 0.008153203874826431, 0.011466206051409245, 0.01984088122844696]
     Zeros: 90, Total: 225

================================================================================
199_model.layers.16.post_attention_layernorm: LlamaRMSNorm (model.layers.16.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.16.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.080420, Std: 11.982368
     First 10: [-0.6359348297119141, -2.755664110183716, -0.2986564636230469, 0.49912405014038086, -11.825831413269043, -7.456363677978516, 5.004725933074951, -0.0536419041454792, -0.616590142250061, -1.990919828414917]
     Last 10:  [0.2036716192960739, -0.20801913738250732, 0.3851730227470398, -0.3921072781085968, 0.043984003365039825, -0.12988469004631042, 0.06809944659471512, -0.040157854557037354, -0.2018122375011444, -0.04700672999024391]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000107, Std: 0.335097
     First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
     Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.400170
     First 10: [0.383544921875, 0.346435546875, 0.409912109375, 0.4248046875, 0.366943359375, 0.2763671875, 0.33154296875, 0.395263671875, 0.44482421875, 0.3095703125]
     Last 10:  [0.45361328125, 0.448486328125, 0.366455078125, 0.3818359375, 0.4248046875, 0.42724609375, 0.4541015625, 0.39404296875, 0.441162109375, 0.409423828125]

================================================================================
200_model.layers.16.mlp.gate_proj: Linear (model.layers.16.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.16.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000107, Std: 0.335097
     First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
     Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.133502, Std: 0.391416
     First 10: [0.16629043221473694, -0.0012838691473007202, 0.09348772466182709, -0.3883487582206726, 0.23453070223331451, -0.24834810197353363, -0.06198611482977867, -0.03591781109571457, -0.0891311913728714, -0.06520441919565201]
     Last 10:  [-0.397527277469635, -0.3585626482963562, -0.15717285871505737, -1.1200917959213257, -0.12538540363311768, -0.2032448947429657, 0.1424315720796585, -0.4279419779777527, 0.05212010443210602, -0.31669309735298157]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000108
     First 10: [-0.003040313720703125, -0.048126220703125, 0.036529541015625, 0.034454345703125, -0.027069091796875, -0.00794219970703125, -0.01522064208984375, 0.0232391357421875, 0.00749969482421875, -0.06341552734375]
     Last 10:  [-0.04974365234375, -0.0252532958984375, -0.001026153564453125, -0.004474639892578125, 0.015899658203125, -0.0182037353515625, 0.0206298828125, -0.00370025634765625, 0.00371551513671875, 0.0115509033203125]

================================================================================
201_model.layers.16.mlp.act_fn: SiLU (model.layers.16.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.16.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.133502, Std: 0.391416
     First 10: [0.16629043221473694, -0.0012838691473007202, 0.09348772466182709, -0.3883487582206726, 0.23453070223331451, -0.24834810197353363, -0.06198611482977867, -0.03591781109571457, -0.0891311913728714, -0.06520441919565201]
     Last 10:  [-0.397527277469635, -0.3585626482963562, -0.15717285871505737, -1.1200917959213257, -0.12538540363311768, -0.2032448947429657, 0.1424315720796585, -0.4279419779777527, 0.05212010443210602, -0.31669309735298157]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.16.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.026482, Std: 0.173021
     First 10: [0.09004245698451996, -0.0006415225216187537, 0.04892726242542267, -0.15693749487400055, 0.1309538334608078, -0.10883361846208572, -0.030032793059945107, -0.017636418342590332, -0.04258081689476967, -0.0315396822988987]
     Last 10:  [-0.15976883471012115, -0.14747953414916992, -0.07242327928543091, -0.2755361497402191, -0.058767467737197876, -0.09133072942495346, 0.076278917491436, -0.16887354850769043, 0.026739023625850677, -0.13348039984703064]
     Zeros: 0, Total: 7680

================================================================================
202_model.layers.16.mlp.up_proj: Linear (model.layers.16.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.16.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000107, Std: 0.335097
     First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
     Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000451, Std: 0.283919
     First 10: [-0.2658904790878296, 0.24787864089012146, -0.09355218708515167, 0.03402865678071976, -0.027910618111491203, 0.11032726615667343, 0.01559099368751049, -0.20008787512779236, 0.22914975881576538, -0.10552684962749481]
     Last 10:  [0.19494220614433289, 0.03199974074959755, 0.03491540253162384, -0.025975383818149567, -0.044475894421339035, 0.002219259738922119, 0.48689377307891846, -0.040170423686504364, 0.14113067090511322, 0.4499254524707794]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000005
     First 10: [0.045166015625, -0.048309326171875, 0.034332275390625, 0.022705078125, -0.0229339599609375, 0.01165771484375, 0.032012939453125, -0.0167999267578125, -0.03814697265625, 0.02996826171875]
     Last 10:  [-0.0087127685546875, -0.02874755859375, -0.03924560546875, -0.0036163330078125, -0.0005445480346679688, -0.01100921630859375, -0.03564453125, 0.020172119140625, 0.0266876220703125, -0.014434814453125]

================================================================================
203_model.layers.16.mlp.down_proj: Linear (model.layers.16.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.16.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000460, Std: 0.054024
     First 10: [-0.02394143119454384, -0.00015901973529253155, -0.004577252548187971, -0.005340372212231159, -0.0036550024524331093, -0.012007315643131733, -0.0004682410799432546, 0.0035288333892822266, -0.009757383726537228, 0.003328283317387104]
     Last 10:  [-0.031145690008997917, -0.004719306714832783, -0.0025286879390478134, 0.007157157175242901, 0.002613735618069768, -0.0002026866131927818, 0.03713972866535187, 0.006783721968531609, 0.003773696254938841, -0.06005622819066048]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.16.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003232, Std: 0.085038
     First 10: [-0.027936656028032303, 0.046456508338451385, 0.031772419810295105, 0.02008354663848877, 0.029768014326691628, 0.016100645065307617, -0.06484322249889374, 0.06986074149608612, 0.004992889240384102, -0.05312095582485199]
     Last 10:  [-0.12098050117492676, 0.014772661030292511, 0.0014150813221931458, -0.12441963702440262, 0.006079589948058128, 0.15603815019130707, -0.1390717774629593, 0.08101731538772583, 0.023632189258933067, -0.13965603709220886]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000038
     First 10: [0.03192138671875, -0.0259552001953125, 0.006320953369140625, -0.016265869140625, 0.04315185546875, -0.0246429443359375, -0.01499176025390625, -0.00855255126953125, -0.039398193359375, 0.0687255859375]
     Last 10:  [0.01122283935546875, -0.013153076171875, -0.045440673828125, -0.0014791488647460938, 0.0021572113037109375, -0.0465087890625, -0.0119476318359375, 0.02178955078125, 0.00038743019104003906, -0.00603485107421875]

================================================================================
204_model.layers.16.mlp: LlamaMLP (model.layers.16.mlp)
================================================================================

  → INPUT[0]: model.layers.16.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000107, Std: 0.335097
     First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
     Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.16.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003232, Std: 0.085038
     First 10: [-0.027936656028032303, 0.046456508338451385, 0.031772419810295105, 0.02008354663848877, 0.029768014326691628, 0.016100645065307617, -0.06484322249889374, 0.06986074149608612, 0.004992889240384102, -0.05312095582485199]
     Last 10:  [-0.12098050117492676, 0.014772661030292511, 0.0014150813221931458, -0.12441963702440262, 0.006079589948058128, 0.15603815019130707, -0.1390717774629593, 0.08101731538772583, 0.023632189258933067, -0.13965603709220886]
     Zeros: 0, Total: 2880

================================================================================
205_model.layers.17.input_layernorm: LlamaRMSNorm (model.layers.17.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.17.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.083651, Std: 11.991622
     First 10: [-0.6638714671134949, -2.709207534790039, -0.2668840289115906, 0.5192075967788696, -11.796063423156738, -7.440262794494629, 4.939882755279541, 0.01621883735060692, -0.6115972399711609, -2.0440406799316406]
     Last 10:  [0.08269111812114716, -0.1932464838027954, 0.38658809661865234, -0.5165269374847412, 0.0500635951757431, 0.026153460144996643, -0.07097233086824417, 0.04085946083068848, -0.1781800538301468, -0.18666276335716248]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010791, Std: 0.399692
     First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
     Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.531379
     First 10: [0.50146484375, 0.54931640625, 0.59912109375, 0.6240234375, 0.513671875, 0.81494140625, 0.59619140625, 0.63720703125, 0.59130859375, 0.410888671875]
     Last 10:  [0.5810546875, 0.5751953125, 0.482666015625, 0.49755859375, 0.51708984375, 0.5810546875, 0.57275390625, 0.58447265625, 0.57568359375, 0.63916015625]

================================================================================
206_model.layers.17.self_attn.q_proj: Linear (model.layers.17.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010791, Std: 0.399692
     First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
     Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.059662, Std: 1.029027
     First 10: [0.17620018124580383, -0.005415042862296104, 0.49547407031059265, -0.017877716571092606, -0.32422691583633423, 0.18870772421360016, 0.1761510968208313, -0.13226057589054108, 0.038664113730192184, -0.301192969083786]
     Last 10:  [-3.7760860919952393, -0.6261692047119141, -0.1570054590702057, -0.7031912803649902, 2.3285138607025146, 2.1023638248443604, -1.1395542621612549, -1.1463192701339722, -1.2893290519714355, -2.6063854694366455]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000077
     First 10: [0.022674560546875, -0.0206146240234375, 0.04339599609375, 0.033172607421875, 0.03216552734375, 0.01165008544921875, -0.0157623291015625, 0.068115234375, -0.0017576217651367188, -0.06365966796875]
     Last 10:  [0.00191497802734375, 0.10552978515625, -0.006443023681640625, -0.01042938232421875, -0.04364013671875, 0.0341796875, -0.005657196044921875, -0.008819580078125, -0.059173583984375, 0.051605224609375]

================================================================================
207_model.layers.17.self_attn.k_proj: Linear (model.layers.17.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010791, Std: 0.399692
     First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
     Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.050232, Std: 1.297941
     First 10: [0.009398944675922394, 0.014877453446388245, -0.004718877375125885, -0.010775990784168243, 0.016413161531090736, 0.004908842965960503, -6.683915853500366e-05, 0.011351363733410835, -0.004613161087036133, 0.004563651978969574]
     Last 10:  [-1.0040006637573242, -1.1255080699920654, -1.067479133605957, 4.940567493438721, 0.9456954002380371, -0.722148060798645, -1.3144100904464722, 1.9216970205307007, 2.6383395195007324, 0.9680902361869812]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000040
     First 10: [-0.016326904296875, 0.007289886474609375, 0.01435089111328125, -0.037109375, -0.02471923828125, 0.0113067626953125, -0.041290283203125, 0.0254364013671875, 0.01995849609375, -0.03607177734375]
     Last 10:  [-0.064208984375, -0.122802734375, -0.0200653076171875, 0.1072998046875, -0.0200347900390625, 0.0139923095703125, 0.0205841064453125, 0.09722900390625, 0.02410888671875, 0.0330810546875]

================================================================================
208_model.layers.17.self_attn.v_proj: Linear (model.layers.17.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010791, Std: 0.399692
     First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
     Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.007038, Std: 0.230105
     First 10: [0.00912422128021717, 0.004715672228485346, 0.015031802468001842, 0.0020664026960730553, 0.004894088953733444, -0.006862367503345013, -0.007586793974041939, 0.008753946051001549, 0.0024590101093053818, 0.008908390067517757]
     Last 10:  [-0.21751290559768677, 0.25524118542671204, 0.692430853843689, 0.20677316188812256, -0.2521459460258484, 0.2303818166255951, 0.11270415782928467, 0.24939972162246704, -0.09557916224002838, -0.2958414852619171]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000092
     First 10: [0.01277923583984375, -0.007015228271484375, 0.0287322998046875, -0.0132904052734375, 0.028900146484375, -0.0146942138671875, -0.019683837890625, 0.00787353515625, 0.056732177734375, 0.00015413761138916016]
     Last 10:  [0.05810546875, -0.0239410400390625, -0.01727294921875, 0.061370849609375, -0.0153045654296875, 0.041412353515625, 0.01763916015625, -0.01605224609375, 0.034027099609375, 0.0029582977294921875]

================================================================================
209_model.layers.17.self_attn.o_proj: Linear (model.layers.17.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004303, Std: 0.047361
     First 10: [0.00912422128021717, 0.004715672228485346, 0.015031802468001842, 0.0020664026960730553, 0.004894088953733444, -0.006862367503345013, -0.007586793974041939, 0.008753946051001549, 0.0024590101093053818, 0.008908390067517757]
     Last 10:  [-0.034934308379888535, 0.023565512150526047, 0.02834794484078884, -0.0021806051954627037, -0.019562136381864548, 0.02060643769800663, -0.032415006309747696, 0.012397365644574165, -0.0006528444355353713, 0.024543169885873795]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001037, Std: 0.031296
     First 10: [0.024034274742007256, 0.009686790406703949, -0.011641263030469418, 0.03035816177725792, -0.055995676666498184, -0.005811348557472229, 0.001816493459045887, -0.00855021458119154, 0.008600113913416862, -0.03423628583550453]
     Last 10:  [0.021419771015644073, -0.01344669796526432, 0.037874288856983185, -0.006192639470100403, 0.05086163058876991, 0.03190179541707039, -0.006532394327223301, 0.048124026507139206, 0.008581060916185379, 0.011034717783331871]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000028
     First 10: [0.018157958984375, 0.0028362274169921875, -0.00585174560546875, 0.01470184326171875, 0.01425933837890625, 0.034759521484375, -0.01268768310546875, -0.03997802734375, 0.0026302337646484375, 0.01480865478515625]
     Last 10:  [-0.07562255859375, 0.043792724609375, 0.028228759765625, -0.03863525390625, 0.050018310546875, -0.05841064453125, -0.0285186767578125, 0.05133056640625, -0.06060791015625, 0.01508331298828125]

================================================================================
210_model.layers.17.self_attn: LlamaSdpaAttention (model.layers.17.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.17.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001037, Std: 0.031296
     First 10: [0.024034274742007256, 0.009686790406703949, -0.011641263030469418, 0.03035816177725792, -0.055995676666498184, -0.005811348557472229, 0.001816493459045887, -0.00855021458119154, 0.008600113913416862, -0.03423628583550453]
     Last 10:  [0.021419771015644073, -0.01344669796526432, 0.037874288856983185, -0.006192639470100403, 0.05086163058876991, 0.03190179541707039, -0.006532394327223301, 0.048124026507139206, 0.008581060916185379, 0.011034717783331871]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.17.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.273551
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4997972249984741, 0.5002027750015259, 0.0, 0.0, 0.0]
     Last 10:  [0.4557938575744629, 0.45536404848098755, 0.050707828253507614, 0.038134243339300156, 0.0, 0.4610772132873535, 0.46151381731033325, 0.02490229345858097, 0.026277411729097366, 0.026229215785861015]
     Zeros: 90, Total: 225

================================================================================
211_model.layers.17.post_attention_layernorm: LlamaRMSNorm (model.layers.17.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.17.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.082614, Std: 11.993440
     First 10: [-0.6398372054100037, -2.6995208263397217, -0.27852529287338257, 0.5495657324790955, -11.852059364318848, -7.44607400894165, 4.941699028015137, 0.007668622769415379, -0.6029971241950989, -2.0782768726348877]
     Last 10:  [0.10411088913679123, -0.20669318735599518, 0.42446237802505493, -0.5227195620536804, 0.10092522203922272, 0.05805525556206703, -0.0775047242641449, 0.08898349106311798, -0.1695989966392517, -0.17562805116176605]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000918, Std: 0.344057
     First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
     Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.406987
     First 10: [0.407470703125, 0.365478515625, 0.40234375, 0.42529296875, 0.428955078125, 0.28125, 0.35595703125, 0.389404296875, 0.42724609375, 0.344970703125]
     Last 10:  [0.45751953125, 0.424072265625, 0.395263671875, 0.38623046875, 0.421630859375, 0.406982421875, 0.458251953125, 0.40478515625, 0.434326171875, 0.40283203125]

================================================================================
212_model.layers.17.mlp.gate_proj: Linear (model.layers.17.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.17.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000918, Std: 0.344057
     First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
     Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.128233, Std: 0.383859
     First 10: [0.21205765008926392, -0.26580917835235596, -0.21272900700569153, -0.5500901341438293, -0.5428280830383301, 0.2676239609718323, -1.086139440536499, 0.16430847346782684, -0.12101077288389206, -0.6859495639801025]
     Last 10:  [-0.37082260847091675, -0.09694718569517136, 0.2986826002597809, 0.460945725440979, -0.17704330384731293, -1.3766473531723022, 0.519015908241272, 0.4678449332714081, -0.2165176421403885, -0.12170661985874176]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000151
     First 10: [-0.0156402587890625, -0.0014600753784179688, -0.12890625, -0.00530242919921875, 0.01519012451171875, 0.0628662109375, 0.0211944580078125, 0.0226593017578125, 0.0039520263671875, -0.046356201171875]
     Last 10:  [0.0794677734375, -0.031585693359375, -0.06634521484375, 0.049224853515625, -0.039154052734375, -0.0250244140625, -0.0034046173095703125, 0.030303955078125, -0.0280609130859375, -0.0134735107421875]

================================================================================
213_model.layers.17.mlp.act_fn: SiLU (model.layers.17.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.17.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.128233, Std: 0.383859
     First 10: [0.21205765008926392, -0.26580917835235596, -0.21272900700569153, -0.5500901341438293, -0.5428280830383301, 0.2676239609718323, -1.086139440536499, 0.16430847346782684, -0.12101077288389206, -0.6859495639801025]
     Last 10:  [-0.37082260847091675, -0.09694718569517136, 0.2986826002597809, 0.460945725440979, -0.17704330384731293, -1.3766473531723022, 0.519015908241272, 0.4678449332714081, -0.2165176421403885, -0.12170661985874176]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.17.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.025368, Std: 0.172134
     First 10: [0.11722899973392487, -0.11534423381090164, -0.09509357064962387, -0.20124688744544983, -0.19950558245182037, 0.15161150693893433, -0.27408286929130554, 0.08888841420412064, -0.05684894695878029, -0.22974832355976105]
     Last 10:  [-0.15142254531383514, -0.04612573981285095, 0.17147979140281677, 0.28266966342926025, -0.08070597797632217, -0.277460515499115, 0.32538023591041565, 0.2876654863357544, -0.09658442437648773, -0.05715474858880043]
     Zeros: 0, Total: 7680

================================================================================
214_model.layers.17.mlp.up_proj: Linear (model.layers.17.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.17.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000918, Std: 0.344057
     First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
     Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.006167, Std: 0.289655
     First 10: [0.2331659495830536, -0.20070266723632812, 0.05258927494287491, -0.16163861751556396, -0.36914438009262085, -0.14993810653686523, -0.19461660087108612, -0.11860565841197968, 0.4599470794200897, 0.013090181164443493]
     Last 10:  [0.10169914364814758, 0.43642550706863403, -0.13328209519386292, -0.4647698700428009, -0.46214526891708374, 0.0015610717236995697, -0.42340147495269775, -0.6607855558395386, 0.028821490705013275, -0.2917616069316864]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000002
     First 10: [-0.01462554931640625, -0.043701171875, -0.0423583984375, -0.09967041015625, -0.0224151611328125, 0.00567626953125, 0.09283447265625, -0.053253173828125, -0.0031871795654296875, -0.017608642578125]
     Last 10:  [0.021942138671875, 0.0178070068359375, -0.03790283203125, 0.0374755859375, 0.020660400390625, -0.04833984375, -0.0088348388671875, -0.0172271728515625, 0.014129638671875, -0.0177154541015625]

================================================================================
215_model.layers.17.mlp.down_proj: Linear (model.layers.17.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.17.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.001624, Std: 0.048779
     First 10: [0.027333810925483704, 0.023149894550442696, -0.005000901874154806, 0.03252926841378212, 0.07364636659622192, -0.022732341662049294, 0.05334107577800751, -0.010542669333517551, -0.0261475071310997, -0.003007447114214301]
     Last 10:  [-0.015399543568491936, -0.020130449905991554, -0.022855184972286224, -0.13137634098529816, 0.03729788586497307, -0.0004331357777118683, -0.13776646554470062, -0.19008520245552063, -0.002783707110211253, 0.016675561666488647]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.17.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000992, Std: 0.078279
     First 10: [-0.026727400720119476, 0.04562339559197426, -0.049648717045784, 0.03508767485618591, -0.018014345318078995, 0.0794500932097435, -0.08577456325292587, -0.07889021933078766, -0.021168801933526993, -0.014344832859933376]
     Last 10:  [0.24368222057819366, -0.011579791083931923, 0.223648801445961, -0.06548430025577545, 0.14418953657150269, 0.01916997879743576, 0.13177256286144257, 0.01455237902700901, 0.015568599104881287, -0.05008528381586075]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000000
     First 10: [-0.0426025390625, -0.01467132568359375, -0.0009069442749023438, -0.0565185546875, 0.0206298828125, 0.005779266357421875, 0.0174102783203125, 0.049957275390625, -0.033050537109375, -0.013641357421875]
     Last 10:  [-0.02911376953125, 0.03961181640625, -0.01416015625, 0.01055908203125, 0.031402587890625, -0.020355224609375, 0.01386260986328125, 0.0255889892578125, -0.0362548828125, -0.016357421875]

================================================================================
216_model.layers.17.mlp: LlamaMLP (model.layers.17.mlp)
================================================================================

  → INPUT[0]: model.layers.17.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000918, Std: 0.344057
     First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
     Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.17.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000992, Std: 0.078279
     First 10: [-0.026727400720119476, 0.04562339559197426, -0.049648717045784, 0.03508767485618591, -0.018014345318078995, 0.0794500932097435, -0.08577456325292587, -0.07889021933078766, -0.021168801933526993, -0.014344832859933376]
     Last 10:  [0.24368222057819366, -0.011579791083931923, 0.223648801445961, -0.06548430025577545, 0.14418953657150269, 0.01916997879743576, 0.13177256286144257, 0.01455237902700901, 0.015568599104881287, -0.05008528381586075]
     Zeros: 0, Total: 2880

================================================================================
217_model.layers.18.input_layernorm: LlamaRMSNorm (model.layers.18.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.18.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.081622, Std: 12.001882
     First 10: [-0.6665645837783813, -2.653897523880005, -0.32817399501800537, 0.584653377532959, -11.870073318481445, -7.366623878479004, 4.855924606323242, -0.07122159749269485, -0.624165952205658, -2.0926218032836914]
     Last 10:  [0.3477931022644043, -0.21827298402786255, 0.6481111645698547, -0.5882038474082947, 0.2451147586107254, 0.07722523808479309, 0.05426783859729767, 0.10353586822748184, -0.15403039753437042, -0.2257133424282074]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004963, Std: 0.364190
     First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
     Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.496113
     First 10: [0.46923828125, 0.54833984375, 0.52587890625, 0.525390625, 0.3955078125, 0.69091796875, 0.544921875, 0.61083984375, 0.5498046875, 0.43017578125]
     Last 10:  [0.50830078125, 0.5888671875, 0.4345703125, 0.473388671875, 0.47607421875, 0.4921875, 0.56201171875, 0.5478515625, 0.513671875, 0.548828125]

================================================================================
218_model.layers.18.self_attn.q_proj: Linear (model.layers.18.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.18.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004963, Std: 0.364190
     First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
     Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.042951, Std: 1.120729
     First 10: [-0.8377794623374939, 0.8824523091316223, 0.2115900218486786, 0.32088714838027954, 0.35283565521240234, -0.4460007846355438, 0.3452777862548828, 0.3937453627586365, 0.15842032432556152, -0.05418047308921814]
     Last 10:  [-0.16999435424804688, 0.5271382927894592, 1.0528688430786133, -0.14003366231918335, 0.0482865646481514, 0.3681071996688843, -0.0046808719635009766, -1.1384668350219727, 2.352933883666992, 2.8094520568847656]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000061
     First 10: [0.052398681640625, 0.0292816162109375, -0.0240478515625, -0.0986328125, -0.01934814453125, -0.0023784637451171875, 0.02374267578125, -0.0192718505859375, 0.016265869140625, 0.055328369140625]
     Last 10:  [-0.033355712890625, 0.01018524169921875, 0.00797271728515625, 0.046966552734375, 0.0193939208984375, -0.0364990234375, 0.01654052734375, 0.07757568359375, -0.049652099609375, 0.030059814453125]

================================================================================
219_model.layers.18.self_attn.k_proj: Linear (model.layers.18.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.18.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004963, Std: 0.364190
     First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
     Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.080262, Std: 1.316830
     First 10: [0.005962073802947998, 0.0027903690934181213, 0.0018459558486938477, -0.0059988126158714294, 0.009695593267679214, -0.011615023016929626, 0.007661469280719757, -0.0024742260575294495, -0.0025738850235939026, 0.01688353717327118]
     Last 10:  [0.7259094715118408, -8.153075218200684, 3.7973084449768066, 1.25607430934906, 0.051473021507263184, -2.106125593185425, -0.4597930908203125, 0.7789705991744995, -2.2604992389678955, -1.1115267276763916]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000312
     First 10: [-0.005176544189453125, 0.032562255859375, -0.03265380859375, -0.016845703125, -0.048919677734375, 0.0107879638671875, 0.022430419921875, -0.0261077880859375, 0.08331298828125, -0.01483917236328125]
     Last 10:  [0.052459716796875, 0.07275390625, -0.06884765625, -0.017486572265625, -0.021026611328125, -0.021148681640625, -0.0232696533203125, 0.075439453125, 0.0531005859375, 0.03118896484375]

================================================================================
220_model.layers.18.self_attn.v_proj: Linear (model.layers.18.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.18.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004963, Std: 0.364190
     First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
     Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.026012, Std: 0.231529
     First 10: [-0.025765325874090195, -0.32301434874534607, 0.01865028589963913, -0.007378639653325081, -0.006604405120015144, -0.011182085610926151, 0.014351952821016312, -0.005257573910057545, -0.009171336889266968, 0.010653816163539886]
     Last 10:  [0.3005211055278778, -0.5478452444076538, 0.24131830036640167, 0.17114663124084473, -0.4464709162712097, 0.22537609934806824, -0.0653851181268692, 0.39671340584754944, 0.1975804567337036, -0.13870105147361755]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000073
     First 10: [0.039581298828125, -0.01390838623046875, -0.039093017578125, -0.0321044921875, 0.0011930465698242188, 0.024139404296875, -0.0250091552734375, -0.02801513671875, -0.03558349609375, 0.00675201416015625]
     Last 10:  [-0.0006275177001953125, -0.0423583984375, 0.00020241737365722656, -0.0264434814453125, -0.00252532958984375, 0.058441162109375, -0.001476287841796875, -0.0494384765625, -0.036773681640625, -0.0279388427734375]

================================================================================
221_model.layers.18.self_attn.o_proj: Linear (model.layers.18.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.18.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003647, Std: 0.044973
     First 10: [-0.025765325874090195, -0.32301434874534607, 0.01865028589963913, -0.007378639653325081, -0.006604405120015144, -0.011182085610926151, 0.014351952821016312, -0.005257573910057545, -0.009171336889266968, 0.010653816163539886]
     Last 10:  [0.009222244843840599, -0.014640099368989468, 0.01489559281617403, 0.007551583927124739, -0.020604563876986504, -0.0031008003279566765, -0.0035804761573672295, 0.010456638410687447, 0.017065126448869705, -0.0016377160791307688]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002427, Std: 0.044122
     First 10: [0.058850452303886414, -0.02426319569349289, -0.0020857914350926876, -0.04168229550123215, 0.05155494809150696, 0.08311007916927338, -0.0810142531991005, -0.017859669402241707, -0.0020665014162659645, 0.014075408689677715]
     Last 10:  [-0.006787922233343124, -0.03319380059838295, -0.006946871988475323, 0.023417925462126732, 0.0062087830156087875, -0.003955204039812088, -0.0059850080870091915, -0.050419051200151443, -0.04298160970211029, -0.000788138248026371]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000025
     First 10: [-0.004451751708984375, 0.014190673828125, -0.0266265869140625, -0.0029506683349609375, -0.0008502006530761719, -0.011383056640625, -0.0093994140625, 0.029144287109375, -0.00885772705078125, -0.0109710693359375]
     Last 10:  [-0.07177734375, 0.0025157928466796875, 0.0018463134765625, 0.05718994140625, 0.01062774658203125, -0.037322998046875, -0.022796630859375, 0.03265380859375, -0.055084228515625, -0.0654296875]

================================================================================
222_model.layers.18.self_attn: LlamaSdpaAttention (model.layers.18.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.18.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002427, Std: 0.044122
     First 10: [0.058850452303886414, -0.02426319569349289, -0.0020857914350926876, -0.04168229550123215, 0.05155494809150696, 0.08311007916927338, -0.0810142531991005, -0.017859669402241707, -0.0020665014162659645, 0.014075408689677715]
     Last 10:  [-0.006787922233343124, -0.03319380059838295, -0.006946871988475323, 0.023417925462126732, 0.0062087830156087875, -0.003955204039812088, -0.0059850080870091915, -0.050419051200151443, -0.04298160970211029, -0.000788138248026371]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.18.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.272157
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001047849655151, 0.4998951852321625, 0.0, 0.0, 0.0]
     Last 10:  [0.48647257685661316, 0.48779740929603577, 0.013902934268116951, 0.0118270767852664, 0.0, 0.4796624481678009, 0.4806508421897888, 0.005016630049794912, 0.009345166385173798, 0.025324858725070953]
     Zeros: 90, Total: 225

================================================================================
223_model.layers.18.post_attention_layernorm: LlamaRMSNorm (model.layers.18.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.18.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.084049, Std: 12.005649
     First 10: [-0.6077141165733337, -2.6781606674194336, -0.3302597999572754, 0.5429710745811462, -11.81851863861084, -7.283514022827148, 4.7749104499816895, -0.08908126503229141, -0.6262324452400208, -2.0785462856292725]
     Last 10:  [0.3410051763057709, -0.2514667809009552, 0.6411643028259277, -0.564785897731781, 0.25132355093955994, 0.0732700377702713, 0.04828283190727234, 0.0531168170273304, -0.1970120072364807, -0.2265014797449112]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000532, Std: 0.362479
     First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
     Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.433940
     First 10: [0.437255859375, 0.398193359375, 0.4326171875, 0.43603515625, 0.44384765625, 0.306640625, 0.390625, 0.4072265625, 0.4658203125, 0.39599609375]
     Last 10:  [0.46484375, 0.470947265625, 0.42236328125, 0.416748046875, 0.456787109375, 0.44775390625, 0.47705078125, 0.43115234375, 0.480712890625, 0.41357421875]

================================================================================
224_model.layers.18.mlp.gate_proj: Linear (model.layers.18.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.18.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000532, Std: 0.362479
     First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
     Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.175681, Std: 0.419672
     First 10: [-0.2780911326408386, 0.5901973247528076, 0.25485217571258545, -0.3146638572216034, -0.18665599822998047, 0.024738967418670654, -0.018943332135677338, -0.1688479483127594, 0.26470211148262024, -0.04802200570702553]
     Last 10:  [-0.07905992120504379, -0.5438293218612671, -0.624908983707428, 0.278026282787323, 0.1272514909505844, -0.1826837807893753, -0.5444859266281128, -0.22942592203617096, 0.034127384424209595, -0.27837133407592773]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000243
     First 10: [0.0242919921875, -0.00927734375, 0.016326904296875, 0.00042748451232910156, -0.01335906982421875, -0.0007529258728027344, -0.0478515625, 0.0237579345703125, 0.0279541015625, 0.0002593994140625]
     Last 10:  [-0.019317626953125, 0.040191650390625, -0.0186004638671875, 0.032257080078125, -0.0024566650390625, -0.028045654296875, -0.040435791015625, -0.07391357421875, 0.0211944580078125, 0.04345703125]

================================================================================
225_model.layers.18.mlp.act_fn: SiLU (model.layers.18.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.18.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.175681, Std: 0.419672
     First 10: [-0.2780911326408386, 0.5901973247528076, 0.25485217571258545, -0.3146638572216034, -0.18665599822998047, 0.024738967418670654, -0.018943332135677338, -0.1688479483127594, 0.26470211148262024, -0.04802200570702553]
     Last 10:  [-0.07905992120504379, -0.5438293218612671, -0.624908983707428, 0.278026282787323, 0.1272514909505844, -0.1826837807893753, -0.5444859266281128, -0.22942592203617096, 0.034127384424209595, -0.27837133407592773]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.18.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.039594, Std: 0.181315
     First 10: [-0.11983554065227509, 0.37973910570144653, 0.14357617497444153, -0.13278083503246307, -0.08464308828115463, 0.012522480450570583, -0.009381955489516258, -0.07731346040964127, 0.14976628124713898, -0.023434584960341454]
     Last 10:  [-0.03796815499663353, -0.1997470110654831, -0.21788440644741058, 0.158214271068573, 0.06766852736473083, -0.08302167803049088, -0.19990509748458862, -0.10161130875349045, 0.01735483482480049, -0.11993715167045593]
     Zeros: 0, Total: 7680

================================================================================
226_model.layers.18.mlp.up_proj: Linear (model.layers.18.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.18.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000532, Std: 0.362479
     First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
     Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000021, Std: 0.303260
     First 10: [0.2583382725715637, 0.06000901758670807, 0.2526709735393524, -0.38453179597854614, -0.027783101424574852, -0.05933084338903427, -0.03775890916585922, 0.41567912697792053, -0.08919097483158112, 0.3805236518383026]
     Last 10:  [0.48384466767311096, 0.5883837938308716, 0.97945237159729, 0.028534285724163055, 0.39286166429519653, -0.2969404458999634, -0.15566077828407288, -0.09004196524620056, 0.5595129132270813, -0.23826977610588074]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000024
     First 10: [-0.0231475830078125, -0.024383544921875, -0.0183868408203125, -0.07257080078125, -0.0026035308837890625, 0.0246124267578125, 0.01375579833984375, -0.06591796875, -0.01490020751953125, 0.058837890625]
     Last 10:  [-0.01282501220703125, -0.0308685302734375, -0.0379638671875, 0.0284576416015625, -0.0198211669921875, -0.07037353515625, -0.04473876953125, 0.0168914794921875, 0.039886474609375, -0.0772705078125]

================================================================================
227_model.layers.18.mlp.down_proj: Linear (model.layers.18.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.18.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000844, Std: 0.066806
     First 10: [-0.030958106741309166, 0.02278777025640011, 0.03627753257751465, 0.05105845257639885, 0.002351647475734353, -0.000742969335988164, 0.0003542524063959718, -0.032137591391801834, -0.013357800431549549, -0.008917413651943207]
     Last 10:  [-0.01837068982422352, -0.11752790212631226, -0.21340739727020264, 0.00451453123241663, 0.02658437006175518, 0.024652494117617607, 0.031117383390665054, 0.009149282239377499, 0.009710254147648811, 0.028577398508787155]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.18.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000535, Std: 0.097557
     First 10: [-0.019346637651324272, 0.05246886983513832, -0.018062008544802666, -0.010608246549963951, 0.033735714852809906, 0.06311169266700745, -0.09063869714736938, 0.05476351082324982, -0.028151072561740875, 0.03922653570771217]
     Last 10:  [-0.13474345207214355, 0.1326989233493805, -0.031219426542520523, -0.032321397215127945, -0.06690109521150589, 0.006207451224327087, -0.21834003925323486, 0.19003926217556, 0.23019525408744812, -0.05090992525219917]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000052
     First 10: [-0.0132904052734375, -0.002902984619140625, 0.04315185546875, -0.0211181640625, 0.0799560546875, 0.044891357421875, 0.02392578125, -0.07562255859375, -0.041015625, -0.08062744140625]
     Last 10:  [0.0265655517578125, -0.015380859375, 0.0438232421875, 0.0135040283203125, -0.055999755859375, 0.09027099609375, -0.0008258819580078125, -0.0413818359375, 0.0704345703125, -0.07525634765625]

================================================================================
228_model.layers.18.mlp: LlamaMLP (model.layers.18.mlp)
================================================================================

  → INPUT[0]: model.layers.18.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000532, Std: 0.362479
     First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
     Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.18.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000535, Std: 0.097557
     First 10: [-0.019346637651324272, 0.05246886983513832, -0.018062008544802666, -0.010608246549963951, 0.033735714852809906, 0.06311169266700745, -0.09063869714736938, 0.05476351082324982, -0.028151072561740875, 0.03922653570771217]
     Last 10:  [-0.13474345207214355, 0.1326989233493805, -0.031219426542520523, -0.032321397215127945, -0.06690109521150589, 0.006207451224327087, -0.21834003925323486, 0.19003926217556, 0.23019525408744812, -0.05090992525219917]
     Zeros: 0, Total: 2880

================================================================================
229_model.layers.19.input_layernorm: LlamaRMSNorm (model.layers.19.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.19.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.083514, Std: 12.008832
     First 10: [-0.6270607709884644, -2.6256918907165527, -0.348321795463562, 0.5323628187179565, -11.784783363342285, -7.220402240753174, 4.684271812438965, -0.034317754209041595, -0.6543835401535034, -2.0393197536468506]
     Last 10:  [0.20626172423362732, -0.11876785755157471, 0.6099448800086975, -0.5971072912216187, 0.18442246317863464, 0.07947748899459839, -0.17005720734596252, 0.2431560754776001, 0.03318324685096741, -0.27741140127182007]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007799, Std: 0.419179
     First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
     Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.526974
     First 10: [0.6181640625, 0.603515625, 0.552734375, 0.5908203125, 0.470703125, 0.751953125, 0.734375, 0.67529296875, 0.5361328125, 0.52099609375]
     Last 10:  [0.486572265625, 0.6376953125, 0.48193359375, 0.479248046875, 0.489990234375, 0.517578125, 0.57763671875, 0.615234375, 0.50732421875, 0.58203125]

================================================================================
230_model.layers.19.self_attn.q_proj: Linear (model.layers.19.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.19.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007799, Std: 0.419179
     First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
     Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.013251, Std: 1.160804
     First 10: [0.5734501481056213, 0.25922608375549316, 0.2998431324958801, -0.1883755326271057, -0.5414149165153503, 0.14954331517219543, 0.20416440069675446, 0.0311285313218832, 0.1654435396194458, -0.5489588379859924]
     Last 10:  [0.9710652232170105, -4.291116237640381, -1.591453194618225, 0.5787240862846375, 0.9696335196495056, 3.266785144805908, 5.116537094116211, -2.0767135620117188, -5.148368835449219, -2.04774808883667]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000013
     First 10: [0.054534912109375, -0.022216796875, -0.0011138916015625, 0.07965087890625, 0.047698974609375, 0.0296783447265625, 0.024017333984375, 0.01291656494140625, 0.00684356689453125, 0.00563812255859375]
     Last 10:  [0.00897216796875, 0.0262908935546875, -0.126953125, -0.1358642578125, 0.07147216796875, -0.1256103515625, 0.0216064453125, -0.08001708984375, 0.007595062255859375, 0.07757568359375]

================================================================================
231_model.layers.19.self_attn.k_proj: Linear (model.layers.19.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.19.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007799, Std: 0.419179
     First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
     Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.023870, Std: 1.529393
     First 10: [-0.0066927894949913025, 0.003967344760894775, 0.010434471070766449, -0.013717111200094223, -0.013788595795631409, -0.015797004103660583, -0.015063539147377014, -0.03939242660999298, 0.0008821636438369751, 0.01291651837527752]
     Last 10:  [0.4805268347263336, 0.25657936930656433, -0.919264554977417, 1.3373773097991943, 1.3127280473709106, 1.02220618724823, -1.0766745805740356, 0.6707425713539124, 1.4604045152664185, 0.6491736769676208]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000294
     First 10: [0.0213165283203125, 0.01355743408203125, -0.040069580078125, -0.0283050537109375, 0.0188140869140625, 0.07977294921875, -0.051025390625, 0.09246826171875, 0.08074951171875, 0.0130615234375]
     Last 10:  [-0.13916015625, -0.1400146484375, -0.056488037109375, -0.0117950439453125, 0.0615234375, -0.11376953125, 0.018463134765625, 0.003612518310546875, -0.0762939453125, 0.054534912109375]

================================================================================
232_model.layers.19.self_attn.v_proj: Linear (model.layers.19.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.19.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007799, Std: 0.419179
     First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
     Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.018465, Std: 0.236915
     First 10: [-0.0012876521795988083, 0.005803906358778477, -0.0031116995960474014, 0.0019115209579467773, 0.007089809514582157, -0.005595693364739418, 0.0014660404995083809, 0.0013264268636703491, -0.007427733391523361, -0.002674686722457409]
     Last 10:  [0.6521482467651367, -0.05542096495628357, -0.511698842048645, 0.13921010494232178, -0.7942172288894653, 0.1645403355360031, 0.045300811529159546, -0.18486541509628296, 0.1070793867111206, -0.09608727693557739]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000167
     First 10: [0.03826904296875, -0.040283203125, 0.037567138671875, -0.01195526123046875, -0.0014019012451171875, -0.0135040283203125, -0.00450897216796875, -0.035003662109375, -0.0284271240234375, -0.021728515625]
     Last 10:  [0.037994384765625, 0.02398681640625, -0.0016803741455078125, 0.062469482421875, 0.007129669189453125, 0.0177154541015625, 0.023406982421875, -0.007228851318359375, 0.08935546875, -0.035888671875]

================================================================================
233_model.layers.19.self_attn.o_proj: Linear (model.layers.19.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.19.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004222, Std: 0.068545
     First 10: [-0.0012876521795988083, 0.005803906358778477, -0.0031116995960474014, 0.0019115209579467773, 0.007089809514582157, -0.005595693364739418, 0.0014660404995083809, 0.0013264268636703491, -0.007427733391523361, -0.002674686722457409]
     Last 10:  [0.01340756006538868, -0.005559442564845085, -0.0501440092921257, 0.02397761307656765, 0.023086370900273323, -0.010344645008444786, -0.010802729055285454, 0.0032507518772035837, 0.0050797127187252045, 0.005924539640545845]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000955, Std: 0.051300
     First 10: [0.01912875659763813, -0.044397465884685516, -0.00924941897392273, -0.07033971697092056, -0.039318837225437164, 0.022757938131690025, 0.08105392754077911, -0.005851780064404011, 0.027946799993515015, -0.01991271786391735]
     Last 10:  [0.0374181792140007, 0.05294226109981537, -0.013717522844672203, 0.014362967573106289, 0.031184270977973938, 0.04060351848602295, 0.012982919812202454, -0.0878903791308403, -0.024067485705018044, -0.03837566450238228]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000089
     First 10: [-0.03125, 0.10614013671875, -0.0633544921875, -0.0135345458984375, -0.0016632080078125, -0.0849609375, 0.0283355712890625, 0.0005793571472167969, -0.10430908203125, -0.048431396484375]
     Last 10:  [-0.046630859375, 0.024658203125, -0.04071044921875, -0.028656005859375, 0.006591796875, 0.045684814453125, 0.0036449432373046875, 0.06304931640625, -0.0166168212890625, 0.08984375]

================================================================================
234_model.layers.19.self_attn: LlamaSdpaAttention (model.layers.19.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.19.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000955, Std: 0.051300
     First 10: [0.01912875659763813, -0.044397465884685516, -0.00924941897392273, -0.07033971697092056, -0.039318837225437164, 0.022757938131690025, 0.08105392754077911, -0.005851780064404011, 0.027946799993515015, -0.01991271786391735]
     Last 10:  [0.0374181792140007, 0.05294226109981537, -0.013717522844672203, 0.014362967573106289, 0.031184270977973938, 0.04060351848602295, 0.012982919812202454, -0.0878903791308403, -0.024067485705018044, -0.03837566450238228]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.19.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.272694
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5002147555351257, 0.49978533387184143, 0.0, 0.0, 0.0]
     Last 10:  [0.49461960792541504, 0.4943769872188568, 0.0046204435639083385, 0.006382955238223076, 0.0, 0.48945072293281555, 0.4891625642776489, 0.005610514897853136, 0.008785570971667767, 0.0069905673153698444]
     Zeros: 90, Total: 225

================================================================================
235_model.layers.19.post_attention_layernorm: LlamaRMSNorm (model.layers.19.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.19.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.084470, Std: 12.012135
     First 10: [-0.6079320311546326, -2.6700892448425293, -0.35757121443748474, 0.4620231091976166, -11.824102401733398, -7.197644233703613, 4.765325546264648, -0.04016953334212303, -0.626436710357666, -2.059232473373413]
     Last 10:  [0.24367991089820862, -0.06582559645175934, 0.5962273478507996, -0.582744300365448, 0.21560673415660858, 0.12008100748062134, -0.15707428753376007, 0.1552656888961792, 0.009115761145949364, -0.31578707695007324]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000201, Std: 0.364422
     First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
     Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.437132
     First 10: [0.440673828125, 0.395263671875, 0.426513671875, 0.435302734375, 0.46142578125, 0.353271484375, 0.391357421875, 0.4228515625, 0.455810546875, 0.413818359375]
     Last 10:  [0.467529296875, 0.478271484375, 0.432373046875, 0.43505859375, 0.43017578125, 0.45947265625, 0.466064453125, 0.430908203125, 0.479248046875, 0.412353515625]

================================================================================
236_model.layers.19.mlp.gate_proj: Linear (model.layers.19.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.19.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000201, Std: 0.364422
     First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
     Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.185284, Std: 0.452420
     First 10: [-0.1248796135187149, -0.09433577209711075, 0.23153480887413025, 0.08005581796169281, 0.027399277314543724, -0.7590510845184326, 0.08980295807123184, -0.34578049182891846, -0.9712328910827637, 0.09294278919696808]
     Last 10:  [-0.005166793242096901, -0.36636078357696533, -1.0826754570007324, -1.2720487117767334, -0.5106630325317383, -0.3924185633659363, -0.9873599410057068, -0.02358730137348175, 0.4882686734199524, 0.08582982420921326]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000195
     First 10: [0.0784912109375, -0.00212860107421875, 0.0379638671875, 0.0182342529296875, -0.0643310546875, -0.006618499755859375, 0.015655517578125, -0.045074462890625, 0.01204681396484375, -0.0194091796875]
     Last 10:  [0.0531005859375, 0.0772705078125, -0.0111083984375, -0.043487548828125, -0.01303863525390625, -0.03204345703125, -0.039642333984375, -0.035400390625, 0.049530029296875, 0.05572509765625]

================================================================================
237_model.layers.19.mlp.act_fn: SiLU (model.layers.19.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.19.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.185284, Std: 0.452420
     First 10: [-0.1248796135187149, -0.09433577209711075, 0.23153480887413025, 0.08005581796169281, 0.027399277314543724, -0.7590510845184326, 0.08980295807123184, -0.34578049182891846, -0.9712328910827637, 0.09294278919696808]
     Last 10:  [-0.005166793242096901, -0.36636078357696533, -1.0826754570007324, -1.2720487117767334, -0.5106630325317383, -0.3924185633659363, -0.9873599410057068, -0.02358730137348175, 0.4882686734199524, 0.08582982420921326]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.19.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.038270, Std: 0.191010
     First 10: [-0.05854613706469536, -0.044944725930690765, 0.1291099488735199, 0.04162928834557533, 0.013887307606637478, -0.2420251965522766, 0.04691626876592636, -0.14329351484775543, -0.26673439145088196, 0.048629432916641235]
     Last 10:  [-0.002576722763478756, -0.14999568462371826, -0.27391692996025085, -0.2784600555896759, -0.19151809811592102, -0.15819774568080902, -0.2680029273033142, -0.0116545669734478, 0.3025793433189392, 0.0447554737329483]
     Zeros: 0, Total: 7680

================================================================================
238_model.layers.19.mlp.up_proj: Linear (model.layers.19.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.19.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000201, Std: 0.364422
     First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
     Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.009823, Std: 0.310440
     First 10: [-0.011288153938949108, -0.206289604306221, -0.3903566896915436, 0.10192419588565826, 0.25222182273864746, -0.07980801165103912, -0.22571825981140137, -0.43774253129959106, -0.01914067752659321, 0.6913800835609436]
     Last 10:  [-0.08968715369701385, -0.05830085277557373, -0.2642518877983093, 0.04021197557449341, -0.1031121164560318, -1.0470170974731445, -0.11213459819555283, -0.08668024092912674, -0.25721248984336853, 0.1661846935749054]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000010
     First 10: [-0.02117919921875, -0.021240234375, 0.055908203125, -0.053009033203125, -0.008148193359375, 0.0005230903625488281, 0.0154571533203125, 0.05670166015625, 0.09979248046875, -0.023223876953125]
     Last 10:  [-0.06988525390625, 0.0015516281127929688, -0.0294036865234375, -0.0477294921875, 0.00891876220703125, -0.03656005859375, 0.033477783203125, -0.003692626953125, 0.0281982421875, -0.00585174560546875]

================================================================================
239_model.layers.19.mlp.down_proj: Linear (model.layers.19.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.19.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.001336, Std: 0.062591
     First 10: [0.0006608777912333608, 0.009271630086004734, -0.05039893090724945, 0.004243031609803438, 0.0035026820842176676, 0.019315550103783607, -0.010589858517050743, 0.06272566318511963, 0.005105476826429367, 0.033621422946453094]
     Last 10:  [0.00023109893663786352, 0.008744875900447369, 0.07238306850194931, -0.011197429150342941, 0.019747836515307426, 0.16563574969768524, 0.03005240112543106, 0.001010220730677247, -0.07782718539237976, 0.007437674794346094]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.19.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001719, Std: 0.098107
     First 10: [-0.019145194441080093, 0.04688078910112381, -0.11684485524892807, 0.08168193697929382, 0.1020248532295227, 0.07961532473564148, -0.07758274674415588, -0.12421499192714691, 0.10597354173660278, 0.03329408913850784]
     Last 10:  [0.17151546478271484, 0.044858887791633606, -0.0059884823858737946, -0.09607034921646118, 0.15282511711120605, 0.20279699563980103, -0.21713387966156006, -0.10902567207813263, 0.02681189402937889, -0.1693846881389618]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000064
     First 10: [-0.0159149169921875, 0.03851318359375, -0.0128326416015625, 0.01861572265625, -0.00592041015625, 0.045654296875, 0.004436492919921875, 0.06304931640625, -0.08306884765625, 2.396106719970703e-05]
     Last 10:  [0.0157928466796875, -0.006610870361328125, 0.036712646484375, -0.0635986328125, 0.00521087646484375, -0.08349609375, 0.0298309326171875, -0.031097412109375, 0.0099945068359375, -0.03741455078125]

================================================================================
240_model.layers.19.mlp: LlamaMLP (model.layers.19.mlp)
================================================================================

  → INPUT[0]: model.layers.19.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000201, Std: 0.364422
     First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
     Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.19.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001719, Std: 0.098107
     First 10: [-0.019145194441080093, 0.04688078910112381, -0.11684485524892807, 0.08168193697929382, 0.1020248532295227, 0.07961532473564148, -0.07758274674415588, -0.12421499192714691, 0.10597354173660278, 0.03329408913850784]
     Last 10:  [0.17151546478271484, 0.044858887791633606, -0.0059884823858737946, -0.09607034921646118, 0.15282511711120605, 0.20279699563980103, -0.21713387966156006, -0.10902567207813263, 0.02681189402937889, -0.1693846881389618]
     Zeros: 0, Total: 2880

================================================================================
241_model.layers.20.input_layernorm: LlamaRMSNorm (model.layers.20.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.20.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.086189, Std: 12.014653
     First 10: [-0.6270772218704224, -2.623208522796631, -0.4744160771369934, 0.5437050461769104, -11.722077369689941, -7.1180291175842285, 4.687742710113525, -0.16438452899456024, -0.5204631686210632, -2.0259382724761963]
     Last 10:  [0.41519537568092346, -0.020966708660125732, 0.5902388691902161, -0.6788146495819092, 0.36843186616897583, 0.32287800312042236, -0.37420815229415894, 0.04624001681804657, 0.035927653312683105, -0.48517176508903503]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004985, Std: 0.423120
     First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
     Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.595183
     First 10: [0.615234375, 0.646484375, 0.63916015625, 0.73291015625, 0.488037109375, 0.90380859375, 0.75390625, 0.77197265625, 0.6005859375, 0.59130859375]
     Last 10:  [0.49853515625, 0.66552734375, 0.5869140625, 0.52978515625, 0.50146484375, 0.60205078125, 0.67724609375, 0.615234375, 0.5947265625, 0.58154296875]

================================================================================
242_model.layers.20.self_attn.q_proj: Linear (model.layers.20.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.20.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004985, Std: 0.423120
     First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
     Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.012340, Std: 1.053530
     First 10: [0.23265299201011658, -0.31718090176582336, 0.05648844316601753, 0.37458574771881104, 0.18924075365066528, -0.056706465780735016, -0.1733427494764328, -0.042458172887563705, -0.2921498417854309, 0.0738520622253418]
     Last 10:  [-0.5507814288139343, 8.078080177307129, -0.040261536836624146, -0.5807448029518127, -0.18600089848041534, 0.8330078125, -0.8552126884460449, -0.18775293231010437, -2.929267406463623, -3.3478779792785645]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000116
     First 10: [0.037445068359375, -0.0213623046875, 0.043792724609375, 0.05413818359375, -0.038299560546875, -0.0177459716796875, 0.038726806640625, -0.0052032470703125, 0.06036376953125, 0.002613067626953125]
     Last 10:  [0.0108795166015625, 0.04705810546875, 0.10394287109375, -0.0118408203125, 0.0906982421875, 0.0252227783203125, 0.00897216796875, -0.070556640625, 0.00691986083984375, -0.05059814453125]

================================================================================
243_model.layers.20.self_attn.k_proj: Linear (model.layers.20.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.20.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004985, Std: 0.423120
     First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
     Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.093698, Std: 1.269144
     First 10: [0.0042570047080516815, 0.0051924362778663635, -0.0021750181913375854, -0.0070402175188064575, 0.002317999489605427, 0.00647111888974905, -0.005890600383281708, -0.01019047200679779, 0.007202997803688049, -0.0017686462961137295]
     Last 10:  [-1.1240390539169312, -0.44455721974372864, 2.5238137245178223, -1.7886484861373901, -0.07196877151727676, -1.8105428218841553, -1.6128242015838623, 1.778730869293213, 0.8856378197669983, 0.8938947319984436]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000064
     First 10: [-0.01513671875, 0.01204681396484375, -0.0166168212890625, 0.046295166015625, -0.00567626953125, 0.036865234375, 0.040740966796875, -0.048065185546875, -0.01068878173828125, -0.0304107666015625]
     Last 10:  [-0.11053466796875, -0.0751953125, 0.0090789794921875, -0.019134521484375, -0.0254364013671875, 0.005809783935546875, 0.00691986083984375, 0.02215576171875, -0.060455322265625, -0.00516510009765625]

================================================================================
244_model.layers.20.self_attn.v_proj: Linear (model.layers.20.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.20.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004985, Std: 0.423120
     First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
     Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.010529, Std: 0.350973
     First 10: [-0.023633981123566628, -0.006339972838759422, 0.00045689381659030914, 0.0005622822791337967, 0.0064045824110507965, -0.011260639876127243, 0.012442640960216522, -0.005054449662566185, -0.010427902452647686, 0.004508761689066887]
     Last 10:  [0.045087650418281555, -0.5410366654396057, 0.4805414378643036, -0.19812090694904327, 0.31287962198257446, 0.25048092007637024, -0.45792415738105774, -0.2764427363872528, -0.12499526143074036, -0.4779697060585022]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000151
     First 10: [-0.044647216796875, -0.02508544921875, -0.0286712646484375, -0.057342529296875, -0.03436279296875, -0.00279998779296875, 0.021942138671875, -0.027252197265625, 0.033966064453125, 0.039581298828125]
     Last 10:  [-0.01629638671875, 0.0260772705078125, 0.0307769775390625, 0.04266357421875, 0.034515380859375, -0.0225830078125, 0.0005621910095214844, -0.007537841796875, -0.0119476318359375, 0.039215087890625]

================================================================================
245_model.layers.20.self_attn.o_proj: Linear (model.layers.20.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.20.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.004341, Std: 0.057772
     First 10: [-0.023633981123566628, -0.006339972838759422, 0.00045689381659030914, 0.0005622822791337967, 0.0064045824110507965, -0.011260639876127243, 0.012442640960216522, -0.005054449662566185, -0.010427902452647686, 0.004508761689066887]
     Last 10:  [-0.014603732153773308, -0.004075275268405676, 0.016180209815502167, -0.006033035460859537, 0.029000816866755486, 0.006662741303443909, -0.007959072478115559, -0.003721515880897641, 0.010445047169923782, -0.000668529886752367]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001363, Std: 0.053707
     First 10: [0.012760322540998459, -0.00035351887345314026, 0.05874323099851608, -0.0026317350566387177, 0.0025421753525733948, 0.03461454436182976, -0.0007169647142291069, -0.028233088552951813, 0.007607476785778999, -0.011008314788341522]
     Last 10:  [0.001615370623767376, -0.10300357639789581, -0.0019391151145100594, -0.04226933419704437, -0.02460837922990322, 0.03137750178575516, 0.07179649919271469, 0.04638942331075668, -0.028725646436214447, -0.04564177617430687]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000067
     First 10: [-0.05010986328125, -0.079345703125, 0.0198974609375, -0.020965576171875, 0.007843017578125, 0.024017333984375, 0.026641845703125, 0.0484619140625, -0.0142822265625, 0.039337158203125]
     Last 10:  [0.07244873046875, -0.08282470703125, 0.041656494140625, 0.00342559814453125, -0.01947021484375, 0.0496826171875, 0.067626953125, -0.0131378173828125, 0.035064697265625, -0.031463623046875]

================================================================================
246_model.layers.20.self_attn: LlamaSdpaAttention (model.layers.20.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.20.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001363, Std: 0.053707
     First 10: [0.012760322540998459, -0.00035351887345314026, 0.05874323099851608, -0.0026317350566387177, 0.0025421753525733948, 0.03461454436182976, -0.0007169647142291069, -0.028233088552951813, 0.007607476785778999, -0.011008314788341522]
     Last 10:  [0.001615370623767376, -0.10300357639789581, -0.0019391151145100594, -0.04226933419704437, -0.02460837922990322, 0.03137750178575516, 0.07179649919271469, 0.04638942331075668, -0.028725646436214447, -0.04564177617430687]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.20.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.272864
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4997909367084503, 0.5002090334892273, 0.0, 0.0, 0.0]
     Last 10:  [0.49530720710754395, 0.4940468966960907, 0.003903933335095644, 0.006742018740624189, 0.0, 0.491840660572052, 0.49170583486557007, 0.0033464606385678053, 0.00603004964068532, 0.007077015936374664]
     Zeros: 90, Total: 225

================================================================================
247_model.layers.20.post_attention_layernorm: LlamaRMSNorm (model.layers.20.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.20.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.087552, Std: 12.016282
     First 10: [-0.6143168807029724, -2.6235620975494385, -0.41567283868789673, 0.5410733222961426, -11.719534873962402, -7.083414554595947, 4.687025547027588, -0.19261762499809265, -0.5128557085990906, -2.0369465351104736]
     Last 10:  [0.4168107509613037, -0.12397028505802155, 0.5882997512817383, -0.7210839986801147, 0.34382349252700806, 0.3542554974555969, -0.30241164565086365, 0.09262944012880325, 0.0072020068764686584, -0.5308135151863098]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001203, Std: 0.383795
     First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
     Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.468847
     First 10: [0.4853515625, 0.446044921875, 0.473388671875, 0.462890625, 0.444091796875, 0.31982421875, 0.400146484375, 0.419921875, 0.49169921875, 0.4541015625]
     Last 10:  [0.484619140625, 0.4853515625, 0.455078125, 0.478515625, 0.4580078125, 0.48828125, 0.50439453125, 0.46435546875, 0.52734375, 0.430908203125]

================================================================================
248_model.layers.20.mlp.gate_proj: Linear (model.layers.20.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.20.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001203, Std: 0.383795
     First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
     Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.248543, Std: 0.482246
     First 10: [-0.07429102808237076, -0.31238383054733276, -0.19426292181015015, -0.738709568977356, -0.08010382950305939, 0.8217036724090576, 0.05887015536427498, -0.26759710907936096, -0.32694217562675476, -0.5683956742286682]
     Last 10:  [-0.8872087001800537, -0.91950523853302, -0.6837807893753052, -0.6551245450973511, 0.28999125957489014, -0.4399922788143158, -0.21439611911773682, -0.3913847506046295, 0.7021210789680481, -0.26141926646232605]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000116
     First 10: [-0.0626220703125, -0.006275177001953125, -0.0200347900390625, 0.00893402099609375, 0.0154571533203125, -0.034423828125, -0.0631103515625, -0.0880126953125, 0.01271820068359375, -0.07012939453125]
     Last 10:  [-0.06561279296875, -0.06121826171875, -0.0108795166015625, -0.09869384765625, -0.032470703125, -0.055450439453125, -0.024871826171875, 0.056365966796875, 0.03289794921875, 0.01313018798828125]

================================================================================
249_model.layers.20.mlp.act_fn: SiLU (model.layers.20.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.20.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.248543, Std: 0.482246
     First 10: [-0.07429102808237076, -0.31238383054733276, -0.19426292181015015, -0.738709568977356, -0.08010382950305939, 0.8217036724090576, 0.05887015536427498, -0.26759710907936096, -0.32694217562675476, -0.5683956742286682]
     Last 10:  [-0.8872087001800537, -0.91950523853302, -0.6837807893753052, -0.6551245450973511, 0.28999125957489014, -0.4399922788143158, -0.21439611911773682, -0.3913847506046295, 0.7021210789680481, -0.26141926646232605]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.20.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.058577, Std: 0.185715
     First 10: [-0.0357663594186306, -0.13199247419834137, -0.08772649616003036, -0.23881474137306213, -0.03844861686229706, 0.5707535743713379, 0.030301252380013466, -0.11600257456302643, -0.13698381185531616, -0.2055359184741974]
     Last 10:  [-0.25878652930259705, -0.26211297512054443, -0.22935236990451813, -0.22394494712352753, 0.16587325930595398, -0.17236380279064178, -0.09575045108795166, -0.1578783392906189, 0.4694787859916687, -0.11372126638889313]
     Zeros: 0, Total: 7680

================================================================================
250_model.layers.20.mlp.up_proj: Linear (model.layers.20.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.20.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001203, Std: 0.383795
     First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
     Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.011350, Std: 0.323297
     First 10: [0.1132466122508049, 0.004511840641498566, 0.06690139323472977, -0.05303361266851425, -0.15887556970119476, -0.6011021137237549, 0.3616740107536316, -0.27920079231262207, -0.10968785732984543, -0.3025050163269043]
     Last 10:  [-0.6118732690811157, 0.031049460172653198, 0.5534160137176514, -0.561439037322998, -0.0431094616651535, -0.19828274846076965, -0.20971760153770447, -0.2855392098426819, -0.25892388820648193, 0.5381672978401184]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000034
     First 10: [-0.00872802734375, 0.01381683349609375, -0.0212249755859375, 0.0293121337890625, -0.00777435302734375, 0.0284271240234375, -0.0027637481689453125, 0.046844482421875, -0.005962371826171875, 0.0843505859375]
     Last 10:  [0.055816650390625, -0.05029296875, -0.026580810546875, -0.0146942138671875, -0.02490234375, -0.0391845703125, 0.0118560791015625, 0.005481719970703125, -0.0243072509765625, -0.0537109375]

================================================================================
251_model.layers.20.mlp.down_proj: Linear (model.layers.20.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.20.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000480, Std: 0.061231
     First 10: [-0.004050419200211763, -0.0005955289816483855, -0.005869024898856878, 0.01266520842909813, 0.006108545698225498, -0.34308117628097534, 0.010959175415337086, 0.03238800913095474, 0.015025461092591286, 0.06217564642429352]
     Last 10:  [0.158344566822052, -0.008138466626405716, -0.12692727148532867, 0.1257314383983612, -0.007150706835091114, 0.034176766872406006, 0.020080555230379105, 0.045080456882715225, -0.1215592697262764, -0.0612010657787323]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.20.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.005363, Std: 0.093906
     First 10: [-0.0224256981164217, 0.06704564392566681, 0.12639197707176208, 0.018606672063469887, 0.06070858612656593, 0.010594639927148819, -0.08867188543081284, 0.09653672575950623, -0.01186444517225027, -0.02915659174323082]
     Last 10:  [-0.05422423034906387, 0.0390188992023468, 0.010260045528411865, 0.2214052379131317, -0.06394590437412262, 0.10702542960643768, 0.036005109548568726, 0.0019258782267570496, -0.13415798544883728, 0.1235584169626236]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000034
     First 10: [0.02459716796875, 0.04071044921875, 0.05322265625, -0.0516357421875, -0.008544921875, 0.006900787353515625, -0.0528564453125, -0.0072479248046875, -0.02532958984375, -0.0288543701171875]
     Last 10:  [0.0186920166015625, 0.0032558441162109375, 0.041046142578125, -0.005657196044921875, -0.025421142578125, -0.049407958984375, 0.06439208984375, 0.0989990234375, -0.0200347900390625, -0.003086090087890625]

================================================================================
252_model.layers.20.mlp: LlamaMLP (model.layers.20.mlp)
================================================================================

  → INPUT[0]: model.layers.20.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001203, Std: 0.383795
     First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
     Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.20.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.005363, Std: 0.093906
     First 10: [-0.0224256981164217, 0.06704564392566681, 0.12639197707176208, 0.018606672063469887, 0.06070858612656593, 0.010594639927148819, -0.08867188543081284, 0.09653672575950623, -0.01186444517225027, -0.02915659174323082]
     Last 10:  [-0.05422423034906387, 0.0390188992023468, 0.010260045528411865, 0.2214052379131317, -0.06394590437412262, 0.10702542960643768, 0.036005109548568726, 0.0019258782267570496, -0.13415798544883728, 0.1235584169626236]
     Zeros: 0, Total: 2880

================================================================================
253_model.layers.21.input_layernorm: LlamaRMSNorm (model.layers.21.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.21.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.082189, Std: 12.014915
     First 10: [-0.6367425918579102, -2.556516408920288, -0.28928086161613464, 0.5596799850463867, -11.658825874328613, -7.072819709777832, 4.598353862762451, -0.09608089923858643, -0.5247201323509216, -2.066103219985962]
     Last 10:  [0.36258652806282043, -0.08495138585567474, 0.5985597968101501, -0.49967876076698303, 0.27987760305404663, 0.4612809419631958, -0.2664065361022949, 0.0945553183555603, -0.12695598602294922, -0.4072551131248474]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003367, Std: 0.376527
     First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
     Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.519418
     First 10: [0.473388671875, 0.6474609375, 0.57470703125, 0.60791015625, 0.4072265625, 0.826171875, 0.5361328125, 0.607421875, 0.552734375, 0.564453125]
     Last 10:  [0.469482421875, 0.56103515625, 0.51123046875, 0.50146484375, 0.497802734375, 0.56103515625, 0.564453125, 0.58251953125, 0.525390625, 0.5712890625]

================================================================================
254_model.layers.21.self_attn.q_proj: Linear (model.layers.21.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.21.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003367, Std: 0.376527
     First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
     Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.016057, Std: 1.183110
     First 10: [0.22984200716018677, 0.15910829603672028, -0.003514103591442108, 0.2691241204738617, 0.35574468970298767, -0.22650867700576782, -0.025627970695495605, 0.02055741287767887, -0.39690181612968445, -0.21535637974739075]
     Last 10:  [-5.280608177185059, -0.1609506905078888, 6.974335193634033, -0.2305663526058197, -0.3093112111091614, -1.1503269672393799, 0.40631794929504395, -1.5499911308288574, -1.7624633312225342, -1.2647048234939575]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000007
     First 10: [-0.0181427001953125, 0.05029296875, 0.06378173828125, 0.044342041015625, 0.012115478515625, -0.05035400390625, 0.00511932373046875, -0.0150146484375, 0.00130462646484375, -0.025482177734375]
     Last 10:  [0.054473876953125, -0.0031337738037109375, -0.021331787109375, -0.1444091796875, -0.0181884765625, 0.004974365234375, 0.038848876953125, -0.08551025390625, 0.04510498046875, -0.055755615234375]

================================================================================
255_model.layers.21.self_attn.k_proj: Linear (model.layers.21.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.21.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003367, Std: 0.376527
     First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
     Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.127560, Std: 1.394830
     First 10: [0.0027727633714675903, 0.01414744183421135, 0.007860049605369568, 0.003395169973373413, -0.006208799779415131, -0.004044666886329651, -0.0023282617330551147, 0.010892018675804138, -0.002220362424850464, 0.004994720220565796]
     Last 10:  [0.700076699256897, -0.922065019607544, -0.8861062526702881, -0.2740546762943268, 1.0790432691574097, 0.4173305332660675, -0.2509743273258209, -1.0885858535766602, 1.43439781665802, 1.6519968509674072]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000310
     First 10: [0.011444091796875, -0.04656982421875, -0.0023365020751953125, 0.09967041015625, 0.0186614990234375, -0.0609130859375, 0.1097412109375, -0.0173492431640625, -0.01032257080078125, -0.00754547119140625]
     Last 10:  [0.04315185546875, 0.07012939453125, -0.1197509765625, -0.01546478271484375, -0.03717041015625, 0.0843505859375, 0.062042236328125, 0.01149749755859375, 0.0372314453125, 0.07135009765625]

================================================================================
256_model.layers.21.self_attn.v_proj: Linear (model.layers.21.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.21.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003367, Std: 0.376527
     First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
     Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.008311, Std: 0.265292
     First 10: [0.0007532346062362194, -0.004728805273771286, 0.0024248436093330383, -0.005464533343911171, -0.005816289223730564, 0.0024835262447595596, 0.0019276496022939682, -0.001150958240032196, 0.2210206389427185, 0.006917043589055538]
     Last 10:  [0.4177420437335968, 0.4196409583091736, 0.04316980019211769, -0.04197131097316742, 0.2942720651626587, 0.23625461757183075, 0.2365373969078064, 0.26631057262420654, -0.35316744446754456, -0.12206064164638519]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000143
     First 10: [0.047271728515625, 0.1065673828125, 0.02716064453125, -0.00669097900390625, -0.01788330078125, -0.005100250244140625, 0.038726806640625, -0.0121307373046875, 0.0023174285888671875, -0.0271148681640625]
     Last 10:  [-0.0227813720703125, -0.025421142578125, 0.058929443359375, 0.01473236083984375, -0.00666046142578125, -0.031219482421875, -0.025909423828125, -0.01377105712890625, -0.0235443115234375, 0.03411865234375]

================================================================================
257_model.layers.21.self_attn.o_proj: Linear (model.layers.21.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.21.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001630, Std: 0.031378
     First 10: [0.0007532346062362194, -0.004728805273771286, 0.0024248436093330383, -0.005464533343911171, -0.005816289223730564, 0.0024835262447595596, 0.0019276496022939682, -0.001150958240032196, 0.2210206389427185, 0.006917043589055538]
     Last 10:  [0.011144109070301056, 0.0029293762054294348, -0.0017982112476602197, 0.0034878933802247047, 0.013988573104143143, 0.010967212729156017, -0.0008642916800454259, 0.006287422496825457, -0.023387420922517776, 0.004245796240866184]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001482, Std: 0.031514
     First 10: [0.006397673394531012, -0.021988656371831894, 0.01723289117217064, -0.005464411340653896, 0.008898677304387093, 0.010687432251870632, 0.012334084138274193, 0.018378661945462227, 0.015047771856188774, -0.008032435551285744]
     Last 10:  [-0.0018701991066336632, -0.03900965303182602, 0.023705456405878067, -0.0012166635133326054, -0.03350949287414551, 0.02829616330564022, -0.009873336181044579, -0.002146713435649872, 0.03482506051659584, -0.023764245212078094]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000071
     First 10: [0.01690673828125, -0.0272674560546875, -0.036834716796875, 0.024993896484375, 0.017181396484375, 0.0189666748046875, -0.0182952880859375, 0.0301055908203125, -0.0155792236328125, -0.0096435546875]
     Last 10:  [0.007717132568359375, 0.0009984970092773438, -0.03363037109375, -0.02093505859375, -0.002025604248046875, 0.046539306640625, 0.049957275390625, -0.022857666015625, -0.03118896484375, -0.0017404556274414062]

================================================================================
258_model.layers.21.self_attn: LlamaSdpaAttention (model.layers.21.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.21.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001482, Std: 0.031514
     First 10: [0.006397673394531012, -0.021988656371831894, 0.01723289117217064, -0.005464411340653896, 0.008898677304387093, 0.010687432251870632, 0.012334084138274193, 0.018378661945462227, 0.015047771856188774, -0.008032435551285744]
     Last 10:  [-0.0018701991066336632, -0.03900965303182602, 0.023705456405878067, -0.0012166635133326054, -0.03350949287414551, 0.02829616330564022, -0.009873336181044579, -0.002146713435649872, 0.03482506051659584, -0.023764245212078094]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.21.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.272971
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49984100461006165, 0.5001590251922607, 0.0, 0.0, 0.0]
     Last 10:  [0.49036628007888794, 0.49185219407081604, 0.010607033967971802, 0.00717449514195323, 0.0, 0.47771021723747253, 0.4788278043270111, 0.015490761026740074, 0.01590866781771183, 0.012062611989676952]
     Zeros: 90, Total: 225

================================================================================
259_model.layers.21.post_attention_layernorm: LlamaRMSNorm (model.layers.21.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.21.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.083671, Std: 12.016568
     First 10: [-0.6303449273109436, -2.578505039215088, -0.2720479667186737, 0.5542155504226685, -11.649927139282227, -7.062132358551025, 4.610687732696533, -0.07770223915576935, -0.5096723437309265, -2.0741355419158936]
     Last 10:  [0.3607163429260254, -0.12396103888750076, 0.6222652792930603, -0.5008954405784607, 0.24636811017990112, 0.48957711458206177, -0.27627986669540405, 0.09240860491991043, -0.09213092923164368, -0.4310193657875061]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002642, Std: 0.381286
     First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
     Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.478825
     First 10: [0.48583984375, 0.460693359375, 0.482177734375, 0.488525390625, 0.4599609375, 0.31494140625, 0.40966796875, 0.4462890625, 0.50048828125, 0.465087890625]
     Last 10:  [0.50927734375, 0.505859375, 0.4775390625, 0.492431640625, 0.46875, 0.486328125, 0.529296875, 0.4677734375, 0.51171875, 0.453857421875]

================================================================================
260_model.layers.21.mlp.gate_proj: Linear (model.layers.21.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.21.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002642, Std: 0.381286
     First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
     Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.237013, Std: 0.460242
     First 10: [-0.22884830832481384, -0.5886576175689697, 0.03616829216480255, -0.0913294330239296, -0.0695096030831337, 0.4416232705116272, 0.12405756115913391, -0.5053423643112183, -0.6672689318656921, 0.08058266341686249]
     Last 10:  [-0.17080862820148468, -0.37617653608322144, -0.8496569395065308, 1.4870314598083496, 0.3391801118850708, -0.4760298728942871, -0.08977946639060974, -0.09948757290840149, 0.2536627948284149, -1.3234496116638184]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000094
     First 10: [0.047149658203125, 0.04400634765625, -0.0129241943359375, -0.051910400390625, -0.034912109375, -0.0126953125, -0.033935546875, 0.0114288330078125, 0.0007219314575195312, -0.00943756103515625]
     Last 10:  [0.006317138671875, -0.10443115234375, 0.187744140625, 0.0241851806640625, 0.004268646240234375, -0.04034423828125, -0.0024814605712890625, 0.05810546875, -0.1162109375, -0.11767578125]

================================================================================
261_model.layers.21.mlp.act_fn: SiLU (model.layers.21.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.21.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.237013, Std: 0.460242
     First 10: [-0.22884830832481384, -0.5886576175689697, 0.03616829216480255, -0.0913294330239296, -0.0695096030831337, 0.4416232705116272, 0.12405756115913391, -0.5053423643112183, -0.6672689318656921, 0.08058266341686249]
     Last 10:  [-0.17080862820148468, -0.37617653608322144, -0.8496569395065308, 1.4870314598083496, 0.3391801118850708, -0.4760298728942871, -0.08977946639060974, -0.09948757290840149, 0.2536627948284149, -1.3234496116638184]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.21.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.057293, Std: 0.180461
     First 10: [-0.1013881117105484, -0.2101171612739563, 0.018411146476864815, -0.04358089715242386, -0.03354739025235176, 0.26879212260246277, 0.06587142497301102, -0.19015325605869293, -0.22627666592597961, 0.041913848370313644]
     Last 10:  [-0.07812810689210892, -0.1531224399805069, -0.2544763684272766, 1.2128709554672241, 0.1980782449245453, -0.18240991234779358, -0.0428759939968586, -0.04727138206362724, 0.14283189177513123, -0.2782485783100128]
     Zeros: 0, Total: 7680

================================================================================
262_model.layers.21.mlp.up_proj: Linear (model.layers.21.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.21.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002642, Std: 0.381286
     First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
     Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.001538, Std: 0.320853
     First 10: [0.3715323507785797, -0.2934679687023163, 0.5119953155517578, -0.1137034073472023, 0.11482010781764984, -0.27615001797676086, 1.0119760036468506, -0.15964308381080627, -0.055060748010873795, 0.06178927421569824]
     Last 10:  [-0.11325860768556595, -0.6864119172096252, 0.3015926778316498, -0.2248462736606598, 0.09742708504199982, -0.014056161046028137, 0.15402640402317047, 0.33341294527053833, 0.17754997313022614, 0.31568098068237305]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000121
     First 10: [0.0022106170654296875, 0.09918212890625, -0.0352783203125, -0.016998291015625, -0.034210205078125, 0.051483154296875, -0.029327392578125, -0.048797607421875, -0.0260162353515625, -0.0209503173828125]
     Last 10:  [0.04266357421875, 0.002666473388671875, -0.05157470703125, -0.02313232421875, 0.0987548828125, 0.0804443359375, 0.011932373046875, 0.08453369140625, 0.0131683349609375, 0.022613525390625]

================================================================================
263_model.layers.21.mlp.down_proj: Linear (model.layers.21.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.21.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000617, Std: 0.063210
     First 10: [-0.03766896203160286, 0.06166265532374382, 0.0094264205545187, 0.004955296404659748, -0.0038519150111824274, -0.07422695308923721, 0.06666029989719391, 0.030356653034687042, 0.012458962388336658, 0.002589826239272952]
     Last 10:  [0.008848680183291435, 0.10510506480932236, -0.07674820721149445, -0.27270951867103577, 0.019298186525702477, 0.002563983201980591, -0.006604035384953022, -0.015760891139507294, 0.025359798222780228, -0.08783778548240662]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.21.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002990, Std: 0.098263
     First 10: [-0.07455923408269882, 0.005484214052557945, 0.12612131237983704, -0.04730931669473648, 0.009857622906565666, 0.02295755222439766, -0.10611513257026672, -0.07391823828220367, 0.08659259974956512, -0.028750231489539146]
     Last 10:  [-0.0646699070930481, 0.04960818588733673, 0.15533052384853363, -0.0018791472539305687, 0.07151983678340912, -0.1355585753917694, -0.08350910246372223, -0.08633317053318024, -0.025011468678712845, 0.03650433197617531]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000027
     First 10: [0.053985595703125, -0.07012939453125, -0.040802001953125, -0.03363037109375, -0.0010690689086914062, 0.00222015380859375, -0.0792236328125, -0.0235595703125, 0.037628173828125, 0.053070068359375]
     Last 10:  [0.012786865234375, -0.009674072265625, 0.07208251953125, -0.033447265625, -0.02392578125, 0.021270751953125, 0.031585693359375, 0.034698486328125, -0.01019287109375, -0.033843994140625]

================================================================================
264_model.layers.21.mlp: LlamaMLP (model.layers.21.mlp)
================================================================================

  → INPUT[0]: model.layers.21.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002642, Std: 0.381286
     First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
     Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.21.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002990, Std: 0.098263
     First 10: [-0.07455923408269882, 0.005484214052557945, 0.12612131237983704, -0.04730931669473648, 0.009857622906565666, 0.02295755222439766, -0.10611513257026672, -0.07391823828220367, 0.08659259974956512, -0.028750231489539146]
     Last 10:  [-0.0646699070930481, 0.04960818588733673, 0.15533052384853363, -0.0018791472539305687, 0.07151983678340912, -0.1355585753917694, -0.08350910246372223, -0.08633317053318024, -0.025011468678712845, 0.03650433197617531]
     Zeros: 0, Total: 2880

================================================================================
265_model.layers.22.input_layernorm: LlamaRMSNorm (model.layers.22.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.22.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.086661, Std: 12.013081
     First 10: [-0.7049041390419006, -2.5730209350585938, -0.14592665433883667, 0.5069062113761902, -11.640069961547852, -7.039175033569336, 4.50457239151001, -0.15162047743797302, -0.4230797290802002, -2.1028857231140137]
     Last 10:  [0.2960464358329773, -0.07435285300016403, 0.7775958180427551, -0.5027745962142944, 0.31788796186447144, 0.35401853919029236, -0.3597889542579651, 0.006075434386730194, -0.11714239418506622, -0.3945150375366211]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007081, Std: 0.402962
     First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
     Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.550805
     First 10: [0.599609375, 0.6689453125, 0.64306640625, 0.6396484375, 0.36181640625, 0.89404296875, 0.71875, 0.6591796875, 0.57958984375, 0.642578125]
     Last 10:  [0.51171875, 0.64013671875, 0.556640625, 0.51513671875, 0.495361328125, 0.556640625, 0.60205078125, 0.642578125, 0.5009765625, 0.626953125]

================================================================================
266_model.layers.22.self_attn.q_proj: Linear (model.layers.22.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.22.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007081, Std: 0.402962
     First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
     Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.046878, Std: 1.223713
     First 10: [-0.36074739694595337, -0.009188666939735413, -0.2593041956424713, -0.05504351109266281, 0.21059226989746094, 0.1324395388364792, 0.2643706798553467, 0.14050404727458954, 0.39174655079841614, -0.03540651127696037]
     Last 10:  [-0.5488568544387817, -1.5286352634429932, -1.4471421241760254, 1.2971949577331543, 2.5139453411102295, 0.323557585477829, -1.2507191896438599, -0.5471152067184448, 1.510875940322876, 5.33905029296875]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000088
     First 10: [-0.0168609619140625, -0.0005030632019042969, -0.00815582275390625, -0.038177490234375, -0.021728515625, 0.0099639892578125, -0.0019197463989257812, 0.01166534423828125, -0.03192138671875, 0.00890350341796875]
     Last 10:  [0.002147674560546875, -0.13916015625, 0.0200958251953125, 0.0872802734375, -0.11932373046875, 0.0245513916015625, -0.0665283203125, 0.2432861328125, -0.1146240234375, -0.0226593017578125]

================================================================================
267_model.layers.22.self_attn.k_proj: Linear (model.layers.22.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.22.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007081, Std: 0.402962
     First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
     Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.082793, Std: 1.238845
     First 10: [0.012216828763484955, -0.023319728672504425, -0.023002414032816887, 0.010548101738095284, -0.004133984446525574, 0.013665445148944855, 0.0017748773097991943, 0.01568520814180374, 0.00408385694026947, 0.019474200904369354]
     Last 10:  [6.705820083618164, 0.47833529114723206, -0.9782044291496277, -0.7091226577758789, -0.4627963602542877, 1.531082034111023, 4.5554656982421875, 4.5525665283203125, -2.009323835372925, -0.33737683296203613]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000056
     First 10: [0.00788116455078125, 0.0241851806640625, -0.0294647216796875, -0.04730224609375, -0.0303955078125, 0.01148223876953125, -0.0290679931640625, -0.01751708984375, -0.0244598388671875, -0.05218505859375]
     Last 10:  [0.0927734375, 0.04107666015625, -0.00021588802337646484, -0.02227783203125, 0.046173095703125, 0.03228759765625, -0.08447265625, 0.137451171875, -0.020477294921875, -0.0325927734375]

================================================================================
268_model.layers.22.self_attn.v_proj: Linear (model.layers.22.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.22.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.007081, Std: 0.402962
     First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
     Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.017767, Std: 0.336811
     First 10: [0.01535709761083126, 0.002594597637653351, -0.0005996804684400558, -0.0014284218195825815, 0.0032699727453291416, -0.002136276103556156, -0.032770104706287384, 0.009553901851177216, -0.0060053253546357155, -0.010167919099330902]
     Last 10:  [1.1591767072677612, 0.6215489506721497, -0.9111776351928711, -0.451712042093277, 0.35181891918182373, 0.27174293994903564, 0.5214964747428894, -0.06354756653308868, -0.38733887672424316, -0.5985794067382812]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000051
     First 10: [-0.0283355712890625, -0.01885986328125, 0.0035858154296875, -0.061279296875, -0.009918212890625, 0.02105712890625, -0.03143310546875, 0.01016998291015625, 0.00237274169921875, 0.0303497314453125]
     Last 10:  [-0.0141448974609375, -0.037109375, -0.0018720626831054688, -0.005207061767578125, -0.04486083984375, 0.0102081298828125, 0.0096435546875, -0.0213623046875, -0.04449462890625, 0.0011882781982421875]

================================================================================
269_model.layers.22.self_attn.o_proj: Linear (model.layers.22.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.22.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002043, Std: 0.062767
     First 10: [0.01535709761083126, 0.002594597637653351, -0.0005996804684400558, -0.0014284218195825815, 0.0032699727453291416, -0.002136276103556156, -0.032770104706287384, 0.009553901851177216, -0.0060053253546357155, -0.010167919099330902]
     Last 10:  [0.04433678463101387, 0.013130789622664452, -0.04815905913710594, 0.0011169093195348978, 0.025027208030223846, 0.016393307596445084, 0.030104847624897957, 0.009956265799701214, -0.008847092278301716, -0.008229238912463188]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000114, Std: 0.068613
     First 10: [0.05316454917192459, -0.02331623062491417, -0.04013150930404663, -0.01521223969757557, 0.0427003875374794, -0.060108482837677, 0.008881362155079842, 0.013814093545079231, 0.002582073677331209, -0.017367791384458542]
     Last 10:  [-0.042863644659519196, -0.024657342582941055, -0.22159329056739807, 0.14144231379032135, -0.26581746339797974, -0.14955508708953857, -0.11467471718788147, 0.044134750962257385, 0.05820775777101517, -0.005239246413111687]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000074
     First 10: [-0.039947509765625, 0.0297393798828125, -0.0083160400390625, 0.05181884765625, 0.05224609375, -0.0281982421875, 0.0171356201171875, 0.00301361083984375, -0.011444091796875, 0.0789794921875]
     Last 10:  [-0.0034351348876953125, 0.026092529296875, 0.00160980224609375, 0.035980224609375, -0.003620147705078125, 0.0035419464111328125, -0.0271148681640625, -0.0079193115234375, 0.05303955078125, -0.055877685546875]

================================================================================
270_model.layers.22.self_attn: LlamaSdpaAttention (model.layers.22.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.22.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000114, Std: 0.068613
     First 10: [0.05316454917192459, -0.02331623062491417, -0.04013150930404663, -0.01521223969757557, 0.0427003875374794, -0.060108482837677, 0.008881362155079842, 0.013814093545079231, 0.002582073677331209, -0.017367791384458542]
     Last 10:  [-0.042863644659519196, -0.024657342582941055, -0.22159329056739807, 0.14144231379032135, -0.26581746339797974, -0.14955508708953857, -0.11467471718788147, 0.044134750962257385, 0.05820775777101517, -0.005239246413111687]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.22.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.262732
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000764727592468, 0.4999235272407532, 0.0, 0.0, 0.0]
     Last 10:  [0.48176485300064087, 0.4822445213794708, 0.013340338133275509, 0.022650232538580894, 0.0, 0.4707629084587097, 0.4716876745223999, 0.012161492370069027, 0.012615066953003407, 0.03277279809117317]
     Zeros: 90, Total: 225

================================================================================
271_model.layers.22.post_attention_layernorm: LlamaRMSNorm (model.layers.22.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.22.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.086547, Std: 12.018270
     First 10: [-0.6517395973205566, -2.596337080001831, -0.1860581636428833, 0.49169397354125977, -11.597369194030762, -7.099283695220947, 4.513453960418701, -0.13780638575553894, -0.4204976558685303, -2.120253562927246]
     Last 10:  [0.2531827986240387, -0.09901019930839539, 0.5560024976730347, -0.3613322973251343, 0.0520704984664917, 0.20446345210075378, -0.47446367144584656, 0.05021018534898758, -0.058934636414051056, -0.39975428581237793]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000812, Std: 0.365180
     First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
     Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.470074
     First 10: [0.47998046875, 0.457763671875, 0.462646484375, 0.491455078125, 0.4384765625, 0.3486328125, 0.37646484375, 0.445068359375, 0.48828125, 0.44775390625]
     Last 10:  [0.493408203125, 0.493408203125, 0.46728515625, 0.485595703125, 0.464599609375, 0.509765625, 0.5146484375, 0.447998046875, 0.483642578125, 0.4521484375]

================================================================================
272_model.layers.22.mlp.gate_proj: Linear (model.layers.22.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.22.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000812, Std: 0.365180
     First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
     Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.194071, Std: 0.451186
     First 10: [0.12825141847133636, -0.1199914738535881, -0.33811861276626587, -0.9641696214675903, -0.453379362821579, -0.0516669787466526, -0.4012378454208374, -0.6591695547103882, -0.41465336084365845, -0.3029075860977173]
     Last 10:  [-1.0374350547790527, -0.2553068995475769, 0.3562857508659363, -0.8226343989372253, 0.6884980797767639, 0.16277694702148438, -0.20076915621757507, -1.2174700498580933, 0.46374034881591797, -0.11898411810398102]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000001
     First 10: [0.037811279296875, -0.038360595703125, 0.0018148422241210938, -0.0265350341796875, 0.08099365234375, -0.006145477294921875, -0.042999267578125, -0.07818603515625, 0.00714111328125, 0.0198822021484375]
     Last 10:  [-0.025054931640625, 0.017242431640625, 0.0160369873046875, 0.0501708984375, -0.0275115966796875, -0.04376220703125, 0.0223236083984375, -0.01509857177734375, 0.0110015869140625, 0.03326416015625]

================================================================================
273_model.layers.22.mlp.act_fn: SiLU (model.layers.22.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.22.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.194071, Std: 0.451186
     First 10: [0.12825141847133636, -0.1199914738535881, -0.33811861276626587, -0.9641696214675903, -0.453379362821579, -0.0516669787466526, -0.4012378454208374, -0.6591695547103882, -0.41465336084365845, -0.3029075860977173]
     Last 10:  [-1.0374350547790527, -0.2553068995475769, 0.3562857508659363, -0.8226343989372253, 0.6884980797767639, 0.16277694702148438, -0.20076915621757507, -1.2174700498580933, 0.46374034881591797, -0.11898411810398102]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.22.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.041171, Std: 0.205181
     First 10: [0.06823218613862991, -0.056400563567876816, -0.14074747264385223, -0.26615336537361145, -0.17616400122642517, -0.02516626939177513, -0.16090239584445953, -0.22472815215587616, -0.16494780778884888, -0.12868933379650116]
     Last 10:  [-0.27143990993499756, -0.11144598573446274, 0.2095462530851364, -0.2510719299316406, 0.458286851644516, 0.08799797296524048, -0.0903412327170372, -0.27804821729660034, 0.2846907079219818, -0.055956922471523285]
     Zeros: 0, Total: 7680

================================================================================
274_model.layers.22.mlp.up_proj: Linear (model.layers.22.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.22.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000812, Std: 0.365180
     First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
     Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.002648, Std: 0.323809
     First 10: [-0.3480209410190582, 0.2867393493652344, -0.26820698380470276, 0.332044780254364, 0.16729779541492462, 0.03233139216899872, -0.17248675227165222, 0.11719851940870285, 0.2806438207626343, 0.10768653452396393]
     Last 10:  [-0.4167974293231964, -0.3732747733592987, -0.10734303295612335, -0.2530028820037842, 0.1769219934940338, -0.1868470013141632, -0.2023690938949585, 0.19844475388526917, 0.002392515540122986, 0.11281710863113403]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000028
     First 10: [0.018218994140625, -0.0035953521728515625, 0.01345062255859375, -0.0274658203125, -0.0217437744140625, -0.0027313232421875, -0.018768310546875, -0.034332275390625, -0.0193023681640625, -0.06512451171875]
     Last 10:  [-0.0014085769653320312, -0.034271240234375, -0.01324462890625, 0.0478515625, -0.0247955322265625, -0.11083984375, 0.01265716552734375, -0.0106048583984375, 0.033843994140625, 0.054656982421875]

================================================================================
275_model.layers.22.mlp.down_proj: Linear (model.layers.22.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.22.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.001360, Std: 0.080548
     First 10: [-0.023746229708194733, -0.01617226004600525, 0.037749454379081726, -0.08837483823299408, -0.029471848160028458, -0.0008136605028994381, 0.02775353193283081, -0.02633780613541603, -0.04629158228635788, -0.013858108781278133]
     Last 10:  [0.1131354570388794, 0.041599974036216736, -0.022493330761790276, 0.06352192163467407, 0.08108102530241013, -0.016442157328128815, 0.018282273784279823, -0.05517721176147461, 0.000681126955896616, -0.006312898360192776]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.22.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004049, Std: 0.123073
     First 10: [0.02610795758664608, 0.038270462304353714, -0.09737083315849304, -0.08154627680778503, 0.07211573421955109, 0.05738777294754982, 0.022405140101909637, -0.007602822966873646, 0.02503429725766182, -0.03890591487288475]
     Last 10:  [-0.44651857018470764, -0.23423157632350922, 0.2262629121541977, -0.2898651659488678, 0.022039901465177536, 0.1453578621149063, 0.04107243940234184, -0.07388395816087723, 0.1357453465461731, -0.12350669503211975]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000059
     First 10: [0.055206298828125, 0.0166778564453125, 0.06451416015625, 0.060577392578125, 0.0455322265625, 0.07098388671875, -0.0797119140625, 0.003658294677734375, 0.058380126953125, -0.035675048828125]
     Last 10:  [-0.0146484375, 0.0246124267578125, -0.012969970703125, 0.00832366943359375, 0.09210205078125, 0.0254058837890625, -0.02349853515625, -0.02911376953125, -0.06610107421875, -0.06365966796875]

================================================================================
276_model.layers.22.mlp: LlamaMLP (model.layers.22.mlp)
================================================================================

  → INPUT[0]: model.layers.22.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000812, Std: 0.365180
     First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
     Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.22.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004049, Std: 0.123073
     First 10: [0.02610795758664608, 0.038270462304353714, -0.09737083315849304, -0.08154627680778503, 0.07211573421955109, 0.05738777294754982, 0.022405140101909637, -0.007602822966873646, 0.02503429725766182, -0.03890591487288475]
     Last 10:  [-0.44651857018470764, -0.23423157632350922, 0.2262629121541977, -0.2898651659488678, 0.022039901465177536, 0.1453578621149063, 0.04107243940234184, -0.07388395816087723, 0.1357453465461731, -0.12350669503211975]
     Zeros: 0, Total: 2880

================================================================================
277_model.layers.23.input_layernorm: LlamaRMSNorm (model.layers.23.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.23.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.082499, Std: 12.007148
     First 10: [-0.6256316304206848, -2.5580666065216064, -0.28342899680137634, 0.41014769673347473, -11.525253295898438, -7.041895866394043, 4.535859107971191, -0.1454092115163803, -0.39546334743499756, -2.1591594219207764]
     Last 10:  [-0.19333577156066895, -0.3332417607307434, 0.7822654247283936, -0.6511974334716797, 0.07411040365695953, 0.3498213291168213, -0.4333912432193756, -0.02367377281188965, 0.07681071013212204, -0.5232609510421753]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000381, Std: 0.404781
     First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
     Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.582079
     First 10: [0.55908203125, 0.6904296875, 0.7080078125, 0.68017578125, 0.3984375, 1.0107421875, 0.70263671875, 0.810546875, 0.62744140625, 0.7109375]
     Last 10:  [0.4716796875, 0.615234375, 0.57666015625, 0.572265625, 0.466064453125, 0.57177734375, 0.673828125, 0.74267578125, 0.53759765625, 0.671875]

================================================================================
278_model.layers.23.self_attn.q_proj: Linear (model.layers.23.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.23.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000381, Std: 0.404781
     First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
     Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.016844, Std: 1.013739
     First 10: [0.03882542997598648, -0.0109908077865839, 0.18332095444202423, 0.030024807900190353, 0.15492121875286102, -0.4824647903442383, -0.20886868238449097, -0.16639557480812073, -0.06901206076145172, -0.1604500412940979]
     Last 10:  [1.6566965579986572, 6.271746635437012, 0.6227735280990601, 0.7228705883026123, 1.2723091840744019, 0.09891939163208008, 0.24935078620910645, -1.673363447189331, -0.07355526089668274, -0.053754061460494995]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000010
     First 10: [-0.04266357421875, -0.001956939697265625, -0.019073486328125, -0.058746337890625, 0.016632080078125, 0.0233154296875, 0.01271820068359375, 0.0273590087890625, -0.050537109375, -0.03765869140625]
     Last 10:  [0.0172576904296875, 0.09710693359375, 0.05035400390625, -0.02581787109375, -0.056182861328125, 0.042205810546875, -0.0322265625, -0.108642578125, -0.11956787109375, -0.01422882080078125]

================================================================================
279_model.layers.23.self_attn.k_proj: Linear (model.layers.23.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.23.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000381, Std: 0.404781
     First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
     Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.028193, Std: 1.279865
     First 10: [0.0010672901989892125, -0.012887310236692429, -0.01687832549214363, -0.006928585469722748, -0.003949351608753204, -0.00751158595085144, -0.006269879639148712, 0.008447722531855106, 0.006100103259086609, 0.0034252703189849854]
     Last 10:  [0.8110791444778442, -0.46858835220336914, 0.9879734516143799, -0.5486447811126709, 2.29508900642395, -0.5165367722511292, 1.1878749132156372, 2.5627260208129883, 1.7888284921646118, -1.3071839809417725]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000223
     First 10: [0.0062713623046875, 0.0063934326171875, -0.040313720703125, -0.03955078125, -0.10333251953125, 0.037384033203125, -0.0382080078125, 0.0124359130859375, 0.0188446044921875, 0.0181121826171875]
     Last 10:  [0.0269622802734375, -0.09051513671875, 0.10528564453125, 0.011688232421875, -0.12353515625, 0.0103912353515625, 0.00136566162109375, -0.1544189453125, -0.0014629364013671875, -0.062469482421875]

================================================================================
280_model.layers.23.self_attn.v_proj: Linear (model.layers.23.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.23.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000381, Std: 0.404781
     First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
     Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.028437, Std: 0.393747
     First 10: [-0.0025939475744962692, 0.0065566785633563995, -0.005376212298870087, 0.007756686769425869, 0.00014266371726989746, -0.009234514087438583, 0.010661184787750244, 0.014128042384982109, 0.0024462398141622543, -0.0037422841414809227]
     Last 10:  [0.1966671645641327, 0.7185508608818054, 0.06158833205699921, 0.40007007122039795, -0.03582869470119476, -0.2765061855316162, 0.22976237535476685, -0.532347559928894, -0.33457404375076294, -0.2990342080593109]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000215
     First 10: [-0.0051116943359375, -0.07818603515625, 0.00012969970703125, -0.0257110595703125, -0.076171875, -0.02337646484375, -0.048736572265625, 0.01531219482421875, -0.0298919677734375, 0.0026092529296875]
     Last 10:  [-0.019439697265625, 0.022674560546875, -0.026092529296875, -0.054443359375, -0.0161590576171875, 0.028411865234375, -0.04095458984375, 0.0195770263671875, -0.05950927734375, 0.0256805419921875]

================================================================================
281_model.layers.23.self_attn.o_proj: Linear (model.layers.23.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.23.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003804, Std: 0.064498
     First 10: [-0.0025939475744962692, 0.0065566785633563995, -0.005376212298870087, 0.007756686769425869, 0.00014266371726989746, -0.009234514087438583, 0.010661184787750244, 0.014128042384982109, 0.0024462398141622543, -0.0037422841414809227]
     Last 10:  [0.005524877924472094, 0.022212868556380272, -0.0008092504576779902, 0.012956112623214722, -0.021154744550585747, -0.030004682019352913, 0.0014769032131880522, -0.009680322371423244, -0.01673680730164051, -0.013859687373042107]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001076, Std: 0.068838
     First 10: [0.01896200142800808, -0.017869537696242332, 0.03212352097034454, 0.004578854888677597, -0.00672895135357976, -0.0015109654050320387, 0.012132639065384865, -0.008294843137264252, -0.008047648705542088, -0.015722960233688354]
     Last 10:  [0.004189356695860624, 0.04497360810637474, -0.11405371129512787, -0.11616293340921402, -0.14106105268001556, -0.09966025501489639, -0.1514626443386078, 0.017721250653266907, -0.12847736477851868, -0.3146999478340149]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000082
     First 10: [-0.01910400390625, 0.0204620361328125, -0.0249176025390625, 0.004177093505859375, 0.012359619140625, -0.0272216796875, 0.06524658203125, -0.038360595703125, 0.0274200439453125, 0.00762939453125]
     Last 10:  [-0.041839599609375, -0.078125, 0.0214385986328125, -0.04766845703125, -0.09478759765625, -0.0207977294921875, -0.0352783203125, -0.005157470703125, -0.035675048828125, -0.02911376953125]

================================================================================
282_model.layers.23.self_attn: LlamaSdpaAttention (model.layers.23.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.23.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001076, Std: 0.068838
     First 10: [0.01896200142800808, -0.017869537696242332, 0.03212352097034454, 0.004578854888677597, -0.00672895135357976, -0.0015109654050320387, 0.012132639065384865, -0.008294843137264252, -0.008047648705542088, -0.015722960233688354]
     Last 10:  [0.004189356695860624, 0.04497360810637474, -0.11405371129512787, -0.11616293340921402, -0.14106105268001556, -0.09966025501489639, -0.1514626443386078, 0.017721250653266907, -0.12847736477851868, -0.3146999478340149]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.23.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.266964
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49996840953826904, 0.5000316500663757, 0.0, 0.0, 0.0]
     Last 10:  [0.4842434227466583, 0.4848483204841614, 0.013631118461489677, 0.017277171835303307, 0.0, 0.4552130699157715, 0.4570446312427521, 0.02106368914246559, 0.022347334772348404, 0.044331151992082596]
     Zeros: 90, Total: 225

================================================================================
283_model.layers.23.post_attention_layernorm: LlamaRMSNorm (model.layers.23.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.23.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.083575, Std: 12.009032
     First 10: [-0.6066696047782898, -2.5759360790252686, -0.2513054609298706, 0.4147265553474426, -11.531982421875, -7.043406963348389, 4.547991752624512, -0.15370404720306396, -0.4035109877586365, -2.174882411956787]
     Last 10:  [-0.18914641439914703, -0.28826814889907837, 0.6682116985321045, -0.7673603892326355, -0.06695064902305603, 0.2501610815525055, -0.5848538875579834, -0.005952522158622742, -0.05166665464639664, -0.8379608988761902]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003453, Std: 0.355253
     First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
     Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.478013
     First 10: [0.491455078125, 0.4541015625, 0.4697265625, 0.483154296875, 0.396484375, 0.2763671875, 0.396484375, 0.45361328125, 0.485595703125, 0.478271484375]
     Last 10:  [0.492919921875, 0.50146484375, 0.478271484375, 0.482666015625, 0.455078125, 0.50341796875, 0.52197265625, 0.481689453125, 0.485107421875, 0.458251953125]

================================================================================
284_model.layers.23.mlp.gate_proj: Linear (model.layers.23.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.23.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003453, Std: 0.355253
     First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
     Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.196718, Std: 0.401999
     First 10: [-0.04783173277974129, 0.13219603896141052, 0.1513558179140091, -0.4377827048301697, -0.32963457703590393, -0.00872887670993805, 0.13432136178016663, 0.0012616664171218872, 0.03157933056354523, -0.4618930220603943]
     Last 10:  [-0.4584723114967346, -0.42884188890457153, -0.5990346670150757, -0.23482072353363037, -1.0211139917373657, -0.12887078523635864, 0.3433792293071747, -0.4555142819881439, -0.49022194743156433, 0.393224835395813]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000056
     First 10: [-0.03662109375, -0.048065185546875, -0.056671142578125, -0.005825042724609375, 0.004543304443359375, -0.07232666015625, 0.0234222412109375, -0.01129150390625, 0.01288604736328125, -0.00611114501953125]
     Last 10:  [0.0491943359375, -0.0823974609375, -0.01323699951171875, 0.031341552734375, -0.037139892578125, -0.060394287109375, 0.00030875205993652344, 0.048187255859375, -0.041595458984375, 0.0231475830078125]

================================================================================
285_model.layers.23.mlp.act_fn: SiLU (model.layers.23.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.23.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.196718, Std: 0.401999
     First 10: [-0.04783173277974129, 0.13219603896141052, 0.1513558179140091, -0.4377827048301697, -0.32963457703590393, -0.00872887670993805, 0.13432136178016663, 0.0012616664171218872, 0.03157933056354523, -0.4618930220603943]
     Last 10:  [-0.4584723114967346, -0.42884188890457153, -0.5990346670150757, -0.23482072353363037, -1.0211139917373657, -0.12887078523635864, 0.3433792293071747, -0.4555142819881439, -0.49022194743156433, 0.393224835395813]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.23.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.051588, Std: 0.166738
     First 10: [-0.02334400825202465, 0.07046061754226685, 0.08139415085315704, -0.17172877490520477, -0.1378958821296692, -0.004345390014350414, 0.0716644674539566, 0.000631231174338609, 0.016038957983255386, -0.17853868007659912]
     Last 10:  [-0.17758846282958984, -0.16913649439811707, -0.21239647269248962, -0.10368816554546356, -0.27040165662765503, -0.060289207845926285, 0.20088067650794983, -0.17676255106925964, -0.18620653450489044, 0.23477834463119507]
     Zeros: 0, Total: 7680

================================================================================
286_model.layers.23.mlp.up_proj: Linear (model.layers.23.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.23.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003453, Std: 0.355253
     First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
     Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.005359, Std: 0.319773
     First 10: [-0.30181047320365906, -0.3488282859325409, 0.4746686816215515, -0.0848032608628273, 0.015756061300635338, 0.043790869414806366, 1.05527925491333, -0.9231560230255127, -0.2956707775592804, 0.12498873472213745]
     Last 10:  [-0.44497841596603394, -0.3743751049041748, -0.2455146610736847, 0.04781195521354675, 0.3584684431552887, 0.20256289839744568, 0.1163095235824585, -0.18338069319725037, -0.09849771857261658, -0.22898846864700317]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000106
     First 10: [-0.01055908203125, 0.0300445556640625, 0.0142822265625, 0.026031494140625, 0.0546875, -0.0137481689453125, 0.0012416839599609375, 0.03729248046875, -0.014190673828125, -0.0413818359375]
     Last 10:  [-0.0693359375, -0.0037937164306640625, 0.04541015625, -0.05206298828125, -0.0157470703125, 0.0037403106689453125, 0.043670654296875, 0.0294952392578125, 0.0458984375, -0.006046295166015625]

================================================================================
287_model.layers.23.mlp.down_proj: Linear (model.layers.23.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.23.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000852, Std: 0.062536
     First 10: [0.007045465987175703, -0.024578657001256943, 0.03863525390625, 0.014563160017132759, -0.0021726959384977818, -0.0001902884105220437, 0.07562602311372757, -0.0005827248678542674, -0.004742251243442297, -0.02231532335281372]
     Last 10:  [0.07902303338050842, 0.06332049518823624, 0.05214644968509674, -0.004957533907145262, -0.09693045914173126, -0.012212356552481651, 0.023364335298538208, 0.032414838671684265, 0.01834091916680336, -0.05376153439283371]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.23.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000066, Std: 0.108549
     First 10: [0.0661521777510643, 0.062001004815101624, 0.031662195920944214, 0.026480158790946007, 0.05145028233528137, 0.24532906711101532, 0.03555052727460861, 0.008698836900293827, 0.0245942585170269, 0.04240277782082558]
     Last 10:  [0.0575750470161438, 0.11860759556293488, 0.25804969668388367, -0.12256695330142975, 0.21863412857055664, 0.1768510341644287, 0.20613552629947662, 0.059022217988967896, 0.02508348599076271, 0.224784255027771]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000036
     First 10: [-0.0185394287109375, -0.038299560546875, -0.0115966796875, -0.0357666015625, 0.0213623046875, -0.032012939453125, -0.042724609375, -0.007476806640625, 0.09027099609375, 0.00034928321838378906]
     Last 10:  [-0.0004012584686279297, -0.06610107421875, 0.010284423828125, 0.00872802734375, -0.0277099609375, 0.041046142578125, 0.04644775390625, 0.0167083740234375, 0.0753173828125, -0.02484130859375]

================================================================================
288_model.layers.23.mlp: LlamaMLP (model.layers.23.mlp)
================================================================================

  → INPUT[0]: model.layers.23.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003453, Std: 0.355253
     First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
     Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.23.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000066, Std: 0.108549
     First 10: [0.0661521777510643, 0.062001004815101624, 0.031662195920944214, 0.026480158790946007, 0.05145028233528137, 0.24532906711101532, 0.03555052727460861, 0.008698836900293827, 0.0245942585170269, 0.04240277782082558]
     Last 10:  [0.0575750470161438, 0.11860759556293488, 0.25804969668388367, -0.12256695330142975, 0.21863412857055664, 0.1768510341644287, 0.20613552629947662, 0.059022217988967896, 0.02508348599076271, 0.224784255027771]
     Zeros: 0, Total: 2880

================================================================================
289_model.layers.24.input_layernorm: LlamaRMSNorm (model.layers.24.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.24.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.083509, Std: 11.998659
     First 10: [-0.5405174493789673, -2.513935089111328, -0.2196432650089264, 0.4412067234516144, -11.480531692504883, -6.7980780601501465, 4.583542346954346, -0.1450052112340927, -0.37891674041748047, -2.132479667663574]
     Last 10:  [-0.13157136738300323, -0.1696605533361435, 0.9262614250183105, -0.889927327632904, 0.1516834795475006, 0.4270121157169342, -0.37871837615966797, 0.053069695830345154, -0.026583168655633926, -0.6131766438484192]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000504, Std: 0.372201
     First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
     Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.547602
     First 10: [0.54248046875, 0.666015625, 0.68798828125, 0.65576171875, 0.43359375, 1.00390625, 0.759765625, 0.72705078125, 0.5625, 0.7822265625]
     Last 10:  [0.430419921875, 0.525390625, 0.54833984375, 0.533203125, 0.431396484375, 0.53271484375, 0.6025390625, 0.6826171875, 0.492431640625, 0.6396484375]

================================================================================
290_model.layers.24.self_attn.q_proj: Linear (model.layers.24.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.24.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000504, Std: 0.372201
     First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
     Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.008843, Std: 1.075763
     First 10: [-0.19828028976917267, 0.21985313296318054, -0.11545799672603607, -0.16001473367214203, 0.13438065350055695, -0.19457554817199707, -0.04535700008273125, -0.001008361577987671, -0.030495628714561462, -0.10290385782718658]
     Last 10:  [-0.47785496711730957, -1.8432698249816895, -1.4449994564056396, 0.6939730644226074, 1.4835745096206665, 0.27159950137138367, -0.9502145051956177, 0.9896942377090454, 0.8951778411865234, 0.4132666289806366]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000309
     First 10: [-0.0301055908203125, -0.0694580078125, -0.0308074951171875, 0.0460205078125, -0.028106689453125, 0.01236724853515625, 0.01873779296875, -0.0005173683166503906, 0.00079345703125, -0.0169219970703125]
     Last 10:  [0.04205322265625, -0.02972412109375, 0.0206146240234375, 0.072265625, 0.041717529296875, 0.03509521484375, 0.0435791015625, 0.02618408203125, 0.0574951171875, -0.0094146728515625]

================================================================================
291_model.layers.24.self_attn.k_proj: Linear (model.layers.24.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.24.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000504, Std: 0.372201
     First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
     Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.010645, Std: 1.181718
     First 10: [0.015383579768240452, -0.017470505088567734, -0.004446686245501041, 0.014349602162837982, 0.00897890329360962, 0.013567976653575897, 0.0016046389937400818, 0.009090833365917206, -0.0011606290936470032, -0.012900218367576599]
     Last 10:  [-6.41807746887207, -0.13302984833717346, -0.04055327549576759, -0.8856514692306519, 1.6969667673110962, 2.1277787685394287, 6.560985088348389, -2.6855905055999756, 0.024928748607635498, 0.02905094623565674]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000118
     First 10: [-0.01430511474609375, 0.00963592529296875, -0.0102081298828125, -0.11383056640625, -0.05145263671875, 0.01316070556640625, -0.0288543701171875, 0.0474853515625, 0.0841064453125, -0.00376129150390625]
     Last 10:  [-0.036102294921875, -0.09185791015625, -0.0487060546875, 0.0272369384765625, 0.0487060546875, -0.08599853515625, -0.01007843017578125, -0.035919189453125, 0.12017822265625, 0.031890869140625]

================================================================================
292_model.layers.24.self_attn.v_proj: Linear (model.layers.24.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.24.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000504, Std: 0.372201
     First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
     Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.001971, Std: 0.346682
     First 10: [-0.012662391178309917, 0.0010635113576427102, 0.001180274412035942, -0.004955079406499863, -0.013432323932647705, -0.0087968111038208, 0.025990135967731476, 0.0015112105756998062, 0.007911326363682747, 0.0035303710028529167]
     Last 10:  [0.11490588635206223, -0.29836198687553406, 0.44008973240852356, -0.009852483868598938, 0.008230469189584255, 0.46456316113471985, -0.16702018678188324, 0.3121829628944397, -0.8828476071357727, -0.1978970170021057]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000072
     First 10: [0.045318603515625, 0.036895751953125, -0.01448822021484375, 0.00405120849609375, 0.059600830078125, -0.006458282470703125, -0.0260772705078125, -0.03985595703125, -0.01549530029296875, -0.04034423828125]
     Last 10:  [-0.0222320556640625, 0.054779052734375, -0.00860595703125, -0.00553131103515625, 0.005977630615234375, -0.039398193359375, -0.01126861572265625, 0.0226287841796875, -0.073486328125, -0.002017974853515625]

================================================================================
293_model.layers.24.self_attn.o_proj: Linear (model.layers.24.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.24.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001293, Std: 0.041928
     First 10: [-0.012662391178309917, 0.0010635113576427102, 0.001180274412035942, -0.004955079406499863, -0.013432323932647705, -0.0087968111038208, 0.025990135967731476, 0.0015112105756998062, 0.007911326363682747, 0.0035303710028529167]
     Last 10:  [0.016313951462507248, -0.11438614130020142, -0.019928060472011566, -0.024368036538362503, 0.01062038354575634, 0.0442165732383728, -0.017899664118885994, 0.00136107939761132, -0.10151694715023041, -0.006969610694795847]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001010, Std: 0.035017
     First 10: [0.009150363504886627, 0.01459546759724617, 0.049958765506744385, 0.007941214367747307, -0.009602834470570087, -0.02898387424647808, 0.01245711650699377, -0.003556998446583748, 0.0050198836252093315, -0.02520291693508625]
     Last 10:  [-0.007110577076673508, -0.026778966188430786, -0.007922408170998096, -0.01079603098332882, 0.03251740336418152, 0.05992622300982475, -0.0009817946702241898, 0.026423141360282898, -0.0322178453207016, 0.00858977995812893]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000100
     First 10: [0.0850830078125, -0.10308837890625, -0.07196044921875, 0.016448974609375, 0.10870361328125, 0.038909912109375, 0.0087738037109375, -0.0592041015625, 0.059173583984375, 0.0242767333984375]
     Last 10:  [0.0477294921875, -0.03497314453125, -0.0016803741455078125, -0.08984375, -0.0517578125, 0.0308837890625, 0.05419921875, 0.07672119140625, -0.08123779296875, -0.045074462890625]

================================================================================
294_model.layers.24.self_attn: LlamaSdpaAttention (model.layers.24.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.24.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001010, Std: 0.035017
     First 10: [0.009150363504886627, 0.01459546759724617, 0.049958765506744385, 0.007941214367747307, -0.009602834470570087, -0.02898387424647808, 0.01245711650699377, -0.003556998446583748, 0.0050198836252093315, -0.02520291693508625]
     Last 10:  [-0.007110577076673508, -0.026778966188430786, -0.007922408170998096, -0.01079603098332882, 0.03251740336418152, 0.05992622300982475, -0.0009817946702241898, 0.026423141360282898, -0.0322178453207016, 0.00858977995812893]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.24.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.265362
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001118183135986, 0.4998881220817566, 0.0, 0.0, 0.0]
     Last 10:  [0.4846850633621216, 0.48368605971336365, 0.017937304452061653, 0.013691598549485207, 0.0, 0.4358241558074951, 0.4360167384147644, 0.042052559554576874, 0.04222005605697632, 0.043886519968509674]
     Zeros: 90, Total: 225

================================================================================
295_model.layers.24.post_attention_layernorm: LlamaRMSNorm (model.layers.24.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.24.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.082499, Std: 11.997826
     First 10: [-0.5313670635223389, -2.4993395805358887, -0.169684499502182, 0.44914793968200684, -11.490134239196777, -6.827062129974365, 4.595999240875244, -0.1485622078180313, -0.37389686703681946, -2.1576826572418213]
     Last 10:  [-0.13868194818496704, -0.19643951952457428, 0.9183390140533447, -0.9007233381271362, 0.18420088291168213, 0.48693832755088806, -0.3797001838684082, 0.07949283719062805, -0.058801013976335526, -0.6045868396759033]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003819, Std: 0.358905
     First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
     Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.479659
     First 10: [0.49462890625, 0.450927734375, 0.461669921875, 0.483154296875, 0.408935546875, 0.266357421875, 0.37890625, 0.463134765625, 0.49267578125, 0.49267578125]
     Last 10:  [0.494873046875, 0.49365234375, 0.48291015625, 0.48779296875, 0.475830078125, 0.48974609375, 0.51318359375, 0.47900390625, 0.490966796875, 0.46484375]

================================================================================
296_model.layers.24.mlp.gate_proj: Linear (model.layers.24.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.24.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003819, Std: 0.358905
     First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
     Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.202436, Std: 0.393278
     First 10: [0.016141436994075775, 0.737815260887146, -0.49998190999031067, -1.0092025995254517, -0.5337592959403992, 0.5319071412086487, -0.3162623643875122, 0.09798599779605865, -0.02206818014383316, 0.4006209373474121]
     Last 10:  [-0.5592445731163025, -0.4574618935585022, -0.018152162432670593, -0.3723921775817871, -0.5067622661590576, -0.6970515847206116, -0.24927160143852234, -1.013108253479004, -0.02153243124485016, -0.8761065006256104]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000015
     First 10: [0.040130615234375, -0.07159423828125, 0.055206298828125, 0.06097412109375, -0.01154327392578125, 0.0309906005859375, 0.11029052734375, -0.06036376953125, 0.044464111328125, 0.06396484375]
     Last 10:  [-0.01155853271484375, 0.125, -0.0037994384765625, 0.0460205078125, 0.057281494140625, 0.00010508298873901367, 0.0017490386962890625, -0.045806884765625, 0.0141754150390625, 0.0032634735107421875]

================================================================================
297_model.layers.24.mlp.act_fn: SiLU (model.layers.24.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.24.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.202436, Std: 0.393278
     First 10: [0.016141436994075775, 0.737815260887146, -0.49998190999031067, -1.0092025995254517, -0.5337592959403992, 0.5319071412086487, -0.3162623643875122, 0.09798599779605865, -0.02206818014383316, 0.4006209373474121]
     Last 10:  [-0.5592445731163025, -0.4574618935585022, -0.018152162432670593, -0.3723921775817871, -0.5067622661590576, -0.6970515847206116, -0.24927160143852234, -1.013108253479004, -0.02153243124485016, -0.8761065006256104]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.24.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.055078, Std: 0.165897
     First 10: [0.008135853335261345, 0.4991452693939209, -0.18876563012599945, -0.26959428191185, -0.197299063205719, 0.3350631296634674, -0.13333207368850708, 0.051391392946243286, -0.010912342928349972, 0.23990656435489655]
     Last 10:  [-0.20340970158576965, -0.17730678617954254, -0.008993708528578281, -0.15192227065563202, -0.19051869213581085, -0.2317461222410202, -0.10918164998292923, -0.26986366510391235, -0.010650308802723885, -0.25756239891052246]
     Zeros: 0, Total: 7680

================================================================================
298_model.layers.24.mlp.up_proj: Linear (model.layers.24.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.24.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003819, Std: 0.358905
     First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
     Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.004803, Std: 0.311171
     First 10: [-0.06243281811475754, -0.03320425748825073, 0.0640900582075119, -0.13079240918159485, -0.06173759326338768, 0.1487858146429062, 0.024105902761220932, 0.6352505087852478, -0.5493402481079102, -0.2756800353527069]
     Last 10:  [-0.32803279161453247, -0.5124449133872986, -0.04404153674840927, 0.3406355082988739, 0.23057328164577484, 0.20433661341667175, -0.0625760480761528, 0.38127774000167847, -0.13489696383476257, -0.18704092502593994]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000038
     First 10: [0.06591796875, -0.10272216796875, -0.1220703125, 0.0258026123046875, 0.00763702392578125, 0.0094451904296875, -0.042633056640625, 0.032806396484375, 0.038604736328125, -0.004848480224609375]
     Last 10:  [0.088623046875, -0.06365966796875, -0.0066070556640625, -0.016754150390625, -0.05120849609375, -0.020477294921875, -0.054931640625, -0.07623291015625, -0.01168060302734375, -0.09747314453125]

================================================================================
299_model.layers.24.mlp.down_proj: Linear (model.layers.24.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.24.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.000543, Std: 0.054245
     First 10: [-0.0005079442635178566, -0.016573747619986534, -0.012098000384867191, 0.0352608859539032, 0.012180768884718418, 0.0498526394367218, -0.003214089898392558, 0.032646410167217255, 0.005994589067995548, -0.06613744795322418]
     Last 10:  [0.06672505289316177, 0.09085995703935623, 0.0003960967587772757, -0.051750119775533676, -0.043928518891334534, -0.04735421761870384, 0.006832156330347061, -0.10289300978183746, 0.0014366942923516035, 0.04817470908164978]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.24.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000277, Std: 0.096120
     First 10: [0.035520002245903015, 0.022818995639681816, 0.044538166373968124, -0.04142330586910248, 0.05920391529798508, 0.16030186414718628, 0.05606876313686371, -0.09191669523715973, 0.010938156396150589, 0.020804349333047867]
     Last 10:  [0.025621481239795685, 0.0801917165517807, -0.13025996088981628, 0.15346160531044006, 0.07203134149312973, 0.0707186609506607, 0.1362040638923645, -0.0017171497456729412, 0.0033317506313323975, 0.06908836960792542]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000046
     First 10: [0.0019502639770507812, -0.0013875961303710938, 0.0119171142578125, 0.0153045654296875, 0.019744873046875, -0.0758056640625, -0.0009074211120605469, 0.035919189453125, 0.00662994384765625, -0.0054779052734375]
     Last 10:  [-0.0309600830078125, 0.0039215087890625, 0.00905609130859375, 0.038482666015625, -0.070556640625, 0.01910400390625, 0.061920166015625, 0.0650634765625, -0.03900146484375, -0.03363037109375]

================================================================================
300_model.layers.24.mlp: LlamaMLP (model.layers.24.mlp)
================================================================================

  → INPUT[0]: model.layers.24.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003819, Std: 0.358905
     First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
     Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.24.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000277, Std: 0.096120
     First 10: [0.035520002245903015, 0.022818995639681816, 0.044538166373968124, -0.04142330586910248, 0.05920391529798508, 0.16030186414718628, 0.05606876313686371, -0.09191669523715973, 0.010938156396150589, 0.020804349333047867]
     Last 10:  [0.025621481239795685, 0.0801917165517807, -0.13025996088981628, 0.15346160531044006, 0.07203134149312973, 0.0707186609506607, 0.1362040638923645, -0.0017171497456729412, 0.0033317506313323975, 0.06908836960792542]
     Zeros: 0, Total: 2880

================================================================================
301_model.layers.25.input_layernorm: LlamaRMSNorm (model.layers.25.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.25.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.082222, Std: 11.978249
     First 10: [-0.49584704637527466, -2.476520538330078, -0.12514632940292358, 0.40772461891174316, -11.430930137634277, -6.666760444641113, 4.652068138122559, -0.24047890305519104, -0.362958699464798, -2.136878252029419]
     Last 10:  [-0.11306046694517136, -0.11624780297279358, 0.788079023361206, -0.7472617626190186, 0.25623223185539246, 0.55765700340271, -0.2434961199760437, 0.07777568697929382, -0.05546926334500313, -0.5354984998703003]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000666, Std: 0.381757
     First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
     Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.563047
     First 10: [0.52685546875, 0.712890625, 0.693359375, 0.62744140625, 0.51025390625, 1.12109375, 0.80029296875, 0.6962890625, 0.55908203125, 0.7587890625]
     Last 10:  [0.45703125, 0.572265625, 0.60205078125, 0.5224609375, 0.467529296875, 0.55126953125, 0.56982421875, 0.666015625, 0.51416015625, 0.69140625]

================================================================================
302_model.layers.25.self_attn.q_proj: Linear (model.layers.25.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.25.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000666, Std: 0.381757
     First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
     Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001547, Std: 1.141639
     First 10: [-0.4261331856250763, 0.022199584171175957, -0.12311774492263794, 0.3504631519317627, 0.17115673422813416, 0.8101059794425964, 0.9295114278793335, -0.560868501663208, 0.29431062936782837, -0.2072330117225647]
     Last 10:  [1.6453437805175781, -0.6023577451705933, 2.08278751373291, -2.7431681156158447, -0.6756157279014587, 0.6408123970031738, -0.4929950535297394, 0.16256083548069, -1.7509284019470215, 0.29147717356681824]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000124
     First 10: [0.01105499267578125, -0.017669677734375, -0.038482666015625, -0.042816162109375, 0.0469970703125, 0.075927734375, -0.0167083740234375, 0.0587158203125, -0.027191162109375, 0.00011807680130004883]
     Last 10:  [-0.0262451171875, 0.05621337890625, -0.1746826171875, -0.052459716796875, 0.009307861328125, 0.10662841796875, -0.0908203125, -0.0970458984375, -0.107666015625, 0.1187744140625]

================================================================================
303_model.layers.25.self_attn.k_proj: Linear (model.layers.25.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.25.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000666, Std: 0.381757
     First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
     Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.016736, Std: 1.203004
     First 10: [0.007772316690534353, 0.01077929139137268, 0.0017339438199996948, 0.008605072274804115, -0.011484041810035706, 0.002602696418762207, 0.003750041127204895, 0.015081807971000671, -0.0021987035870552063, 0.020899023860692978]
     Last 10:  [-0.06104101240634918, -2.9472105503082275, 2.0914993286132812, -2.3398914337158203, -3.185286521911621, -1.6877696514129639, -0.6444486379623413, 2.022949457168579, 1.5233832597732544, -0.7443884611129761]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000140
     First 10: [0.0233612060546875, 0.014434814453125, 0.0266265869140625, 0.0019931793212890625, -0.0657958984375, 0.1190185546875, -0.01314544677734375, -0.01401519775390625, -0.02947998046875, 0.07781982421875]
     Last 10:  [0.01412200927734375, 0.00618743896484375, 0.06427001953125, 0.0323486328125, 0.0460205078125, -0.0030193328857421875, 0.0321044921875, 0.033843994140625, -0.008331298828125, -0.0024318695068359375]

================================================================================
304_model.layers.25.self_attn.v_proj: Linear (model.layers.25.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.25.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000666, Std: 0.381757
     First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
     Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.017579, Std: 0.277501
     First 10: [-0.0016400497406721115, 0.013595083728432655, -0.009687155485153198, -0.007029416039586067, -0.0005712909623980522, 0.013242900371551514, -0.005197100341320038, -0.008372807875275612, 0.007671581115573645, 0.0024570776149630547]
     Last 10:  [-0.020114701241254807, -0.29736828804016113, 0.45363229513168335, -0.1583147644996643, -0.20929983258247375, 0.6248188614845276, -0.08766117691993713, 0.15424302220344543, 0.5982751250267029, -0.13916878402233124]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000054
     First 10: [0.0152130126953125, -0.033660888671875, 0.037139892578125, -0.051666259765625, -0.02850341796875, -0.0017766952514648438, 0.018524169921875, -0.017181396484375, 0.093994140625, -0.004550933837890625]
     Last 10:  [-0.038543701171875, -0.055206298828125, 0.00698089599609375, -0.10638427734375, -0.0009026527404785156, 0.052490234375, 0.034210205078125, 0.007049560546875, 0.018646240234375, -0.02435302734375]

================================================================================
305_model.layers.25.self_attn.o_proj: Linear (model.layers.25.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.25.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.005277, Std: 0.056906
     First 10: [-0.0016400497406721115, 0.013595083728432655, -0.009687155485153198, -0.007029416039586067, -0.0005712909623980522, 0.013242900371551514, -0.005197100341320038, -0.008372807875275612, 0.007671581115573645, 0.0024570776149630547]
     Last 10:  [-0.0016340846195816994, -0.006946050561964512, -0.01437340583652258, -0.04054759442806244, -0.017665838822722435, 0.036401551216840744, 0.027456242591142654, 0.01905916817486286, 0.03235683962702751, -0.002345101675018668]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000867, Std: 0.054672
     First 10: [0.09278392791748047, -0.03881445527076721, -0.03408059850335121, -0.0062532005831599236, -0.05229702219367027, -0.05254851654171944, 0.03209899365901947, -0.05480077862739563, -0.040882449597120285, -0.015055214986205101]
     Last 10:  [-0.04049097001552582, -0.02880406752228737, 0.027155382558703423, -0.012123499996960163, 0.041778918355703354, -0.05336150527000427, 0.0019043106585741043, 0.03547871857881546, -0.012475385330617428, -0.0745677500963211]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000045
     First 10: [0.01262664794921875, 0.055908203125, -0.019927978515625, -0.006687164306640625, 0.0051116943359375, -0.035125732421875, 0.005001068115234375, -0.09625244140625, -0.0855712890625, -0.093505859375]
     Last 10:  [-0.01168060302734375, 0.036041259765625, -0.036712646484375, 0.045196533203125, 0.0284576416015625, 0.04473876953125, 0.0810546875, 0.05145263671875, -0.03753662109375, 0.0030612945556640625]

================================================================================
306_model.layers.25.self_attn: LlamaSdpaAttention (model.layers.25.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.25.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000867, Std: 0.054672
     First 10: [0.09278392791748047, -0.03881445527076721, -0.03408059850335121, -0.0062532005831599236, -0.05229702219367027, -0.05254851654171944, 0.03209899365901947, -0.05480077862739563, -0.040882449597120285, -0.015055214986205101]
     Last 10:  [-0.04049097001552582, -0.02880406752228737, 0.027155382558703423, -0.012123499996960163, 0.041778918355703354, -0.05336150527000427, 0.0019043106585741043, 0.03547871857881546, -0.012475385330617428, -0.0745677500963211]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.25.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.264015
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001692771911621, 0.4998306930065155, 0.0, 0.0, 0.0]
     Last 10:  [0.47128477692604065, 0.4710506796836853, 0.04585563763976097, 0.011808880604803562, 0.0, 0.44682812690734863, 0.4461047351360321, 0.06272093206644058, 0.015955068171024323, 0.028391094878315926]
     Zeros: 90, Total: 225

================================================================================
307_model.layers.25.post_attention_layernorm: LlamaRMSNorm (model.layers.25.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.25.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.083089, Std: 11.985290
     First 10: [-0.4030631184577942, -2.5153350830078125, -0.1592269241809845, 0.40147140622138977, -11.483226776123047, -6.719308853149414, 4.68416690826416, -0.29527968168258667, -0.40384113788604736, -2.151933431625366]
     Last 10:  [-0.15355142951011658, -0.14505186676979065, 0.8152344226837158, -0.7593852877616882, 0.2980111539363861, 0.5042954683303833, -0.24159181118011475, 0.11325440555810928, -0.06794464588165283, -0.6100662350654602]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002872, Std: 0.359445
     First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
     Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.494041
     First 10: [0.505859375, 0.47802734375, 0.4912109375, 0.5107421875, 0.419677734375, 0.2919921875, 0.4111328125, 0.474365234375, 0.49365234375, 0.494384765625]
     Last 10:  [0.5234375, 0.5068359375, 0.50830078125, 0.5107421875, 0.493896484375, 0.53369140625, 0.52099609375, 0.50537109375, 0.51123046875, 0.469970703125]

================================================================================
308_model.layers.25.mlp.gate_proj: Linear (model.layers.25.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.25.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002872, Std: 0.359445
     First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
     Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.219452, Std: 0.402099
     First 10: [0.1971726268529892, -0.19388937950134277, -0.06389018893241882, 0.15800416469573975, -0.1179538369178772, -1.2225337028503418, -0.2434474527835846, -0.6076571941375732, -0.12232544273138046, 0.3323368430137634]
     Last 10:  [-0.5415199995040894, -0.08415818214416504, -0.11723829805850983, -0.40189647674560547, -0.21628065407276154, -0.48351824283599854, -0.7556737065315247, -0.6128374338150024, -0.4286273717880249, -0.14685137569904327]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000084
     First 10: [0.0290985107421875, -0.031005859375, 0.1339111328125, 0.01094818115234375, 0.0293426513671875, -0.040771484375, 0.0240478515625, 0.027557373046875, 0.031982421875, -0.0264434814453125]
     Last 10:  [0.01119232177734375, 0.01751708984375, -0.0025959014892578125, 0.0085296630859375, -0.0136260986328125, -0.06298828125, -0.00847625732421875, 0.0626220703125, -0.0282440185546875, 0.041290283203125]

================================================================================
309_model.layers.25.mlp.act_fn: SiLU (model.layers.25.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.25.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.219452, Std: 0.402099
     First 10: [0.1971726268529892, -0.19388937950134277, -0.06389018893241882, 0.15800416469573975, -0.1179538369178772, -1.2225337028503418, -0.2434474527835846, -0.6076571941375732, -0.12232544273138046, 0.3323368430137634]
     Last 10:  [-0.5415199995040894, -0.08415818214416504, -0.11723829805850983, -0.40189647674560547, -0.21628065407276154, -0.48351824283599854, -0.7556737065315247, -0.6128374338150024, -0.4286273717880249, -0.14685137569904327]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.25.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.060265, Std: 0.170447
     First 10: [0.10827421396970749, -0.0875757485628128, -0.030924955382943153, 0.08523046225309372, -0.055502668023109436, -0.27811527252197266, -0.10697980970144272, -0.2142561674118042, -0.05742650479078293, 0.19352899491786957]
     Last 10:  [-0.19918949902057648, -0.04030948504805565, -0.05518687516450882, -0.16110292077064514, -0.09649136662483215, -0.18442434072494507, -0.24150294065475464, -0.21535855531692505, -0.16907384991645813, -0.0680440291762352]
     Zeros: 0, Total: 7680

================================================================================
310_model.layers.25.mlp.up_proj: Linear (model.layers.25.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.25.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002872, Std: 0.359445
     First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
     Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000159, Std: 0.317446
     First 10: [0.07091669738292694, -0.11660479009151459, 0.42214035987854004, -0.2542322874069214, 0.039477813988924026, -0.37491315603256226, 0.019831696525216103, 0.14064109325408936, -0.10694609582424164, -0.07383676618337631]
     Last 10:  [0.6308544874191284, -0.37563756108283997, -0.04934529960155487, -0.42159026861190796, 0.09808884561061859, -0.16135691106319427, -0.10801555216312408, -0.17016498744487762, -0.04450578987598419, 0.16235703229904175]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000052
     First 10: [-0.0211029052734375, 0.0199127197265625, -0.0192718505859375, 0.01407623291015625, -0.01100921630859375, -0.054962158203125, -0.08544921875, 0.0382080078125, -0.0008730888366699219, -0.0670166015625]
     Last 10:  [-0.08807373046875, -0.0214691162109375, 0.016571044921875, -0.046661376953125, 0.0670166015625, 0.06048583984375, -0.05352783203125, 0.03826904296875, -0.0172882080078125, -0.042633056640625]

================================================================================
311_model.layers.25.mlp.down_proj: Linear (model.layers.25.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.25.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000443, Std: 0.061079
     First 10: [0.007678449619561434, 0.01021175179630518, -0.013054671697318554, -0.021668335422873497, -0.0021911240182816982, 0.10426907241344452, -0.0021215910091996193, -0.030133221298456192, 0.006141540594398975, -0.01428955513983965]
     Last 10:  [-0.12565958499908447, 0.015141756273806095, 0.002723212819546461, 0.06791942566633224, -0.009464726783335209, 0.029758142307400703, 0.026086073368787766, 0.036646485328674316, 0.007524765096604824, -0.011047426611185074]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.25.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002428, Std: 0.110176
     First 10: [0.08096560090780258, 0.012979015707969666, 0.006341487169265747, -0.026800407096743584, 0.062232568860054016, 0.10063852369785309, 0.02401314117014408, -0.030360879376530647, 0.01913442835211754, 0.212269589304924]
     Last 10:  [-0.13092075288295746, -0.014723449945449829, -0.1526506543159485, -0.023363303393125534, -0.09001479297876358, 0.14186719059944153, -0.1713767647743225, -0.06416232138872147, -0.013386063277721405, 0.003587535582482815]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000092
     First 10: [0.0283966064453125, 0.06683349609375, 0.011810302734375, 0.05657958984375, 0.0186920166015625, -0.0285797119140625, 0.019989013671875, 0.11676025390625, -0.05194091796875, -0.03265380859375]
     Last 10:  [-0.076171875, -0.0435791015625, 0.00537109375, -0.0179901123046875, 0.0294036865234375, 0.03277587890625, -0.031585693359375, 0.0693359375, 0.0027217864990234375, 0.0060882568359375]

================================================================================
312_model.layers.25.mlp: LlamaMLP (model.layers.25.mlp)
================================================================================

  → INPUT[0]: model.layers.25.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.002872, Std: 0.359445
     First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
     Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.25.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002428, Std: 0.110176
     First 10: [0.08096560090780258, 0.012979015707969666, 0.006341487169265747, -0.026800407096743584, 0.062232568860054016, 0.10063852369785309, 0.02401314117014408, -0.030360879376530647, 0.01913442835211754, 0.212269589304924]
     Last 10:  [-0.13092075288295746, -0.014723449945449829, -0.1526506543159485, -0.023363303393125534, -0.09001479297876358, 0.14186719059944153, -0.1713767647743225, -0.06416232138872147, -0.013386063277721405, 0.003587535582482815]
     Zeros: 0, Total: 2880

================================================================================
313_model.layers.26.input_layernorm: LlamaRMSNorm (model.layers.26.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.26.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.085517, Std: 11.965765
     First 10: [-0.322097510099411, -2.5023560523986816, -0.15288543701171875, 0.37467101216316223, -11.42099380493164, -6.618670463562012, 4.708179950714111, -0.32564055919647217, -0.3847067058086395, -1.9396638870239258]
     Last 10:  [-0.28447216749191284, -0.15977531671524048, 0.6625837683677673, -0.7827485799789429, 0.20799636840820312, 0.6461626291275024, -0.41296857595443726, 0.04909208416938782, -0.08133070915937424, -0.6064786911010742]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003127, Std: 0.379457
     First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
     Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.565114
     First 10: [0.51611328125, 0.70458984375, 0.7265625, 0.66064453125, 0.49169921875, 1.16015625, 0.77685546875, 0.68798828125, 0.580078125, 0.755859375]
     Last 10:  [0.46826171875, 0.560546875, 0.60498046875, 0.487548828125, 0.513671875, 0.537109375, 0.55078125, 0.6669921875, 0.5224609375, 0.6904296875]

================================================================================
314_model.layers.26.self_attn.q_proj: Linear (model.layers.26.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.26.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003127, Std: 0.379457
     First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
     Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.016609, Std: 1.112635
     First 10: [-0.021311167627573013, 0.22213736176490784, -0.4895819425582886, 0.10150492191314697, -0.553885281085968, -0.7210612893104553, 0.008784223347902298, -0.3497462868690491, 0.2059897780418396, -0.9525598287582397]
     Last 10:  [-0.7325423955917358, -1.0361206531524658, -0.41487154364585876, 0.2520964443683624, -0.725663959980011, 1.922345757484436, 3.227874279022217, -2.7098734378814697, -4.9439287185668945, 1.5238721370697021]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000033
     First 10: [-0.052947998046875, 0.1031494140625, -0.05902099609375, -0.08831787109375, 0.06256103515625, -0.07177734375, 0.001621246337890625, -0.0711669921875, 0.034393310546875, 0.0270233154296875]
     Last 10:  [-0.0014553070068359375, 0.071533203125, -0.084716796875, -0.035430908203125, 0.05780029296875, -0.00316619873046875, 0.045013427734375, 0.044952392578125, 0.00623321533203125, 0.0826416015625]

================================================================================
315_model.layers.26.self_attn.k_proj: Linear (model.layers.26.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.26.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003127, Std: 0.379457
     First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
     Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.012143, Std: 1.279046
     First 10: [0.005120024085044861, 0.0022451579570770264, -0.0007396042346954346, 0.0080175269395113, -0.010106161236763, 0.015942241996526718, -0.011179491877555847, 0.0036654025316238403, 0.0007702205330133438, -0.026198625564575195]
     Last 10:  [-1.2997297048568726, -0.8487428426742554, 8.742570877075195, 1.1087367534637451, -0.9479537010192871, 0.5693605542182922, -0.6856707334518433, -1.5405899286270142, 2.2890586853027344, -1.3717763423919678]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000123
     First 10: [-0.0125885009765625, -0.083740234375, -0.0198822021484375, 0.02191162109375, 0.01416778564453125, -0.078125, 0.02569580078125, -0.002185821533203125, 0.042327880859375, -0.0181427001953125]
     Last 10:  [0.013214111328125, 0.00011938810348510742, 0.09332275390625, -0.0234222412109375, -0.000858306884765625, 0.052154541015625, 0.048004150390625, 0.04180908203125, 0.0208282470703125, -0.0673828125]

================================================================================
316_model.layers.26.self_attn.v_proj: Linear (model.layers.26.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.26.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003127, Std: 0.379457
     First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
     Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.023115, Std: 0.281833
     First 10: [-0.2879851460456848, -0.006393730640411377, 0.006837252527475357, 0.01450863853096962, -0.0017932765185832977, -0.017116744071245193, -0.014376552775502205, 0.0003307778388261795, 0.005851795896887779, -0.004778750240802765]
     Last 10:  [-0.005627982318401337, 0.44351285696029663, -0.35582131147384644, -0.0038735121488571167, 0.7844586372375488, 0.2800220847129822, 0.09418714791536331, -0.6016528010368347, 0.25983425974845886, 0.49603790044784546]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000039
     First 10: [0.018585205078125, 0.0235748291015625, -0.00026702880859375, 0.035400390625, 0.01558685302734375, 0.036834716796875, 0.0222320556640625, -0.004680633544921875, -0.035919189453125, 0.0128631591796875]
     Last 10:  [-0.002254486083984375, -0.0020771026611328125, 0.050262451171875, -0.0235595703125, -0.042236328125, 0.020263671875, 0.03436279296875, -0.049041748046875, 0.007808685302734375, 0.044769287109375]

================================================================================
317_model.layers.26.self_attn.o_proj: Linear (model.layers.26.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.26.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.005200, Std: 0.095826
     First 10: [-0.2879851460456848, -0.006393730640411377, 0.006837252527475357, 0.01450863853096962, -0.0017932765185832977, -0.017116744071245193, -0.014376552775502205, 0.0003307778388261795, 0.005851795896887779, -0.004778750240802765]
     Last 10:  [-0.0035519334487617016, 0.02178470604121685, -0.0009402817231602967, 0.004095249343663454, 0.041731059551239014, 0.02264028787612915, 0.004501557908952236, -0.029926780611276627, 0.01218757126480341, 0.03811829909682274]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003122, Std: 0.097781
     First 10: [-0.013690924271941185, -0.0008350880816578865, 0.020384829491376877, -0.001758501399308443, -0.0020978241227567196, -0.061941634863615036, 0.007750852964818478, -0.014810938388109207, 0.009858159348368645, -0.031658802181482315]
     Last 10:  [0.05925004556775093, -0.07908536493778229, 0.3524857759475708, -0.07276730239391327, 0.08172174543142319, -0.037860944867134094, -0.009830759838223457, -0.0029500704258680344, -0.19947470724582672, -0.21566785871982574]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000072
     First 10: [-0.019012451171875, 0.066650390625, -0.037841796875, 0.0748291015625, -0.08660888671875, 0.0421142578125, 0.034149169921875, -0.009979248046875, 0.07611083984375, 0.033050537109375]
     Last 10:  [-0.01763916015625, -0.035003662109375, 0.058380126953125, -0.0162200927734375, 0.00040268898010253906, -0.04620361328125, 0.02557373046875, 0.0242767333984375, -0.0154876708984375, 0.03594970703125]

================================================================================
318_model.layers.26.self_attn: LlamaSdpaAttention (model.layers.26.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.26.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.003122, Std: 0.097781
     First 10: [-0.013690924271941185, -0.0008350880816578865, 0.020384829491376877, -0.001758501399308443, -0.0020978241227567196, -0.061941634863615036, 0.007750852964818478, -0.014810938388109207, 0.009858159348368645, -0.031658802181482315]
     Last 10:  [0.05925004556775093, -0.07908536493778229, 0.3524857759475708, -0.07276730239391327, 0.08172174543142319, -0.037860944867134094, -0.009830759838223457, -0.0029500704258680344, -0.19947470724582672, -0.21566785871982574]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.26.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.266404
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000210404396057, 0.4999789893627167, 0.0, 0.0, 0.0]
     Last 10:  [0.4793320596218109, 0.47887924313545227, 0.026855159550905228, 0.01493354793637991, 0.0, 0.4678178131580353, 0.46807965636253357, 0.018439246341586113, 0.007677636109292507, 0.03798556327819824]
     Zeros: 90, Total: 225

================================================================================
319_model.layers.26.post_attention_layernorm: LlamaRMSNorm (model.layers.26.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.26.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.082395, Std: 11.968935
     First 10: [-0.33578842878341675, -2.5031912326812744, -0.13250060379505157, 0.3729124963283539, -11.423091888427734, -6.680612087249756, 4.715930938720703, -0.34045150876045227, -0.37484854459762573, -1.9713226556777954]
     Last 10:  [-0.2252221256494522, -0.23886068165302277, 1.0150694847106934, -0.8555158972740173, 0.2897181212902069, 0.6083016991615295, -0.42279934883117676, 0.046142011880874634, -0.28080540895462036, -0.8221465349197388]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001887, Std: 0.389902
     First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
     Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.510730
     First 10: [0.51416015625, 0.5029296875, 0.499755859375, 0.517578125, 0.426513671875, 0.33447265625, 0.44775390625, 0.5126953125, 0.52099609375, 0.5146484375]
     Last 10:  [0.51708984375, 0.51416015625, 0.517578125, 0.513671875, 0.498046875, 0.52294921875, 0.5283203125, 0.517578125, 0.50537109375, 0.49267578125]

================================================================================
320_model.layers.26.mlp.gate_proj: Linear (model.layers.26.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.26.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001887, Std: 0.389902
     First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
     Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.247667, Std: 0.418645
     First 10: [-0.38888368010520935, -1.3792853355407715, 0.08825710415840149, 0.17754776775836945, -0.05420788750052452, -0.7204705476760864, 0.05992819741368294, 0.7282297015190125, -0.631008505821228, -0.9205172061920166]
     Last 10:  [-0.12042336165904999, -0.331166535615921, -0.9038124680519104, -0.23281939327716827, -0.5538129806518555, 0.6899538040161133, -0.7292208075523376, -0.5827463269233704, -0.10384413599967957, -0.9003647565841675]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000069
     First 10: [-0.037567138671875, -0.0288238525390625, 0.060455322265625, 0.0119781494140625, 0.031982421875, 0.01007843017578125, 0.0297698974609375, 0.01451873779296875, -0.0218048095703125, -0.01123809814453125]
     Last 10:  [0.01494598388671875, 0.009490966796875, -0.0399169921875, -0.09515380859375, 0.0301666259765625, -0.039154052734375, -0.0301361083984375, -0.016387939453125, -0.004131317138671875, -0.034576416015625]

================================================================================
321_model.layers.26.mlp.act_fn: SiLU (model.layers.26.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.26.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.247667, Std: 0.418645
     First 10: [-0.38888368010520935, -1.3792853355407715, 0.08825710415840149, 0.17754776775836945, -0.05420788750052452, -0.7204705476760864, 0.05992819741368294, 0.7282297015190125, -0.631008505821228, -0.9205172061920166]
     Last 10:  [-0.12042336165904999, -0.331166535615921, -0.9038124680519104, -0.23281939327716827, -0.5538129806518555, 0.6899538040161133, -0.7292208075523376, -0.5827463269233704, -0.10384413599967957, -0.9003647565841675]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.26.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.068820, Std: 0.173378
     First 10: [-0.15710358321666718, -0.27740710973739624, 0.04607461765408516, 0.09663404524326324, -0.026369499042630196, -0.23580235242843628, 0.03086167760193348, 0.4911302626132965, -0.21913783252239227, -0.2622116506099701]
     Last 10:  [-0.05659060925245285, -0.13841331005096436, -0.2605399191379547, -0.10291935503482819, -0.20213079452514648, 0.45947930216789246, -0.23726347088813782, -0.20879852771759033, -0.04922858998179436, -0.2601833939552307]
     Zeros: 0, Total: 7680

================================================================================
322_model.layers.26.mlp.up_proj: Linear (model.layers.26.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.26.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001887, Std: 0.389902
     First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
     Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.009516, Std: 0.354890
     First 10: [-0.2236481010913849, 0.019144611433148384, -0.7300968170166016, 0.11007511615753174, -0.08696450293064117, -0.028518103063106537, 0.36674216389656067, -0.27103719115257263, -0.31252819299697876, -0.13719624280929565]
     Last 10:  [0.057018741965293884, -0.5966054201126099, -0.14395949244499207, 0.6542375683784485, -0.29571178555488586, -0.16868045926094055, -0.3908184766769409, -0.2766566276550293, -0.2813835144042969, 0.3547680377960205]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000038
     First 10: [0.06866455078125, 0.0302581787109375, 0.0038471221923828125, -0.04412841796875, 0.01934814453125, 0.031768798828125, 0.027435302734375, -0.016571044921875, -0.031890869140625, -0.0654296875]
     Last 10:  [-0.0089874267578125, 0.07012939453125, -0.002147674560546875, 0.03167724609375, 0.08343505859375, 0.06732177734375, -0.04449462890625, 0.062469482421875, 0.01763916015625, -0.041534423828125]

================================================================================
323_model.layers.26.mlp.down_proj: Linear (model.layers.26.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.26.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000334, Std: 0.066184
     First 10: [0.035135917365550995, -0.005310851149260998, -0.033638931810855865, 0.010637003928422928, 0.0022932104766368866, 0.0067246356047689915, 0.01131827849894762, -0.13311456143856049, 0.06848675012588501, 0.03597445413470268]
     Last 10:  [-0.00322672538459301, 0.0825781300663948, 0.03750719502568245, -0.06733370572328568, 0.05977245792746544, -0.07750517874956131, 0.0927269458770752, 0.05776549503207207, 0.013852113857865334, -0.09230475127696991]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.26.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003252, Std: 0.111876
     First 10: [0.0575503334403038, 0.01184755191206932, -0.05154011398553848, -0.044469334185123444, 0.09567449241876602, 0.04824741557240486, -0.013508006930351257, -0.11702915281057358, -0.04365560784935951, -0.025028301402926445]
     Last 10:  [0.0873878002166748, -0.08558624982833862, -0.27438968420028687, 0.055550336837768555, 0.10353484004735947, -0.1279650330543518, -0.18676774203777313, 0.025006741285324097, 0.02349427342414856, -0.06937399506568909]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000123
     First 10: [-0.033599853515625, 0.045928955078125, 0.0231475830078125, -0.03192138671875, 0.0287628173828125, -0.0287322998046875, -0.024658203125, -0.0029048919677734375, -0.037750244140625, 0.0102386474609375]
     Last 10:  [-0.0207366943359375, -0.019317626953125, -0.118408203125, 0.038482666015625, -0.0283660888671875, -0.01416778564453125, 0.00732421875, 0.0318603515625, -0.04974365234375, 0.0281524658203125]

================================================================================
324_model.layers.26.mlp: LlamaMLP (model.layers.26.mlp)
================================================================================

  → INPUT[0]: model.layers.26.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001887, Std: 0.389902
     First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
     Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.26.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003252, Std: 0.111876
     First 10: [0.0575503334403038, 0.01184755191206932, -0.05154011398553848, -0.044469334185123444, 0.09567449241876602, 0.04824741557240486, -0.013508006930351257, -0.11702915281057358, -0.04365560784935951, -0.025028301402926445]
     Last 10:  [0.0873878002166748, -0.08558624982833862, -0.27438968420028687, 0.055550336837768555, 0.10353484004735947, -0.1279650330543518, -0.18676774203777313, 0.025006741285324097, 0.02349427342414856, -0.06937399506568909]
     Zeros: 0, Total: 2880

================================================================================
325_model.layers.27.input_layernorm: LlamaRMSNorm (model.layers.27.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.27.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.085647, Std: 11.952164
     First 10: [-0.27823808789253235, -2.4913437366485596, -0.18404072523117065, 0.32844316959381104, -11.327417373657227, -6.632364749908447, 4.702423095703125, -0.45748066902160645, -0.41850414872169495, -1.9963510036468506]
     Last 10:  [-0.1378343254327774, -0.3244469165802002, 0.7406798005104065, -0.7999655604362488, 0.393252968788147, 0.48033666610717773, -0.6095671057701111, 0.07114875316619873, -0.2573111355304718, -0.8915205001831055]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002023, Std: 0.363523
     First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
     Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.553649
     First 10: [0.487548828125, 0.63232421875, 0.65869140625, 0.587890625, 0.435302734375, 1.04296875, 0.759765625, 0.60107421875, 0.5380859375, 0.6943359375]
     Last 10:  [0.50146484375, 0.52783203125, 0.5849609375, 0.52392578125, 0.50439453125, 0.55908203125, 0.56689453125, 0.58349609375, 0.50830078125, 0.6201171875]

================================================================================
326_model.layers.27.self_attn.q_proj: Linear (model.layers.27.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.27.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002023, Std: 0.363523
     First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
     Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.114633, Std: 1.183905
     First 10: [0.9597833752632141, -0.6753027439117432, -0.6546114683151245, 0.9736368060112, 0.7561737298965454, 1.0348362922668457, 0.9168868064880371, -0.47723090648651123, -0.8250890970230103, -0.5598512887954712]
     Last 10:  [0.3173826336860657, 0.7274330854415894, 0.44367632269859314, -0.8808079957962036, 0.9024897813796997, 0.23321092128753662, -1.631101131439209, -1.7187466621398926, -1.1417286396026611, 2.1076574325561523]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000154
     First 10: [0.0250244140625, -0.035736083984375, 0.025390625, 0.038726806640625, 0.049224853515625, -0.0213775634765625, 0.1531982421875, 0.0362548828125, 0.03369140625, 0.029144287109375]
     Last 10:  [0.07537841796875, 0.02545166015625, -0.002574920654296875, 0.0245819091796875, 0.058807373046875, -0.0279083251953125, -0.0010080337524414062, 0.035614013671875, -0.02056884765625, -0.0311126708984375]

================================================================================
327_model.layers.27.self_attn.k_proj: Linear (model.layers.27.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.27.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002023, Std: 0.363523
     First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
     Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.076987, Std: 1.231408
     First 10: [0.007489114999771118, -0.030970722436904907, -0.009985923767089844, -0.00609879195690155, 0.015028825029730797, -0.003424830734729767, 0.012594595551490784, -0.01508418470621109, 0.0012328624725341797, 0.0043955594301223755]
     Last 10:  [1.3112461566925049, -8.007617950439453, 1.4307982921600342, 2.168120861053467, -0.22792783379554749, -0.983596920967102, -0.16879892349243164, 1.7389380931854248, -1.717474341392517, -0.2386631965637207]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000147
     First 10: [-0.0226287841796875, 0.0035648345947265625, 0.041534423828125, 0.01219940185546875, -0.0015649795532226562, -0.0928955078125, -0.04833984375, -0.0195770263671875, -0.0455322265625, -0.00926971435546875]
     Last 10:  [0.07574462890625, -0.022430419921875, 0.10467529296875, -0.04583740234375, 0.0273895263671875, 0.036102294921875, -0.0194244384765625, 0.0113525390625, -0.045806884765625, 0.00366973876953125]

================================================================================
328_model.layers.27.self_attn.v_proj: Linear (model.layers.27.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.27.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002023, Std: 0.363523
     First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
     Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.018923, Std: 0.294029
     First 10: [0.00789564661681652, -0.024062111973762512, -0.012917366810142994, -0.02527754381299019, 0.004070125054568052, 0.0023087509907782078, 0.002987094223499298, 0.0062078190967440605, -0.007357854396104813, -0.009961077012121677]
     Last 10:  [1.1650665998458862, 0.4366189241409302, -0.3104100525379181, 0.34271952509880066, -0.0005662739276885986, -0.018488243222236633, -0.4691067039966583, -0.0072876811027526855, 0.3638358414173126, -0.1827058345079422]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000015
     First 10: [-0.050018310546875, 0.05181884765625, -0.036041259765625, -0.0016155242919921875, 0.007415771484375, 0.0036983489990234375, 0.08929443359375, -0.08111572265625, 0.00231170654296875, -0.01149749755859375]
     Last 10:  [-0.04840087890625, -0.038055419921875, 0.053070068359375, -0.0281829833984375, 0.040740966796875, -0.0780029296875, 0.0712890625, 0.0248260498046875, 0.004047393798828125, -0.04351806640625]

================================================================================
329_model.layers.27.self_attn.o_proj: Linear (model.layers.27.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.27.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.008217, Std: 0.105770
     First 10: [0.00789564661681652, -0.024062111973762512, -0.012917366810142994, -0.02527754381299019, 0.004070125054568052, 0.0023087509907782078, 0.002987094223499298, 0.0062078190967440605, -0.007357854396104813, -0.009961077012121677]
     Last 10:  [0.06748168915510178, 0.01946718618273735, -0.016012661159038544, -0.00019510694255586714, -0.016799969598650932, 0.01139384601265192, -0.04447246715426445, -0.028189679607748985, -0.011893120594322681, -0.027149582281708717]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010562, Std: 0.137762
     First 10: [0.007923866622149944, 0.011928820982575417, -0.05008391663432121, -0.09599346667528152, -0.11439596861600876, -0.15517084300518036, 0.039772916585206985, -0.01264312956482172, 0.04029567167162895, -0.02343432419002056]
     Last 10:  [0.07478094100952148, 0.07667869329452515, 0.31073683500289917, -0.016903679817914963, 0.006944466382265091, -0.13665689527988434, -0.17881113290786743, 0.2652333676815033, -0.05741901323199272, -0.08582337945699692]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000081
     First 10: [-0.048187255859375, -0.00568389892578125, -0.11212158203125, 0.018524169921875, -0.001811981201171875, 0.0284271240234375, -0.0211181640625, -0.058135986328125, 0.0207366943359375, 0.040374755859375]
     Last 10:  [-0.0615234375, 0.0148773193359375, 0.040008544921875, -0.0113677978515625, 0.00429534912109375, 0.0195770263671875, 0.09954833984375, 0.029205322265625, -0.041534423828125, 0.053741455078125]

================================================================================
330_model.layers.27.self_attn: LlamaSdpaAttention (model.layers.27.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.27.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.010562, Std: 0.137762
     First 10: [0.007923866622149944, 0.011928820982575417, -0.05008391663432121, -0.09599346667528152, -0.11439596861600876, -0.15517084300518036, 0.039772916585206985, -0.01264312956482172, 0.04029567167162895, -0.02343432419002056]
     Last 10:  [0.07478094100952148, 0.07667869329452515, 0.31073683500289917, -0.016903679817914963, 0.006944466382265091, -0.13665689527988434, -0.17881113290786743, 0.2652333676815033, -0.05741901323199272, -0.08582337945699692]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.27.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.252881
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49851638078689575, 0.5014836192131042, 0.0, 0.0, 0.0]
     Last 10:  [0.48121389746665955, 0.4798073172569275, 0.015549310483038425, 0.02342957817018032, 0.0, 0.4620146155357361, 0.4607367217540741, 0.011911962181329727, 0.024408554658293724, 0.040928225964307785]
     Zeros: 90, Total: 225

================================================================================
331_model.layers.27.post_attention_layernorm: LlamaRMSNorm (model.layers.27.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.27.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.096209, Std: 11.960194
     First 10: [-0.27031421661376953, -2.479414939880371, -0.23412464559078217, 0.2324497103691101, -11.441813468933105, -6.787535667419434, 4.742196083068848, -0.4701237976551056, -0.3782084882259369, -2.0197854042053223]
     Last 10:  [-0.06305338442325592, -0.24776822328567505, 1.0514166355133057, -0.8168692588806152, 0.40019744634628296, 0.3436797857284546, -0.7883782386779785, 0.336382120847702, -0.31473013758659363, -0.9773438572883606]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003049, Std: 0.384611
     First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
     Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.515895
     First 10: [0.505859375, 0.49853515625, 0.51416015625, 0.5263671875, 0.428466796875, 0.3701171875, 0.438232421875, 0.51123046875, 0.529296875, 0.50146484375]
     Last 10:  [0.52392578125, 0.52490234375, 0.52197265625, 0.513671875, 0.513671875, 0.5380859375, 0.52587890625, 0.5185546875, 0.5283203125, 0.50537109375]

================================================================================
332_model.layers.27.mlp.gate_proj: Linear (model.layers.27.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.27.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003049, Std: 0.384611
     First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
     Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.223669, Std: 0.447516
     First 10: [-0.06841742247343063, -0.39616748690605164, -0.32114291191101074, 0.06356251984834671, -0.3092915713787079, 0.09342837333679199, 0.15901362895965576, -0.22823604941368103, -0.612730860710144, 0.42952796816825867]
     Last 10:  [-0.3862904906272888, -0.2690432369709015, -1.252345085144043, -0.43416717648506165, -0.3731456995010376, -0.31602782011032104, -0.4834897220134735, -0.08001965284347534, -0.48378652334213257, -1.11398184299469]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000123
     First 10: [-0.01387786865234375, 0.02606201171875, 0.047943115234375, 0.0643310546875, -0.01477813720703125, -0.029510498046875, 0.04766845703125, -0.005435943603515625, 0.04559326171875, -0.0030727386474609375]
     Last 10:  [0.03619384765625, 0.0126800537109375, 0.0562744140625, 0.1072998046875, 0.01436614990234375, 0.0227813720703125, 0.039703369140625, 0.00620269775390625, 0.005130767822265625, -0.016143798828125]

================================================================================
333_model.layers.27.mlp.act_fn: SiLU (model.layers.27.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.27.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.223669, Std: 0.447516
     First 10: [-0.06841742247343063, -0.39616748690605164, -0.32114291191101074, 0.06356251984834671, -0.3092915713787079, 0.09342837333679199, 0.15901362895965576, -0.22823604941368103, -0.612730860710144, 0.42952796816825867]
     Last 10:  [-0.3862904906272888, -0.2690432369709015, -1.252345085144043, -0.43416717648506165, -0.3731456995010376, -0.31602782011032104, -0.4834897220134735, -0.08001965284347534, -0.48378652334213257, -1.11398184299469]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.27.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.054549, Std: 0.214315
     First 10: [-0.03303893283009529, -0.15935182571411133, -0.1350075900554657, 0.03279096633195877, -0.13091930747032166, 0.048894815146923065, 0.08581486344337463, -0.10115133225917816, -0.21533599495887756, 0.26019126176834106]
     Last 10:  [-0.1562972366809845, -0.11653392016887665, -0.2783893644809723, -0.17068487405776978, -0.15216177701950073, -0.13325126469135284, -0.184416726231575, -0.038409896194934845, -0.18449606001377106, -0.2752975523471832]
     Zeros: 0, Total: 7680

================================================================================
334_model.layers.27.mlp.up_proj: Linear (model.layers.27.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.27.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003049, Std: 0.384611
     First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
     Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.007425, Std: 0.371882
     First 10: [0.11251769959926605, 0.007027017883956432, -0.6513623595237732, 0.8430122137069702, 0.23220808804035187, -0.1367376446723938, -0.2646061182022095, -0.20783928036689758, 0.578169584274292, -0.2442985475063324]
     Last 10:  [-0.1232791543006897, 0.23765291273593903, -0.14259353280067444, -0.9280157685279846, 0.17850349843502045, -0.3006230294704437, -0.05015864968299866, -0.25989896059036255, -0.3430526852607727, 0.18115012347698212]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000125
     First 10: [0.061553955078125, 0.044952392578125, 0.006778717041015625, 0.056243896484375, 0.03961181640625, 0.0288238525390625, -0.016876220703125, 0.053985595703125, -0.029144287109375, 0.020904541015625]
     Last 10:  [0.08929443359375, -0.013458251953125, -0.02838134765625, -0.0294036865234375, 0.020751953125, -0.033294677734375, 0.0394287109375, -0.00524139404296875, 0.00506591796875, -0.0382080078125]

================================================================================
335_model.layers.27.mlp.down_proj: Linear (model.layers.27.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.27.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.000806, Std: 0.082226
     First 10: [-0.003717464627698064, -0.0011197681305930018, 0.08793886005878448, 0.02764318510890007, -0.03040052205324173, -0.006685761734843254, -0.02270713821053505, 0.02102321945130825, -0.12450072169303894, -0.06356434524059296]
     Last 10:  [0.01926819048821926, -0.02769462577998638, 0.03969652205705643, 0.15839825570583344, -0.027161410078406334, 0.040058400481939316, 0.009250094182789326, 0.009982692077755928, 0.06329187005758286, -0.04987018555402756]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.27.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001135, Std: 0.135571
     First 10: [-0.061802081763744354, -0.14234863221645355, 0.040890883654356, -0.029381342232227325, -0.0033243298530578613, 0.12918423116207123, 0.06904754042625427, 0.1607809215784073, 0.004375232383608818, 0.1423788070678711]
     Last 10:  [0.014956608414649963, -0.00470835343003273, -0.11729138344526291, -0.18640510737895966, -0.08044008910655975, 0.05937131494283676, -0.16532473266124725, 0.034114912152290344, -0.06407684087753296, 0.0784221887588501]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000095
     First 10: [-0.0736083984375, -0.005916595458984375, 0.01523590087890625, 0.11566162109375, 0.0297393798828125, -0.0933837890625, -0.0140380859375, -0.08502197265625, 0.034820556640625, 0.037109375]
     Last 10:  [0.0305328369140625, -0.10986328125, -0.058074951171875, 0.0418701171875, 0.0129852294921875, -0.09295654296875, -0.03338623046875, -0.048004150390625, -0.01444244384765625, -0.0053863525390625]

================================================================================
336_model.layers.27.mlp: LlamaMLP (model.layers.27.mlp)
================================================================================

  → INPUT[0]: model.layers.27.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003049, Std: 0.384611
     First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
     Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.27.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.001135, Std: 0.135571
     First 10: [-0.061802081763744354, -0.14234863221645355, 0.040890883654356, -0.029381342232227325, -0.0033243298530578613, 0.12918423116207123, 0.06904754042625427, 0.1607809215784073, 0.004375232383608818, 0.1423788070678711]
     Last 10:  [0.014956608414649963, -0.00470835343003273, -0.11729138344526291, -0.18640510737895966, -0.08044008910655975, 0.05937131494283676, -0.16532473266124725, 0.034114912152290344, -0.06407684087753296, 0.0784221887588501]
     Zeros: 0, Total: 2880

================================================================================
337_model.layers.28.input_layernorm: LlamaRMSNorm (model.layers.28.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.28.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.095075, Std: 11.926068
     First 10: [-0.3321163058280945, -2.6217634677886963, -0.19323375821113586, 0.20306837558746338, -11.445137977600098, -6.658351421356201, 4.811243534088135, -0.3093428611755371, -0.3738332688808441, -1.8774065971374512]
     Last 10:  [-0.04809677600860596, -0.2524765729904175, 0.9341252446174622, -1.0032743215560913, 0.319757342338562, 0.40305110812187195, -0.953702986240387, 0.37049704790115356, -0.3788069784641266, -0.8989216685295105]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006960, Std: 0.374844
     First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
     Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.594132
     First 10: [0.515625, 0.61865234375, 0.6748046875, 0.69677734375, 0.53125, 0.68017578125, 0.6455078125, 0.6083984375, 0.64599609375, 0.65673828125]
     Last 10:  [0.5927734375, 0.55517578125, 0.65966796875, 0.564453125, 0.5712890625, 0.5517578125, 0.57568359375, 0.599609375, 0.62744140625, 0.57470703125]

================================================================================
338_model.layers.28.self_attn.q_proj: Linear (model.layers.28.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.28.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006960, Std: 0.374844
     First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
     Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.073946, Std: 1.154610
     First 10: [0.6149311065673828, -0.4805939495563507, -0.067977674305439, 0.4206625521183014, -0.7993888258934021, -0.2012237310409546, 0.18426552414894104, 0.26233819127082825, 0.4105304777622223, 0.9626890420913696]
     Last 10:  [0.2220304310321808, -1.7292859554290771, -4.643119812011719, -0.7658923268318176, -0.8189964294433594, -0.6285178065299988, 2.1022450923919678, -1.807969093322754, 0.10805679857730865, 1.7081286907196045]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000098
     First 10: [-0.06561279296875, 0.005062103271484375, -0.01145172119140625, -0.01324462890625, -0.08807373046875, 0.06365966796875, -0.0153961181640625, -0.09375, -0.044403076171875, 0.051361083984375]
     Last 10:  [0.0195159912109375, 0.002048492431640625, -0.05743408203125, -0.009307861328125, 0.033660888671875, -0.03082275390625, 0.00984954833984375, -0.07037353515625, -0.1370849609375, 0.0200958251953125]

================================================================================
339_model.layers.28.self_attn.k_proj: Linear (model.layers.28.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.28.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006960, Std: 0.374844
     First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
     Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.130582, Std: 1.410843
     First 10: [0.0035872086882591248, -0.009303197264671326, 0.004126459360122681, -0.01750504970550537, -0.00853826105594635, 0.010820655152201653, 0.013142785057425499, 0.006411999464035034, 0.011238276958465576, -0.027142062783241272]
     Last 10:  [-2.664684772491455, -0.28530246019363403, 1.4246898889541626, -0.07578302919864655, -0.8041360378265381, -0.43867960572242737, -0.869949460029602, 0.8943350315093994, -0.9220335483551025, -1.4576408863067627]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000128
     First 10: [0.032623291015625, -0.00388336181640625, -0.01514434814453125, -0.041778564453125, 0.013702392578125, 0.0291595458984375, -0.0088348388671875, -0.07720947265625, -0.036773681640625, 0.00604248046875]
     Last 10:  [0.12060546875, 0.0328369140625, 0.039306640625, -0.0263214111328125, -0.019866943359375, 0.07763671875, -0.006313323974609375, 0.105712890625, 0.042022705078125, -0.02099609375]

================================================================================
340_model.layers.28.self_attn.v_proj: Linear (model.layers.28.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.28.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.006960, Std: 0.374844
     First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
     Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.008993, Std: 0.298576
     First 10: [0.01655512861907482, 0.010838732123374939, -0.005177323706448078, -0.02836153469979763, 0.0131099047139287, -0.029212556779384613, 0.0023323632776737213, -0.009417103603482246, 0.014474233612418175, 0.0006571020931005478]
     Last 10:  [0.43392443656921387, 0.65770024061203, -0.5855507850646973, 0.0868016853928566, -0.2170192301273346, -0.005132589489221573, 0.10735204070806503, -0.2543153166770935, -0.29783421754837036, -0.18187406659126282]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000065
     First 10: [-0.0188446044921875, -0.00283050537109375, 0.126953125, -0.09893798828125, -0.0288543701171875, -0.03924560546875, -0.055023193359375, -0.00478363037109375, 0.06646728515625, -0.01300048828125]
     Last 10:  [-0.028228759765625, -0.101806640625, -0.0293426513671875, -0.03009033203125, 0.10760498046875, -0.052520751953125, -0.0111541748046875, -0.01073455810546875, -0.0784912109375, 0.0268707275390625]

================================================================================
341_model.layers.28.self_attn.o_proj: Linear (model.layers.28.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.28.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.002733, Std: 0.095840
     First 10: [0.01655512861907482, 0.010838732123374939, -0.005177323706448078, -0.02836153469979763, 0.0131099047139287, -0.029212556779384613, 0.0023323632776737213, -0.009417103603482246, 0.014474233612418175, 0.0006571020931005478]
     Last 10:  [0.05649228394031525, 0.09885520488023758, -0.054442763328552246, 0.06785283982753754, -0.04014575108885765, -0.001224550069309771, -0.0007578872609883547, -0.04289951175451279, -0.06814179569482803, 0.00038398068863898516]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000712, Std: 0.089222
     First 10: [-0.023951619863510132, -0.005104107316583395, 0.13757923245429993, -0.009691664017736912, 0.04752480238676071, -0.0004134867340326309, -0.07798007130622864, -0.029172945767641068, 0.04693637043237686, -0.08623510599136353]
     Last 10:  [0.05704334378242493, -0.024426601827144623, 0.05350762605667114, 0.020676258951425552, 0.05379326269030571, -0.24131079018115997, 0.020830443128943443, 0.12403851747512817, 0.10358887165784836, -0.07847173511981964]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: -0.000039
     First 10: [-0.0310211181640625, -0.026153564453125, -0.068359375, -0.0197296142578125, 0.0301666259765625, -0.02508544921875, 0.01163482666015625, -0.01361083984375, -0.043975830078125, 0.0059814453125]
     Last 10:  [0.01216888427734375, 0.0033721923828125, -0.0018529891967773438, 0.0631103515625, 0.00789642333984375, 0.0694580078125, 0.0081634521484375, -0.0689697265625, -0.00846099853515625, -0.0230865478515625]

================================================================================
342_model.layers.28.self_attn: LlamaSdpaAttention (model.layers.28.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.28.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.000712, Std: 0.089222
     First 10: [-0.023951619863510132, -0.005104107316583395, 0.13757923245429993, -0.009691664017736912, 0.04752480238676071, -0.0004134867340326309, -0.07798007130622864, -0.029172945767641068, 0.04693637043237686, -0.08623510599136353]
     Last 10:  [0.05704334378242493, -0.024426601827144623, 0.05350762605667114, 0.020676258951425552, 0.05379326269030571, -0.24131079018115997, 0.020830443128943443, 0.12403851747512817, 0.10358887165784836, -0.07847173511981964]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.28.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.252674
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49974972009658813, 0.5002503395080566, 0.0, 0.0, 0.0]
     Last 10:  [0.4116165339946747, 0.41173818707466125, 0.06966163218021393, 0.10698362439870834, 0.0, 0.40154793858528137, 0.4018951952457428, 0.028445212170481682, 0.052452102303504944, 0.11565953493118286]
     Zeros: 90, Total: 225

================================================================================
343_model.layers.28.post_attention_layernorm: LlamaRMSNorm (model.layers.28.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.28.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.095786, Std: 11.926423
     First 10: [-0.3560679256916046, -2.6268675327301025, -0.05565452575683594, 0.19337670505046844, -11.397613525390625, -6.658764839172363, 4.7332634925842285, -0.3385158181190491, -0.32689690589904785, -1.96364164352417]
     Last 10:  [0.00894656777381897, -0.2769031822681427, 0.9876328706741333, -0.982598066329956, 0.3735505938529968, 0.16174031794071198, -0.9328725337982178, 0.49453556537628174, -0.27521809935569763, -0.977393388748169]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001621, Std: 0.398450
     First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
     Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.532200
     First 10: [0.541015625, 0.5244140625, 0.5224609375, 0.54443359375, 0.48974609375, 0.44140625, 0.49853515625, 0.51806640625, 0.525390625, 0.53271484375]
     Last 10:  [0.5107421875, 0.53662109375, 0.50244140625, 0.51513671875, 0.51708984375, 0.5498046875, 0.5341796875, 0.5302734375, 0.54150390625, 0.52978515625]

================================================================================
344_model.layers.28.mlp.gate_proj: Linear (model.layers.28.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.28.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001621, Std: 0.398450
     First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
     Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.264424, Std: 0.506641
     First 10: [0.12897004187107086, -0.39608830213546753, -0.2803894579410553, -0.3247537910938263, 0.08042105287313461, -0.09660046547651291, 0.08362066000699997, 0.010287169367074966, 0.07964073121547699, 0.3445656895637512]
     Last 10:  [-0.35239189863204956, -0.29913103580474854, -0.09088311344385147, -0.14011387526988983, 0.3769327998161316, -0.9975695610046387, -0.5995365977287292, -0.6305609941482544, 0.6636433601379395, 0.3247942626476288]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000336
     First 10: [-0.01439666748046875, -0.01509857177734375, 0.00807952880859375, 0.0484619140625, 0.0142669677734375, -0.02313232421875, 0.01177978515625, -0.007320404052734375, 0.08770751953125, 0.043914794921875]
     Last 10:  [0.0229034423828125, -0.0086212158203125, 0.004909515380859375, -0.0205078125, -0.034332275390625, -0.0247802734375, 0.04486083984375, -0.1619873046875, -0.01027679443359375, 0.048248291015625]

================================================================================
345_model.layers.28.mlp.act_fn: SiLU (model.layers.28.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.28.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.264424, Std: 0.506641
     First 10: [0.12897004187107086, -0.39608830213546753, -0.2803894579410553, -0.3247537910938263, 0.08042105287313461, -0.09660046547651291, 0.08362066000699997, 0.010287169367074966, 0.07964073121547699, 0.3445656895637512]
     Last 10:  [-0.35239189863204956, -0.29913103580474854, -0.09088311344385147, -0.14011387526988983, 0.3769327998161316, -0.9975695610046387, -0.5995365977287292, -0.6305609941482544, 0.6636433601379395, 0.3247942626476288]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.28.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.061144, Std: 0.256378
     First 10: [0.06863758713006973, -0.15932752192020416, -0.12066793441772461, -0.1362399458885193, 0.04182653874158859, -0.04596912860870361, 0.04355741664767265, 0.005170040763914585, 0.04140518978238106, 0.20167399942874908]
     Last 10:  [-0.14546826481819153, -0.127360999584198, -0.04337804391980171, -0.0651569738984108, 0.22357133030891418, -0.2687647342681885, -0.21250556409358978, -0.21904638409614563, 0.4380565881729126, 0.1885405331850052]
     Zeros: 0, Total: 7680

================================================================================
346_model.layers.28.mlp.up_proj: Linear (model.layers.28.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.28.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001621, Std: 0.398450
     First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
     Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.007951, Std: 0.432298
     First 10: [0.05869648978114128, -0.018466472625732422, 0.22964343428611755, 0.10962295532226562, 0.25922828912734985, 0.12456062436103821, 0.05788126215338707, -0.36263954639434814, 0.7943211197853088, -0.31878596544265747]
     Last 10:  [0.7832505702972412, -0.24361921846866608, -0.44514787197113037, -1.2266592979431152, 0.3153582811355591, -0.12884919345378876, -0.302031934261322, -0.289600133895874, -0.30363649129867554, -0.3482217490673065]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: -0.000071
     First 10: [0.0260467529296875, -0.080322265625, 0.031158447265625, 0.053741455078125, 0.03192138671875, 0.08135986328125, -0.0258636474609375, 0.097412109375, -0.026397705078125, 0.006290435791015625]
     Last 10:  [-0.03369140625, 0.0024871826171875, 0.00615692138671875, -0.0113983154296875, -0.0024967193603515625, -0.023712158203125, -0.037872314453125, -0.002117156982421875, -0.03363037109375, -0.059722900390625]

================================================================================
347_model.layers.28.mlp.down_proj: Linear (model.layers.28.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.28.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.018814, Std: 1.276110
     First 10: [0.00402878550812602, 0.002942217281088233, -0.027710597962141037, -0.014935025945305824, 0.010842622257769108, -0.005725943483412266, 0.0025211581960320473, -0.0018748611910268664, 0.03288901597261429, -0.0642908439040184]
     Last 10:  [-0.1139381006360054, 0.03102758713066578, 0.019309643656015396, 0.07992541044950485, 0.0705050677061081, 0.03463011980056763, 0.06418346613645554, 0.063435859978199, -0.13300997018814087, -0.06565391272306442]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.28.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.052723, Std: 1.842718
     First 10: [-1.5671627521514893, -2.2775650024414062, -1.4149456024169922, 1.237135887145996, -0.7577803134918213, -0.6648438572883606, 2.409255266189575, 0.5028420686721802, -0.9213648438453674, -0.6347841620445251]
     Last 10:  [0.47384971380233765, -0.034696534276008606, 0.3225051462650299, 0.49267566204071045, -0.16220074892044067, -0.31065380573272705, 0.3274558186531067, 0.05742955580353737, -0.03630128875374794, 0.24124374985694885]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: 0.000030
     First 10: [0.00917816162109375, 0.0225372314453125, 0.0230560302734375, -0.039794921875, 0.00571441650390625, 0.053436279296875, -0.002002716064453125, 0.01861572265625, 0.00994110107421875, -0.031982421875]
     Last 10:  [-0.01502227783203125, -0.1160888671875, 0.0087738037109375, -0.01349639892578125, -0.008453369140625, 0.01403045654296875, 0.00786590576171875, -0.0003383159637451172, -0.0184173583984375, 0.0799560546875]

================================================================================
348_model.layers.28.mlp: LlamaMLP (model.layers.28.mlp)
================================================================================

  → INPUT[0]: model.layers.28.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.001621, Std: 0.398450
     First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
     Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.28.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.052723, Std: 1.842718
     First 10: [-1.5671627521514893, -2.2775650024414062, -1.4149456024169922, 1.237135887145996, -0.7577803134918213, -0.6648438572883606, 2.409255266189575, 0.5028420686721802, -0.9213648438453674, -0.6347841620445251]
     Last 10:  [0.47384971380233765, -0.034696534276008606, 0.3225051462650299, 0.49267566204071045, -0.16220074892044067, -0.31065380573272705, 0.3274558186531067, 0.05742955580353737, -0.03630128875374794, 0.24124374985694885]
     Zeros: 0, Total: 2880

================================================================================
349_model.layers.29.input_layernorm: LlamaRMSNorm (model.layers.29.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.29.input_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.043063, Std: 10.273553
     First 10: [-1.9232306480407715, -4.90443229675293, -1.4706001281738281, 1.430512547492981, -12.155393600463867, -7.323608875274658, 7.142518997192383, 0.1643262505531311, -1.2482616901397705, -2.59842586517334]
     Last 10:  [0.4827962815761566, -0.3115997314453125, 1.3101379871368408, -0.4899224042892456, 0.21134984493255615, -0.14891348779201508, -0.6054167151451111, 0.5519651174545288, -0.3115193843841553, -0.7361496686935425]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.input_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000230, Std: 0.341509
     First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
     Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.584867
     First 10: [0.4736328125, 0.6689453125, 0.65478515625, 0.61181640625, 0.397705078125, 0.8115234375, 0.6748046875, 0.56689453125, 0.56982421875, 0.701171875]
     Last 10:  [0.5380859375, 0.587890625, 0.5703125, 0.56103515625, 0.52685546875, 0.5771484375, 0.5947265625, 0.5654296875, 0.5859375, 0.5576171875]

================================================================================
350_model.layers.29.self_attn.q_proj: Linear (model.layers.29.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.29.self_attn.q_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000230, Std: 0.341509
     First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
     Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.self_attn.q_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.051633, Std: 1.045315
     First 10: [0.2336878478527069, 0.3683914542198181, -0.26508623361587524, -0.531550407409668, 0.14748455584049225, -0.6213107109069824, -0.13117972016334534, 0.5304543972015381, 0.38900327682495117, 0.32231318950653076]
     Last 10:  [-0.2202601432800293, 0.446963906288147, -4.681027412414551, 0.31972789764404297, 1.186026930809021, 1.578558325767517, 0.9774452447891235, -1.9583154916763306, 1.9326210021972656, 0.23209038376808167]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000088
     First 10: [0.0248565673828125, 0.060699462890625, 0.0172882080078125, -0.0562744140625, -0.01119232177734375, 0.04656982421875, 0.064697265625, 0.006099700927734375, 0.0250244140625, -0.0211639404296875]
     Last 10:  [-0.059112548828125, -0.0211181640625, 0.034027099609375, -0.0244140625, 0.054229736328125, 0.0941162109375, 0.049224853515625, -0.044464111328125, 0.044769287109375, -0.0023345947265625]

================================================================================
351_model.layers.29.self_attn.k_proj: Linear (model.layers.29.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.29.self_attn.k_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000230, Std: 0.341509
     First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
     Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.self_attn.k_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: 0.002538, Std: 1.012784
     First 10: [-0.00937582366168499, -0.038926564157009125, 0.005319289863109589, -0.0030039921402931213, 0.01790553703904152, 0.000886041671037674, -0.01605241373181343, -0.0011844933032989502, -0.027588024735450745, -0.006459102034568787]
     Last 10:  [-0.42949652671813965, 0.7413684725761414, 0.9240916967391968, -0.9947003126144409, -1.2200775146484375, 0.0010367855429649353, 1.6633961200714111, 1.2300827503204346, 0.3265140950679779, 1.193665862083435]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: -0.000026
     First 10: [-0.031707763671875, 0.0036144256591796875, 0.056121826171875, 0.0457763671875, 0.0655517578125, -0.03472900390625, -0.04486083984375, -0.02752685546875, -0.044891357421875, 0.015716552734375]
     Last 10:  [-0.194091796875, -0.07366943359375, 0.06463623046875, 0.0312042236328125, 0.037200927734375, 0.06439208984375, 0.0865478515625, -0.07464599609375, -0.044830322265625, -0.011016845703125]

================================================================================
352_model.layers.29.self_attn.v_proj: Linear (model.layers.29.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.29.self_attn.v_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.000230, Std: 0.341509
     First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
     Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.self_attn.v_proj_output
     Shape: [1, 5, 192]
     Dtype: torch.float32
     Mean: -0.003787, Std: 0.249379
     First 10: [0.04828491434454918, -0.0006453683599829674, -0.003548063337802887, 0.012386120855808258, -0.019839972257614136, -0.004468521568924189, 0.004066263325512409, -0.0026262961328029633, -0.009701646864414215, -0.008686428889632225]
     Last 10:  [0.5079941749572754, -0.5518670082092285, -0.00813090056180954, 0.7327921390533447, 0.34959426522254944, 0.4754205644130707, 0.12692445516586304, 0.349627822637558, 0.1239483430981636, -0.4577659070491791]
     Zeros: 0, Total: 960

  → PARAMETER: weight
     Shape: [192, 576]
     Mean: 0.000220
     First 10: [0.01462554931640625, -0.0618896484375, 0.0189666748046875, 0.04241943359375, 0.07855224609375, -0.00356292724609375, -0.0264739990234375, 0.03607177734375, -0.012847900390625, -0.0046844482421875]
     Last 10:  [0.0111236572265625, -0.01331329345703125, -0.03753662109375, 0.057525634765625, 0.023468017578125, 0.013427734375, -0.006198883056640625, 0.041900634765625, -0.0299530029296875, 0.0765380859375]

================================================================================
353_model.layers.29.self_attn.o_proj: Linear (model.layers.29.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.29.self_attn.o_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.003314, Std: 0.072479
     First 10: [0.04828491434454918, -0.0006453683599829674, -0.003548063337802887, 0.012386120855808258, -0.019839972257614136, -0.004468521568924189, 0.004066263325512409, -0.0026262961328029633, -0.009701646864414215, -0.008686428889632225]
     Last 10:  [0.01654069311916828, -0.17360030114650726, 0.014826632104814053, 0.1264973282814026, 0.04803786426782608, 0.032845038920640945, -0.029415013268589973, 0.0766880214214325, 0.06580642610788345, -0.04277350753545761]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.self_attn.o_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.007071, Std: 0.121768
     First 10: [-0.03164166212081909, -0.012140659615397453, -0.02638525702059269, -0.02672414854168892, -0.09785962104797363, -0.02832343615591526, -0.2260829359292984, 0.004762471653521061, -0.049237266182899475, -0.05203139781951904]
     Last 10:  [-0.06317279487848282, -0.03762347251176834, 0.04144430533051491, 0.038063451647758484, 0.021388234570622444, 0.04907140135765076, 0.13077114522457123, 0.06544061750173569, -0.09651215374469757, -0.013896534219384193]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 576]
     Mean: 0.000014
     First 10: [-0.024871826171875, -0.0021610260009765625, 0.0928955078125, 0.012664794921875, 0.06585693359375, -0.0814208984375, 0.0291900634765625, -0.019744873046875, -0.04669189453125, 0.0139617919921875]
     Last 10:  [-0.0233612060546875, -0.048736572265625, -0.06427001953125, 0.02886962890625, -0.0546875, 0.00797271728515625, -0.057220458984375, -0.00574493408203125, 0.0032863616943359375, -0.054168701171875]

================================================================================
354_model.layers.29.self_attn: LlamaSdpaAttention (model.layers.29.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.29.self_attn_output_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.007071, Std: 0.121768
     First 10: [-0.03164166212081909, -0.012140659615397453, -0.02638525702059269, -0.02672414854168892, -0.09785962104797363, -0.02832343615591526, -0.2260829359292984, 0.004762471653521061, -0.049237266182899475, -0.05203139781951904]
     Last 10:  [-0.06317279487848282, -0.03762347251176834, 0.04144430533051491, 0.038063451647758484, 0.021388234570622444, 0.04907140135765076, 0.13077114522457123, 0.06544061750173569, -0.09651215374469757, -0.013896534219384193]
     Zeros: 0, Total: 2880

  → OUTPUT[1]: model.layers.29.self_attn_output_1
     Shape: [1, 9, 5, 5]
     Dtype: torch.float32
     Mean: 0.200000, Std: 0.254665
     First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5004143118858337, 0.49958574771881104, 0.0, 0.0, 0.0]
     Last 10:  [0.4367995262145996, 0.43619590997695923, 0.08444936573505402, 0.042555153369903564, 0.0, 0.38690683245658875, 0.38735440373420715, 0.10769648104906082, 0.0684686228632927, 0.04957360401749611]
     Zeros: 90, Total: 225

================================================================================
355_model.layers.29.post_attention_layernorm: LlamaRMSNorm (model.layers.29.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.29.post_attention_layernorm_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: -0.035992, Std: 10.270823
     First 10: [-1.9548723697662354, -4.9165730476379395, -1.4969854354858398, 1.403788447380066, -12.253252983093262, -7.351932525634766, 6.916436195373535, 0.1690887212753296, -1.2974989414215088, -2.6504573822021484]
     Last 10:  [0.4196234941482544, -0.34922319650650024, 1.3515822887420654, -0.4518589377403259, 0.23273807764053345, -0.09984208643436432, -0.47464555501937866, 0.6174057126045227, -0.40803152322769165, -0.7500461935997009]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.post_attention_layernorm_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004375, Std: 0.387368
     First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
     Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576]
     Mean: 0.502549
     First 10: [0.409423828125, 0.488525390625, 0.458984375, 0.49609375, 0.472412109375, 0.440673828125, 0.499267578125, 0.4814453125, 0.5029296875, 0.52587890625]
     Last 10:  [0.52392578125, 0.51806640625, 0.4443359375, 0.47509765625, 0.51416015625, 0.52490234375, 0.496337890625, 0.52197265625, 0.4912109375, 0.505859375]

================================================================================
356_model.layers.29.mlp.gate_proj: Linear (model.layers.29.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.29.mlp.gate_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004375, Std: 0.387368
     First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
     Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.mlp.gate_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.211077, Std: 0.662286
     First 10: [-0.5671274065971375, -1.2409058809280396, -0.29080745577812195, -0.03479953855276108, 0.4953216016292572, -0.4618515968322754, -0.6326705813407898, 0.23212365806102753, -0.5857346653938293, 0.6488878130912781]
     Last 10:  [-0.3486367464065552, -0.030581653118133545, -1.0788029432296753, -0.7312259674072266, -0.34351158142089844, -0.14589543640613556, -0.04152501001954079, 0.43124714493751526, -0.5620436668395996, -0.9064484238624573]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000235
     First 10: [-0.05426025390625, -0.013885498046875, -0.031402587890625, -0.045379638671875, -0.0290985107421875, 0.04833984375, 0.005512237548828125, 0.020294189453125, 0.0163726806640625, 0.0287628173828125]
     Last 10:  [-0.040496826171875, 0.017791748046875, -0.022613525390625, 0.04608154296875, -0.044219970703125, 0.01125335693359375, 0.05584716796875, -0.041168212890625, -0.0745849609375, 0.0309600830078125]

================================================================================
357_model.layers.29.mlp.act_fn: SiLU (model.layers.29.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.29.mlp.act_fn_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.211077, Std: 0.662286
     First 10: [-0.5671274065971375, -1.2409058809280396, -0.29080745577812195, -0.03479953855276108, 0.4953216016292572, -0.4618515968322754, -0.6326705813407898, 0.23212365806102753, -0.5857346653938293, 0.6488878130912781]
     Last 10:  [-0.3486367464065552, -0.030581653118133545, -1.0788029432296753, -0.7312259674072266, -0.34351158142089844, -0.14589543640613556, -0.04152501001954079, 0.43124714493751526, -0.5620436668395996, -0.9064484238624573]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.29.mlp.act_fn_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: -0.016233, Std: 0.425680
     First 10: [-0.20524336397647858, -0.27830833196640015, -0.12440923601388931, -0.017097048461437225, 0.3077726662158966, -0.17852719128131866, -0.21947674453258514, 0.129472017288208, -0.20946697890758514, 0.4261634349822998]
     Last 10:  [-0.14423556625843048, -0.015057035721838474, -0.2737274765968323, -0.2375941425561905, -0.14254245162010193, -0.06763576716184616, -0.020331483334302902, 0.2614096999168396, -0.20406386256217957, -0.2608098089694977]
     Zeros: 0, Total: 7680

================================================================================
358_model.layers.29.mlp.up_proj: Linear (model.layers.29.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.29.mlp.up_proj_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004375, Std: 0.387368
     First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
     Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.mlp.up_proj_output
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.007754, Std: 0.650116
     First 10: [-0.6334491968154907, -0.4209849238395691, -0.22874568402767181, 0.2435075044631958, -0.22569306194782257, -0.4182506799697876, 0.3145217001438141, -1.746135950088501, -0.12157414853572845, -0.2710261344909668]
     Last 10:  [-0.5976570844650269, 0.575960636138916, 0.23476943373680115, -0.21857641637325287, -0.19871574640274048, -0.24691295623779297, -0.044295236468315125, -0.5169350504875183, -0.7826733589172363, -0.6185628771781921]
     Zeros: 0, Total: 7680

  → PARAMETER: weight
     Shape: [1536, 576]
     Mean: 0.000084
     First 10: [0.04248046875, -0.0011320114135742188, -0.054962158203125, 0.055755615234375, 0.016082763671875, 0.02117919921875, 0.055755615234375, 0.01456451416015625, -0.038787841796875, -0.10906982421875]
     Last 10:  [-0.00791168212890625, 0.07501220703125, 0.00962066650390625, 0.103759765625, -0.0191192626953125, 0.06793212890625, -0.02410888671875, 0.00922393798828125, -0.0352783203125, -0.055999755859375]

================================================================================
359_model.layers.29.mlp.down_proj: Linear (model.layers.29.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.29.mlp.down_proj_input_0
     Shape: [1, 5, 1536]
     Dtype: torch.float32
     Mean: 0.015903, Std: 3.356709
     First 10: [0.13001124560832977, 0.11716361343860626, 0.02845807559788227, -0.004163259640336037, -0.06946215778589249, 0.07466912269592285, -0.06903019547462463, -0.22607573866844177, 0.025465769693255424, -0.11550142616033554]
     Last 10:  [0.08620341122150421, -0.008672259747982025, -0.06426284462213516, 0.051932476460933685, 0.028325429186224937, 0.016700146719813347, 0.0009005878819152713, -0.1351318359375, 0.15971535444259644, 0.1613272726535797]
     Zeros: 0, Total: 7680

  → OUTPUT[0]: model.layers.29.mlp.down_proj_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.048424, Std: 10.158028
     First 10: [2.0837559700012207, 5.01254415512085, 1.7442330121994019, -1.349542498588562, 11.678293228149414, 7.412636756896973, -6.787923812866211, -0.03134290128946304, 1.1584196090698242, 2.5779271125793457]
     Last 10:  [-0.4681185781955719, 0.24617637693881989, 0.013099242001771927, 0.4093452990055084, -0.0956038311123848, -0.06480144709348679, 0.044930458068847656, -0.15281924605369568, -0.18177638947963715, 0.47355028986930847]
     Zeros: 0, Total: 2880

  → PARAMETER: weight
     Shape: [576, 1536]
     Mean: -0.000026
     First 10: [0.003002166748046875, -0.0182952880859375, -2.6524066925048828e-05, -0.042999267578125, 0.0140228271484375, -0.00902557373046875, -0.025482177734375, -0.004337310791015625, -0.04632568359375, -0.0103302001953125]
     Last 10:  [-0.0222930908203125, 0.007610321044921875, 0.0300140380859375, 0.094970703125, 0.0180206298828125, 0.003528594970703125, -0.058074951171875, 0.00732421875, -0.0260467529296875, -0.055267333984375]

================================================================================
360_model.layers.29.mlp: LlamaMLP (model.layers.29.mlp)
================================================================================

  → INPUT[0]: model.layers.29.mlp_input_0
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.004375, Std: 0.387368
     First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
     Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
     Zeros: 0, Total: 2880

  → OUTPUT[0]: model.layers.29.mlp_output
     Shape: [1, 5, 576]
     Dtype: torch.float32
     Mean: 0.048424, Std: 10.158028
     First 10: [2.0837559700012207, 5.01254415512085, 1.7442330121994019, -1.349542498588562, 11.678293228149414, 7.412636756896973, -6.787923812866211, -0.03134290128946304, 1.1584196090698242, 2.5779271125793457]
     Last 10:  [-0.4681185781955719, 0.24617637693881989, 0.013099242001771927, 0.4093452990055084, -0.0956038311123848, -0.06480144709348679, 0.044930458068847656, -0.15281924605369568, -0.18177638947963715, 0.47355028986930847]
     Zeros: 0, Total: 2880

Forward pass completed. Tracked 361 operations.

================================================================================
OPERATION SUMMARY (361 operations)
================================================================================

000_model.embed_tokens: model.embed_tokens (Embedding)
  IN[0]:  [1, 5] | min=1 max=5 | torch.int64
    First 10: [1, 2, 3, 4, 5]
    Last 10:  [1, 2, 3, 4, 5]
  OUT[0]: [1, 5, 576] | μ=0.000728 σ=0.049521
    First 10: [-0.105712890625, -0.023468017578125, 0.051483154296875, 0.0313720703125, 0.047332763671875, -0.0167999267578125, -0.0188751220703125, 0.01373291015625, -0.002361297607421875, 0.005970001220703125]
    Last 10:  [0.041229248046875, 0.034027099609375, 0.0277557373046875, -0.01296234130859375, 0.006580352783203125, -0.083251953125, 0.0091400146484375, 0.05645751953125, 0.05078125, 0.00970458984375]
  WEIGHT: [32000, 576] | μ=0.001268

001_model.layers.0.input_layernorm: model.layers.0.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=0.000728 σ=0.049521
    First 10: [-0.105712890625, -0.023468017578125, 0.051483154296875, 0.0313720703125, 0.047332763671875, -0.0167999267578125, -0.0188751220703125, 0.01373291015625, -0.002361297607421875, 0.005970001220703125]
    Last 10:  [0.041229248046875, 0.034027099609375, 0.0277557373046875, -0.01296234130859375, 0.006580352783203125, -0.083251953125, 0.0091400146484375, 0.05645751953125, 0.05078125, 0.00970458984375]
  OUT[0]: [1, 5, 576] | μ=-0.001793 σ=0.255825
    First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
    Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
  WEIGHT: [576] | μ=0.041261

002_model.layers.0.self_attn.q_proj: model.layers.0.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001793 σ=0.255825
    First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
    Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
  OUT[0]: [1, 5, 576] | μ=0.112946 σ=1.552935
    First 10: [0.5033838152885437, 0.5412465333938599, 0.14994597434997559, -0.008902221918106079, -0.1874096691608429, 0.176373690366745, -0.44474470615386963, 0.03787432238459587, -0.31698372960090637, -0.2557930052280426]
    Last 10:  [4.165809631347656, -0.12427692860364914, -0.25147971510887146, -0.6601132750511169, 4.253054141998291, 0.5314412117004395, -0.09199923276901245, 0.17124396562576294, -0.2839110493659973, -0.10712286829948425]
  WEIGHT: [576, 576] | μ=-0.000126

003_model.layers.0.self_attn.k_proj: model.layers.0.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001793 σ=0.255825
    First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
    Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
  OUT[0]: [1, 5, 192] | μ=0.133324 σ=1.404734
    First 10: [0.7571072578430176, 1.4551655054092407, -0.005290508270263672, 0.7294079065322876, -1.4618819952011108, 0.385603129863739, -0.6978878378868103, 1.6107616424560547, -0.5896987318992615, 0.29151690006256104]
    Last 10:  [-0.04396691545844078, 3.99662709236145, 0.016898587346076965, 1.3481683731079102, -1.0014292001724243, 0.45284271240234375, -1.1064977645874023, -0.21121539175510406, 0.4838324785232544, 1.4160747528076172]
  WEIGHT: [192, 576] | μ=-0.000144

004_model.layers.0.self_attn.v_proj: model.layers.0.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001793 σ=0.255825
    First 10: [-0.5468090176582336, -0.039798445999622345, 0.05963956564664841, -0.07286541908979416, 0.22328203916549683, -0.033767182379961014, -0.04389414191246033, 0.022616326808929443, 0.004610024858266115, 0.010614580474793911]
    Last 10:  [0.06179852411150932, 0.04462289437651634, 0.029941365122795105, 0.01390804536640644, 0.00873605813831091, 0.07993072271347046, 0.009314920753240585, 0.06881023198366165, -0.049637049436569214, 0.012108736671507359]
  OUT[0]: [1, 5, 192] | μ=-0.003835 σ=0.055827
    First 10: [0.15630188584327698, 0.018999285995960236, -0.004712186753749847, -0.02590961381793022, 0.023915179073810577, -0.00251717958599329, 0.014611541293561459, -0.015919966623187065, 0.0029330477118492126, 0.009663678705692291]
    Last 10:  [0.017394091933965683, -0.06894639879465103, 0.01122063398361206, 0.016725104302167892, 0.10629651695489883, 0.007913182489573956, -0.000908607617020607, -0.05060180649161339, -0.08660925179719925, -0.001465432345867157]
  WEIGHT: [192, 576] | μ=-0.000019

005_model.layers.0.self_attn.o_proj: model.layers.0.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.005900 σ=0.060204
    First 10: [0.15630188584327698, 0.018999285995960236, -0.004712186753749847, -0.02590961381793022, 0.023915179073810577, -0.00251717958599329, 0.014611541293561459, -0.015919966623187065, 0.0029330477118492126, 0.009663678705692291]
    Last 10:  [-0.0021775299683213234, -0.06498672813177109, -0.002680807374417782, 0.01607847400009632, 0.07597081363201141, 0.023918841034173965, 0.00777264591306448, -0.01605452038347721, -0.06435714662075043, 0.007037031464278698]
  OUT[0]: [1, 5, 576] | μ=-0.001761 σ=0.039185
    First 10: [-0.025306524708867073, -0.021873250603675842, 0.0016701433341950178, 0.03850219398736954, 0.2911089360713959, 0.0014683189801871777, -0.08217266201972961, 0.007354758679866791, -0.01887727901339531, 0.037398651242256165]
    Last 10:  [0.0035476339980959892, 0.03285942226648331, -0.005317067727446556, 0.004096785560250282, 0.029731327667832375, -0.014254536479711533, -0.0008613960817456245, 0.038928065448999405, -0.02528518997132778, 0.014403929002583027]
  WEIGHT: [576, 576] | μ=0.000013

006_model.layers.0.self_attn: model.layers.0.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.001761 σ=0.039185
    First 10: [-0.025306524708867073, -0.021873250603675842, 0.0016701433341950178, 0.03850219398736954, 0.2911089360713959, 0.0014683189801871777, -0.08217266201972961, 0.007354758679866791, -0.01887727901339531, 0.037398651242256165]
    Last 10:  [0.0035476339980959892, 0.03285942226648331, -0.005317067727446556, 0.004096785560250282, 0.029731327667832375, -0.014254536479711533, -0.0008613960817456245, 0.038928065448999405, -0.02528518997132778, 0.014403929002583027]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.255701
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5031329393386841, 0.49686703085899353, 0.0, 0.0, 0.0]
    Last 10:  [0.046319182962179184, 0.6555679440498352, 0.13640841841697693, 0.161704421043396, 0.0, 0.015517447143793106, 0.09633219242095947, 0.13731218874454498, 0.5063778758049011, 0.2444603592157364]

007_model.layers.0.post_attention_layernorm: model.layers.0.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.001033 σ=0.058745
    First 10: [-0.13101941347122192, -0.04534126818180084, 0.053153298795223236, 0.06987426429986954, 0.3384416997432709, -0.015331607311964035, -0.10104778409004211, 0.02108766883611679, -0.021238576620817184, 0.04336865246295929]
    Last 10:  [0.044776882976293564, 0.0668865218758583, 0.022438669577240944, -0.008865555748343468, 0.03631167858839035, -0.09750649333000183, 0.008278618566691875, 0.0953855812549591, 0.02549606002867222, 0.024108517915010452]
  OUT[0]: [1, 5, 576] | μ=-0.003832 σ=0.212775
    First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
    Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
  WEIGHT: [576] | μ=0.211037

008_model.layers.0.mlp.gate_proj: model.layers.0.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003832 σ=0.212775
    First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
    Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
  OUT[0]: [1, 5, 1536] | μ=0.249022 σ=0.454324
    First 10: [0.774133563041687, -0.1935971975326538, 0.42558181285858154, 0.5865437388420105, -0.17026937007904053, 0.4110816717147827, 0.3665488362312317, 0.5453060865402222, 0.6536177396774292, 0.5717343091964722]
    Last 10:  [-0.1495017558336258, -0.1377575397491455, 0.7607302665710449, 0.44456207752227783, 0.2130720615386963, 0.32657790184020996, 0.05526885390281677, 0.6562900543212891, -0.2257138341665268, 0.2352910041809082]
  WEIGHT: [1536, 576] | μ=0.000107

009_model.layers.0.mlp.act_fn: model.layers.0.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=0.249022 σ=0.454324
    First 10: [0.774133563041687, -0.1935971975326538, 0.42558181285858154, 0.5865437388420105, -0.17026937007904053, 0.4110816717147827, 0.3665488362312317, 0.5453060865402222, 0.6536177396774292, 0.5717343091964722]
    Last 10:  [-0.1495017558336258, -0.1377575397491455, 0.7607302665710449, 0.44456207752227783, 0.2130720615386963, 0.32657790184020996, 0.05526885390281677, 0.6562900543212891, -0.2257138341665268, 0.2352910041809082]
  OUT[0]: [1, 5, 1536] | μ=0.178735 σ=0.201166
    First 10: [0.5298281311988831, -0.08745779097080231, 0.25739961862564087, 0.3768964111804962, -0.07790423184633255, 0.2472028285264969, 0.21649283170700073, 0.3452037572860718, 0.4299662411212921, 0.365431547164917]
    Last 10:  [-0.06917357444763184, -0.06414197385311127, 0.5184470415115356, 0.270891934633255, 0.11784321814775467, 0.18971776962280273, 0.028397895395755768, 0.4321187734603882, -0.10017403215169907, 0.13142246007919312]

010_model.layers.0.mlp.up_proj: model.layers.0.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003832 σ=0.212775
    First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
    Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
  OUT[0]: [1, 5, 1536] | μ=0.007905 σ=0.451230
    First 10: [0.02607899159193039, 0.9251092672348022, -0.015188708901405334, -0.04484395682811737, 0.8570455312728882, 0.41126400232315063, 0.20365285873413086, -0.04396922141313553, -0.08920896053314209, 0.14263316988945007]
    Last 10:  [0.6473357677459717, 0.11297062039375305, -0.011756278574466705, 0.08435392379760742, 0.147542342543602, -0.5049746036529541, -0.7381752729415894, 0.013063106685876846, -0.3355069160461426, 0.4289051294326782]
  WEIGHT: [1536, 576] | μ=-0.000033

011_model.layers.0.mlp.down_proj: model.layers.0.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000639 σ=0.067761
    First 10: [0.013817382976412773, -0.08090801537036896, -0.003909567836672068, -0.016901526600122452, -0.06676747649908066, 0.10166562348604202, 0.044089384377002716, -0.01517834048718214, -0.03835684061050415, 0.05212265998125076]
    Last 10:  [-0.044778529554605484, -0.007246158551424742, -0.006095007993280888, 0.022850798442959785, 0.017386864870786667, -0.0958026573061943, -0.02096262387931347, 0.005644813645631075, 0.03360908105969429, 0.05636776611208916]
  OUT[0]: [1, 5, 576] | μ=-0.006827 σ=0.147617
    First 10: [0.02055155485868454, 0.2455417662858963, -0.007903367280960083, 0.18999998271465302, -0.3644137680530548, 0.06386332213878632, 0.05789285525679588, -0.12974077463150024, 0.007551177404820919, -0.20064906775951385]
    Last 10:  [-0.013377692550420761, 0.0752212405204773, 0.0833132192492485, -0.03686423599720001, 0.06534500420093536, 0.18399345874786377, -0.01678416132926941, -0.01757822558283806, -0.05646134167909622, -0.03271164372563362]
  WEIGHT: [576, 1536] | μ=0.000001

012_model.layers.0.mlp: model.layers.0.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.003832 σ=0.212775
    First 10: [-0.5636857151985168, -0.16461291909217834, 0.19901920855045319, 0.26324528455734253, 0.6703099608421326, -0.05701792240142822, -0.36621883511543274, 0.07256273180246353, -0.07527357339859009, 0.14713110029697418]
    Last 10:  [0.16693629324436188, 0.23981788754463196, 0.08351199328899384, -0.030767053365707397, 0.1310444325208664, -0.33132416009902954, 0.02962961606681347, 0.35520824790000916, 0.09298987686634064, 0.08731287717819214]
  OUT[0]: [1, 5, 576] | μ=-0.006827 σ=0.147617
    First 10: [0.02055155485868454, 0.2455417662858963, -0.007903367280960083, 0.18999998271465302, -0.3644137680530548, 0.06386332213878632, 0.05789285525679588, -0.12974077463150024, 0.007551177404820919, -0.20064906775951385]
    Last 10:  [-0.013377692550420761, 0.0752212405204773, 0.0833132192492485, -0.03686423599720001, 0.06534500420093536, 0.18399345874786377, -0.01678416132926941, -0.01757822558283806, -0.05646134167909622, -0.03271164372563362]

013_model.layers.1.input_layernorm: model.layers.1.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.007860 σ=0.150465
    First 10: [-0.11046785861253738, 0.20020049810409546, 0.04524993151426315, 0.25987425446510315, -0.025972068309783936, 0.04853171482682228, -0.04315492883324623, -0.10865310579538345, -0.013687399215996265, -0.15728041529655457]
    Last 10:  [0.0313991904258728, 0.142107754945755, 0.1057518869638443, -0.04572979360818863, 0.10165668278932571, 0.08648696541786194, -0.008505542762577534, 0.07780735194683075, -0.030965281650424004, -0.008603125810623169]
  OUT[0]: [1, 5, 576] | μ=-0.016017 σ=0.314043
    First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
    Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
  WEIGHT: [576] | μ=0.284939

014_model.layers.1.self_attn.q_proj: model.layers.1.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016017 σ=0.314043
    First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
    Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
  OUT[0]: [1, 5, 576] | μ=0.033712 σ=1.402174
    First 10: [5.6544270515441895, -2.2833328247070312, 1.322256088256836, 1.120871901512146, -0.2613333761692047, 0.3489985167980194, 0.3242741525173187, -0.08701068162918091, -2.2208964824676514, 0.5514967441558838]
    Last 10:  [-1.7431881427764893, -0.31974127888679504, 2.33636212348938, 0.10411253571510315, 0.5894709229469299, 0.8580677509307861, 0.32326075434684753, -1.1342636346817017, 1.266931176185608, -0.6298837661743164]
  WEIGHT: [576, 576] | μ=0.000128

015_model.layers.1.self_attn.k_proj: model.layers.1.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016017 σ=0.314043
    First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
    Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
  OUT[0]: [1, 5, 192] | μ=0.174347 σ=1.890776
    First 10: [4.307790279388428, 0.24903547763824463, -0.38460683822631836, -0.048388659954071045, -0.9271780252456665, -0.0726182609796524, -1.7457337379455566, -2.346543788909912, -1.1731445789337158, 0.9714718461036682]
    Last 10:  [-1.2557222843170166, 5.824743270874023, -0.5365407466888428, 3.121462821960449, -0.6908756494522095, 2.3866610527038574, 2.171079158782959, 1.6431232690811157, -1.3889046907424927, 0.9824296832084656]
  WEIGHT: [192, 576] | μ=0.000185

016_model.layers.1.self_attn.v_proj: model.layers.1.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016017 σ=0.314043
    First 10: [-0.2257457971572876, 0.6318710446357727, 0.07773788273334503, 0.6744063496589661, -0.02226852811872959, 0.1004805862903595, -0.08600638806819916, -0.1899251639842987, -0.022281447425484657, -0.2982916831970215]
    Last 10:  [0.05213909596204758, 0.2720264196395874, 0.2273782640695572, -0.08342418074607849, 0.176825150847435, 0.15014487504959106, -0.014621627517044544, 0.12418343126773834, -0.05575381964445114, -0.014132398180663586]
  OUT[0]: [1, 5, 192] | μ=-0.006574 σ=0.150328
    First 10: [-0.013140656054019928, 0.05268750712275505, 0.08544564247131348, 0.08652074635028839, 0.016232751309871674, -0.07923436164855957, 0.09547887742519379, -0.022125933319330215, 0.1006070002913475, -0.021838292479515076]
    Last 10:  [-0.04853528365492821, -0.05762871354818344, -0.04027634114027023, 0.15780788660049438, -0.12782247364521027, 0.09446044266223907, 0.05632761865854263, -0.0265656765550375, -0.008354637771844864, -0.2001357078552246]
  WEIGHT: [192, 576] | μ=0.000014

017_model.layers.1.self_attn.o_proj: model.layers.1.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.009280 σ=0.142116
    First 10: [-0.013140656054019928, 0.05268750712275505, 0.08544564247131348, 0.08652074635028839, 0.016232751309871674, -0.07923436164855957, 0.09547887742519379, -0.022125933319330215, 0.1006070002913475, -0.021838292479515076]
    Last 10:  [-0.06786902993917465, -0.011088712140917778, 0.023502111434936523, 0.07350620627403259, -0.14521102607250214, -0.06621630489826202, 0.16627566516399384, 0.08805088698863983, -0.06611942499876022, -0.10407154262065887]
  OUT[0]: [1, 5, 576] | μ=-0.003439 σ=0.094421
    First 10: [0.14699488878250122, 0.16522417962551117, 0.018222704529762268, 0.039799533784389496, -0.36835792660713196, -0.45509135723114014, 0.6053880453109741, -0.1511288583278656, 0.01855621114373207, 0.06573210656642914]
    Last 10:  [-0.007342725992202759, -0.06285791099071503, -0.0969674289226532, 0.032375916838645935, -0.009292319416999817, -0.09914619475603104, 0.015424775891005993, -0.07605130970478058, -0.055568400770425797, 0.057337209582328796]
  WEIGHT: [576, 576] | μ=-0.000006

018_model.layers.1.self_attn: model.layers.1.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.003439 σ=0.094421
    First 10: [0.14699488878250122, 0.16522417962551117, 0.018222704529762268, 0.039799533784389496, -0.36835792660713196, -0.45509135723114014, 0.6053880453109741, -0.1511288583278656, 0.01855621114373207, 0.06573210656642914]
    Last 10:  [-0.007342725992202759, -0.06285791099071503, -0.0969674289226532, 0.032375916838645935, -0.009292319416999817, -0.09914619475603104, 0.015424775891005993, -0.07605130970478058, -0.055568400770425797, 0.057337209582328796]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.295431
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.9421323537826538, 0.057867635041475296, 0.0, 0.0, 0.0]
    Last 10:  [0.1998409777879715, 0.20282889902591705, 0.0888049304485321, 0.5085251927375793, 0.0, 0.2701634168624878, 0.26139888167381287, 0.040388479828834534, 0.2363702803850174, 0.19167892634868622]

019_model.layers.1.post_attention_layernorm: model.layers.1.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.011299 σ=0.179623
    First 10: [0.03652703016996384, 0.3654246926307678, 0.06347263604402542, 0.29967379570007324, -0.3943299949169159, -0.40655964612960815, 0.5622330904006958, -0.25978195667266846, 0.0048688119277358055, -0.09154830873012543]
    Last 10:  [0.024056464433670044, 0.07924984395503998, 0.008784458041191101, -0.013353876769542694, 0.0923643633723259, -0.012659229338169098, 0.006919233128428459, 0.001756042242050171, -0.08653368055820465, 0.04873408377170563]
  OUT[0]: [1, 5, 576] | μ=-0.009972 σ=0.203663
    First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
    Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
  WEIGHT: [576] | μ=0.266304

020_model.layers.1.mlp.gate_proj: model.layers.1.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.009972 σ=0.203663
    First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
    Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
  OUT[0]: [1, 5, 1536] | μ=-0.030218 σ=0.220124
    First 10: [-0.2788000702857971, -0.15132349729537964, -0.29731041193008423, 0.05133878067135811, 0.10717805474996567, 0.19758184254169464, 0.11098693311214447, 0.17757414281368256, 0.026973171159625053, -0.048807285726070404]
    Last 10:  [-0.099079929292202, 0.07228939235210419, -0.3826596140861511, -0.1305699199438095, 0.19183552265167236, 0.4133463501930237, 0.06703313440084457, 0.24891294538974762, -0.0895785391330719, -0.027612023055553436]
  WEIGHT: [1536, 576] | μ=0.000043

021_model.layers.1.mlp.act_fn: model.layers.1.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.030218 σ=0.220124
    First 10: [-0.2788000702857971, -0.15132349729537964, -0.29731041193008423, 0.05133878067135811, 0.10717805474996567, 0.19758184254169464, 0.11098693311214447, 0.17757414281368256, 0.026973171159625053, -0.048807285726070404]
    Last 10:  [-0.099079929292202, 0.07228939235210419, -0.3826596140861511, -0.1305699199438095, 0.19183552265167236, 0.4133463501930237, 0.06703313440084457, 0.24891294538974762, -0.0895785391330719, -0.027612023055553436]
  OUT[0]: [1, 5, 1536] | μ=-0.003073 σ=0.116947
    First 10: [-0.12009256333112717, -0.06994794309139252, -0.1267181932926178, 0.026328163221478462, 0.056458063423633575, 0.10851893573999405, 0.05856983736157417, 0.09664956480264664, 0.013668462634086609, -0.023808224126696587]
    Last 10:  [-0.047087762504816055, 0.03745056688785553, -0.1551629602909088, -0.06102887541055679, 0.10508987307548523, 0.24878902733325958, 0.03463950753211975, 0.1398664116859436, -0.04278453066945076, -0.013615417294204235]

022_model.layers.1.mlp.up_proj: model.layers.1.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.009972 σ=0.203663
    First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
    Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
  OUT[0]: [1, 5, 1536] | μ=-0.001334 σ=0.187504
    First 10: [0.04637036845088005, 0.03801455348730087, 0.24506424367427826, 0.0020537003874778748, -0.10567010194063187, -0.13597896695137024, -0.3662850260734558, -0.2959253787994385, -0.1478966772556305, -0.014155663549900055]
    Last 10:  [-0.08734194934368134, 0.13003328442573547, -0.1292160451412201, -0.014719143509864807, -0.06583994626998901, 0.0639127641916275, -0.25653275847435, 0.2551459074020386, -0.04634295403957367, -0.22183072566986084]
  WEIGHT: [1536, 576] | μ=0.000013

023_model.layers.1.mlp.down_proj: model.layers.1.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000776 σ=0.055321
    First 10: [-0.005568736232817173, -0.002659039804711938, -0.031054098159074783, 5.4070158512331545e-05, -0.005965929478406906, -0.014756293036043644, -0.02145325392484665, -0.028601059690117836, -0.0020215201657265425, 0.0003370212216395885]
    Last 10:  [0.004112736787647009, 0.004869820084422827, 0.02004954405128956, 0.0008982927538454533, -0.006919111590832472, 0.01590079441666603, -0.008886168710887432, 0.035686343908309937, 0.0019827615469694138, 0.003020317992195487]
  OUT[0]: [1, 5, 576] | μ=-0.001952 σ=0.099599
    First 10: [-0.057210881263017654, 0.1283363699913025, -0.048121389001607895, 0.12318471074104309, -0.6774101257324219, -0.1365349292755127, 0.037335556000471115, -0.09794369339942932, 0.0053521692752838135, -0.059597574174404144]
    Last 10:  [-0.011542011983692646, -0.019285093992948532, -0.006983857601881027, -0.011420756578445435, 0.04144163429737091, 0.05083943158388138, -0.08604114502668381, 0.056537989526987076, -0.046440817415714264, 0.0858861654996872]
  WEIGHT: [576, 1536] | μ=0.000023

024_model.layers.1.mlp: model.layers.1.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.009972 σ=0.203663
    First 10: [0.04591430723667145, 0.2841661870479584, 0.09497812390327454, 0.4187515079975128, -0.22176703810691833, -0.42926642298698425, 0.45059844851493835, -0.377658873796463, 0.00731602031737566, -0.07773065567016602]
    Last 10:  [0.045423489063978195, 0.16359646618366241, 0.013882902450859547, -0.02276083268225193, 0.179494708776474, -0.02151869237422943, 0.0131602818146348, 0.0032700481824576855, -0.16339315474033356, 0.09082575142383575]
  OUT[0]: [1, 5, 576] | μ=-0.001952 σ=0.099599
    First 10: [-0.057210881263017654, 0.1283363699913025, -0.048121389001607895, 0.12318471074104309, -0.6774101257324219, -0.1365349292755127, 0.037335556000471115, -0.09794369339942932, 0.0053521692752838135, -0.059597574174404144]
    Last 10:  [-0.011542011983692646, -0.019285093992948532, -0.006983857601881027, -0.011420756578445435, 0.04144163429737091, 0.05083943158388138, -0.08604114502668381, 0.056537989526987076, -0.046440817415714264, 0.0858861654996872]

025_model.layers.2.input_layernorm: model.layers.2.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.013250 σ=0.240525
    First 10: [-0.020683851093053818, 0.4937610626220703, 0.015351247042417526, 0.42285850644111633, -1.0717401504516602, -0.5430945754051208, 0.5995686650276184, -0.3577256500720978, 0.010220981203019619, -0.15114587545394897]
    Last 10:  [0.012514452449977398, 0.059964749962091446, 0.0018006004393100739, -0.02477463334798813, 0.1338059902191162, 0.03818020224571228, -0.0791219100356102, 0.05829403176903725, -0.1329745054244995, 0.13462024927139282]
  OUT[0]: [1, 5, 576] | μ=-0.015760 σ=0.335650
    First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
    Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
  WEIGHT: [576] | μ=0.404170

026_model.layers.2.self_attn.q_proj: model.layers.2.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.015760 σ=0.335650
    First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
    Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
  OUT[0]: [1, 5, 576] | μ=-0.000261 σ=1.064702
    First 10: [0.7333023548126221, -0.9111893773078918, 0.19315338134765625, -0.022953733801841736, 0.21768642961978912, 0.5415355563163757, -0.07642477750778198, 0.15493005514144897, -0.011029332876205444, 0.14526233077049255]
    Last 10:  [-0.3527904748916626, -0.7480610609054565, 1.3232334852218628, 2.100210428237915, -0.5787128806114197, -0.020605504512786865, -0.4597489833831787, 0.5251026153564453, 0.24047750234603882, -0.43320098519325256]
  WEIGHT: [576, 576] | μ=-0.000047

027_model.layers.2.self_attn.k_proj: model.layers.2.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.015760 σ=0.335650
    First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
    Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
  OUT[0]: [1, 5, 192] | μ=0.030963 σ=1.076693
    First 10: [0.3599778413772583, 0.8220376968383789, -0.18712687492370605, 0.24025574326515198, 0.18656666576862335, -0.38477277755737305, 0.32027801871299744, -0.38603395223617554, 0.8414723873138428, 1.2157974243164062]
    Last 10:  [-0.11390848457813263, 3.0013866424560547, 0.17063909769058228, 1.1882054805755615, 1.1870455741882324, -0.8459323048591614, -0.30034971237182617, 0.3669646680355072, -3.0457301139831543, -0.19939634203910828]
  WEIGHT: [192, 576] | μ=-0.000021

028_model.layers.2.self_attn.v_proj: model.layers.2.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.015760 σ=0.335650
    First 10: [-0.03284836560487747, 0.8172115683555603, 0.022495094686746597, 0.5935037732124329, -0.748441219329834, -1.1198481321334839, 1.000051498413086, -0.5423147678375244, 0.013599876314401627, -0.15582960844039917]
    Last 10:  [0.027622269466519356, 0.13598105311393738, 0.0032051114831119776, -0.05355169624090195, 0.3093615770339966, 0.09191495180130005, -0.19271011650562286, 0.12952975928783417, -0.2902897298336029, 0.3456057906150818]
  OUT[0]: [1, 5, 192] | μ=-0.005181 σ=0.124867
    First 10: [0.10930074751377106, 0.01852099969983101, -0.03498408943414688, -0.016908537596464157, -0.10928560793399811, 0.12025588005781174, 0.06853493303060532, -0.0369487889111042, -0.033113833516836166, -0.01424737274646759]
    Last 10:  [-0.09038268029689789, 0.09866522252559662, -0.1454405039548874, 0.0017186123877763748, 0.09347393363714218, 0.02997349388897419, 0.40196937322616577, 0.03717218339443207, -0.009397335350513458, 0.07749859988689423]
  WEIGHT: [192, 576] | μ=-0.000105

029_model.layers.2.self_attn.o_proj: model.layers.2.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007934 σ=0.094966
    First 10: [0.10930074751377106, 0.01852099969983101, -0.03498408943414688, -0.016908537596464157, -0.10928560793399811, 0.12025588005781174, 0.06853493303060532, -0.0369487889111042, -0.033113833516836166, -0.01424737274646759]
    Last 10:  [-0.07381939888000488, 0.05289873480796814, -0.07953393459320068, -0.024912891909480095, -0.0200420580804348, 0.03782053664326668, 0.039256174117326736, -0.08953704684972763, 0.03561319410800934, 0.020485728979110718]
  OUT[0]: [1, 5, 576] | μ=-0.000555 σ=0.040133
    First 10: [0.03333117812871933, -0.04117128252983093, -0.045981816947460175, -0.014533319510519505, -0.01351021695882082, -0.047197747975587845, 0.043454091995954514, 0.04102688655257225, -0.031258583068847656, 0.05345965176820755]
    Last 10:  [-0.005331395659595728, -0.07464171200990677, 0.03104270249605179, -0.003783557564020157, 0.03477132320404053, 0.0362672433257103, 0.016830790787935257, 0.03133479878306389, 0.033532146364450455, -0.09051783382892609]
  WEIGHT: [576, 576] | μ=-0.000005

030_model.layers.2.self_attn: model.layers.2.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000555 σ=0.040133
    First 10: [0.03333117812871933, -0.04117128252983093, -0.045981816947460175, -0.014533319510519505, -0.01351021695882082, -0.047197747975587845, 0.043454091995954514, 0.04102688655257225, -0.031258583068847656, 0.05345965176820755]
    Last 10:  [-0.005331395659595728, -0.07464171200990677, 0.03104270249605179, -0.003783557564020157, 0.03477132320404053, 0.0362672433257103, 0.016830790787935257, 0.03133479878306389, 0.033532146364450455, -0.09051783382892609]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.253912
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4496649503707886, 0.5503350496292114, 0.0, 0.0, 0.0]
    Last 10:  [0.24087342619895935, 0.48508960008621216, 0.08232543617486954, 0.19171148538589478, 0.0, 0.16241221129894257, 0.3128097951412201, 0.1413467824459076, 0.29635104537010193, 0.08708019554615021]

031_model.layers.2.post_attention_layernorm: model.layers.2.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.013805 σ=0.241902
    First 10: [0.012647327035665512, 0.4525897800922394, -0.03063056990504265, 0.4083251953125, -1.0852503776550293, -0.5902923345565796, 0.6430227756500244, -0.31669875979423523, -0.021037600934505463, -0.09768622368574142]
    Last 10:  [0.00718305679038167, -0.014676962047815323, 0.03284330293536186, -0.028558190912008286, 0.16857731342315674, 0.07444744557142258, -0.06229111924767494, 0.08962883055210114, -0.09944236278533936, 0.044102415442466736]
  OUT[0]: [1, 5, 576] | μ=-0.007226 σ=0.221753
    First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
    Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
  WEIGHT: [576] | μ=0.304605

032_model.layers.2.mlp.gate_proj: model.layers.2.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007226 σ=0.221753
    First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
    Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
  OUT[0]: [1, 5, 1536] | μ=-0.083423 σ=0.222432
    First 10: [-0.14181184768676758, -0.041152454912662506, -0.07861782610416412, 0.14319467544555664, 0.05370566248893738, 0.1934967339038849, 0.15107406675815582, -0.14219583570957184, 0.30142146348953247, 0.1317731738090515]
    Last 10:  [0.2716795802116394, 0.07624541223049164, 0.10985670238733292, -0.1743449568748474, -0.145074725151062, -0.41468238830566406, -0.2947149872779846, 0.00977267324924469, 0.11093562841415405, 0.20685385167598724]
  WEIGHT: [1536, 576] | μ=-0.000018

033_model.layers.2.mlp.act_fn: model.layers.2.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.083423 σ=0.222432
    First 10: [-0.14181184768676758, -0.041152454912662506, -0.07861782610416412, 0.14319467544555664, 0.05370566248893738, 0.1934967339038849, 0.15107406675815582, -0.14219583570957184, 0.30142146348953247, 0.1317731738090515]
    Last 10:  [0.2716795802116394, 0.07624541223049164, 0.10985670238733292, -0.1743449568748474, -0.145074725151062, -0.41468238830566406, -0.2947149872779846, 0.00977267324924469, 0.11093562841415405, 0.20685385167598724]
  OUT[0]: [1, 5, 1536] | μ=-0.027995 σ=0.105524
    First 10: [-0.06588667631149292, -0.020152907818555832, -0.03776451572775841, 0.07671477645635605, 0.027573732659220695, 0.10607951134443283, 0.08123204857110977, -0.06605149805545807, 0.17325404286384583, 0.07022135704755783]
    Last 10:  [0.15417957305908203, 0.03957534208893776, 0.05794244632124901, -0.07959263026714325, -0.06728489696979523, -0.1649564653635025, -0.12579907476902008, 0.004910212941467762, 0.05854133889079094, 0.1140860766172409]

034_model.layers.2.mlp.up_proj: model.layers.2.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007226 σ=0.221753
    First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
    Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
  OUT[0]: [1, 5, 1536] | μ=0.005740 σ=0.218702
    First 10: [-0.0445304773747921, -0.06631763279438019, 0.09514583647251129, 0.01395963504910469, 0.2227368950843811, 0.1034187376499176, 0.21252581477165222, 0.08791664242744446, 0.28555843234062195, 0.11279655992984772]
    Last 10:  [-0.010365337133407593, 0.05311800539493561, -0.10396414995193481, 0.08182191848754883, 0.23828716576099396, -0.215040922164917, 0.29237526655197144, 0.09003258496522903, -0.16172713041305542, -0.43905413150787354]
  WEIGHT: [1536, 576] | μ=0.000011

035_model.layers.2.mlp.down_proj: model.layers.2.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000894 σ=0.102264
    First 10: [0.002933965064585209, 0.0013364931801334023, -0.0035931363236159086, 0.0010709102498367429, 0.006141687743365765, 0.01097060926258564, 0.017263907939195633, -0.005807025823742151, 0.04947415366768837, 0.007920727133750916]
    Last 10:  [-0.0015981232281774282, 0.002102163154631853, -0.0060239373706281185, -0.006512421648949385, -0.016033127903938293, 0.03547238931059837, -0.03678053617477417, 0.0004420791519805789, -0.009467722848057747, -0.050089962780475616]
  OUT[0]: [1, 5, 576] | μ=0.003222 σ=0.148650
    First 10: [0.013843273743987083, 0.0741475448012352, 0.233220174908638, 0.08282329887151718, -0.094773069024086, -0.12372307479381561, -0.36185935139656067, -0.15723083913326263, 0.19096963107585907, -0.35864683985710144]
    Last 10:  [-0.027818966656923294, 0.026171445846557617, -0.03871838375926018, -0.07840397953987122, 0.04126948118209839, -0.06331732869148254, 0.05096636712551117, 0.02210124582052231, 0.01394903939217329, 0.01598244160413742]
  WEIGHT: [576, 1536] | μ=-0.000012

036_model.layers.2.mlp: model.layers.2.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.007226 σ=0.221753
    First 10: [0.012949680909514427, 0.3019125461578369, -0.036179836839437485, 0.4421669840812683, -0.47675642371177673, -0.45733922719955444, 0.3822319507598877, -0.3562096953392029, -0.026035642251372337, -0.06808646023273468]
    Last 10:  [0.013339575380086899, -0.0275681484490633, 0.053842943161726, -0.04753825441002846, 0.31932878494262695, 0.12827424705028534, -0.11716852337121964, 0.16133269667625427, -0.181109219789505, 0.08237045258283615]
  OUT[0]: [1, 5, 576] | μ=0.003222 σ=0.148650
    First 10: [0.013843273743987083, 0.0741475448012352, 0.233220174908638, 0.08282329887151718, -0.094773069024086, -0.12372307479381561, -0.36185935139656067, -0.15723083913326263, 0.19096963107585907, -0.35864683985710144]
    Last 10:  [-0.027818966656923294, 0.026171445846557617, -0.03871838375926018, -0.07840397953987122, 0.04126948118209839, -0.06331732869148254, 0.05096636712551117, 0.02210124582052231, 0.01394903939217329, 0.01598244160413742]

037_model.layers.3.input_layernorm: model.layers.3.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.010583 σ=0.303761
    First 10: [0.026490600779652596, 0.5267373323440552, 0.20258960127830505, 0.4911485016345978, -1.180023431777954, -0.7140154242515564, 0.28116342425346375, -0.47392958402633667, 0.1699320375919342, -0.45633307099342346]
    Last 10:  [-0.02063591033220291, 0.011494483798742294, -0.005875080823898315, -0.1069621741771698, 0.20984679460525513, 0.011130116879940033, -0.011324752122163773, 0.11173007637262344, -0.08549332618713379, 0.060084857046604156]
  OUT[0]: [1, 5, 576] | μ=-0.012797 σ=0.347329
    First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
    Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
  WEIGHT: [576] | μ=0.389791

038_model.layers.3.self_attn.q_proj: model.layers.3.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.012797 σ=0.347329
    First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
    Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
  OUT[0]: [1, 5, 576] | μ=-0.034215 σ=0.915691
    First 10: [-0.9603913426399231, 1.0111327171325684, 1.2033007144927979, -0.6479910612106323, -1.7761136293411255, -1.1252028942108154, -0.48106515407562256, 1.0533173084259033, 0.48715144395828247, -0.9295320510864258]
    Last 10:  [0.04749685525894165, -3.303799629211426, -1.9044828414916992, -1.8732447624206543, -0.6835434436798096, -1.5139360427856445, 0.14144492149353027, -0.5830146074295044, -1.2441668510437012, -1.078377604484558]
  WEIGHT: [576, 576] | μ=0.000111

039_model.layers.3.self_attn.k_proj: model.layers.3.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.012797 σ=0.347329
    First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
    Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
  OUT[0]: [1, 5, 192] | μ=-0.068470 σ=1.258778
    First 10: [0.527774453163147, -0.252979576587677, -0.666265606880188, -0.08112695068120956, 0.5307960510253906, 0.1443297266960144, -0.0904371440410614, -0.4899388551712036, 1.4791536331176758, 0.4138545095920563]
    Last 10:  [1.6532325744628906, -0.17497801780700684, -1.088820219039917, 0.6564754247665405, -1.3117616176605225, 3.0680859088897705, -0.4599526524543762, 0.8695080280303955, 0.9346773624420166, 0.20680835843086243]
  WEIGHT: [192, 576] | μ=-0.000202

040_model.layers.3.self_attn.v_proj: model.layers.3.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.012797 σ=0.347329
    First 10: [0.02543431520462036, 0.5940873026847839, 0.1936589628458023, 0.4627060294151306, -0.5909611582756042, -0.9821706414222717, 0.35599222779273987, -0.4821004271507263, 0.16611893475055695, -0.3001391291618347]
    Last 10:  [-0.04051274433732033, 0.026094013825058937, -0.010461477562785149, -0.20348070561885834, 0.4724937081336975, 0.02392694540321827, -0.02504950389266014, 0.23220974206924438, -0.1712348759174347, 0.14236222207546234]
  OUT[0]: [1, 5, 192] | μ=-0.000508 σ=0.134544
    First 10: [-0.06963096559047699, -0.0823480486869812, -0.16832402348518372, 0.035402826964855194, 0.04505624249577522, 0.030097216367721558, 0.015270955860614777, 0.08999179303646088, 0.07497339695692062, -0.03490070998668671]
    Last 10:  [-0.03366328775882721, 0.19623829424381256, -0.018137972801923752, 0.1117287129163742, -0.03150084614753723, 0.07688938081264496, -0.08506433665752411, 0.08511245250701904, 0.005982831120491028, -0.10437114536762238]
  WEIGHT: [192, 576] | μ=0.000110

041_model.layers.3.self_attn.o_proj: model.layers.3.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.001692 σ=0.092180
    First 10: [-0.06963096559047699, -0.0823480486869812, -0.16832402348518372, 0.035402826964855194, 0.04505624249577522, 0.030097216367721558, 0.015270955860614777, 0.08999179303646088, 0.07497339695692062, -0.03490070998668671]
    Last 10:  [0.07921987771987915, 0.04688526317477226, 0.004165417514741421, 0.0407702811062336, -0.018677763640880585, -0.026585672050714493, -0.05888291075825691, 0.23680880665779114, 0.027741873636841774, 0.006870264653116465]
  OUT[0]: [1, 5, 576] | μ=0.002011 σ=0.075345
    First 10: [0.04790180176496506, 0.00790725089609623, 0.017801187932491302, -0.029068030416965485, 0.044126514345407486, -0.07165082544088364, 0.22726190090179443, 0.0024346911814063787, 0.017036156728863716, 0.0885390043258667]
    Last 10:  [-0.0018596882000565529, -0.058536283671855927, 0.0740852802991867, -0.04166227951645851, -0.030921295285224915, 0.0032780999317765236, 0.051121316850185394, 0.0303204283118248, 0.06873306632041931, -0.036461178213357925]
  WEIGHT: [576, 576] | μ=0.000009

042_model.layers.3.self_attn: model.layers.3.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.002011 σ=0.075345
    First 10: [0.04790180176496506, 0.00790725089609623, 0.017801187932491302, -0.029068030416965485, 0.044126514345407486, -0.07165082544088364, 0.22726190090179443, 0.0024346911814063787, 0.017036156728863716, 0.0885390043258667]
    Last 10:  [-0.0018596882000565529, -0.058536283671855927, 0.0740852802991867, -0.04166227951645851, -0.030921295285224915, 0.0032780999317765236, 0.051121316850185394, 0.0303204283118248, 0.06873306632041931, -0.036461178213357925]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.270087
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5563852787017822, 0.4436147212982178, 0.0, 0.0, 0.0]
    Last 10:  [0.3896559476852417, 0.42685142159461975, 0.054475583136081696, 0.12901698052883148, 0.0, 0.35147666931152344, 0.3832237422466278, 0.05836702510714531, 0.1851484179496765, 0.021784164011478424]

043_model.layers.3.post_attention_layernorm: model.layers.3.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.008572 σ=0.303909
    First 10: [0.0743924006819725, 0.534644603729248, 0.22039079666137695, 0.4620804786682129, -1.135896921157837, -0.7856662273406982, 0.5084253549575806, -0.4714948832988739, 0.18696819245815277, -0.36779406666755676]
    Last 10:  [-0.02249559760093689, -0.04704179987311363, 0.06821019947528839, -0.148624449968338, 0.1789254993200302, 0.014408216811716557, 0.03979656472802162, 0.14205050468444824, -0.016760259866714478, 0.02362367883324623]
  OUT[0]: [1, 5, 576] | μ=-0.001490 σ=0.272908
    First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
    Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
  WEIGHT: [576] | μ=0.329189

044_model.layers.3.mlp.gate_proj: model.layers.3.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001490 σ=0.272908
    First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
    Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
  OUT[0]: [1, 5, 1536] | μ=-0.140182 σ=0.327649
    First 10: [-0.12652917206287384, -0.12910917401313782, -0.16115781664848328, -0.08507073670625687, -0.2554311454296112, -0.38588207960128784, -0.05802881345152855, -0.18510855734348297, -0.04753420501947403, -0.46895045042037964]
    Last 10:  [0.16312150657176971, 0.30525004863739014, -0.20691262185573578, 0.34166014194488525, -0.5112568140029907, -0.4452665150165558, -0.15119311213493347, -0.10216635465621948, -0.15853510797023773, 0.1006893664598465]
  WEIGHT: [1536, 576] | μ=0.000112

045_model.layers.3.mlp.act_fn: model.layers.3.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.140182 σ=0.327649
    First 10: [-0.12652917206287384, -0.12910917401313782, -0.16115781664848328, -0.08507073670625687, -0.2554311454296112, -0.38588207960128784, -0.05802881345152855, -0.18510855734348297, -0.04753420501947403, -0.46895045042037964]
    Last 10:  [0.16312150657176971, 0.30525004863739014, -0.20691262185573578, 0.34166014194488525, -0.5112568140029907, -0.4452665150165558, -0.15119311213493347, -0.10216635465621948, -0.15853510797023773, 0.1006893664598465]
  OUT[0]: [1, 5, 1536] | μ=-0.042037 σ=0.191050
    First 10: [-0.05926751345396042, -0.0603930726647377, -0.07409995794296265, -0.040727198123931885, -0.11149241775274277, -0.15616995096206665, -0.028172805905342102, -0.08401235938072205, -0.023202331736683846, -0.18048247694969177]
    Last 10:  [0.08819819241762161, 0.17574021220207214, -0.0927911251783371, 0.19973237812519073, -0.19166962802410126, -0.17387068271636963, -0.06989257782697678, -0.048475950956344604, -0.07299733906984329, 0.05287713184952736]

046_model.layers.3.mlp.up_proj: model.layers.3.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001490 σ=0.272908
    First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
    Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
  OUT[0]: [1, 5, 1536] | μ=-0.005932 σ=0.272456
    First 10: [0.08344591408967972, 0.1461186408996582, -0.2568758428096771, -0.02086719125509262, 0.0530514121055603, 0.31716015934944153, 0.19432389736175537, 0.009800553321838379, -0.07615585625171661, 0.12208016216754913]
    Last 10:  [-0.49951571226119995, 0.0481812059879303, 0.12930969893932343, -0.15984658896923065, -0.11159844696521759, -0.0447094701230526, 0.15453165769577026, 0.06432764232158661, -0.49257153272628784, -0.48752832412719727]
  WEIGHT: [1536, 576] | μ=-0.000027

047_model.layers.3.mlp.down_proj: model.layers.3.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000615 σ=0.068785
    First 10: [-0.004945631604641676, -0.008824553340673447, 0.0190344899892807, 0.000849862233735621, -0.005914830137044191, -0.049530886113643646, -0.005474649369716644, -0.0008233676198869944, 0.0017669934313744307, -0.02203333005309105]
    Last 10:  [-0.04405638203024864, 0.008467375300824642, -0.011998792178928852, -0.03192653879523277, 0.021390032023191452, 0.007773666176944971, -0.010800615884363651, -0.0031183436512947083, 0.03595641255378723, -0.025779100134968758]
  OUT[0]: [1, 5, 576] | μ=0.004070 σ=0.096762
    First 10: [0.10987060517072678, -0.19733841717243195, 0.13236036896705627, 0.054239749908447266, 0.12869082391262054, 0.17019742727279663, 0.004337344318628311, 0.15330487489700317, 0.09178666770458221, 0.04711531847715378]
    Last 10:  [0.06788361072540283, 0.06107707694172859, 0.007321672514081001, 0.18564552068710327, 0.046938370913267136, -0.006687292829155922, 0.07814761251211166, 0.02487228810787201, -0.09398137778043747, 0.027552496641874313]
  WEIGHT: [576, 1536] | μ=-0.000017

048_model.layers.3.mlp: model.layers.3.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.001490 σ=0.272908
    First 10: [0.05903168395161629, 0.3243979513645172, 0.18831613659858704, 0.3934372663497925, -0.4438227713108063, -0.4650910198688507, 0.2742809057235718, -0.41994673013687134, 0.17555297911167145, -0.19697174429893494]
    Last 10:  [-0.044095028191804886, -0.09549150615930557, 0.12061646580696106, -0.26899492740631104, 0.37352848052978516, 0.026773300021886826, 0.07998329401016235, 0.2843501567840576, -0.033729810267686844, 0.04484828561544418]
  OUT[0]: [1, 5, 576] | μ=0.004070 σ=0.096762
    First 10: [0.10987060517072678, -0.19733841717243195, 0.13236036896705627, 0.054239749908447266, 0.12869082391262054, 0.17019742727279663, 0.004337344318628311, 0.15330487489700317, 0.09178666770458221, 0.04711531847715378]
    Last 10:  [0.06788361072540283, 0.06107707694172859, 0.007321672514081001, 0.18564552068710327, 0.046938370913267136, -0.006687292829155922, 0.07814761251211166, 0.02487228810787201, -0.09398137778043747, 0.027552496641874313]

049_model.layers.4.input_layernorm: model.layers.4.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.004501 σ=0.291158
    First 10: [0.18426300585269928, 0.3373062014579773, 0.3527511656284332, 0.5163202285766602, -1.0072060823440552, -0.6154688000679016, 0.512762725353241, -0.3181900084018707, 0.278754860162735, -0.3206787407398224]
    Last 10:  [0.04538801312446594, 0.01403527706861496, 0.07553187012672424, 0.03702107071876526, 0.22586387395858765, 0.007720923982560635, 0.11794418096542358, 0.16692279279232025, -0.11074163764715195, 0.051176175475120544]
  OUT[0]: [1, 5, 576] | μ=-0.008128 σ=0.345495
    First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
    Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
  WEIGHT: [576] | μ=0.368273

050_model.layers.4.self_attn.q_proj: model.layers.4.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008128 σ=0.345495
    First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
    Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
  OUT[0]: [1, 5, 576] | μ=0.033955 σ=0.969834
    First 10: [0.3499539792537689, -0.6262015104293823, -0.07102936506271362, 0.6608253717422485, 0.7933931946754456, -0.07798846065998077, -0.5186876058578491, 0.0365929901599884, 0.49463796615600586, -0.29091185331344604]
    Last 10:  [0.015421181917190552, 1.6089082956314087, 0.7674106359481812, 0.9109204411506653, 0.12071073055267334, 1.076124906539917, -0.3998664617538452, 0.8778401017189026, 0.8417953252792358, 0.43057602643966675]
  WEIGHT: [576, 576] | μ=0.000168

051_model.layers.4.self_attn.k_proj: model.layers.4.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008128 σ=0.345495
    First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
    Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
  OUT[0]: [1, 5, 192] | μ=0.065226 σ=1.312274
    First 10: [-0.45099467039108276, 0.48654043674468994, -0.03908967971801758, 0.3915303945541382, 0.140016108751297, 0.6874562501907349, 0.3279276490211487, 1.121958613395691, -0.2037087380886078, 0.30507588386535645]
    Last 10:  [4.374963760375977, 0.6331333518028259, 1.1623709201812744, 0.5452325940132141, -0.4822693169116974, -0.8844747543334961, -0.5815154910087585, 0.8653417229652405, -0.41775965690612793, 0.2902383506298065]
  WEIGHT: [192, 576] | μ=0.000452

052_model.layers.4.self_attn.v_proj: model.layers.4.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008128 σ=0.345495
    First 10: [0.1935109943151474, 0.3906458616256714, 0.34247055649757385, 0.531154215335846, -0.8291765451431274, -1.1230230331420898, 0.6448636651039124, -0.3219519853591919, 0.282231867313385, -0.22458505630493164]
    Last 10:  [0.07776407897472382, 0.02768602967262268, 0.13046610355377197, 0.06672263145446777, 0.47625675797462463, 0.015014424920082092, 0.2669859528541565, 0.326302170753479, -0.2015588879585266, 0.10153569281101227]
  OUT[0]: [1, 5, 192] | μ=0.019253 σ=0.255743
    First 10: [-0.013895686715841293, -0.07613816112279892, 0.04684597626328468, 0.043456338346004486, -0.03315308690071106, 0.0287503432482481, 0.08841660618782043, -0.021907487884163857, -0.004777014255523682, 0.028528928756713867]
    Last 10:  [-0.05991671606898308, -0.09439192712306976, -0.424213707447052, -0.222523495554924, -0.014070529490709305, 0.1818118393421173, -0.116051584482193, 0.061205532401800156, -0.3673054575920105, -0.23282015323638916]
  WEIGHT: [192, 576] | μ=0.000016

053_model.layers.4.self_attn.o_proj: model.layers.4.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.020297 σ=0.239129
    First 10: [-0.013895686715841293, -0.07613816112279892, 0.04684597626328468, 0.043456338346004486, -0.03315308690071106, 0.0287503432482481, 0.08841660618782043, -0.021907487884163857, -0.004777014255523682, 0.028528928756713867]
    Last 10:  [-0.07677724957466125, 0.02002410963177681, -0.11201566457748413, -0.0919637605547905, -0.04795454069972038, 0.24534744024276733, -0.05155344679951668, 0.06982751190662384, -0.22088414430618286, -0.17089876532554626]
  OUT[0]: [1, 5, 576] | μ=0.004233 σ=0.207638
    First 10: [0.23437198996543884, -0.652759313583374, 0.295864999294281, 0.18938946723937988, 0.3568170964717865, 0.18249580264091492, 0.1303810179233551, -0.09144800901412964, 0.1684950441122055, -0.14709945023059845]
    Last 10:  [-0.093539297580719, -0.04752352088689804, -0.002180315088480711, -0.009834744036197662, -0.021878289058804512, -0.032222360372543335, -0.01297493651509285, -0.027456313371658325, -0.00022863643243908882, -0.061685558408498764]
  WEIGHT: [576, 576] | μ=0.000018

054_model.layers.4.self_attn: model.layers.4.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.004233 σ=0.207638
    First 10: [0.23437198996543884, -0.652759313583374, 0.295864999294281, 0.18938946723937988, 0.3568170964717865, 0.18249580264091492, 0.1303810179233551, -0.09144800901412964, 0.1684950441122055, -0.14709945023059845]
    Last 10:  [-0.093539297580719, -0.04752352088689804, -0.002180315088480711, -0.009834744036197662, -0.021878289058804512, -0.032222360372543335, -0.01297493651509285, -0.027456313371658325, -0.00022863643243908882, -0.061685558408498764]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.259131
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5383939146995544, 0.46160608530044556, 0.0, 0.0, 0.0]
    Last 10:  [0.09336786717176437, 0.08206779509782791, 0.16843755543231964, 0.6561267375946045, 0.0, 0.10221681743860245, 0.1113375723361969, 0.13872957229614258, 0.3980114161968231, 0.24970459938049316]

055_model.layers.4.post_attention_layernorm: model.layers.4.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.000269 σ=0.367799
    First 10: [0.4186350107192993, -0.31545311212539673, 0.6486161947250366, 0.70570969581604, -0.6503889560699463, -0.4329729974269867, 0.6431437730789185, -0.40963801741600037, 0.4472498893737793, -0.46777820587158203]
    Last 10:  [-0.04815128445625305, -0.03348824381828308, 0.07335155457258224, 0.027186326682567596, 0.20398558676242828, -0.024501435458660126, 0.10496924817562103, 0.13946647942066193, -0.11097027361392975, -0.01050938293337822]
  OUT[0]: [1, 5, 576] | μ=-0.000176 σ=0.321012
    First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
    Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
  WEIGHT: [576] | μ=0.340310

056_model.layers.4.mlp.gate_proj: model.layers.4.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000176 σ=0.321012
    First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
    Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
  OUT[0]: [1, 5, 1536] | μ=-0.213641 σ=0.595861
    First 10: [-0.26438701152801514, -0.09240249544382095, -0.17170540988445282, -0.1680271178483963, -0.3767663538455963, -0.6068867444992065, -0.4031659960746765, -0.36431801319122314, -0.4601881206035614, -0.5172584652900696]
    Last 10:  [-0.8473682403564453, -0.6399716138839722, -0.803383469581604, -0.775547444820404, 0.1463160514831543, 0.038333386182785034, -0.49034029245376587, -0.3351455330848694, -0.5544290542602539, 0.31126660108566284]
  WEIGHT: [1536, 576] | μ=-0.000102

057_model.layers.4.mlp.act_fn: model.layers.4.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.213641 σ=0.595861
    First 10: [-0.26438701152801514, -0.09240249544382095, -0.17170540988445282, -0.1680271178483963, -0.3767663538455963, -0.6068867444992065, -0.4031659960746765, -0.36431801319122314, -0.4601881206035614, -0.5172584652900696]
    Last 10:  [-0.8473682403564453, -0.6399716138839722, -0.803383469581604, -0.775547444820404, 0.1463160514831543, 0.038333386182785034, -0.49034029245376587, -0.3351455330848694, -0.5544290542602539, 0.31126660108566284]
  OUT[0]: [1, 5, 1536] | μ=-0.055302 σ=0.486111
    First 10: [-0.1148194670677185, -0.04406821355223656, -0.07850007712841034, -0.07697184383869171, -0.15330888330936432, -0.2140912562608719, -0.16148890554904938, -0.14933931827545166, -0.17806574702262878, -0.19319269061088562]
    Last 10:  [-0.25419795513153076, -0.2209520787000656, -0.24848829209804535, -0.2445148229598999, 0.07850059121847153, 0.01953401044011116, -0.18623782694339752, -0.13975206017494202, -0.20227648317813873, 0.1796613335609436]

058_model.layers.4.mlp.up_proj: model.layers.4.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000176 σ=0.321012
    First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
    Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
  OUT[0]: [1, 5, 1536] | μ=0.005057 σ=0.558208
    First 10: [0.3182176649570465, 0.23535531759262085, 0.22958579659461975, -0.05688841640949249, 0.1990031599998474, 0.22303371131420135, -0.1264333724975586, -0.22615531086921692, -0.05546984821557999, -0.2018885463476181]
    Last 10:  [0.19127242267131805, -0.0013832151889801025, -0.2616879343986511, 0.11673107743263245, -0.25986170768737793, -0.2821997404098511, -0.0696256011724472, -0.49264949560165405, -0.19463461637496948, 0.3575946092605591]
  WEIGHT: [1536, 576] | μ=-0.000052

059_model.layers.4.mlp.down_proj: model.layers.4.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.038350 σ=6.269567
    First 10: [-0.03653758391737938, -0.01037168875336647, -0.018022501841187477, 0.004378806333988905, -0.03050895221531391, -0.04774956777691841, 0.0204175878316164, 0.03377388045191765, 0.009877280332148075, 0.039003390818834305]
    Last 10:  [-0.04862105846405029, 0.00030562427127733827, 0.06502638757228851, -0.028542479500174522, -0.020399298518896103, -0.00551249273121357, 0.012966920621693134, 0.06884878128767014, 0.03937000408768654, 0.06424592435359955]
  OUT[0]: [1, 5, 576] | μ=-0.071432 σ=11.620458
    First 10: [-1.4895509481430054, -2.5261735916137695, -0.7691031098365784, -0.06825529783964157, -11.86688232421875, -7.811278820037842, 5.723982334136963, 0.520793080329895, -0.7017213106155396, -1.7934167385101318]
    Last 10:  [-0.02422521449625492, -0.01246379129588604, -0.1001502275466919, 0.12633441388607025, -0.049669299274683, 0.04701988399028778, -0.07920810580253601, -0.044595733284950256, 0.07090699672698975, 0.07987726479768753]
  WEIGHT: [576, 1536] | μ=0.000007

060_model.layers.4.mlp: model.layers.4.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.000176 σ=0.321012
    First 10: [0.25670403242111206, -0.12329547107219696, 0.4414464235305786, 0.4687424600124359, -0.2194998562335968, -0.18250314891338348, 0.29111215472221375, -0.2786068022251129, 0.31109631061553955, -0.22618621587753296]
    Last 10:  [-0.10022466629743576, -0.06643613427877426, 0.12600579857826233, 0.04950006306171417, 0.45049259066581726, -0.04470975697040558, 0.20796412229537964, 0.2837674617767334, -0.21644125878810883, -0.020947536453604698]
  OUT[0]: [1, 5, 576] | μ=-0.071432 σ=11.620458
    First 10: [-1.4895509481430054, -2.5261735916137695, -0.7691031098365784, -0.06825529783964157, -11.86688232421875, -7.811278820037842, 5.723982334136963, 0.520793080329895, -0.7017213106155396, -1.7934167385101318]
    Last 10:  [-0.02422521449625492, -0.01246379129588604, -0.1001502275466919, 0.12633441388607025, -0.049669299274683, 0.04701988399028778, -0.07920810580253601, -0.044595733284950256, 0.07090699672698975, 0.07987726479768753]

061_model.layers.5.input_layernorm: model.layers.5.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.071701 σ=11.735382
    First 10: [-1.070915937423706, -2.8416266441345215, -0.12048691511154175, 0.6374543905258179, -12.517271041870117, -8.24425220489502, 6.367125988006592, 0.11115506291389465, -0.25447142124176025, -2.261194944381714]
    Last 10:  [-0.07237649708986282, -0.04595203697681427, -0.02679867297410965, 0.15352073311805725, 0.15431629121303558, 0.022518448531627655, 0.025761142373085022, 0.09487074613571167, -0.04006327688694, 0.06936788558959961]
  OUT[0]: [1, 5, 576] | μ=-0.006907 σ=0.476014
    First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
    Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
  WEIGHT: [576] | μ=0.552186

062_model.layers.5.self_attn.q_proj: model.layers.5.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006907 σ=0.476014
    First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
    Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
  OUT[0]: [1, 5, 576] | μ=0.006911 σ=1.039437
    First 10: [0.011148429475724697, -0.14922311902046204, -0.09255900979042053, -0.2372174710035324, -0.05077295005321503, -0.31918737292289734, 0.11206366866827011, -0.06453379988670349, 0.08821548521518707, -0.40594813227653503]
    Last 10:  [0.28330445289611816, -4.590604782104492, 0.18568271398544312, -0.2808797359466553, 0.06834368407726288, 0.14365559816360474, -1.5202274322509766, 0.14717307686805725, -0.35367539525032043, -2.315927743911743]
  WEIGHT: [576, 576] | μ=0.000023

063_model.layers.5.self_attn.k_proj: model.layers.5.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006907 σ=0.476014
    First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
    Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
  OUT[0]: [1, 5, 192] | μ=-0.123410 σ=1.410842
    First 10: [0.004205349832773209, -0.015669070184230804, -0.0032307193614542484, 0.02122553251683712, -0.006750196218490601, 0.010638093575835228, 0.0006491690874099731, 0.006316151469945908, 0.0018900725990533829, -0.00372314453125]
    Last 10:  [-0.24470752477645874, 0.31668946146965027, 0.07764565944671631, -0.749000072479248, 0.7401829957962036, 0.43620842695236206, 0.49503323435783386, -1.5563727617263794, 1.2032849788665771, 1.6344879865646362]
  WEIGHT: [192, 576] | μ=-0.000112

064_model.layers.5.self_attn.v_proj: model.layers.5.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006907 σ=0.476014
    First 10: [-0.027215799316763878, -0.10927747935056686, -0.003027117345482111, 0.01751713454723358, -0.3108600974082947, -0.2573649287223816, 0.2311112880706787, 0.004163394682109356, -0.006369905546307564, -0.0960032269358635]
    Last 10:  [-0.20459549129009247, -0.12916983664035797, -0.07717106491327286, 0.496030330657959, 0.43907853960990906, 0.06895037740468979, 0.07377498596906662, 0.2887350618839264, -0.13198593258857727, 0.19572415947914124]
  OUT[0]: [1, 5, 192] | μ=-0.009668 σ=0.243731
    First 10: [0.0011690286919474602, -0.005026388913393021, 0.00967811606824398, -0.010617650113999844, 0.0019956007599830627, 0.005627055186778307, -0.0008182357996702194, 0.0007400680333375931, -0.08969124406576157, -0.007815008983016014]
    Last 10:  [0.05988962948322296, 0.029111593961715698, 0.07845437526702881, 0.17910826206207275, -0.12014491111040115, -0.7699041962623596, -0.7329123616218567, -0.098568394780159, -0.10866663604974747, 0.12320106476545334]
  WEIGHT: [192, 576] | μ=0.000007

065_model.layers.5.self_attn.o_proj: model.layers.5.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001668 σ=0.046043
    First 10: [0.0011690286919474602, -0.005026388913393021, 0.00967811606824398, -0.010617650113999844, 0.0019956007599830627, 0.005627055186778307, -0.0008182357996702194, 0.0007400680333375931, -0.08969124406576157, -0.007815008983016014]
    Last 10:  [-0.012782054953277111, 0.06759936362504959, 0.05926797166466713, 0.016518346965312958, -0.0037660959642380476, -0.16632206737995148, -0.20830589532852173, 0.019624952226877213, -0.002291879616677761, -0.07409363240003586]
  OUT[0]: [1, 5, 576] | μ=-0.002110 σ=0.030210
    First 10: [0.02999141626060009, -0.10225832462310791, 0.002578926272690296, 0.003558077849447727, -0.04219193011522293, 0.060872748494148254, -0.05349038541316986, -0.034452736377716064, -0.02760591171681881, -0.00025088293477892876]
    Last 10:  [-0.031333230435848236, -0.03280074894428253, 0.02215174399316311, -0.005409642122685909, 0.03900017589330673, 0.036158882081508636, 0.014893000945448875, -0.04701649770140648, -0.007887379266321659, -0.059257686138153076]
  WEIGHT: [576, 576] | μ=-0.000016

066_model.layers.5.self_attn: model.layers.5.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.002110 σ=0.030210
    First 10: [0.02999141626060009, -0.10225832462310791, 0.002578926272690296, 0.003558077849447727, -0.04219193011522293, 0.060872748494148254, -0.05349038541316986, -0.034452736377716064, -0.02760591171681881, -0.00025088293477892876]
    Last 10:  [-0.031333230435848236, -0.03280074894428253, 0.02215174399316311, -0.005409642122685909, 0.03900017589330673, 0.036158882081508636, 0.014893000945448875, -0.04701649770140648, -0.007887379266321659, -0.059257686138153076]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.271301
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000327825546265, 0.49996721744537354, 0.0, 0.0, 0.0]
    Last 10:  [0.37404295802116394, 0.37462079524993896, 0.10715237259864807, 0.14418385922908783, 0.0, 0.2872178256511688, 0.2875540256500244, 0.13794679939746857, 0.20291583240032196, 0.08436549454927444]

067_model.layers.5.post_attention_layernorm: model.layers.5.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.073811 σ=11.739354
    First 10: [-1.0409245491027832, -2.94388484954834, -0.11790798604488373, 0.6410124897956848, -12.559462547302246, -8.183379173278809, 6.31363582611084, 0.07670232653617859, -0.2820773422718048, -2.2614457607269287]
    Last 10:  [-0.10370972752571106, -0.0787527859210968, -0.004646928980946541, 0.14811109006404877, 0.19331645965576172, 0.05867733061313629, 0.04065414518117905, 0.04785424843430519, -0.04795065522193909, 0.010110199451446533]
  OUT[0]: [1, 5, 576] | μ=-0.003503 σ=0.317641
    First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
    Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
  WEIGHT: [576] | μ=0.361149

068_model.layers.5.mlp.gate_proj: model.layers.5.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003503 σ=0.317641
    First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
    Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
  OUT[0]: [1, 5, 1536] | μ=-0.154025 σ=0.345780
    First 10: [-0.12451913207769394, -0.22349907457828522, -0.09223880618810654, 0.11339700222015381, -0.1403258740901947, 0.046081554144620895, -0.07643620669841766, -0.17636334896087646, -0.23387077450752258, -0.03838684782385826]
    Last 10:  [-0.33730441331863403, -0.015362799167633057, 0.15328581631183624, -0.013188585638999939, -0.45498764514923096, 0.009647637605667114, -0.20168349146842957, -0.586155891418457, -0.17486010491847992, -0.1503814160823822]
  WEIGHT: [1536, 576] | μ=-0.000246

069_model.layers.5.mlp.act_fn: model.layers.5.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.154025 σ=0.345780
    First 10: [-0.12451913207769394, -0.22349907457828522, -0.09223880618810654, 0.11339700222015381, -0.1403258740901947, 0.046081554144620895, -0.07643620669841766, -0.17636334896087646, -0.23387077450752258, -0.03838684782385826]
    Last 10:  [-0.33730441331863403, -0.015362799167633057, 0.15328581631183624, -0.013188585638999939, -0.45498764514923096, 0.009647637605667114, -0.20168349146842957, -0.586155891418457, -0.17486010491847992, -0.1503814160823822]
  OUT[0]: [1, 5, 1536] | μ=-0.042803 σ=0.145955
    First 10: [-0.05838831514120102, -0.09931330382823944, -0.043993908911943436, 0.05990977957844734, -0.06524816155433655, 0.0235715601593256, -0.0367581881582737, -0.08042575418949127, -0.10332348942756653, -0.01882508210837841]
    Last 10:  [-0.1404752880334854, -0.007622396573424339, 0.08250556886196136, -0.006550808437168598, -0.17661510407924652, 0.004847087897360325, -0.09070701897144318, -0.20956088602542877, -0.07980545610189438, -0.06954769790172577]

070_model.layers.5.mlp.up_proj: model.layers.5.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003503 σ=0.317641
    First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
    Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
  OUT[0]: [1, 5, 1536] | μ=0.000849 σ=0.251089
    First 10: [-0.4126914143562317, 0.06960908323526382, -0.1612505167722702, 0.16509202122688293, -0.37451666593551636, 0.37716835737228394, -0.19824187457561493, -0.02358924224972725, 0.029068293049931526, -0.5193350911140442]
    Last 10:  [0.043286487460136414, 0.06982308626174927, -0.06090734899044037, -0.19634106755256653, 0.3223060369491577, 0.1708851456642151, -0.12156863510608673, -0.07584212720394135, 0.6584208607673645, 0.09324852377176285]
  WEIGHT: [1536, 576] | μ=-0.000092

071_model.layers.5.mlp.down_proj: model.layers.5.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000780 σ=0.040417
    First 10: [0.024096356704831123, -0.006913107819855213, 0.007094040513038635, 0.009890626184642315, 0.024436524137854576, 0.008890446275472641, 0.007287011947482824, 0.001897182548418641, -0.003003437537699938, 0.009776526130735874]
    Last 10:  [-0.006080681923776865, -0.00053221924463287, -0.005025195423513651, 0.001286192680709064, -0.05692411586642265, 0.0008282953058369458, 0.01102712843567133, 0.015893543139100075, -0.05254557728767395, -0.006485220044851303]
  OUT[0]: [1, 5, 576] | μ=-0.001338 σ=0.067386
    First 10: [0.04179523140192032, 0.03470918536186218, 0.037108033895492554, -0.010444579645991325, 0.021473418921232224, 0.021371955052018166, -0.07702352851629257, 0.07774808257818222, 0.05501452460885048, 0.04908186197280884]
    Last 10:  [0.06648523360490799, -0.11129830777645111, -0.003178226761519909, 0.035789813846349716, -0.061983391642570496, -0.04516693949699402, -0.07562772929668427, 0.047824934124946594, -0.082781121134758, 0.036904413253068924]
  WEIGHT: [576, 1536] | μ=0.000002

072_model.layers.5.mlp: model.layers.5.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.003503 σ=0.317641
    First 10: [-0.01984376832842827, -0.04221665486693382, -0.0024044259916990995, 0.012616375461220741, -0.143591046333313, -0.10787899792194366, 0.08622106164693832, 0.001527815475128591, -0.00604541040956974, -0.0279672984033823]
    Last 10:  [-0.23286478221416473, -0.16556937992572784, -0.008495899848639965, 0.28302690386772156, 0.4518113434314728, 0.11512842029333115, 0.0836048498749733, 0.09954151511192322, -0.09647182375192642, 0.02110976353287697]
  OUT[0]: [1, 5, 576] | μ=-0.001338 σ=0.067386
    First 10: [0.04179523140192032, 0.03470918536186218, 0.037108033895492554, -0.010444579645991325, 0.021473418921232224, 0.021371955052018166, -0.07702352851629257, 0.07774808257818222, 0.05501452460885048, 0.04908186197280884]
    Last 10:  [0.06648523360490799, -0.11129830777645111, -0.003178226761519909, 0.035789813846349716, -0.061983391642570496, -0.04516693949699402, -0.07562772929668427, 0.047824934124946594, -0.082781121134758, 0.036904413253068924]

073_model.layers.6.input_layernorm: model.layers.6.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.075150 σ=11.757419
    First 10: [-0.9991292953491211, -2.9091756343841553, -0.08079995214939117, 0.6305679082870483, -12.537988662719727, -8.162007331848145, 6.236612319946289, 0.1544504165649414, -0.22706282138824463, -2.2123639583587646]
    Last 10:  [-0.03722449392080307, -0.1900510936975479, -0.00782515574246645, 0.18390090763568878, 0.13133306801319122, 0.013510391116142273, -0.03497358411550522, 0.09567917883396149, -0.13073177635669708, 0.04701461270451546]
  OUT[0]: [1, 5, 576] | μ=-0.014552 σ=0.359396
    First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
    Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
  WEIGHT: [576] | μ=0.450693

074_model.layers.6.self_attn.q_proj: model.layers.6.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.014552 σ=0.359396
    First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
    Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
  OUT[0]: [1, 5, 576] | μ=-0.001164 σ=0.995748
    First 10: [0.16988146305084229, -0.18693286180496216, -0.31996291875839233, 0.10492569208145142, -0.006624385714530945, -0.35189545154571533, -0.09926792234182358, 0.060010507702827454, 0.20669510960578918, -0.01383022591471672]
    Last 10:  [0.2204882800579071, 0.38752418756484985, 0.890075147151947, 0.30162888765335083, 0.15894903242588043, -0.7649171948432922, 0.2146669626235962, 1.6500415802001953, 1.4152417182922363, 1.4316480159759521]
  WEIGHT: [576, 576] | μ=0.000046

075_model.layers.6.self_attn.k_proj: model.layers.6.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.014552 σ=0.359396
    First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
    Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
  OUT[0]: [1, 5, 192] | μ=-0.024756 σ=1.308856
    First 10: [-0.010584324598312378, 0.002955809235572815, -0.00781702995300293, -0.006952986121177673, 0.007539138197898865, -0.0057596489787101746, -0.007018059492111206, -0.02408599853515625, 0.006329894065856934, -0.009182603098452091]
    Last 10:  [0.6661744713783264, -4.13813591003418, -7.789874076843262, -0.23498621582984924, 0.7908744812011719, -0.5723912715911865, -2.9560964107513428, -0.3319821357727051, -0.17184317111968994, 0.6571153998374939]
  WEIGHT: [192, 576] | μ=-0.000379

076_model.layers.6.self_attn.v_proj: model.layers.6.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.014552 σ=0.359396
    First 10: [-0.029032088816165924, -0.09255833923816681, -0.001844728016294539, 0.015597452409565449, -0.2610529661178589, -0.3049285113811493, 0.197440966963768, 0.004159244243055582, -0.006514335982501507, -0.03932111710309982]
    Last 10:  [-0.0879216194152832, -0.438821405172348, -0.016400350257754326, 0.40073448419570923, 0.3670057952404022, 0.031041692942380905, -0.08860315382480621, 0.24806736409664154, -0.3150438070297241, 0.1266377568244934]
  OUT[0]: [1, 5, 192] | μ=-0.006922 σ=0.188651
    First 10: [0.0011938447132706642, -0.00031461846083402634, -0.0017507178708910942, -0.006220131181180477, 0.004878289997577667, -0.0014283442869782448, 0.0026737451553344727, -0.004144820384681225, 0.00033484864979982376, -0.01543469913303852]
    Last 10:  [0.42791229486465454, -0.1636705994606018, 0.04263182356953621, 0.44120025634765625, -0.11101143062114716, -0.5337561964988708, 0.2755693197250366, -0.3248146176338196, 0.17731505632400513, -0.08723072707653046]
  WEIGHT: [192, 576] | μ=0.000029

077_model.layers.6.self_attn.o_proj: model.layers.6.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.000718 σ=0.045032
    First 10: [0.0011938447132706642, -0.00031461846083402634, -0.0017507178708910942, -0.006220131181180477, 0.004878289997577667, -0.0014283442869782448, 0.0026737451553344727, -0.004144820384681225, 0.00033484864979982376, -0.01543469913303852]
    Last 10:  [0.03290938958525658, -0.01516047865152359, 0.0003158330509904772, 0.030431047081947327, -0.012317821383476257, -0.036268800497055054, 0.02889242395758629, -0.07076777517795563, 0.02717437967658043, 0.007521362509578466]
  OUT[0]: [1, 5, 576] | μ=-0.000639 σ=0.030342
    First 10: [0.010518603026866913, -0.0713873952627182, 0.030366849154233932, 0.0012832274660468102, -0.10351624339818954, 0.035623472183942795, -0.03942456096410751, 0.00708397850394249, -0.03945886716246605, 0.012444126419723034]
    Last 10:  [0.0036127367056906223, 0.018163451924920082, -0.013205692172050476, -0.01990918442606926, 0.03495420143008232, 0.00119050033390522, -0.01550910621881485, -0.08906139433383942, 0.012268740683794022, -0.026396457105875015]
  WEIGHT: [576, 576] | μ=0.000030

078_model.layers.6.self_attn: model.layers.6.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000639 σ=0.030342
    First 10: [0.010518603026866913, -0.0713873952627182, 0.030366849154233932, 0.0012832274660468102, -0.10351624339818954, 0.035623472183942795, -0.03942456096410751, 0.00708397850394249, -0.03945886716246605, 0.012444126419723034]
    Last 10:  [0.0036127367056906223, 0.018163451924920082, -0.013205692172050476, -0.01990918442606926, 0.03495420143008232, 0.00119050033390522, -0.01550910621881485, -0.08906139433383942, 0.012268740683794022, -0.026396457105875015]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.268045
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.499860554933548, 0.5001394152641296, 0.0, 0.0, 0.0]
    Last 10:  [0.4635840952396393, 0.46304911375045776, 0.024189665913581848, 0.04917712137103081, 0.0, 0.42398008704185486, 0.42348578572273254, 0.01828223094344139, 0.05518507957458496, 0.07906688004732132]

079_model.layers.6.post_attention_layernorm: model.layers.6.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.075789 σ=11.759266
    First 10: [-0.9886106848716736, -2.980562925338745, -0.05043310299515724, 0.6318511366844177, -12.641505241394043, -8.126383781433105, 6.197187900543213, 0.1615343987941742, -0.266521692276001, -2.1999199390411377]
    Last 10:  [-0.033611755818128586, -0.17188763618469238, -0.02103084698319435, 0.16399171948432922, 0.16628727316856384, 0.014700891450047493, -0.05048269033432007, 0.00661778450012207, -0.11846303939819336, 0.020618155598640442]
  OUT[0]: [1, 5, 576] | μ=-0.005965 σ=0.321562
    First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
    Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
  WEIGHT: [576] | μ=0.372873

080_model.layers.6.mlp.gate_proj: model.layers.6.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.005965 σ=0.321562
    First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
    Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
  OUT[0]: [1, 5, 1536] | μ=-0.152544 σ=0.342788
    First 10: [-0.040437571704387665, -0.18226295709609985, -0.022938815876841545, 0.13532845675945282, 0.346408873796463, -0.4484005272388458, -0.17972970008850098, -0.33383217453956604, -0.5342963337898254, -0.03976327180862427]
    Last 10:  [0.6838191747665405, -0.3304096758365631, -0.226284459233284, 0.2253202497959137, -0.3593934178352356, 0.4935992658138275, -0.08089885860681534, -0.11824265867471695, -0.6270008683204651, 0.1258198618888855]
  WEIGHT: [1536, 576] | μ=0.000057

081_model.layers.6.mlp.act_fn: model.layers.6.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.152544 σ=0.342788
    First 10: [-0.040437571704387665, -0.18226295709609985, -0.022938815876841545, 0.13532845675945282, 0.346408873796463, -0.4484005272388458, -0.17972970008850098, -0.33383217453956604, -0.5342963337898254, -0.03976327180862427]
    Last 10:  [0.6838191747665405, -0.3304096758365631, -0.226284459233284, 0.2253202497959137, -0.3593934178352356, 0.4935992658138275, -0.08089885860681534, -0.11824265867471695, -0.6270008683204651, 0.1258198618888855]
  OUT[0]: [1, 5, 1536] | μ=-0.042578 σ=0.147085
    First 10: [-0.019810041412711143, -0.08284944295883179, -0.011337867006659508, 0.07223569601774216, 0.2029077708721161, -0.1747601330280304, -0.0818108320236206, -0.13931100070476532, -0.19743071496486664, -0.019486406818032265]
    Last 10:  [0.4544597566127777, -0.13815781474113464, -0.10039541125297546, 0.12529900670051575, -0.1477489322423935, 0.30650243163108826, -0.03881416469812393, -0.05563006550073624, -0.218315988779068, 0.06686237454414368]

082_model.layers.6.mlp.up_proj: model.layers.6.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.005965 σ=0.321562
    First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
    Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
  OUT[0]: [1, 5, 1536] | μ=-0.001380 σ=0.254316
    First 10: [0.01806211844086647, 0.04115426540374756, -0.27983665466308594, -0.040245961397886276, -0.3854528069496155, -0.026170141994953156, 0.0709039717912674, 0.08914915472269058, -0.014766678214073181, -0.05836687237024307]
    Last 10:  [0.19518424570560455, 0.5093384981155396, -0.06118357926607132, 0.21455642580986023, -0.3395906388759613, -0.17478854954242706, -0.01667008548974991, -0.262503981590271, 0.5017397999763489, -0.07819198817014694]
  WEIGHT: [1536, 576] | μ=0.000066

083_model.layers.6.mlp.down_proj: model.layers.6.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000177 σ=0.044493
    First 10: [-0.00035781131009571254, -0.003409608034417033, 0.0031727508176118135, -0.0029071951285004616, -0.07821136713027954, 0.0045734974555671215, -0.00580071285367012, -0.012419457547366619, 0.002915395889431238, 0.0011373605811968446]
    Last 10:  [0.08870338648557663, -0.07036909461021423, 0.006142550613731146, 0.02688370645046234, 0.050174154341220856, -0.0535731166601181, 0.0006470354273915291, 0.014603113755583763, -0.10953781753778458, -0.005228102207183838]
  OUT[0]: [1, 5, 576] | μ=0.001956 σ=0.074355
    First 10: [0.09686530381441116, 0.06077098473906517, -0.002336648292839527, 0.01037447340786457, 0.10881470888853073, 0.038441069424152374, -0.14172066748142242, -0.04874054342508316, 0.02985478565096855, 0.024338584393262863]
    Last 10:  [-0.10427617281675339, -0.03805746138095856, 0.20600228011608124, -0.04282350838184357, -0.09786099940538406, 0.023553866893053055, -0.09372316300868988, -0.05960901081562042, 0.05307437852025032, 0.03354959562420845]
  WEIGHT: [576, 1536] | μ=0.000010

084_model.layers.6.mlp: model.layers.6.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.005965 σ=0.321562
    First 10: [-0.01869785599410534, -0.0473291389644146, -0.0010909712873399258, 0.013294785283505917, -0.1569872498512268, -0.11986188590526581, 0.09531375020742416, 0.003621617564931512, -0.006258987821638584, -0.02658267877995968]
    Last 10:  [-0.0765155553817749, -0.37241578102111816, -0.03714042901992798, 0.29472580552101135, 0.40490207076072693, 0.028622135519981384, -0.10817988216876984, 0.014363025315105915, -0.2380359023809433, 0.04418285936117172]
  OUT[0]: [1, 5, 576] | μ=0.001956 σ=0.074355
    First 10: [0.09686530381441116, 0.06077098473906517, -0.002336648292839527, 0.01037447340786457, 0.10881470888853073, 0.038441069424152374, -0.14172066748142242, -0.04874054342508316, 0.02985478565096855, 0.024338584393262863]
    Last 10:  [-0.10427617281675339, -0.03805746138095856, 0.20600228011608124, -0.04282350838184357, -0.09786099940538406, 0.023553866893053055, -0.09372316300868988, -0.05960901081562042, 0.05307437852025032, 0.03354959562420845]

085_model.layers.7.input_layernorm: model.layers.7.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.073834 σ=11.786163
    First 10: [-0.891745388507843, -2.9197919368743896, -0.052769750356674194, 0.6422256231307983, -12.53269100189209, -8.087943077087402, 6.055467128753662, 0.11279385536909103, -0.23666690289974213, -2.175581455230713]
    Last 10:  [-0.13788792490959167, -0.20994509756565094, 0.18497143685817719, 0.12116821110248566, 0.06842627376317978, 0.0382547602057457, -0.14420585334300995, -0.05299122631549835, -0.06538866460323334, 0.05416775122284889]
  OUT[0]: [1, 5, 576] | μ=-0.013281 σ=0.389365
    First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
    Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
  WEIGHT: [576] | μ=0.466474

086_model.layers.7.self_attn.q_proj: model.layers.7.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.013281 σ=0.389365
    First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
    Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
  OUT[0]: [1, 5, 576] | μ=-0.028081 σ=1.073232
    First 10: [-0.163141667842865, 0.14344964921474457, 0.3623453974723816, -0.20442359149456024, -0.017802029848098755, 0.0024100691080093384, -0.21329712867736816, 0.06796086579561234, 0.17820033431053162, -0.02573048323392868]
    Last 10:  [-3.3963112831115723, 0.44770246744155884, -0.7187802791595459, 0.783698558807373, -3.9939937591552734, -0.22990316152572632, 0.08183640241622925, 3.3939690589904785, 0.3571989834308624, -0.08588109910488129]
  WEIGHT: [576, 576] | μ=-0.000023

087_model.layers.7.self_attn.k_proj: model.layers.7.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.013281 σ=0.389365
    First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
    Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
  OUT[0]: [1, 5, 192] | μ=-0.072052 σ=1.327435
    First 10: [-0.005859445780515671, 0.013216912746429443, -0.005568936467170715, 0.011307068169116974, -0.010726341977715492, 0.0036410391330718994, -0.003086097538471222, -0.0008528083562850952, -0.0008198175928555429, -0.002969607710838318]
    Last 10:  [-0.906575083732605, -0.5878944396972656, 1.990466833114624, -3.1886727809906006, 1.2782139778137207, -1.192990779876709, -0.2717127799987793, -2.788429021835327, -0.34668365120887756, 0.39768052101135254]
  WEIGHT: [192, 576] | μ=-0.000041

088_model.layers.7.self_attn.v_proj: model.layers.7.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.013281 σ=0.389365
    First 10: [-0.0301494337618351, -0.09619129449129105, -0.001276542665436864, 0.01531713642179966, -0.31697148084640503, -0.3565436005592346, 0.21631771326065063, 0.003751419484615326, -0.00724484259262681, -0.04376251995563507]
    Last 10:  [-0.3150322437286377, -0.4220733642578125, 0.35077792406082153, 0.26438623666763306, 0.2168893665075302, 0.08096644282341003, -0.2926785349845886, -0.12382051348686218, -0.12355951964855194, 0.1235123723745346]
  OUT[0]: [1, 5, 192] | μ=0.021394 σ=0.186402
    First 10: [0.003178112208843231, -0.009626634418964386, 0.0038685426115989685, 0.001936180517077446, 0.0014038663357496262, 0.002399721648544073, -0.011415351182222366, 0.003929476719349623, 0.005665038712322712, -0.0008842595852911472]
    Last 10:  [-0.08630238473415375, 0.11324853450059891, 0.23456799983978271, -0.44920745491981506, 0.07723478972911835, 0.032636433839797974, 0.1038743257522583, -0.0991995707154274, 0.08751662075519562, 0.05932270362973213]
  WEIGHT: [192, 576] | μ=-0.000019

089_model.layers.7.self_attn.o_proj: model.layers.7.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.002824 σ=0.059541
    First 10: [0.003178112208843231, -0.009626634418964386, 0.0038685426115989685, 0.001936180517077446, 0.0014038663357496262, 0.002399721648544073, -0.011415351182222366, 0.003929476719349623, 0.005665038712322712, -0.0008842595852911472]
    Last 10:  [0.009981338866055012, 0.02078321948647499, 0.017580674961209297, -0.03162253275513649, 0.016499441117048264, 0.005469663068652153, 5.365723336581141e-05, 0.017046190798282623, -0.015902500599622726, -0.003825886407867074]
  OUT[0]: [1, 5, 576] | μ=-0.000842 σ=0.034819
    First 10: [0.011671651154756546, -0.10711812227964401, 0.0315721333026886, -0.04139361530542374, -0.1153922826051712, 0.14532168209552765, -0.03303227946162224, 0.002347170375287533, -0.05380215495824814, 0.02514120563864708]
    Last 10:  [-0.0012159813195466995, -0.0018751071766018867, -0.03404180705547333, 0.005855739116668701, 0.0419999398291111, 0.012451693415641785, -0.011214371770620346, -0.09693868458271027, 0.00971376895904541, 0.010746289044618607]
  WEIGHT: [576, 576] | μ=-0.000025

090_model.layers.7.self_attn: model.layers.7.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000842 σ=0.034819
    First 10: [0.011671651154756546, -0.10711812227964401, 0.0315721333026886, -0.04139361530542374, -0.1153922826051712, 0.14532168209552765, -0.03303227946162224, 0.002347170375287533, -0.05380215495824814, 0.02514120563864708]
    Last 10:  [-0.0012159813195466995, -0.0018751071766018867, -0.03404180705547333, 0.005855739116668701, 0.0419999398291111, 0.012451693415641785, -0.011214371770620346, -0.09693868458271027, 0.00971376895904541, 0.010746289044618607]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.263115
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4999862313270569, 0.5000138282775879, 0.0, 0.0, 0.0]
    Last 10:  [0.47835108637809753, 0.4797568619251251, 0.023529866710305214, 0.018362192437052727, 0.0, 0.43640467524528503, 0.4358574151992798, 0.04408802464604378, 0.05303559452295303, 0.030614394694566727]

091_model.layers.7.post_attention_layernorm: model.layers.7.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.074675 σ=11.786484
    First 10: [-0.8800737261772156, -3.0269100666046143, -0.021197617053985596, 0.6008319854736328, -12.648083686828613, -7.942621231079102, 6.022434711456299, 0.11514102667570114, -0.29046905040740967, -2.150440216064453]
    Last 10:  [-0.13910390436649323, -0.21182020008563995, 0.15092962980270386, 0.12702395021915436, 0.11042621731758118, 0.05070645362138748, -0.1554202288389206, -0.14992991089820862, -0.05567489564418793, 0.0649140402674675]
  OUT[0]: [1, 5, 576] | μ=-0.003303 σ=0.312913
    First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
    Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
  WEIGHT: [576] | μ=0.370987

092_model.layers.7.mlp.gate_proj: model.layers.7.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003303 σ=0.312913
    First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
    Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
  OUT[0]: [1, 5, 1536] | μ=-0.143692 σ=0.335674
    First 10: [-0.44644343852996826, -0.6296501755714417, -0.3028048872947693, -0.3355351686477661, 0.16969412565231323, -0.060166798532009125, -0.9633184671401978, 0.21841850876808167, -0.1178133562207222, 0.006205204874277115]
    Last 10:  [0.15233208239078522, -0.20627819001674652, 0.11936020851135254, -0.6294994354248047, -0.012607485055923462, -0.25003352761268616, -0.3742865025997162, -0.11403638124465942, -0.22093722224235535, -0.4997594356536865]
  WEIGHT: [1536, 576] | μ=0.000115

093_model.layers.7.mlp.act_fn: model.layers.7.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.143692 σ=0.335674
    First 10: [-0.44644343852996826, -0.6296501755714417, -0.3028048872947693, -0.3355351686477661, 0.16969412565231323, -0.060166798532009125, -0.9633184671401978, 0.21841850876808167, -0.1178133562207222, 0.006205204874277115]
    Last 10:  [0.15233208239078522, -0.20627819001674652, 0.11936020851135254, -0.6294994354248047, -0.012607485055923462, -0.25003352761268616, -0.3742865025997162, -0.11403638124465942, -0.22093722224235535, -0.4997594356536865]
  OUT[0]: [1, 5, 1536] | μ=-0.039899 σ=0.145827
    First 10: [-0.1742052286863327, -0.21886001527309418, -0.12865330278873444, -0.13988274335861206, 0.09202886372804642, -0.029178662225604057, -0.2660823166370392, 0.12108872830867767, -0.055440690368413925, 0.003112228587269783]
    Last 10:  [0.08195611089468002, -0.092538982629776, 0.06323759257793427, -0.21882915496826172, -0.0062640062533319, -0.1094684824347496, -0.15252386033535004, -0.05377063527703285, -0.09831469506025314, -0.18870776891708374]

094_model.layers.7.mlp.up_proj: model.layers.7.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003303 σ=0.312913
    First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
    Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
  OUT[0]: [1, 5, 1536] | μ=0.000920 σ=0.246985
    First 10: [0.1448216438293457, 0.07980825006961823, -0.12147991359233856, 0.08897227048873901, 0.036001063883304596, 0.007466818671673536, 0.07499468326568604, 0.21740218997001648, 0.0670018270611763, 0.0916796624660492]
    Last 10:  [0.22887952625751495, 0.7798824310302734, 0.36847352981567383, 0.5763276219367981, 0.267422080039978, 0.16564378142356873, 0.11106894910335541, -0.26560235023498535, 0.5022395253181458, -0.2580797076225281]
  WEIGHT: [1536, 576] | μ=-0.000039

095_model.layers.7.mlp.down_proj: model.layers.7.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000164 σ=0.036808
    First 10: [-0.02522868849337101, -0.01746683567762375, 0.015628792345523834, -0.01244568545371294, 0.003313136985525489, -0.0002178717841161415, -0.019954759627580643, 0.026324953883886337, -0.0037146275863051414, 0.00028532807482406497]
    Last 10:  [0.018758075311779976, -0.07216952741146088, 0.02330137975513935, -0.1261172890663147, -0.0016751335933804512, -0.018132774159312248, -0.016940664499998093, 0.01428160723298788, -0.049377527087926865, 0.04870164766907692]
  OUT[0]: [1, 5, 576] | μ=-0.002309 σ=0.065699
    First 10: [0.06277474761009216, 0.07845433056354523, 0.011200192384421825, 0.019179776310920715, 0.15895119309425354, -0.009320810437202454, -0.05352012440562248, -0.08226831257343292, -0.01105063408613205, 0.032855160534381866]
    Last 10:  [-0.0831308662891388, -0.017722953110933304, -0.07431754469871521, 0.023814208805561066, -0.011128347367048264, -0.058775145560503006, -0.032205622643232346, 0.07332844287157059, -0.034068942070007324, 0.09669757634401321]
  WEIGHT: [576, 1536] | μ=-0.000055

096_model.layers.7.mlp: model.layers.7.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.003303 σ=0.312913
    First 10: [-0.01653772033751011, -0.04438508301973343, -0.00045166353811509907, 0.013180015608668327, -0.15455427765846252, -0.11282376199960709, 0.08420617878437042, 0.002610259223729372, -0.006775280460715294, -0.025178471580147743]
    Last 10:  [-0.2769329845905304, -0.4065369665622711, 0.2253570556640625, 0.2035856693983078, 0.21872887015342712, 0.0928950384259224, -0.3021152913570404, -0.29026904702186584, -0.10554665327072144, 0.12175507843494415]
  OUT[0]: [1, 5, 576] | μ=-0.002309 σ=0.065699
    First 10: [0.06277474761009216, 0.07845433056354523, 0.011200192384421825, 0.019179776310920715, 0.15895119309425354, -0.009320810437202454, -0.05352012440562248, -0.08226831257343292, -0.01105063408613205, 0.032855160534381866]
    Last 10:  [-0.0831308662891388, -0.017722953110933304, -0.07431754469871521, 0.023814208805561066, -0.011128347367048264, -0.058775145560503006, -0.032205622643232346, 0.07332844287157059, -0.034068942070007324, 0.09669757634401321]

097_model.layers.8.input_layernorm: model.layers.8.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.076984 σ=11.809260
    First 10: [-0.8172990083694458, -2.948455810546875, -0.00999742466956377, 0.6200117468833923, -12.48913288116455, -7.951941967010498, 5.96891450881958, 0.03287271410226822, -0.3015196919441223, -2.1175849437713623]
    Last 10:  [-0.22223477065563202, -0.22954314947128296, 0.07661208510398865, 0.15083816647529602, 0.09929786622524261, -0.008068691939115524, -0.18762585520744324, -0.07660146802663803, -0.08974383771419525, 0.1616116166114807]
  OUT[0]: [1, 5, 576] | μ=-0.013028 σ=0.361611
    First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
    Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
  WEIGHT: [576] | μ=0.480348

098_model.layers.8.self_attn.q_proj: model.layers.8.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.013028 σ=0.361611
    First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
    Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
  OUT[0]: [1, 5, 576] | μ=-0.007449 σ=0.930965
    First 10: [0.5700347423553467, -0.03055143542587757, 0.17181363701820374, 0.4668569564819336, -0.19051846861839294, -0.13547587394714355, -0.10261942446231842, 0.05653965473175049, 0.15724077820777893, 0.1988602578639984]
    Last 10:  [0.32177528738975525, 1.141357660293579, -4.713472366333008, -0.6389011144638062, 0.15223488211631775, -1.4306690692901611, 0.8448787927627563, 1.2303187847137451, -1.3698142766952515, -1.591043472290039]
  WEIGHT: [576, 576] | μ=0.000201

099_model.layers.8.self_attn.k_proj: model.layers.8.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.013028 σ=0.361611
    First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
    Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
  OUT[0]: [1, 5, 192] | μ=-0.036599 σ=1.187968
    First 10: [-0.02051757276058197, -0.009375354275107384, -0.0025253184139728546, 0.0029267892241477966, 0.0033593736588954926, -0.007810264825820923, 0.009312622249126434, -0.004092715680599213, 0.004238307476043701, 0.0013017654418945312]
    Last 10:  [0.7285547852516174, 2.060030460357666, 0.6305367350578308, -2.146085262298584, -1.142793893814087, -1.1884491443634033, -1.6811960935592651, -1.4684768915176392, 0.4585420489311218, 2.4857797622680664]
  WEIGHT: [192, 576] | μ=0.000376

100_model.layers.8.self_attn.v_proj: model.layers.8.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.013028 σ=0.361611
    First 10: [-0.025205478072166443, -0.08607141673564911, -0.00027249340200796723, 0.01670464500784874, -0.2865048944950104, -0.3155432641506195, 0.15988048911094666, 0.0010714051313698292, -0.008383958600461483, -0.03630894795060158]
    Last 10:  [-0.5671412348747253, -0.5388062000274658, 0.1686665117740631, 0.3309054672718048, 0.2653375566005707, -0.018077915534377098, -0.4575294256210327, -0.2052009552717209, -0.19078801572322845, 0.40559956431388855]
  OUT[0]: [1, 5, 192] | μ=-0.004660 σ=0.185265
    First 10: [-0.006926373578608036, 4.363991320133209e-05, 0.001378399319946766, -0.0014966316521167755, -0.005546521861106157, 0.0017741546034812927, 0.007012592628598213, 0.001315896981395781, -0.009965930134057999, -0.0034552570432424545]
    Last 10:  [-0.9632606506347656, -0.0759386271238327, 0.18810619413852692, -0.38354724645614624, -0.13642236590385437, 0.018882527947425842, -0.140649676322937, 0.2591759264469147, -0.07496477663516998, 0.5457326769828796]
  WEIGHT: [192, 576] | μ=-0.000050

101_model.layers.8.self_attn.o_proj: model.layers.8.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000838 σ=0.045013
    First 10: [-0.006926373578608036, 4.363991320133209e-05, 0.001378399319946766, -0.0014966316521167755, -0.005546521861106157, 0.0017741546034812927, 0.007012592628598213, 0.001315896981395781, -0.009965930134057999, -0.0034552570432424545]
    Last 10:  [-0.13573822379112244, -0.0035587006714195013, 0.013239198364317417, -0.015732279047369957, -0.00821719877421856, 0.04775655269622803, -0.021149393171072006, -8.777123730396852e-05, 0.00849158875644207, 0.07303531467914581]
  OUT[0]: [1, 5, 576] | μ=-0.000537 σ=0.028551
    First 10: [-0.013519397005438805, -0.014633879996836185, 0.0028122453950345516, -0.011699226684868336, -0.06821851432323456, 0.024635016918182373, 0.047196321189403534, 0.00959493312984705, -0.027519261464476585, 0.011014912277460098]
    Last 10:  [0.009924521669745445, -0.02715279534459114, 0.0016385400667786598, -0.03373562544584274, 0.0574520118534565, -0.009041397832334042, -0.00010344292968511581, -0.0816773921251297, -0.004547027871012688, -0.027889255434274673]
  WEIGHT: [576, 576] | μ=-0.000031

102_model.layers.8.self_attn: model.layers.8.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000537 σ=0.028551
    First 10: [-0.013519397005438805, -0.014633879996836185, 0.0028122453950345516, -0.011699226684868336, -0.06821851432323456, 0.024635016918182373, 0.047196321189403534, 0.00959493312984705, -0.027519261464476585, 0.011014912277460098]
    Last 10:  [0.009924521669745445, -0.02715279534459114, 0.0016385400667786598, -0.03373562544584274, 0.0574520118534565, -0.009041397832334042, -0.00010344292968511581, -0.0816773921251297, -0.004547027871012688, -0.027889255434274673]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.265354
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5004750490188599, 0.4995250403881073, 0.0, 0.0, 0.0]
    Last 10:  [0.4536062180995941, 0.4518175423145294, 0.070783831179142, 0.023792341351509094, 0.0, 0.4170399606227875, 0.4159311056137085, 0.07248721271753311, 0.06135969236493111, 0.033182017505168915]

103_model.layers.8.post_attention_layernorm: model.layers.8.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.077521 σ=11.808899
    First 10: [-0.8308184146881104, -2.96308970451355, -0.007185179274529219, 0.6083125472068787, -12.557351112365723, -7.92730712890625, 6.016110897064209, 0.042467646300792694, -0.32903894782066345, -2.10657000541687]
    Last 10:  [-0.21231025457382202, -0.256695955991745, 0.07825062423944473, 0.11710254102945328, 0.1567498743534088, -0.01711009070277214, -0.18772929906845093, -0.15827885270118713, -0.09429086744785309, 0.13372236490249634]
  OUT[0]: [1, 5, 576] | μ=-0.005885 σ=0.315244
    First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
    Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
  WEIGHT: [576] | μ=0.377048

104_model.layers.8.mlp.gate_proj: model.layers.8.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.005885 σ=0.315244
    First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
    Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
  OUT[0]: [1, 5, 1536] | μ=-0.126363 σ=0.331795
    First 10: [0.11922789365053177, 0.07598148286342621, -0.26246118545532227, -0.244480162858963, -0.009143909439444542, -0.18244251608848572, 0.13987009227275848, 0.07401307672262192, -0.2145039439201355, -0.7887895107269287]
    Last 10:  [-0.46190840005874634, -0.3206123113632202, -0.22908928990364075, 0.29855889081954956, -0.24688836932182312, 0.49990299344062805, -0.6661768555641174, 0.11332497000694275, -0.8373229503631592, 0.1334666609764099]
  WEIGHT: [1536, 576] | μ=0.000332

105_model.layers.8.mlp.act_fn: model.layers.8.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.126363 σ=0.331795
    First 10: [0.11922789365053177, 0.07598148286342621, -0.26246118545532227, -0.244480162858963, -0.009143909439444542, -0.18244251608848572, 0.13987009227275848, 0.07401307672262192, -0.2145039439201355, -0.7887895107269287]
    Last 10:  [-0.46190840005874634, -0.3206123113632202, -0.22908928990364075, 0.29855889081954956, -0.24688836932182312, 0.49990299344062805, -0.6661768555641174, 0.11332497000694275, -0.8373229503631592, 0.1334666609764099]
  OUT[0]: [1, 5, 1536] | μ=-0.032932 σ=0.145950
    First 10: [0.06316357105970383, 0.03943334519863129, -0.11410730332136154, -0.10737143456935883, -0.004551052115857601, -0.08292294293642044, 0.07481800019741058, 0.03837539628148079, -0.09579289704561234, -0.24644044041633606]
    Last 10:  [-0.17854292690753937, -0.13482598960399628, -0.10148125141859055, 0.17139972746372223, -0.1082826554775238, 0.31115788221359253, -0.2260694056749344, 0.05986969172954559, -0.2529543340206146, 0.0711800679564476]

106_model.layers.8.mlp.up_proj: model.layers.8.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.005885 σ=0.315244
    First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
    Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
  OUT[0]: [1, 5, 1536] | μ=-0.002450 σ=0.247449
    First 10: [0.03304416313767433, 0.14383885264396667, 0.40053126215934753, -0.13585264980793, 0.11536655575037003, -0.040009111166000366, -0.12442360073328018, 0.3450770378112793, 0.17298424243927002, -0.04370095580816269]
    Last 10:  [0.3887243866920471, 0.21747443079948425, -0.313065767288208, -0.39275580644607544, -0.3752979040145874, 0.058640554547309875, 0.019689293578267097, -0.34404757618904114, -0.08564433455467224, -0.35616177320480347]
  WEIGHT: [1536, 576] | μ=-0.000031

107_model.layers.8.mlp.down_proj: model.layers.8.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000049 σ=0.045002
    First 10: [0.0020871872548013926, 0.005672046914696693, -0.04570354148745537, 0.014586693607270718, -0.0005250392132438719, 0.003317673224955797, -0.009309125132858753, 0.013242468237876892, -0.01657066121697426, 0.010769682936370373]
    Last 10:  [-0.0694039911031723, -0.029321204870939255, 0.03177030757069588, -0.06731823831796646, 0.04063825309276581, 0.018246470019221306, -0.004451146814972162, -0.020598022267222404, 0.021664105355739594, -0.025351619347929955]
  OUT[0]: [1, 5, 576] | μ=-0.002781 σ=0.076271
    First 10: [0.043941691517829895, 0.07703574001789093, -0.02891324646770954, -0.0001335269771516323, 0.2610950171947479, -0.06291749328374863, -0.04276524484157562, -0.03892497345805168, -0.09670408070087433, 0.009521453641355038]
    Last 10:  [0.12537935376167297, -0.028978396207094193, 0.03350488096475601, -0.04899672046303749, -0.012718352489173412, -0.012230819091200829, 0.04065439850091934, 0.006573688238859177, -0.006543746683746576, 0.0736502930521965]
  WEIGHT: [576, 1536] | μ=0.000032

108_model.layers.8.mlp: model.layers.8.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.005885 σ=0.315244
    First 10: [-0.01627810299396515, -0.047087643295526505, -0.00016267488535959274, 0.01363714411854744, -0.16572004556655884, -0.11135656386613846, 0.0822276622056961, 0.0009737017680890858, -0.007729288190603256, -0.025982052087783813]
    Last 10:  [-0.42142254114151, -0.5028284192085266, 0.11347224563360214, 0.1802501380443573, 0.30756038427352905, -0.029089421033859253, -0.3650802969932556, -0.30711930990219116, -0.17404170334339142, 0.24827782809734344]
  OUT[0]: [1, 5, 576] | μ=-0.002781 σ=0.076271
    First 10: [0.043941691517829895, 0.07703574001789093, -0.02891324646770954, -0.0001335269771516323, 0.2610950171947479, -0.06291749328374863, -0.04276524484157562, -0.03892497345805168, -0.09670408070087433, 0.009521453641355038]
    Last 10:  [0.12537935376167297, -0.028978396207094193, 0.03350488096475601, -0.04899672046303749, -0.012718352489173412, -0.012230819091200829, 0.04065439850091934, 0.006573688238859177, -0.006543746683746576, 0.0736502930521965]

109_model.layers.9.input_layernorm: model.layers.9.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080302 σ=11.837265
    First 10: [-0.7868767380714417, -2.886054039001465, -0.0360984243452549, 0.6081790328025818, -12.296256065368652, -7.990224838256836, 5.973345756530762, 0.0035426728427410126, -0.425743043422699, -2.09704852104187]
    Last 10:  [-0.08693090081214905, -0.2856743633747101, 0.11175550520420074, 0.06810581684112549, 0.14403152465820312, -0.02934090979397297, -0.14707490801811218, -0.15170516073703766, -0.10083461552858353, 0.20737266540527344]
  OUT[0]: [1, 5, 576] | μ=-0.016054 σ=0.355860
    First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
    Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
  WEIGHT: [576] | μ=0.473485

110_model.layers.9.self_attn.q_proj: model.layers.9.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016054 σ=0.355860
    First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
    Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
  OUT[0]: [1, 5, 576] | μ=-0.027000 σ=0.963388
    First 10: [0.04387283697724342, -0.09526120871305466, 0.25692039728164673, -0.13929833471775055, 0.15903016924858093, 0.011951208114624023, -0.4834483563899994, -0.30207350850105286, -0.11317446827888489, -0.05286040157079697]
    Last 10:  [-0.3126358985900879, -0.02996121346950531, -0.6781793832778931, -0.05468013882637024, -0.8086473345756531, 0.9408209323883057, -1.4899678230285645, 0.4838234484195709, 2.1043386459350586, 2.251519203186035]
  WEIGHT: [576, 576] | μ=0.000177

111_model.layers.9.self_attn.k_proj: model.layers.9.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016054 σ=0.355860
    First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
    Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
  OUT[0]: [1, 5, 192] | μ=0.081130 σ=1.248149
    First 10: [-0.003329940140247345, 0.007019057869911194, -0.0015396997332572937, 0.0020980872213840485, 0.015500739216804504, 0.0009484663605690002, 0.004050256684422493, -0.010980360209941864, -0.005229022353887558, -0.003665059804916382]
    Last 10:  [-0.6097809672355652, 5.3869171142578125, 0.06584972143173218, -0.5834517478942871, -1.6502655744552612, -0.766019880771637, 0.7581527233123779, -0.1505623459815979, -1.8711820840835571, -2.11316180229187]
  WEIGHT: [192, 576] | μ=0.000276

112_model.layers.9.self_attn.v_proj: model.layers.9.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016054 σ=0.355860
    First 10: [-0.023717328906059265, -0.06996764987707138, -0.0009750024182721972, 0.01682342402637005, -0.33692967891693115, -0.3098524212837219, 0.17614617943763733, 0.00011167991760885343, -0.012765700928866863, -0.03647419810295105]
    Last 10:  [-0.19826951622962952, -0.6321807503700256, 0.1787300854921341, 0.11960413306951523, 0.34102028608322144, -0.0614469014108181, -0.35040828585624695, -0.40356504917144775, -0.21747709810733795, 0.49538734555244446]
  OUT[0]: [1, 5, 192] | μ=-0.012590 σ=0.214816
    First 10: [0.0038800013717263937, 0.010776727460324764, 0.004395492374897003, 0.020588960498571396, 0.006884204223752022, -0.012203143909573555, -0.002978650387376547, -0.007464400492608547, -0.00919080525636673, -0.0036137141287326813]
    Last 10:  [-0.020976930856704712, -0.2573348879814148, 0.29238787293434143, 0.22658833861351013, -0.15203285217285156, 0.1117548793554306, 0.19532284140586853, -0.2886280417442322, 0.1203177273273468, 0.3968658447265625]
  WEIGHT: [192, 576] | μ=-0.000011

113_model.layers.9.self_attn.o_proj: model.layers.9.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.002527 σ=0.056748
    First 10: [0.0038800013717263937, 0.010776727460324764, 0.004395492374897003, 0.020588960498571396, 0.006884204223752022, -0.012203143909573555, -0.002978650387376547, -0.007464400492608547, -0.00919080525636673, -0.0036137141287326813]
    Last 10:  [-0.0032318830490112305, -0.023941343650221825, -0.0016776962438598275, 0.017985839396715164, 0.011797924526035786, -0.00509921507909894, 0.0051344591192901134, -0.026174673810601234, 0.002657231641933322, 0.006500010844320059]
  OUT[0]: [1, 5, 576] | μ=0.000683 σ=0.035899
    First 10: [-0.00839797593653202, -0.035551298409700394, -0.019028956070542336, -0.02648957632482052, -0.11190793663263321, 0.08060210198163986, -0.06207432225346565, -0.05581294745206833, -0.023520302027463913, 0.026259351521730423]
    Last 10:  [-0.007665117736905813, -0.06332117319107056, -0.009396101348102093, 0.00953938253223896, 0.02488362230360508, 0.04025761038064957, 0.026513539254665375, -0.08103131502866745, 0.007841799408197403, 0.04468400031328201]
  WEIGHT: [576, 576] | μ=0.000034

114_model.layers.9.self_attn: model.layers.9.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.000683 σ=0.035899
    First 10: [-0.00839797593653202, -0.035551298409700394, -0.019028956070542336, -0.02648957632482052, -0.11190793663263321, 0.08060210198163986, -0.06207432225346565, -0.05581294745206833, -0.023520302027463913, 0.026259351521730423]
    Last 10:  [-0.007665117736905813, -0.06332117319107056, -0.009396101348102093, 0.00953938253223896, 0.02488362230360508, 0.04025761038064957, 0.026513539254665375, -0.08103131502866745, 0.007841799408197403, 0.04468400031328201]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.266120
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4998621642589569, 0.5001378059387207, 0.0, 0.0, 0.0]
    Last 10:  [0.4769057035446167, 0.47668829560279846, 0.02589985355734825, 0.02050611935555935, 0.0, 0.4565238356590271, 0.4567650556564331, 0.033541686832904816, 0.03426787629723549, 0.018901566043496132]

115_model.layers.9.post_attention_layernorm: model.layers.9.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.079619 σ=11.838077
    First 10: [-0.7952747344970703, -2.921605348587036, -0.05512738227844238, 0.5816894769668579, -12.408164024353027, -7.909622669219971, 5.911271572113037, -0.052270274609327316, -0.449263334274292, -2.070789098739624]
    Last 10:  [-0.0945960208773613, -0.34899553656578064, 0.10235940665006638, 0.0776451975107193, 0.16891515254974365, 0.010916700586676598, -0.12056136876344681, -0.2327364683151245, -0.09299281239509583, 0.25205665826797485]
  OUT[0]: [1, 5, 576] | μ=-0.007798 σ=0.313722
    First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
    Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
  WEIGHT: [576] | μ=0.371665

116_model.layers.9.mlp.gate_proj: model.layers.9.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007798 σ=0.313722
    First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
    Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
  OUT[0]: [1, 5, 1536] | μ=-0.143020 σ=0.338150
    First 10: [-0.11589538305997849, 0.13814741373062134, -0.07678347826004028, -0.0069809406995773315, -0.15250791609287262, -0.09538917243480682, 0.267318457365036, 0.2933540344238281, -0.01690390706062317, -0.02467486262321472]
    Last 10:  [-0.3502469062805176, -0.16496935486793518, 0.11747929453849792, -0.2565889358520508, -0.20227660238742828, 0.04262363910675049, -0.6897583603858948, -0.4268530607223511, 0.3874856233596802, -0.486378014087677]
  WEIGHT: [1536, 576] | μ=0.000324

117_model.layers.9.mlp.act_fn: model.layers.9.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.143020 σ=0.338150
    First 10: [-0.11589538305997849, 0.13814741373062134, -0.07678347826004028, -0.0069809406995773315, -0.15250791609287262, -0.09538917243480682, 0.267318457365036, 0.2933540344238281, -0.01690390706062317, -0.02467486262321472]
    Last 10:  [-0.3502469062805176, -0.16496935486793518, 0.11747929453849792, -0.2565889358520508, -0.20227660238742828, 0.04262363910675049, -0.6897583603858948, -0.4268530607223511, 0.3874856233596802, -0.486378014087677]
  OUT[0]: [1, 5, 1536] | μ=-0.039259 σ=0.143964
    First 10: [-0.05459350720047951, 0.07383731007575989, -0.03691853582859039, -0.0034782872535288334, -0.0704505443572998, -0.0454215370118618, 0.15141838788986206, 0.16803818941116333, -0.008380519226193428, -0.012185226194560528]
    Last 10:  [-0.1447649449110031, -0.07569634169340134, 0.06218602880835533, -0.11192470788955688, -0.0909440889954567, 0.021765943616628647, -0.2304392009973526, -0.16855491697788239, 0.23081637918949127, -0.18518705666065216]

118_model.layers.9.mlp.up_proj: model.layers.9.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007798 σ=0.313722
    First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
    Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
  OUT[0]: [1, 5, 1536] | μ=-0.000083 σ=0.252712
    First 10: [-0.3458192050457001, -0.21221843361854553, 0.1545608639717102, -0.3760257363319397, 0.12089995294809341, -0.13314272463321686, -0.05786123871803284, -0.1671293079853058, -0.19515642523765564, -0.15980640053749084]
    Last 10:  [0.019852448254823685, -0.20478665828704834, -0.02680964767932892, -0.10130223631858826, 0.515162467956543, 0.22985829412937164, 0.06860505044460297, 0.502947211265564, -0.2903328537940979, -0.08740289509296417]
  WEIGHT: [1536, 576] | μ=-0.000006

119_model.layers.9.mlp.down_proj: model.layers.9.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000276 σ=0.039275
    First 10: [0.01887948252260685, -0.01566963829100132, -0.005706160794943571, 0.0013079255586490035, -0.00851746741682291, 0.0060475473292171955, -0.008761255070567131, -0.028084106743335724, 0.001635512220673263, 0.0019472771091386676]
    Last 10:  [-0.0028739385306835175, 0.015501600690186024, -0.0016671855701133609, 0.01133822277188301, -0.046850983053445816, 0.005003082565963268, -0.01580929383635521, -0.08477422595024109, -0.06701357662677765, 0.016185885295271873]
  OUT[0]: [1, 5, 576] | μ=0.000030 σ=0.070793
    First 10: [0.026561398059129715, 0.07255397737026215, -0.011190158315002918, 0.0182608962059021, 0.12309234589338303, 0.11817166209220886, -0.040946267545223236, -0.0007919203490018845, -0.07040541619062424, 0.03770525008440018]
    Last 10:  [0.13253718614578247, 0.08999583125114441, -5.2306801080703735e-05, 0.0688304528594017, -0.007980171591043472, 0.02235664241015911, 0.09866877645254135, 0.0845712423324585, -0.15372058749198914, -0.15473106503486633]
  WEIGHT: [576, 1536] | μ=-0.000047

120_model.layers.9.mlp: model.layers.9.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.007798 σ=0.313722
    First 10: [-0.015014293603599072, -0.0416639968752861, -0.0012198667973279953, 0.012507416307926178, -0.1962137073278427, -0.10990680009126663, 0.08267894387245178, -0.0011900615645572543, -0.010556838475167751, -0.025383519008755684]
    Last 10:  [-0.17594078183174133, -0.6710938811302185, 0.13813571631908417, 0.11236702650785446, 0.2997991442680359, 0.017357707023620605, -0.23347702622413635, -0.45609062910079956, -0.16426704823970795, 0.4410097897052765]
  OUT[0]: [1, 5, 576] | μ=0.000030 σ=0.070793
    First 10: [0.026561398059129715, 0.07255397737026215, -0.011190158315002918, 0.0182608962059021, 0.12309234589338303, 0.11817166209220886, -0.040946267545223236, -0.0007919203490018845, -0.07040541619062424, 0.03770525008440018]
    Last 10:  [0.13253718614578247, 0.08999583125114441, -5.2306801080703735e-05, 0.0688304528594017, -0.007980171591043472, 0.02235664241015911, 0.09866877645254135, 0.0845712423324585, -0.15372058749198914, -0.15473106503486633]

121_model.layers.10.input_layernorm: model.layers.10.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.079589 σ=11.856653
    First 10: [-0.7687133550643921, -2.8490514755249023, -0.06631754338741302, 0.59995037317276, -12.28507137298584, -7.7914509773254395, 5.870325088500977, -0.05306219309568405, -0.5196687579154968, -2.033083915710449]
    Last 10:  [0.03794116526842117, -0.25899970531463623, 0.10230709612369537, 0.1464756429195404, 0.16093498468399048, 0.03327334299683571, -0.021892592310905457, -0.14816522598266602, -0.24671339988708496, 0.09732559323310852]
  OUT[0]: [1, 5, 576] | μ=-0.016196 σ=0.380332
    First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
    Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
  WEIGHT: [576] | μ=0.467250

122_model.layers.10.self_attn.q_proj: model.layers.10.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016196 σ=0.380332
    First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
    Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
  OUT[0]: [1, 5, 576] | μ=-0.061932 σ=0.978175
    First 10: [0.14946657419204712, 0.4237329065799713, 0.016238979995250702, 0.10533840209245682, 0.3687155842781067, -0.1618715077638626, -0.3842354714870453, 0.03658205270767212, -0.04985567554831505, -0.2313602864742279]
    Last 10:  [-0.022825270891189575, -5.202563285827637, -0.22738832235336304, 1.337790846824646, -0.27868229150772095, -1.1344449520111084, -0.23994173109531403, -1.1602038145065308, 0.3008744716644287, -0.7587815523147583]
  WEIGHT: [576, 576] | μ=-0.000124

123_model.layers.10.self_attn.k_proj: model.layers.10.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016196 σ=0.380332
    First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
    Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
  OUT[0]: [1, 5, 192] | μ=0.047379 σ=1.453040
    First 10: [-0.006445109844207764, 2.396106719970703e-05, 0.0011231075040996075, 0.01953183300793171, 0.013939488679170609, -0.013579700142145157, 0.009705308824777603, -0.011269599199295044, -0.004460237920284271, -0.0007031559944152832]
    Last 10:  [0.10211500525474548, 1.3010547161102295, -0.8465775847434998, -4.474518299102783, 0.7756940126419067, 0.29951944947242737, 0.3242015838623047, 0.0409388542175293, -0.08240259438753128, -0.6977173089981079]
  WEIGHT: [192, 576] | μ=-0.000026

124_model.layers.10.self_attn.v_proj: model.layers.10.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016196 σ=0.380332
    First 10: [-0.026957524940371513, -0.07578732818365097, -0.0017485556891188025, 0.013348845764994621, -0.3405570387840271, -0.2949541509151459, 0.18185050785541534, -0.001538689131848514, -0.013864273205399513, -0.05159228667616844]
    Last 10:  [0.08254580944776535, -0.49019619822502136, 0.15954159200191498, 0.2947981059551239, 0.36102455854415894, 0.061337366700172424, -0.04691183939576149, -0.35881051421165466, -0.4745309352874756, 0.22292007505893707]
  OUT[0]: [1, 5, 192] | μ=0.002164 σ=0.229439
    First 10: [-0.01095241867005825, 0.01454925537109375, 0.0038912040181457996, -0.012795633636415005, -0.008487294428050518, 0.010548923164606094, -0.002227945253252983, -0.01663871482014656, -0.007873760536313057, 0.04521648585796356]
    Last 10:  [-0.12089092284440994, -0.152170792222023, 0.1492007076740265, -0.14464828372001648, -0.013770520687103271, 0.12011227756738663, 0.01828405261039734, 0.31870919466018677, 0.3168059289455414, 0.1735941767692566]
  WEIGHT: [192, 576] | μ=-0.000071

125_model.layers.10.self_attn.o_proj: model.layers.10.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004843 σ=0.075206
    First 10: [-0.01095241867005825, 0.01454925537109375, 0.0038912040181457996, -0.012795633636415005, -0.008487294428050518, 0.010548923164606094, -0.002227945253252983, -0.01663871482014656, -0.007873760536313057, 0.04521648585796356]
    Last 10:  [0.041268475353717804, -0.028927715495228767, 0.07998748868703842, 0.013751420192420483, -0.07310836762189865, 0.04812069609761238, -0.06191546469926834, 0.06021188572049141, 0.0625753328204155, 0.026494214311242104]
  OUT[0]: [1, 5, 576] | μ=-0.000155 σ=0.037771
    First 10: [0.011354760266840458, -0.018301716074347496, -0.023903844878077507, 0.0033173873089253902, -0.08320734649896622, 0.051929011940956116, -0.017452513799071312, -0.02396094799041748, -0.017407327890396118, 0.004157497081905603]
    Last 10:  [0.016488492488861084, 0.00819007121026516, -0.03847689554095268, -0.007706239819526672, 0.018173955380916595, -0.021537160500884056, 0.0006799343973398209, 0.011152059771120548, 0.01965831033885479, -0.04831507056951523]
  WEIGHT: [576, 576] | μ=-0.000067

126_model.layers.10.self_attn: model.layers.10.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000155 σ=0.037771
    First 10: [0.011354760266840458, -0.018301716074347496, -0.023903844878077507, 0.0033173873089253902, -0.08320734649896622, 0.051929011940956116, -0.017452513799071312, -0.02396094799041748, -0.017407327890396118, 0.004157497081905603]
    Last 10:  [0.016488492488861084, 0.00819007121026516, -0.03847689554095268, -0.007706239819526672, 0.018173955380916595, -0.021537160500884056, 0.0006799343973398209, 0.011152059771120548, 0.01965831033885479, -0.04831507056951523]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.263318
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001978278160095, 0.4998021423816681, 0.0, 0.0, 0.0]
    Last 10:  [0.330867737531662, 0.33217498660087585, 0.24370139837265015, 0.09325594455003738, 0.0, 0.3100683093070984, 0.3114705979824066, 0.12488590180873871, 0.19453102350234985, 0.05904412269592285]

127_model.layers.10.post_attention_layernorm: model.layers.10.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.079744 σ=11.857547
    First 10: [-0.7573586106300354, -2.8673532009124756, -0.09022139012813568, 0.6032677888870239, -12.368278503417969, -7.7395219802856445, 5.852872371673584, -0.07702314108610153, -0.5370761156082153, -2.028926372528076]
    Last 10:  [0.05442965775728226, -0.2508096396923065, 0.06383019685745239, 0.13876940310001373, 0.17910894751548767, 0.011736182495951653, -0.021212657913565636, -0.13701316714286804, -0.22705508768558502, 0.04901052266359329]
  OUT[0]: [1, 5, 576] | μ=-0.007058 σ=0.316369
    First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
    Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
  WEIGHT: [576] | μ=0.374137

128_model.layers.10.mlp.gate_proj: model.layers.10.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007058 σ=0.316369
    First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
    Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
  OUT[0]: [1, 5, 1536] | μ=-0.150160 σ=0.365669
    First 10: [0.12533587217330933, 0.539575457572937, 0.5159253478050232, 0.23019398748874664, 0.0697816014289856, 0.05280643701553345, 0.8693276047706604, 0.36892059445381165, 0.02790701389312744, -0.42387571930885315]
    Last 10:  [-0.3378849923610687, -0.09051961451768875, -0.26919126510620117, -0.14221619069576263, -0.34570538997650146, -0.5091465711593628, 0.621299147605896, -0.11430764943361282, -0.15690205991268158, 0.21985876560211182]
  WEIGHT: [1536, 576] | μ=0.000269

129_model.layers.10.mlp.act_fn: model.layers.10.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.150160 σ=0.365669
    First 10: [0.12533587217330933, 0.539575457572937, 0.5159253478050232, 0.23019398748874664, 0.0697816014289856, 0.05280643701553345, 0.8693276047706604, 0.36892059445381165, 0.02790701389312744, -0.42387571930885315]
    Last 10:  [-0.3378849923610687, -0.09051961451768875, -0.26919126510620117, -0.14221619069576263, -0.34570538997650146, -0.5091465711593628, 0.621299147605896, -0.11430764943361282, -0.15690205991268158, 0.21985876560211182]
  OUT[0]: [1, 5, 1536] | μ=-0.038094 σ=0.156071
    First 10: [0.0665900781750679, 0.34085720777511597, 0.32306960225105286, 0.12828612327575684, 0.03610767796635628, 0.02710018679499626, 0.6125332713127136, 0.21810515224933624, 0.014148194342851639, -0.16768087446689606]
    Last 10:  [-0.14066940546035767, -0.04321275278925896, -0.11658824980258942, -0.06606023758649826, -0.14326868951320648, -0.1911303699016571, 0.40416377782821655, -0.053890813142061234, -0.07230906933546066, 0.12196540832519531]

130_model.layers.10.mlp.up_proj: model.layers.10.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007058 σ=0.316369
    First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
    Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
  OUT[0]: [1, 5, 1536] | μ=-0.003681 σ=0.265156
    First 10: [0.3073301911354065, 0.023201871663331985, 0.010693086311221123, -0.671901285648346, -0.035838931798934937, -0.060866743326187134, 0.06572648137807846, 0.14378011226654053, 0.007574064657092094, 0.15290313959121704]
    Last 10:  [0.0033807307481765747, 0.06142779067158699, 0.056572698056697845, -0.027657799422740936, 0.334308385848999, -0.23611915111541748, 0.3624194264411926, -0.5461869835853577, -0.5727048516273499, -0.1320411115884781]
  WEIGHT: [1536, 576] | μ=-0.000047

131_model.layers.10.mlp.down_proj: model.layers.10.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000311 σ=0.056046
    First 10: [0.020465141162276268, 0.007908524945378304, 0.0034546111710369587, -0.08619561046361923, -0.0012940606102347374, -0.0016495001036673784, 0.04025965556502342, 0.03135918453335762, 0.00010715933603933081, -0.02563893236219883]
    Last 10:  [-0.00047556537901982665, -0.0026544639840722084, -0.006595711689442396, 0.0018270808504894376, -0.047895923256874084, 0.04512954130768776, 0.14647680521011353, 0.029434461146593094, 0.04141175374388695, -0.016104448586702347]
  OUT[0]: [1, 5, 576] | μ=-0.000507 σ=0.092877
    First 10: [0.006723128259181976, 0.12636102735996246, -0.07643591612577438, -0.06954610347747803, 0.1770104169845581, -0.010447674430906773, -0.1546364426612854, -0.09481354802846909, 0.03530086949467659, 0.011641738936305046]
    Last 10:  [-0.14670315384864807, 0.13639436662197113, -0.005013647023588419, 0.15865878760814667, 0.016409333795309067, -0.014368413016200066, 0.02435031160712242, -0.017596062272787094, 0.06264224648475647, -0.11068691313266754]
  WEIGHT: [576, 1536] | μ=0.000031

132_model.layers.10.mlp: model.layers.10.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.007058 σ=0.316369
    First 10: [-0.014196217991411686, -0.04302728921175003, -0.0020331337582319975, 0.013484589755535126, -0.19574733078479767, -0.11472728103399277, 0.09163973480463028, -0.0017166491597890854, -0.012830555438995361, -0.023178013041615486]
    Last 10:  [0.09761377424001694, -0.4734342396259308, 0.08267112821340561, 0.1919720321893692, 0.2991282343864441, 0.01836516708135605, -0.03874437138438225, -0.23775219917297363, -0.3921765983104706, 0.08303117007017136]
  OUT[0]: [1, 5, 576] | μ=-0.000507 σ=0.092877
    First 10: [0.006723128259181976, 0.12636102735996246, -0.07643591612577438, -0.06954610347747803, 0.1770104169845581, -0.010447674430906773, -0.1546364426612854, -0.09481354802846909, 0.03530086949467659, 0.011641738936305046]
    Last 10:  [-0.14670315384864807, 0.13639436662197113, -0.005013647023588419, 0.15865878760814667, 0.016409333795309067, -0.014368413016200066, 0.02435031160712242, -0.017596062272787094, 0.06264224648475647, -0.11068691313266754]

133_model.layers.11.input_layernorm: model.layers.11.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080251 σ=11.887290
    First 10: [-0.7506355047225952, -2.7409920692443848, -0.16665729880332947, 0.5337216854095459, -12.191267967224121, -7.749969482421875, 5.698235988616943, -0.17183668911457062, -0.5017752647399902, -2.017284631729126]
    Last 10:  [-0.09227349609136581, -0.11441527307033539, 0.05881654843688011, 0.2974281907081604, 0.19551828503608704, -0.002632230520248413, 0.0031376536935567856, -0.15460923314094543, -0.16441284120082855, -0.06167639046907425]
  OUT[0]: [1, 5, 576] | μ=-0.016261 σ=0.419355
    First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
    Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
  WEIGHT: [576] | μ=0.538685

134_model.layers.11.self_attn.q_proj: model.layers.11.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016261 σ=0.419355
    First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
    Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
  OUT[0]: [1, 5, 576] | μ=0.013588 σ=0.869937
    First 10: [-0.09833730012178421, -0.3380144536495209, -0.10580829530954361, 0.37589961290359497, 0.39564716815948486, -0.3084254264831543, -0.4078513979911804, 0.5017313361167908, -0.5694472789764404, -0.05693710595369339]
    Last 10:  [3.198655128479004, -0.7434597611427307, 1.5761477947235107, 0.6453534960746765, 0.005273308604955673, 1.7134997844696045, -0.44339093565940857, -2.44767427444458, -1.5307910442352295, 2.3896775245666504]
  WEIGHT: [576, 576] | μ=0.000011

135_model.layers.11.self_attn.k_proj: model.layers.11.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016261 σ=0.419355
    First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
    Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
  OUT[0]: [1, 5, 192] | μ=0.055707 σ=1.275205
    First 10: [-0.0022728517651557922, -0.0015389621257781982, -0.0026853345334529877, 0.004023056477308273, 0.007059872150421143, -0.011105760931968689, -0.012318063527345657, -0.006543412804603577, -0.012114412151277065, 0.012987462803721428]
    Last 10:  [0.8687489628791809, -0.9701200723648071, 0.5929964780807495, -0.859692394733429, 1.3973126411437988, 0.6041626930236816, 4.163776397705078, -1.3620789051055908, 0.347576379776001, -1.899458408355713]
  WEIGHT: [192, 576] | μ=-0.000033

136_model.layers.11.self_attn.v_proj: model.layers.11.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.016261 σ=0.419355
    First 10: [-0.02807055227458477, -0.07222820073366165, -0.004993611015379429, 0.01575630158185959, -0.426753968000412, -0.34419387578964233, 0.21501469612121582, -0.006466146558523178, -0.01718643307685852, -0.04851820319890976]
    Last 10:  [-0.23517175018787384, -0.2497883290052414, 0.09638987481594086, 0.5944152474403381, 0.4558083713054657, -0.00523524172604084, 0.007109557744115591, -0.414860337972641, -0.36589911580085754, -0.1516149640083313]
  OUT[0]: [1, 5, 192] | μ=-0.017497 σ=0.223269
    First 10: [-0.02910691127181053, -0.04331636428833008, 0.05339854583144188, -0.5225426554679871, -0.04208200424909592, 0.002118171425536275, 0.047071706503629684, -0.014669853262603283, 0.0578739270567894, -0.05996791273355484]
    Last 10:  [-0.06093572825193405, -0.01800355315208435, -0.08862394094467163, -0.3583317995071411, 0.09321369230747223, 0.014956891536712646, -0.4446098804473877, -0.05460396409034729, -0.23791399598121643, -0.051828183233737946]
  WEIGHT: [192, 576] | μ=0.000143

137_model.layers.11.self_attn.o_proj: model.layers.11.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006694 σ=0.099394
    First 10: [-0.02910691127181053, -0.04331636428833008, 0.05339854583144188, -0.5225426554679871, -0.04208200424909592, 0.002118171425536275, 0.047071706503629684, -0.014669853262603283, 0.0578739270567894, -0.05996791273355484]
    Last 10:  [0.01429824810475111, -0.011123277246952057, -0.008670673705637455, -0.018727988004684448, 0.01208626851439476, -0.010177751071751118, -0.022329753264784813, 0.0027228668332099915, 0.00968257151544094, 0.01294622290879488]
  OUT[0]: [1, 5, 576] | μ=0.000955 σ=0.036818
    First 10: [0.03644127398729324, -0.060190871357917786, -0.07995402067899704, -0.05023517459630966, -0.07148309797048569, 0.029158759862184525, -0.02042042277753353, 0.04403755068778992, -0.00829369854182005, 0.006124582607299089]
    Last 10:  [0.09288059175014496, -0.06529207527637482, -0.01697033829987049, 0.02100716531276703, -0.00387251703068614, 0.03574519231915474, -0.03610561415553093, -0.015736475586891174, -0.06841997802257538, 0.01371303852647543]
  WEIGHT: [576, 576] | μ=-0.000046

138_model.layers.11.self_attn: model.layers.11.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.000955 σ=0.036818
    First 10: [0.03644127398729324, -0.060190871357917786, -0.07995402067899704, -0.05023517459630966, -0.07148309797048569, 0.029158759862184525, -0.02042042277753353, 0.04403755068778992, -0.00829369854182005, 0.006124582607299089]
    Last 10:  [0.09288059175014496, -0.06529207527637482, -0.01697033829987049, 0.02100716531276703, -0.00387251703068614, 0.03574519231915474, -0.03610561415553093, -0.015736475586891174, -0.06841997802257538, 0.01371303852647543]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.267236
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000377893447876, 0.4999622404575348, 0.0, 0.0, 0.0]
    Last 10:  [0.48177382349967957, 0.48069870471954346, 0.021917590871453285, 0.015609879978001118, 0.0, 0.45844319462776184, 0.45742303133010864, 0.03090348094701767, 0.02063487283885479, 0.032595373690128326]

139_model.layers.11.post_attention_layernorm: model.layers.11.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.079296 σ=11.887081
    First 10: [-0.7141942381858826, -2.801182985305786, -0.2466113269329071, 0.48348650336265564, -12.262750625610352, -7.720810890197754, 5.6778154373168945, -0.1277991384267807, -0.510068953037262, -2.011160135269165]
    Last 10:  [0.0006070956587791443, -0.1797073483467102, 0.04184620827436447, 0.3184353709220886, 0.1916457712650299, 0.033112961798906326, -0.03296796232461929, -0.1703457087278366, -0.23283281922340393, -0.047963351011276245]
  OUT[0]: [1, 5, 576] | μ=-0.006562 σ=0.312966
    First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
    Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
  WEIGHT: [576] | μ=0.378323

140_model.layers.11.mlp.gate_proj: model.layers.11.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006562 σ=0.312966
    First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
    Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
  OUT[0]: [1, 5, 1536] | μ=-0.129474 σ=0.367356
    First 10: [0.7379658818244934, -0.02564872056245804, 0.5850489735603333, -0.18778732419013977, 0.14048448204994202, 0.3175482451915741, -0.09952732175588608, 0.12970271706581116, 0.008425537496805191, -0.4662023186683655]
    Last 10:  [-0.4869888722896576, -0.48205459117889404, -0.8015052080154419, -0.6381601691246033, -0.911293625831604, 0.009540095925331116, -0.006061370950192213, -0.20772233605384827, 0.05674118921160698, -0.19846075773239136]
  WEIGHT: [1536, 576] | μ=0.000218

141_model.layers.11.mlp.act_fn: model.layers.11.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.129474 σ=0.367356
    First 10: [0.7379658818244934, -0.02564872056245804, 0.5850489735603333, -0.18778732419013977, 0.14048448204994202, 0.3175482451915741, -0.09952732175588608, 0.12970271706581116, 0.008425537496805191, -0.4662023186683655]
    Last 10:  [-0.4869888722896576, -0.48205459117889404, -0.8015052080154419, -0.6381601691246033, -0.911293625831604, 0.009540095925331116, -0.006061370950192213, -0.20772233605384827, 0.05674118921160698, -0.19846075773239136]
  OUT[0]: [1, 5, 1536] | μ=-0.028836 σ=0.158227
    First 10: [0.4992714822292328, -0.012659905478358269, 0.37573501467704773, -0.08510345965623856, 0.07516811788082123, 0.18377363681793213, -0.04728928208351135, 0.06905117630958557, 0.004230516031384468, -0.17972822487354279]
    Last 10:  [-0.1853495091199875, -0.18403257429599762, -0.24822908639907837, -0.2205880731344223, -0.2612999677658081, 0.004792801104485989, -0.003021500539034605, -0.09311265498399734, 0.029175270348787308, -0.08941590040922165]

142_model.layers.11.mlp.up_proj: model.layers.11.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006562 σ=0.312966
    First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
    Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
  OUT[0]: [1, 5, 1536] | μ=-0.004999 σ=0.299657
    First 10: [-0.366992324590683, 0.3556661009788513, 0.11972010880708694, -0.28342050313949585, 0.006526939570903778, 0.4807150363922119, -0.028140781447291374, -0.046711307018995285, 0.09118714183568954, 0.2354607880115509]
    Last 10:  [0.03171835094690323, 0.046058982610702515, 0.2318703532218933, -0.07702048122882843, -0.06078110262751579, 0.20739692449569702, -0.0663781613111496, 0.42122286558151245, -0.119338758289814, 0.027966633439064026]
  WEIGHT: [1536, 576] | μ=0.000009

143_model.layers.11.mlp.down_proj: model.layers.11.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.002258 σ=0.050446
    First 10: [-0.18322880566120148, -0.0045026992447674274, 0.04498303681612015, 0.024120064452290535, 0.0004906177637167275, 0.08834274858236313, 0.0013307573972269893, -0.0032254706602543592, 0.00038576865335926414, -0.04231894761323929]
    Last 10:  [-0.005878980737179518, -0.008476353250443935, -0.057556964457035065, 0.016989799216389656, 0.01588210090994835, 0.000994012225419283, 0.0002005616552196443, -0.039221178740262985, -0.0034817405976355076, -0.00250066164880991]
  OUT[0]: [1, 5, 576] | μ=0.001173 σ=0.084820
    First 10: [0.120505690574646, 0.0981190949678421, 0.06592634320259094, -0.013438425958156586, 0.2713161110877991, 0.08287996053695679, -0.16010701656341553, -0.012011315673589706, -0.018759645521640778, 0.0857534408569336]
    Last 10:  [-0.02987774834036827, 0.13103070855140686, 0.022475168108940125, -0.03254164755344391, -0.01837688684463501, 0.011171840131282806, 0.01572682335972786, -0.0866255909204483, -0.015201804228127003, -0.12918591499328613]
  WEIGHT: [576, 1536] | μ=0.000036

144_model.layers.11.mlp: model.layers.11.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.006562 σ=0.312966
    First 10: [-0.012964341789484024, -0.0475359745323658, -0.005296922288835049, 0.010811923071742058, -0.24219726026058197, -0.12801218032836914, 0.08749911934137344, -0.0028844664338976145, -0.011963102035224438, -0.028327852487564087]
    Last 10:  [0.0011846035486087203, -0.3366581201553345, 0.05076826736330986, 0.41392412781715393, 0.3000568151473999, 0.048330117017030716, -0.06086234003305435, -0.2973922789096832, -0.38449281454086304, -0.077850840985775]
  OUT[0]: [1, 5, 576] | μ=0.001173 σ=0.084820
    First 10: [0.120505690574646, 0.0981190949678421, 0.06592634320259094, -0.013438425958156586, 0.2713161110877991, 0.08287996053695679, -0.16010701656341553, -0.012011315673589706, -0.018759645521640778, 0.0857534408569336]
    Last 10:  [-0.02987774834036827, 0.13103070855140686, 0.022475168108940125, -0.03254164755344391, -0.01837688684463501, 0.011171840131282806, 0.01572682335972786, -0.0866255909204483, -0.015201804228127003, -0.12918591499328613]

145_model.layers.12.input_layernorm: model.layers.12.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.078123 σ=11.912354
    First 10: [-0.5936885476112366, -2.70306396484375, -0.18068498373031616, 0.47004806995391846, -11.991434097290039, -7.637930870056152, 5.5177083015441895, -0.1398104578256607, -0.5288286209106445, -1.9254066944122314]
    Last 10:  [-0.029270652681589127, -0.048676639795303345, 0.0643213763833046, 0.2858937382698059, 0.1732688844203949, 0.04428480193018913, -0.017241138964891434, -0.2569712996482849, -0.24803462624549866, -0.17714926600456238]
  OUT[0]: [1, 5, 576] | μ=-0.008111 σ=0.309859
    First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
    Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
  WEIGHT: [576] | μ=0.398369

146_model.layers.12.self_attn.q_proj: model.layers.12.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008111 σ=0.309859
    First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
    Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
  OUT[0]: [1, 5, 576] | μ=-0.099025 σ=1.247658
    First 10: [-0.10300732403993607, -0.12910595536231995, 0.3526054918766022, 0.516670286655426, 0.06717849522829056, 0.31428688764572144, -0.5736387372016907, -0.06254216283559799, 0.020749956369400024, -0.5572965741157532]
    Last 10:  [-8.515111923217773, 0.3810027241706848, -5.5177531242370605, -0.641026496887207, -0.21589386463165283, 1.0270981788635254, -1.1573641300201416, -0.3658088445663452, -0.40399372577667236, -0.11495378613471985]
  WEIGHT: [576, 576] | μ=-0.000376

147_model.layers.12.self_attn.k_proj: model.layers.12.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008111 σ=0.309859
    First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
    Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
  OUT[0]: [1, 5, 192] | μ=0.024225 σ=1.566500
    First 10: [0.014684587717056274, -0.0053712427616119385, -0.009041905403137207, 0.016889702528715134, 0.008277103304862976, 0.010395310819149017, -0.012290023267269135, -0.020806701853871346, 0.0028519518673419952, -0.025799276307225227]
    Last 10:  [0.354488730430603, 1.8524433374404907, 0.8638663291931152, -1.8554483652114868, 1.3563103675842285, 0.01034882664680481, 3.426816940307617, -0.22806595265865326, 0.08597460389137268, 0.7460355162620544]
  WEIGHT: [192, 576] | μ=0.000167

148_model.layers.12.self_attn.v_proj: model.layers.12.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008111 σ=0.309859
    First 10: [-0.012486266903579235, -0.057831354439258575, -0.00428273668512702, 0.010964695364236832, -0.26059630513191223, -0.23333190381526947, 0.13758184015750885, -0.0038976354990154505, -0.013426126912236214, -0.03180643543601036]
    Last 10:  [-0.05448824539780617, -0.08796779066324234, 0.06848844140768051, 0.4094860851764679, 0.2693997025489807, 0.06216469407081604, -0.03268251195549965, -0.44664308428764343, -0.40186694264411926, -0.31035158038139343]
  OUT[0]: [1, 5, 192] | μ=0.007348 σ=0.209788
    First 10: [0.007480494678020477, -0.003598351962864399, -0.012262886390089989, 0.00414435938000679, -0.012407689355313778, -0.0067106736823916435, 0.005282680969685316, -0.024111688137054443, 0.04743749648332596, 0.03158693015575409]
    Last 10:  [0.0037540867924690247, 0.3114430606365204, 0.02938469871878624, 0.08932285010814667, -0.17938753962516785, -0.16625963151454926, 0.2751380205154419, -0.017767369747161865, -0.1637609601020813, -0.16993016004562378]
  WEIGHT: [192, 576] | μ=0.000014

149_model.layers.12.self_attn.o_proj: model.layers.12.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.005420 σ=0.078160
    First 10: [0.007480494678020477, -0.003598351962864399, -0.012262886390089989, 0.00414435938000679, -0.012407689355313778, -0.0067106736823916435, 0.005282680969685316, -0.024111688137054443, 0.04743749648332596, 0.03158693015575409]
    Last 10:  [-0.0017163162119686604, 0.06303659826517105, 0.003836127230897546, 0.008229025639593601, 0.037445344030857086, 0.0185327660292387, 0.038581494241952896, -0.002250899560749531, -0.007847270928323269, -0.04468422755599022]
  OUT[0]: [1, 5, 576] | μ=-0.001117 σ=0.076270
    First 10: [0.014697681181132793, -0.06356437504291534, -0.03209638223052025, -0.09019242227077484, -0.0296503733843565, 0.18400844931602478, -0.18186622858047485, 0.053008757531642914, -0.06632836163043976, -0.07376118004322052]
    Last 10:  [0.10826735198497772, -0.11999920755624771, -0.1100778728723526, 0.05835971608757973, 0.027590161189436913, 0.028105370700359344, 0.100612573325634, -0.062093883752822876, -0.056943461298942566, 0.13751718401908875]
  WEIGHT: [576, 576] | μ=0.000104

150_model.layers.12.self_attn: model.layers.12.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.001117 σ=0.076270
    First 10: [0.014697681181132793, -0.06356437504291534, -0.03209638223052025, -0.09019242227077484, -0.0296503733843565, 0.18400844931602478, -0.18186622858047485, 0.053008757531642914, -0.06632836163043976, -0.07376118004322052]
    Last 10:  [0.10826735198497772, -0.11999920755624771, -0.1100778728723526, 0.05835971608757973, 0.027590161189436913, 0.028105370700359344, 0.100612573325634, -0.062093883752822876, -0.056943461298942566, 0.13751718401908875]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.262263
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4999360740184784, 0.500063955783844, 0.0, 0.0, 0.0]
    Last 10:  [0.44887983798980713, 0.4486074447631836, 0.06463519483804703, 0.03787752613425255, 0.0, 0.4395343065261841, 0.43919113278388977, 0.050784882158041, 0.03833407536149025, 0.032155636698007584]

151_model.layers.12.post_attention_layernorm: model.layers.12.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.079240 σ=11.918379
    First 10: [-0.5789908766746521, -2.7666282653808594, -0.2127813696861267, 0.3798556327819824, -12.021084785461426, -7.453922271728516, 5.335842132568359, -0.08680170029401779, -0.5951569676399231, -1.9991679191589355]
    Last 10:  [0.0789967030286789, -0.16867583990097046, -0.045756496489048004, 0.34425345063209534, 0.20085904002189636, 0.07239016890525818, 0.08337143063545227, -0.3190651834011078, -0.30497807264328003, -0.03963208198547363]
  OUT[0]: [1, 5, 576] | μ=-0.003346 σ=0.278439
    First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
    Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
  WEIGHT: [576] | μ=0.319480

152_model.layers.12.mlp.gate_proj: model.layers.12.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003346 σ=0.278439
    First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
    Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
  OUT[0]: [1, 5, 1536] | μ=-0.120033 σ=0.340734
    First 10: [-0.270221471786499, 0.12549084424972534, -0.1775200068950653, 0.06510748714208603, -0.3582253158092499, 0.03724752739071846, -0.5686708688735962, -0.12960727512836456, -0.06560668349266052, -0.13023902475833893]
    Last 10:  [-0.20723649859428406, 0.7631307244300842, -0.5525973439216614, -0.4322618544101715, -0.5279912948608398, -0.2163187563419342, 0.050342097878456116, -0.4461264908313751, -0.007415442727506161, -0.041411831974983215]
  WEIGHT: [1536, 576] | μ=-0.000056

153_model.layers.12.mlp.act_fn: model.layers.12.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.120033 σ=0.340734
    First 10: [-0.270221471786499, 0.12549084424972534, -0.1775200068950653, 0.06510748714208603, -0.3582253158092499, 0.03724752739071846, -0.5686708688735962, -0.12960727512836456, -0.06560668349266052, -0.13023902475833893]
    Last 10:  [-0.20723649859428406, 0.7631307244300842, -0.5525973439216614, -0.4322618544101715, -0.5279912948608398, -0.2163187563419342, 0.050342097878456116, -0.4461264908313751, -0.007415442727506161, -0.041411831974983215]
  OUT[0]: [1, 5, 1536] | μ=-0.028904 σ=0.158054
    First 10: [-0.11696609854698181, 0.06667724996805191, -0.08090228587388992, 0.033613115549087524, -0.14737004041671753, 0.018970569595694542, -0.20559930801391602, -0.06060999259352684, -0.03172766789793968, -0.06088494881987572]
    Last 10:  [-0.09291976690292358, 0.5204803347587585, -0.20184281468391418, -0.1701323539018631, -0.19587711989879608, -0.09650633484125137, 0.025804495438933372, -0.17411518096923828, -0.003693974344059825, -0.020277243107557297]

154_model.layers.12.mlp.up_proj: model.layers.12.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003346 σ=0.278439
    First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
    Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
  OUT[0]: [1, 5, 1536] | μ=0.001043 σ=0.276864
    First 10: [-0.14680781960487366, 0.07992859929800034, -0.42700833082199097, 0.0538935549557209, 0.25258222222328186, -0.18725481629371643, 0.16362294554710388, 0.4287998080253601, -0.14947807788848877, -0.048913855105638504]
    Last 10:  [-0.21148604154586792, -0.42731842398643494, -0.25878578424453735, 0.45757076144218445, 0.0678255558013916, -0.13032351434230804, -0.1819869577884674, 0.0992664247751236, 0.14924320578575134, 0.29146289825439453]
  WEIGHT: [1536, 576] | μ=-0.000021

155_model.layers.12.mlp.down_proj: model.layers.12.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.001120 σ=0.047342
    First 10: [0.01717153750360012, 0.005329419393092394, 0.03454595059156418, 0.0018115303246304393, -0.037223052233457565, -0.0035523304250091314, -0.03364076465368271, -0.025989552959799767, 0.004742590710520744, 0.002978117670863867]
    Last 10:  [0.01965123414993286, -0.22241084277629852, 0.05223404988646507, -0.07784759253263474, -0.013285474851727486, 0.012577044777572155, -0.004696081392467022, -0.017283791676163673, -0.0005513005889952183, -0.005910064093768597]
  OUT[0]: [1, 5, 576] | μ=-0.000092 σ=0.088985
    First 10: [0.0234675370156765, 0.12398941814899445, -0.011473916471004486, -0.01738768070936203, 0.2233818620443344, 0.01835508458316326, -0.09885233640670776, -0.010740630328655243, 0.07286752760410309, 0.0022932225838303566]
    Last 10:  [0.10431592166423798, -0.11954665184020996, 0.1165568009018898, 0.010377686470746994, -0.028768960386514664, -0.031969573348760605, -0.10366721451282501, 0.033560849726200104, 0.18722179532051086, -0.06568539142608643]
  WEIGHT: [576, 1536] | μ=0.000069

156_model.layers.12.mlp: model.layers.12.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.003346 σ=0.278439
    First 10: [-0.00928967073559761, -0.037576824426651, -0.0037256060168147087, 0.007305676117539406, -0.17262010276317596, -0.10375215113162994, 0.0672512799501419, -0.0015636913012713194, -0.012040440924465656, -0.024665743112564087]
    Last 10:  [0.10867597162723541, -0.236487478017807, -0.0440390445291996, 0.3781473934650421, 0.2509457468986511, 0.08262954652309418, 0.1135970875620842, -0.42942097783088684, -0.38022544980049133, -0.04795011505484581]
  OUT[0]: [1, 5, 576] | μ=-0.000092 σ=0.088985
    First 10: [0.0234675370156765, 0.12398941814899445, -0.011473916471004486, -0.01738768070936203, 0.2233818620443344, 0.01835508458316326, -0.09885233640670776, -0.010740630328655243, 0.07286752760410309, 0.0022932225838303566]
    Last 10:  [0.10431592166423798, -0.11954665184020996, 0.1165568009018898, 0.010377686470746994, -0.028768960386514664, -0.031969573348760605, -0.10366721451282501, 0.033560849726200104, 0.18722179532051086, -0.06568539142608643]

157_model.layers.13.input_layernorm: model.layers.13.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.079332 σ=11.952465
    First 10: [-0.5555233359336853, -2.642638921737671, -0.2242552936077118, 0.3624679446220398, -11.79770278930664, -7.435567378997803, 5.236989974975586, -0.09754233062267303, -0.5222894549369812, -1.9968746900558472]
    Last 10:  [0.18331262469291687, -0.2882224917411804, 0.0708003044128418, 0.35463112592697144, 0.172090083360672, 0.040420595556497574, -0.02029578387737274, -0.2855043411254883, -0.11775627732276917, -0.10531747341156006]
  OUT[0]: [1, 5, 576] | μ=-0.004712 σ=0.460520
    First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
    Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
  WEIGHT: [576] | μ=0.566995

158_model.layers.13.self_attn.q_proj: model.layers.13.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004712 σ=0.460520
    First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
    Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
  OUT[0]: [1, 5, 576] | μ=-0.066818 σ=1.190367
    First 10: [-0.42076098918914795, 0.6793600916862488, -0.3703729808330536, 0.435073584318161, -0.2199559509754181, -0.0200016051530838, 0.10038726031780243, -0.01716776192188263, 0.07641714066267014, -0.18805919587612152]
    Last 10:  [-1.7435721158981323, -0.004382014274597168, 1.4221065044403076, -0.6278623938560486, -1.5212430953979492, -1.202261209487915, 2.2697012424468994, -1.72633957862854, 3.7544283866882324, 1.6160914897918701]
  WEIGHT: [576, 576] | μ=-0.000109

159_model.layers.13.self_attn.k_proj: model.layers.13.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004712 σ=0.460520
    First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
    Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
  OUT[0]: [1, 5, 192] | μ=0.035674 σ=1.445020
    First 10: [-0.008639417588710785, -0.010184798389673233, -0.002089519053697586, -0.018481580540537834, 0.006759904325008392, 0.010181590914726257, -0.00013541430234909058, 0.021402381360530853, -0.01400740072131157, -0.024360768496990204]
    Last 10:  [-1.4801174402236938, -2.876662015914917, 0.7733408808708191, -6.402985572814941, 1.1416953802108765, -0.3163928985595703, -3.121788263320923, 4.005972862243652, -1.4569640159606934, -5.875749111175537]
  WEIGHT: [192, 576] | μ=0.000291

160_model.layers.13.self_attn.v_proj: model.layers.13.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004712 σ=0.460520
    First 10: [-0.016152743250131607, -0.08496687561273575, -0.007320445030927658, 0.010342609137296677, -0.30431225895881653, -0.2759688198566437, 0.20235520601272583, -0.0029950288590043783, -0.022043922916054726, -0.08856447041034698]
    Last 10:  [0.6117520332336426, -0.582849383354187, 0.1290869563817978, 1.2643001079559326, 0.29228609800338745, 0.06598369032144547, -0.03839937970042229, -0.977058470249176, -0.22854435443878174, -0.19735443592071533]
  OUT[0]: [1, 5, 192] | μ=-0.005100 σ=0.379252
    First 10: [-0.0030643390491604805, -0.003891207743436098, -0.0007160466630011797, 0.0005905274301767349, 0.005899776704609394, 0.01040727086365223, 0.004341306164860725, -0.005344033241271973, -0.0012756967917084694, 0.0021893689408898354]
    Last 10:  [-0.07414878159761429, -0.1413586139678955, -0.48202940821647644, 0.30420979857444763, -0.22484007477760315, 0.1498182713985443, -0.25613903999328613, -0.7514139413833618, 0.07537653297185898, -0.2919069528579712]
  WEIGHT: [192, 576] | μ=-0.000046

161_model.layers.13.self_attn.o_proj: model.layers.13.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.005242 σ=0.048990
    First 10: [-0.0030643390491604805, -0.003891207743436098, -0.0007160466630011797, 0.0005905274301767349, 0.005899776704609394, 0.01040727086365223, 0.004341306164860725, -0.005344033241271973, -0.0012756967917084694, 0.0021893689408898354]
    Last 10:  [-0.0004892586730420589, -0.003795405151322484, -0.020822331309318542, -0.0026854330208152533, -0.004840119741857052, 0.006202497985213995, -0.004435309208929539, -0.016293026506900787, -9.417619730811566e-05, -0.0021265435498207808]
  OUT[0]: [1, 5, 576] | μ=-0.001198 σ=0.033941
    First 10: [-0.008968332782387733, 0.018781838938593864, -0.05217666178941727, 0.026461176574230194, -0.0163239985704422, 0.027069933712482452, 0.005506984423846006, 0.010798115283250809, -0.02114054188132286, -0.037783071398735046]
    Last 10:  [0.09548792243003845, -0.00459502637386322, -0.019622422754764557, -0.08481042087078094, -0.01826043426990509, -0.03214899078011513, -0.05278797075152397, -0.04224478080868721, -0.009386582300066948, -0.0018432867946103215]
  WEIGHT: [576, 576] | μ=0.000024

162_model.layers.13.self_attn: model.layers.13.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.001198 σ=0.033941
    First 10: [-0.008968332782387733, 0.018781838938593864, -0.05217666178941727, 0.026461176574230194, -0.0163239985704422, 0.027069933712482452, 0.005506984423846006, 0.010798115283250809, -0.02114054188132286, -0.037783071398735046]
    Last 10:  [0.09548792243003845, -0.00459502637386322, -0.019622422754764557, -0.08481042087078094, -0.01826043426990509, -0.03214899078011513, -0.05278797075152397, -0.04224478080868721, -0.009386582300066948, -0.0018432867946103215]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.280217
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000064969062805, 0.49999353289604187, 0.0, 0.0, 0.0]
    Last 10:  [0.4898017644882202, 0.49066632986068726, 0.00640150299295783, 0.01313041616231203, 0.0, 0.49137449264526367, 0.49358707666397095, 0.001001422991976142, 0.0017073185881599784, 0.012329678051173687]

163_model.layers.13.post_attention_layernorm: model.layers.13.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080530 σ=11.956353
    First 10: [-0.5644916892051697, -2.623857021331787, -0.27643194794654846, 0.3889291286468506, -11.814026832580566, -7.408497333526611, 5.242496967315674, -0.08674421906471252, -0.543429970741272, -2.0346577167510986]
    Last 10:  [0.2788005471229553, -0.29281753301620483, 0.05117788165807724, 0.2698206901550293, 0.1538296490907669, 0.008271604776382446, -0.07308375835418701, -0.3277491331100464, -0.12714286148548126, -0.10716076195240021]
  OUT[0]: [1, 5, 576] | μ=-0.003862 σ=0.323448
    First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
    Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
  WEIGHT: [576] | μ=0.371248

164_model.layers.13.mlp.gate_proj: model.layers.13.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003862 σ=0.323448
    First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
    Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
  OUT[0]: [1, 5, 1536] | μ=-0.104733 σ=0.385689
    First 10: [-0.13648712635040283, -0.02419028803706169, 0.1458909809589386, 0.0854378417134285, -0.3010406792163849, -0.11004713922739029, 0.4170633852481842, 0.01187189482152462, -0.10243837535381317, 0.35705187916755676]
    Last 10:  [-0.11883067339658737, 0.282428503036499, 0.5761182904243469, 0.07445305585861206, -0.20090416073799133, -1.3292427062988281, 0.23481516540050507, 0.17988209426403046, -0.01845243200659752, -0.5340774059295654]
  WEIGHT: [1536, 576] | μ=0.000153

165_model.layers.13.mlp.act_fn: model.layers.13.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.104733 σ=0.385689
    First 10: [-0.13648712635040283, -0.02419028803706169, 0.1458909809589386, 0.0854378417134285, -0.3010406792163849, -0.11004713922739029, 0.4170633852481842, 0.01187189482152462, -0.10243837535381317, 0.35705187916755676]
    Last 10:  [-0.11883067339658737, 0.282428503036499, 0.5761182904243469, 0.07445305585861206, -0.20090416073799133, -1.3292427062988281, 0.23481516540050507, 0.17988209426403046, -0.01845243200659752, -0.5340774059295654]
  OUT[0]: [1, 5, 1536] | μ=-0.014712 σ=0.182810
    First 10: [-0.06359358876943588, -0.011948859319090843, 0.07825712114572525, 0.044542718678712845, -0.12803353369235992, -0.05199902877211571, 0.25139760971069336, 0.005971182603389025, -0.048598069697618484, 0.2100631296634674]
    Last 10:  [-0.055889301002025604, 0.16102421283721924, 0.3688157796859741, 0.038611702620983124, -0.09039527177810669, -0.27819010615348816, 0.13112913072109222, 0.09800869971513748, -0.009141094982624054, -0.1973770558834076]

166_model.layers.13.mlp.up_proj: model.layers.13.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003862 σ=0.323448
    First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
    Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
  OUT[0]: [1, 5, 1536] | μ=0.001981 σ=0.276855
    First 10: [-0.26818615198135376, 0.1785079538822174, -0.10966960340738297, -0.14726987481117249, 0.345971018075943, -0.3213123381137848, 0.16391846537590027, 0.3449362516403198, -0.08196382224559784, 0.149034783244133]
    Last 10:  [-0.20740064978599548, 0.08973650634288788, 0.1117028072476387, 0.5778361558914185, -0.058040767908096313, 0.46338027715682983, 0.10986851155757904, 0.16729488968849182, -0.3580135703086853, 0.16964662075042725]
  WEIGHT: [1536, 576] | μ=0.000065

167_model.layers.13.mlp.down_proj: model.layers.13.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000329 σ=0.053829
    First 10: [0.01705491915345192, -0.0021329664159566164, -0.00858242716640234, -0.006559800822287798, -0.044295892119407654, 0.016707928851246834, 0.04120871052145958, 0.0020596773829311132, 0.003983283415436745, 0.031306713819503784]
    Last 10:  [0.011591477319598198, 0.014449750073254108, 0.0411977581679821, 0.022311238572001457, 0.005246610846370459, -0.12890781462192535, 0.01440696232020855, 0.016396354883909225, 0.0032726360950618982, -0.03348435088992119]
  OUT[0]: [1, 5, 576] | μ=0.000468 σ=0.088488
    First 10: [-0.039452143013477325, 0.011190068908035755, -0.008586423471570015, 3.8825906813144684e-05, 0.07798518240451813, 0.010613531805574894, -0.08701080083847046, 0.023920992389321327, -0.04606546461582184, 0.0046095699071884155]
    Last 10:  [-0.22204355895519257, 0.18444284796714783, 0.11296658217906952, -0.2987004518508911, -0.061648618429899216, 0.011459406465291977, -0.005854015238583088, 0.24903547763824463, -0.016605587676167488, 0.054717931896448135]
  WEIGHT: [576, 1536] | μ=0.000004

168_model.layers.13.mlp: model.layers.13.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.003862 σ=0.323448
    First 10: [-0.010289780795574188, -0.041727256029844284, -0.0055567314848303795, 0.008305483497679234, -0.2423650473356247, -0.11628614366054535, 0.08269426971673965, -0.0018400739645585418, -0.011969872750341892, -0.027730990201234818]
    Last 10:  [0.44075703620910645, -0.4742264449596405, 0.060772448778152466, 0.33397766947746277, 0.22812877595424652, 0.011412320658564568, -0.12269394099712372, -0.45778852701187134, -0.19392000138759613, -0.15728265047073364]
  OUT[0]: [1, 5, 576] | μ=0.000468 σ=0.088488
    First 10: [-0.039452143013477325, 0.011190068908035755, -0.008586423471570015, 3.8825906813144684e-05, 0.07798518240451813, 0.010613531805574894, -0.08701080083847046, 0.023920992389321327, -0.04606546461582184, 0.0046095699071884155]
    Last 10:  [-0.22204355895519257, 0.18444284796714783, 0.11296658217906952, -0.2987004518508911, -0.061648618429899216, 0.011459406465291977, -0.005854015238583088, 0.24903547763824463, -0.016605587676167488, 0.054717931896448135]

169_model.layers.14.input_layernorm: model.layers.14.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080062 σ=11.965488
    First 10: [-0.6039438247680664, -2.6126668453216553, -0.2850183844566345, 0.38896796107292175, -11.736042022705078, -7.397883892059326, 5.155486106872559, -0.06282322853803635, -0.5894954204559326, -2.030048131942749]
    Last 10:  [0.056756988167762756, -0.108374685049057, 0.16414445638656616, -0.028879761695861816, 0.09218102693557739, 0.019731011241674423, -0.07893777638673782, -0.07871365547180176, -0.1437484472990036, -0.05244283005595207]
  OUT[0]: [1, 5, 576] | μ=-0.010466 σ=0.422991
    First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
    Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
  WEIGHT: [576] | μ=0.519671

170_model.layers.14.self_attn.q_proj: model.layers.14.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.010466 σ=0.422991
    First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
    Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
  OUT[0]: [1, 5, 576] | μ=-0.061669 σ=1.189427
    First 10: [-0.3514802157878876, -0.05922148376703262, -0.36461183428764343, -0.022580616176128387, 0.13230937719345093, 0.3659815490245819, -0.20977497100830078, -0.34770911931991577, -0.328805536031723, -0.03519643843173981]
    Last 10:  [-0.2600557804107666, -1.1311225891113281, 0.4009439945220947, 0.9139565825462341, 0.7931382656097412, -1.0097589492797852, 0.8571145534515381, -1.6014360189437866, 0.28154757618904114, 2.5096559524536133]
  WEIGHT: [576, 576] | μ=-0.000020

171_model.layers.14.self_attn.k_proj: model.layers.14.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.010466 σ=0.422991
    First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
    Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
  OUT[0]: [1, 5, 192] | μ=-0.011048 σ=1.598904
    First 10: [-0.00015250593423843384, 0.00789109617471695, 0.018104486167430878, -0.016701724380254745, 0.0014400612562894821, -0.00628583412617445, -0.006771024316549301, -0.012862557545304298, 0.0028364062309265137, 0.002975955605506897]
    Last 10:  [0.504362940788269, 10.621129035949707, 1.588103175163269, -0.2511930465698242, 1.3895225524902344, 2.1603503227233887, -1.7744572162628174, 0.7686845064163208, 2.139622926712036, -1.8115251064300537]
  WEIGHT: [192, 576] | μ=-0.000041

172_model.layers.14.self_attn.v_proj: model.layers.14.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.010466 σ=0.422991
    First 10: [-0.015194791369140148, -0.08033636212348938, -0.008741897530853748, 0.012673295103013515, -0.3414773643016815, -0.3002454340457916, 0.183015838265419, -0.0022009818349033594, -0.019952598959207535, -0.04617411643266678]
    Last 10:  [0.12788602709770203, -0.23999221622943878, 0.26792430877685547, -0.061341866850852966, 0.16815364360809326, 0.03613831475377083, -0.2018999457359314, -0.19420909881591797, -0.2801271378993988, -0.12039115279912949]
  OUT[0]: [1, 5, 192] | μ=-0.003793 σ=0.227292
    First 10: [-0.004392341244965792, -0.003490538103505969, 0.008717425167560577, 0.0021784938871860504, -0.3298429846763611, 0.00830900575965643, -0.0037184394896030426, 0.004065722227096558, -0.0016832845285534859, 0.0011702044866979122]
    Last 10:  [-0.6568197011947632, -0.2794167101383209, -0.07752219587564468, 0.009186781942844391, 0.5729794502258301, -0.1233617514371872, 0.005425295792520046, -0.13127289712429047, -0.280194491147995, 0.003177165985107422]
  WEIGHT: [192, 576] | μ=0.000045

173_model.layers.14.self_attn.o_proj: model.layers.14.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003562 σ=0.074555
    First 10: [-0.004392341244965792, -0.003490538103505969, 0.008717425167560577, 0.0021784938871860504, -0.3298429846763611, 0.00830900575965643, -0.0037184394896030426, 0.004065722227096558, -0.0016832845285534859, 0.0011702044866979122]
    Last 10:  [-0.03878210112452507, -0.0024176547303795815, 0.014912809245288372, 0.03185832500457764, 0.06820628046989441, -0.013219917193055153, 0.04151254892349243, 0.01744043454527855, -0.018768716603517532, -0.00031905024661682546]
  OUT[0]: [1, 5, 576] | μ=-0.000517 σ=0.043271
    First 10: [-0.004497063346207142, -0.020656470209360123, -0.008182350546121597, 0.029331877827644348, -0.06959445774555206, 0.011004443280398846, -0.021061347797513008, 0.0316961295902729, 0.013342833146452904, -0.028646090999245644]
    Last 10:  [0.06155891716480255, -0.03895111009478569, -0.01922137476503849, -0.022138241678476334, -0.01845867559313774, 0.010098550468683243, -0.0738924890756607, -0.03796697035431862, -0.035423748195171356, 0.021282747387886047]
  WEIGHT: [576, 576] | μ=0.000034

174_model.layers.14.self_attn: model.layers.14.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000517 σ=0.043271
    First 10: [-0.004497063346207142, -0.020656470209360123, -0.008182350546121597, 0.029331877827644348, -0.06959445774555206, 0.011004443280398846, -0.021061347797513008, 0.0316961295902729, 0.013342833146452904, -0.028646090999245644]
    Last 10:  [0.06155891716480255, -0.03895111009478569, -0.01922137476503849, -0.022138241678476334, -0.01845867559313774, 0.010098550468683243, -0.0738924890756607, -0.03796697035431862, -0.035423748195171356, 0.021282747387886047]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.271420
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000494718551636, 0.4999505877494812, 0.0, 0.0, 0.0]
    Last 10:  [0.42812231183052063, 0.42677605152130127, 0.0884450227022171, 0.05665652081370354, 0.0, 0.45246925950050354, 0.45050230622291565, 0.02913064695894718, 0.02884392999112606, 0.0390537865459919]

175_model.layers.14.post_attention_layernorm: model.layers.14.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080579 σ=11.967939
    First 10: [-0.6084408760070801, -2.6333234310150146, -0.2932007312774658, 0.4182998538017273, -11.805636405944824, -7.3868794441223145, 5.134424686431885, -0.031127098947763443, -0.5761525630950928, -2.0586941242218018]
    Last 10:  [0.11831590533256531, -0.147325798869133, 0.14492307603359222, -0.05101800337433815, 0.07372234761714935, 0.029829561710357666, -0.15283027291297913, -0.11668062210083008, -0.17917218804359436, -0.031160082668066025]
  OUT[0]: [1, 5, 576] | μ=-0.003208 σ=0.323494
    First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
    Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
  WEIGHT: [576] | μ=0.377509

176_model.layers.14.mlp.gate_proj: model.layers.14.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003208 σ=0.323494
    First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
    Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
  OUT[0]: [1, 5, 1536] | μ=-0.106218 σ=0.367682
    First 10: [-0.07407622039318085, -0.31577643752098083, -0.6433165073394775, -0.1772184520959854, 0.1140700951218605, -0.2813716232776642, 0.1797490268945694, -0.7669975161552429, -0.12558990716934204, -0.31941285729408264]
    Last 10:  [-0.6567513346672058, -0.16795724630355835, 0.4937131404876709, 0.1826907992362976, 0.31825947761535645, -0.04405509680509567, 0.6012969613075256, -0.43153178691864014, -0.3819727897644043, -0.3574410080909729]
  WEIGHT: [1536, 576] | μ=0.000200

177_model.layers.14.mlp.act_fn: model.layers.14.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.106218 σ=0.367682
    First 10: [-0.07407622039318085, -0.31577643752098083, -0.6433165073394775, -0.1772184520959854, 0.1140700951218605, -0.2813716232776642, 0.1797490268945694, -0.7669975161552429, -0.12558990716934204, -0.31941285729408264]
    Last 10:  [-0.6567513346672058, -0.16795724630355835, 0.4937131404876709, 0.1826907992362976, 0.31825947761535645, -0.04405509680509567, 0.6012969613075256, -0.43153178691864014, -0.3819727897644043, -0.3574410080909729]
  OUT[0]: [1, 5, 1536] | μ=-0.018214 σ=0.172543
    First 10: [-0.03566691279411316, -0.13316462934017181, -0.22162075340747833, -0.08077811449766159, 0.06028452143073082, -0.12102286517620087, 0.09793026745319366, -0.24323713779449463, -0.05885691940784454, -0.13441495597362518]
    Last 10:  [-0.22426071763038635, -0.07694274187088013, 0.306586354970932, 0.09966625273227692, 0.1842404156923294, -0.021542415022850037, 0.3884095847606659, -0.1699202060699463, -0.1549476981163025, -0.14711527526378632]

178_model.layers.14.mlp.up_proj: model.layers.14.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003208 σ=0.323494
    First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
    Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
  OUT[0]: [1, 5, 1536] | μ=0.004692 σ=0.284830
    First 10: [0.11860073357820511, 0.09151222556829453, -0.017519276589155197, -0.12478967010974884, 0.19767773151397705, -0.14508064091205597, -0.12148916721343994, -0.2326991707086563, -0.32709673047065735, -0.022750381380319595]
    Last 10:  [0.0010524839162826538, 0.32037317752838135, -0.09161709994077682, -0.8842049241065979, 0.7040984630584717, 0.1478009968996048, 0.39878538250923157, 0.17709651589393616, 0.09404692053794861, -0.10393446683883667]
  WEIGHT: [1536, 576] | μ=-0.000011

179_model.layers.14.mlp.down_proj: model.layers.14.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000682 σ=0.057558
    First 10: [-0.004230122081935406, -0.012186191976070404, 0.00388263538479805, 0.010080274194478989, 0.011916907504200935, 0.017558075487613678, -0.011897467076778412, 0.05660108104348183, 0.019251905381679535, 0.0030579916201531887]
    Last 10:  [-0.00023603079898748547, -0.024650391191244125, -0.028088552877306938, -0.08812539279460907, 0.12972339987754822, -0.0031839904841035604, 0.15489207208156586, -0.030092276632785797, -0.014572354033589363, 0.01529034785926342]
  OUT[0]: [1, 5, 576] | μ=-0.000176 σ=0.084054
    First 10: [-0.03280874341726303, -0.04123706370592117, -0.014504763297736645, 0.0506117045879364, 0.016550302505493164, -0.04607955366373062, -0.08920478820800781, -0.020554790273308754, 0.03946232795715332, 0.054748889058828354]
    Last 10:  [0.12243931740522385, -0.044186852872371674, 0.0958390012383461, -0.03137553855776787, -0.00720810703933239, -0.15006276965141296, 0.16627170145511627, -0.02194659411907196, -0.032278984785079956, 0.024131760001182556]
  WEIGHT: [576, 1536] | μ=-0.000080

180_model.layers.14.mlp: model.layers.14.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.003208 σ=0.323494
    First 10: [-0.011410009115934372, -0.04394451528787613, -0.0059032561257481575, 0.009053629823029041, -0.22367499768733978, -0.10839854180812836, 0.08442346751689911, -0.000658846867736429, -0.012663510628044605, -0.03212329372763634]
    Last 10:  [0.18454933166503906, -0.23952177166938782, 0.17835946381092072, -0.06283428519964218, 0.10848323255777359, 0.0415002778172493, -0.24192863702774048, -0.1649329960346222, -0.2657310962677002, -0.04537998139858246]
  OUT[0]: [1, 5, 576] | μ=-0.000176 σ=0.084054
    First 10: [-0.03280874341726303, -0.04123706370592117, -0.014504763297736645, 0.0506117045879364, 0.016550302505493164, -0.04607955366373062, -0.08920478820800781, -0.020554790273308754, 0.03946232795715332, 0.054748889058828354]
    Last 10:  [0.12243931740522385, -0.044186852872371674, 0.0958390012383461, -0.03137553855776787, -0.00720810703933239, -0.15006276965141296, 0.16627170145511627, -0.02194659411907196, -0.032278984785079956, 0.024131760001182556]

181_model.layers.15.input_layernorm: model.layers.15.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080755 σ=11.976205
    First 10: [-0.6412495970726013, -2.674560546875, -0.30770549178123474, 0.4689115583896637, -11.78908634185791, -7.432959079742432, 5.045219898223877, -0.051681891083717346, -0.5366902351379395, -2.0039453506469727]
    Last 10:  [0.24075523018836975, -0.19151264429092407, 0.24076208472251892, -0.08239354193210602, 0.06651423871517181, -0.1202332079410553, 0.013441428542137146, -0.13862721621990204, -0.21145117282867432, -0.007028322666883469]
  OUT[0]: [1, 5, 576] | μ=-0.008878 σ=0.435554
    First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
    Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
  WEIGHT: [576] | μ=0.557016

182_model.layers.15.self_attn.q_proj: model.layers.15.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008878 σ=0.435554
    First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
    Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
  OUT[0]: [1, 5, 576] | μ=-0.092776 σ=0.982665
    First 10: [0.07802552729845047, -0.16831021010875702, -0.15376557409763336, 0.29059451818466187, 0.22482773661613464, 0.2570437788963318, 0.31175166368484497, -0.26605933904647827, -0.13152524828910828, -0.06264762580394745]
    Last 10:  [0.06408193707466125, -4.008188724517822, 0.5420756936073303, -0.7783502340316772, 1.8474944829940796, -0.6108828186988831, 1.1639776229858398, -0.6746225357055664, -0.5543580055236816, 2.634235382080078]
  WEIGHT: [576, 576] | μ=-0.000159

183_model.layers.15.self_attn.k_proj: model.layers.15.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008878 σ=0.435554
    First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
    Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
  OUT[0]: [1, 5, 192] | μ=-0.087334 σ=1.462452
    First 10: [-0.01848674565553665, 0.008560292422771454, -0.0014318674802780151, 0.018284618854522705, 0.002789027988910675, -0.005718842148780823, 0.005018316209316254, 0.012630641460418701, 0.018589362502098083, 0.011986792087554932]
    Last 10:  [0.6881583333015442, 1.3895552158355713, -0.35941582918167114, 0.21085986495018005, 2.3370471000671387, 0.31648313999176025, -3.3314404487609863, 0.5890377163887024, 1.4098063707351685, -1.9833316802978516]
  WEIGHT: [192, 576] | μ=0.000036

184_model.layers.15.self_attn.v_proj: model.layers.15.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008878 σ=0.435554
    First 10: [-0.01784852333366871, -0.080515056848526, -0.009096485562622547, 0.015458793379366398, -0.30365610122680664, -0.30659422278404236, 0.18064391613006592, -0.0018717993516474962, -0.016904141753911972, -0.06270475685596466]
    Last 10:  [0.4590790271759033, -0.3931545317173004, 0.38025808334350586, -0.142563134431839, 0.12117297202348709, -0.221737802028656, 0.028823530301451683, -0.2899267077445984, -0.38215872645378113, -0.01494730357080698]
  OUT[0]: [1, 5, 192] | μ=0.014041 σ=0.245201
    First 10: [-7.308647036552429e-05, -0.0019907504320144653, -0.002632011193782091, 0.0036939848214387894, 0.0016977153718471527, 0.000722990371286869, -0.007085083052515984, -0.008940479718148708, -0.0008091479539871216, 0.0054744696244597435]
    Last 10:  [-0.22604970633983612, 0.37279489636421204, -0.1945694535970688, 0.1265314817428589, -0.13461855053901672, -0.031081005930900574, -0.2440500557422638, 0.21388238668441772, -0.11362393200397491, 0.4031302332878113]
  WEIGHT: [192, 576] | μ=-0.000067

185_model.layers.15.self_attn.o_proj: model.layers.15.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.007818 σ=0.065691
    First 10: [-7.308647036552429e-05, -0.0019907504320144653, -0.002632011193782091, 0.0036939848214387894, 0.0016977153718471527, 0.000722990371286869, -0.007085083052515984, -0.008940479718148708, -0.0008091479539871216, 0.0054744696244597435]
    Last 10:  [-0.011285901069641113, 0.00859407801181078, 0.004827586002647877, 0.021838994696736336, -0.007263955194503069, -0.0479145310819149, 0.010600698180496693, -0.004944524262100458, -0.0014514672802761197, 0.017791738733649254]
  OUT[0]: [1, 5, 576] | μ=-0.000480 σ=0.028471
    First 10: [0.01998063176870346, 0.013053908944129944, 0.05504915118217468, -0.00795756559818983, -0.038467392325401306, -0.011210080236196518, 0.04191960394382477, -0.01814422383904457, -0.010399280115962029, -0.002248473232612014]
    Last 10:  [0.016002409160137177, -0.033594053238630295, -0.005251521244645119, -0.02002592757344246, 0.008774877525866032, -0.001293916255235672, -0.012526373378932476, 0.021626941859722137, -0.021112563088536263, 0.012510336935520172]
  WEIGHT: [576, 576] | μ=0.000059

186_model.layers.15.self_attn: model.layers.15.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000480 σ=0.028471
    First 10: [0.01998063176870346, 0.013053908944129944, 0.05504915118217468, -0.00795756559818983, -0.038467392325401306, -0.011210080236196518, 0.04191960394382477, -0.01814422383904457, -0.010399280115962029, -0.002248473232612014]
    Last 10:  [0.016002409160137177, -0.033594053238630295, -0.005251521244645119, -0.02002592757344246, 0.008774877525866032, -0.001293916255235672, -0.012526373378932476, 0.021626941859722137, -0.021112563088536263, 0.012510336935520172]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.272957
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.499970406293869, 0.5000296235084534, 0.0, 0.0, 0.0]
    Last 10:  [0.4725435972213745, 0.4740278422832489, 0.028379594907164574, 0.025048935785889626, 0.0, 0.4666740298271179, 0.46783795952796936, 0.02236875705420971, 0.01639929786324501, 0.026719938963651657]

187_model.layers.15.post_attention_layernorm: model.layers.15.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.081235 σ=11.975645
    First 10: [-0.6212689876556396, -2.6615066528320312, -0.25265634059906006, 0.4609539806842804, -11.827553749084473, -7.444169044494629, 5.08713960647583, -0.06982611119747162, -0.5470895171165466, -2.0061938762664795]
    Last 10:  [0.2567576467990875, -0.22510670125484467, 0.23551055788993835, -0.10241946578025818, 0.07528911530971527, -0.12152712047100067, 0.00091505516320467, -0.1170002743601799, -0.23256373405456543, 0.0054820142686367035]
  OUT[0]: [1, 5, 576] | μ=-0.001926 σ=0.326463
    First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
    Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
  WEIGHT: [576] | μ=0.386820

188_model.layers.15.mlp.gate_proj: model.layers.15.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001926 σ=0.326463
    First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
    Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
  OUT[0]: [1, 5, 1536] | μ=-0.097665 σ=0.368647
    First 10: [-0.4651666581630707, -0.3400917649269104, 0.052383504807949066, -0.26150426268577576, 0.11982455849647522, -0.3308011591434479, -0.0024910876527428627, 0.3283015489578247, 0.4007241427898407, 0.04473571106791496]
    Last 10:  [0.04808475077152252, 0.5974282026290894, -0.6890300512313843, -0.17770467698574066, -0.27593955397605896, -0.19452960789203644, -0.5481966137886047, 0.6126914620399475, 0.23851102590560913, -0.0579935722053051]
  WEIGHT: [1536, 576] | μ=0.000238

189_model.layers.15.mlp.act_fn: model.layers.15.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.097665 σ=0.368647
    First 10: [-0.4651666581630707, -0.3400917649269104, 0.052383504807949066, -0.26150426268577576, 0.11982455849647522, -0.3308011591434479, -0.0024910876527428627, 0.3283015489578247, 0.4007241427898407, 0.04473571106791496]
    Last 10:  [0.04808475077152252, 0.5974282026290894, -0.6890300512313843, -0.17770467698574066, -0.27593955397605896, -0.19452960789203644, -0.5481966137886047, 0.6126914620399475, 0.23851102590560913, -0.0579935722053051]
  OUT[0]: [1, 5, 1536] | μ=-0.014063 σ=0.175482
    First 10: [-0.1794431060552597, -0.14140580594539642, 0.02687760442495346, -0.11375277489423752, 0.06349747627973557, -0.13829000294208527, -0.0012439923593774438, 0.19085681438446045, 0.23997831344604492, 0.022868093103170395]
    Last 10:  [0.0246203001588583, 0.38538163900375366, -0.23030751943588257, -0.08097831159830093, -0.11905399709939957, -0.08783408999443054, -0.2007950395345688, 0.3973638117313385, 0.13341034948825836, -0.028156209737062454]

190_model.layers.15.mlp.up_proj: model.layers.15.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001926 σ=0.326463
    First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
    Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
  OUT[0]: [1, 5, 1536] | μ=0.002155 σ=0.278305
    First 10: [0.29630032181739807, 0.24168069660663605, 0.0652238130569458, 0.14969247579574585, 0.4160180687904358, 0.02824719063937664, -0.09548992663621902, 0.039694029837846756, 0.5051534175872803, -0.4870710074901581]
    Last 10:  [0.25510597229003906, 0.20596373081207275, 0.47933492064476013, 0.47168609499931335, -0.005025304853916168, 0.09475453197956085, 0.24663090705871582, 0.31489357352256775, -0.3587854504585266, 0.09580744057893753]
  WEIGHT: [1536, 576] | μ=0.000019

191_model.layers.15.mlp.down_proj: model.layers.15.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000566 σ=0.050496
    First 10: [-0.05316904932260513, -0.03417505323886871, 0.0017530597979202867, -0.01702793501317501, 0.026416096836328506, -0.003906304016709328, 0.00011878873920068145, 0.0075758760794997215, 0.12122586369514465, -0.01113838516175747]
    Last 10:  [0.006280785426497459, 0.07937464118003845, -0.1103944331407547, -0.038196343928575516, 0.0005982826114632189, -0.008322678506374359, -0.04952226206660271, 0.12512731552124023, -0.047865692526102066, -0.0026975744403898716]
  OUT[0]: [1, 5, 576] | μ=0.001018 σ=0.078609
    First 10: [-0.020521845668554306, -0.02138121984899044, -0.06537812203168869, 0.0317850336432457, 0.025577450171113014, -0.05370941385626793, -0.05586445704102516, -0.002750442363321781, -0.054448485374450684, 0.023503364995121956]
    Last 10:  [-0.07972723245620728, 0.07079939544200897, 0.145757257938385, -0.3011530935764313, -0.04218877851963043, -0.026874978095293045, 0.08341453969478607, 0.06711170822381973, -0.01482907310128212, -0.05398576334118843]
  WEIGHT: [576, 1536] | μ=0.000065

192_model.layers.15.mlp: model.layers.15.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.001926 σ=0.326463
    First 10: [-0.012380925938487053, -0.04507515951991081, -0.0054098255932331085, 0.009964982978999615, -0.21999113261699677, -0.10408555716276169, 0.08379334956407547, -0.0014356585452333093, -0.012666827067732811, -0.031389087438583374]
    Last 10:  [0.36627331376075745, -0.3301071226596832, 0.2784343361854553, -0.12239442765712738, 0.10361599177122116, -0.1602656990289688, 0.0012790595646947622, -0.14542292058467865, -0.3334307372570038, 0.007163845002651215]
  OUT[0]: [1, 5, 576] | μ=0.001018 σ=0.078609
    First 10: [-0.020521845668554306, -0.02138121984899044, -0.06537812203168869, 0.0317850336432457, 0.025577450171113014, -0.05370941385626793, -0.05586445704102516, -0.002750442363321781, -0.054448485374450684, 0.023503364995121956]
    Last 10:  [-0.07972723245620728, 0.07079939544200897, 0.145757257938385, -0.3011530935764313, -0.04218877851963043, -0.026874978095293045, 0.08341453969478607, 0.06711170822381973, -0.01482907310128212, -0.05398576334118843]

193_model.layers.16.input_layernorm: model.layers.16.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080217 σ=11.980221
    First 10: [-0.6417908072471619, -2.6828877925872803, -0.31803447008132935, 0.4927390217781067, -11.801976203918457, -7.497878551483154, 5.031275272369385, -0.07257655262947083, -0.6015380024909973, -1.982690453529358]
    Last 10:  [0.17703041434288025, -0.1543073058128357, 0.38126781582832336, -0.40357255935668945, 0.03310033679008484, -0.14840209484100342, 0.08432959765195847, -0.04988856613636017, -0.24739280343055725, -0.04850374907255173]
  OUT[0]: [1, 5, 576] | μ=-0.006325 σ=0.362131
    First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
    Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
  WEIGHT: [576] | μ=0.461956

194_model.layers.16.self_attn.q_proj: model.layers.16.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006325 σ=0.362131
    First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
    Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
  OUT[0]: [1, 5, 576] | μ=-0.074436 σ=1.227069
    First 10: [-0.3350387215614319, 0.21581757068634033, 0.003996551036834717, 0.24866832792758942, -0.48115384578704834, -0.4574110507965088, -0.408858060836792, -0.1826893389225006, 0.8016403317451477, 0.014042600989341736]
    Last 10:  [0.8217407464981079, 0.1928538680076599, -0.6873186230659485, 0.4910053014755249, -0.9956344366073608, 0.15623699128627777, -0.7574701309204102, -2.625814914703369, 0.5928845405578613, 2.3561651706695557]
  WEIGHT: [576, 576] | μ=0.000035

195_model.layers.16.self_attn.k_proj: model.layers.16.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006325 σ=0.362131
    First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
    Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
  OUT[0]: [1, 5, 192] | μ=0.009940 σ=1.507380
    First 10: [-0.005198568105697632, 0.00022322498261928558, 0.015129856765270233, -0.0038931481540203094, 0.0008921083062887192, 0.015013113617897034, -0.013862546533346176, -0.0013660825788974762, -0.0019885972142219543, 0.002210184931755066]
    Last 10:  [-3.592010259628296, -1.5219674110412598, 9.552899360656738, 0.5880433917045593, 0.45306408405303955, -0.021950215101242065, -1.1067618131637573, 1.9469947814941406, 0.6273645162582397, -2.558842658996582]
  WEIGHT: [192, 576] | μ=-0.000381

196_model.layers.16.self_attn.v_proj: model.layers.16.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006325 σ=0.362131
    First 10: [-0.013985500670969486, -0.08108824491500854, -0.007779278326779604, 0.013901513069868088, -0.28274717926979065, -0.3062819540500641, 0.14064852893352509, -0.002186084631830454, -0.016443593427538872, -0.03804132714867592]
    Last 10:  [0.2659616768360138, -0.24194200336933136, 0.438348650932312, -0.5463234782218933, 0.04355446621775627, -0.21127451956272125, 0.13701477646827698, -0.07582245767116547, -0.3727521002292633, -0.07463657855987549]
  OUT[0]: [1, 5, 192] | μ=0.003748 σ=0.188717
    First 10: [-0.07723812013864517, 0.014440563507378101, -0.011758588254451752, 0.016981225460767746, 0.002741817384958267, 0.0114127891138196, -0.0004495615139603615, 0.0037898439913988113, 0.005003102123737335, 0.0005356031470000744]
    Last 10:  [0.49229294061660767, -0.1582852005958557, 0.014253538101911545, -0.08009505271911621, 0.2842690646648407, -0.2741151452064514, -0.04489077627658844, 0.08477137237787247, 0.018200833350419998, 0.003392346203327179]
  WEIGHT: [192, 576] | μ=0.000088

197_model.layers.16.self_attn.o_proj: model.layers.16.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.002060 σ=0.046003
    First 10: [-0.07723812013864517, 0.014440563507378101, -0.011758588254451752, 0.016981225460767746, 0.002741817384958267, 0.0114127891138196, -0.0004495615139603615, 0.0037898439913988113, 0.005003102123737335, 0.0005356031470000744]
    Last 10:  [-0.23123101890087128, -0.01605270244181156, -0.0009162032511085272, 0.0184547770768404, 0.0005499367835000157, -0.011951816268265247, -0.006968324538320303, -0.005955745466053486, -0.002505909651517868, -0.014335423707962036]
  OUT[0]: [1, 5, 576] | μ=-0.000202 σ=0.036923
    First 10: [0.005855962168425322, -0.07277625054121017, 0.019377999007701874, 0.006385022774338722, -0.023855526000261307, 0.041514746844768524, -0.02654946781694889, 0.018934648483991623, -0.015052136033773422, -0.008229364641010761]
    Last 10:  [0.026641204953193665, -0.053711824119091034, 0.003905202727764845, 0.01146527100354433, 0.010883665643632412, 0.01851741038262844, -0.016230152919888496, 0.009730711579322815, 0.04558057337999344, 0.0014970204792916775]
  WEIGHT: [576, 576] | μ=0.000007

198_model.layers.16.self_attn: model.layers.16.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000202 σ=0.036923
    First 10: [0.005855962168425322, -0.07277625054121017, 0.019377999007701874, 0.006385022774338722, -0.023855526000261307, 0.041514746844768524, -0.02654946781694889, 0.018934648483991623, -0.015052136033773422, -0.008229364641010761]
    Last 10:  [0.026641204953193665, -0.053711824119091034, 0.003905202727764845, 0.01146527100354433, 0.010883665643632412, 0.01851741038262844, -0.016230152919888496, 0.009730711579322815, 0.04558057337999344, 0.0014970204792916775]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.267624
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49966737627983093, 0.5003325939178467, 0.0, 0.0, 0.0]
    Last 10:  [0.46071377396583557, 0.46109503507614136, 0.03419998660683632, 0.04399118572473526, 0.0, 0.4800853431224823, 0.4804545044898987, 0.008153203874826431, 0.011466206051409245, 0.01984088122844696]

199_model.layers.16.post_attention_layernorm: model.layers.16.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.080420 σ=11.982368
    First 10: [-0.6359348297119141, -2.755664110183716, -0.2986564636230469, 0.49912405014038086, -11.825831413269043, -7.456363677978516, 5.004725933074951, -0.0536419041454792, -0.616590142250061, -1.990919828414917]
    Last 10:  [0.2036716192960739, -0.20801913738250732, 0.3851730227470398, -0.3921072781085968, 0.043984003365039825, -0.12988469004631042, 0.06809944659471512, -0.040157854557037354, -0.2018122375011444, -0.04700672999024391]
  OUT[0]: [1, 5, 576] | μ=-0.000107 σ=0.335097
    First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
    Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
  WEIGHT: [576] | μ=0.400170

200_model.layers.16.mlp.gate_proj: model.layers.16.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000107 σ=0.335097
    First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
    Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
  OUT[0]: [1, 5, 1536] | μ=-0.133502 σ=0.391416
    First 10: [0.16629043221473694, -0.0012838691473007202, 0.09348772466182709, -0.3883487582206726, 0.23453070223331451, -0.24834810197353363, -0.06198611482977867, -0.03591781109571457, -0.0891311913728714, -0.06520441919565201]
    Last 10:  [-0.397527277469635, -0.3585626482963562, -0.15717285871505737, -1.1200917959213257, -0.12538540363311768, -0.2032448947429657, 0.1424315720796585, -0.4279419779777527, 0.05212010443210602, -0.31669309735298157]
  WEIGHT: [1536, 576] | μ=0.000108

201_model.layers.16.mlp.act_fn: model.layers.16.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.133502 σ=0.391416
    First 10: [0.16629043221473694, -0.0012838691473007202, 0.09348772466182709, -0.3883487582206726, 0.23453070223331451, -0.24834810197353363, -0.06198611482977867, -0.03591781109571457, -0.0891311913728714, -0.06520441919565201]
    Last 10:  [-0.397527277469635, -0.3585626482963562, -0.15717285871505737, -1.1200917959213257, -0.12538540363311768, -0.2032448947429657, 0.1424315720796585, -0.4279419779777527, 0.05212010443210602, -0.31669309735298157]
  OUT[0]: [1, 5, 1536] | μ=-0.026482 σ=0.173021
    First 10: [0.09004245698451996, -0.0006415225216187537, 0.04892726242542267, -0.15693749487400055, 0.1309538334608078, -0.10883361846208572, -0.030032793059945107, -0.017636418342590332, -0.04258081689476967, -0.0315396822988987]
    Last 10:  [-0.15976883471012115, -0.14747953414916992, -0.07242327928543091, -0.2755361497402191, -0.058767467737197876, -0.09133072942495346, 0.076278917491436, -0.16887354850769043, 0.026739023625850677, -0.13348039984703064]

202_model.layers.16.mlp.up_proj: model.layers.16.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000107 σ=0.335097
    First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
    Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
  OUT[0]: [1, 5, 1536] | μ=-0.000451 σ=0.283919
    First 10: [-0.2658904790878296, 0.24787864089012146, -0.09355218708515167, 0.03402865678071976, -0.027910618111491203, 0.11032726615667343, 0.01559099368751049, -0.20008787512779236, 0.22914975881576538, -0.10552684962749481]
    Last 10:  [0.19494220614433289, 0.03199974074959755, 0.03491540253162384, -0.025975383818149567, -0.044475894421339035, 0.002219259738922119, 0.48689377307891846, -0.040170423686504364, 0.14113067090511322, 0.4499254524707794]
  WEIGHT: [1536, 576] | μ=-0.000005

203_model.layers.16.mlp.down_proj: model.layers.16.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000460 σ=0.054024
    First 10: [-0.02394143119454384, -0.00015901973529253155, -0.004577252548187971, -0.005340372212231159, -0.0036550024524331093, -0.012007315643131733, -0.0004682410799432546, 0.0035288333892822266, -0.009757383726537228, 0.003328283317387104]
    Last 10:  [-0.031145690008997917, -0.004719306714832783, -0.0025286879390478134, 0.007157157175242901, 0.002613735618069768, -0.0002026866131927818, 0.03713972866535187, 0.006783721968531609, 0.003773696254938841, -0.06005622819066048]
  OUT[0]: [1, 5, 576] | μ=-0.003232 σ=0.085038
    First 10: [-0.027936656028032303, 0.046456508338451385, 0.031772419810295105, 0.02008354663848877, 0.029768014326691628, 0.016100645065307617, -0.06484322249889374, 0.06986074149608612, 0.004992889240384102, -0.05312095582485199]
    Last 10:  [-0.12098050117492676, 0.014772661030292511, 0.0014150813221931458, -0.12441963702440262, 0.006079589948058128, 0.15603815019130707, -0.1390717774629593, 0.08101731538772583, 0.023632189258933067, -0.13965603709220886]
  WEIGHT: [576, 1536] | μ=-0.000038

204_model.layers.16.mlp: model.layers.16.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.000107 σ=0.335097
    First 10: [-0.0128799332305789, -0.05041194707155228, -0.006464685779064894, 0.011196507140994072, -0.22914767265319824, -0.10881739109754562, 0.0876203328371048, -0.001119633438065648, -0.014483374543488026, -0.032546017318964005]
    Last 10:  [0.26996076107025146, -0.27260690927505493, 0.41244015097618103, -0.4374879002571106, 0.054596953094005585, -0.1621512919664383, 0.09036098420619965, -0.04623793438076973, -0.26015374064445496, -0.056236397475004196]
  OUT[0]: [1, 5, 576] | μ=-0.003232 σ=0.085038
    First 10: [-0.027936656028032303, 0.046456508338451385, 0.031772419810295105, 0.02008354663848877, 0.029768014326691628, 0.016100645065307617, -0.06484322249889374, 0.06986074149608612, 0.004992889240384102, -0.05312095582485199]
    Last 10:  [-0.12098050117492676, 0.014772661030292511, 0.0014150813221931458, -0.12441963702440262, 0.006079589948058128, 0.15603815019130707, -0.1390717774629593, 0.08101731538772583, 0.023632189258933067, -0.13965603709220886]

205_model.layers.17.input_layernorm: model.layers.17.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.083651 σ=11.991622
    First 10: [-0.6638714671134949, -2.709207534790039, -0.2668840289115906, 0.5192075967788696, -11.796063423156738, -7.440262794494629, 4.939882755279541, 0.01621883735060692, -0.6115972399711609, -2.0440406799316406]
    Last 10:  [0.08269111812114716, -0.1932464838027954, 0.38658809661865234, -0.5165269374847412, 0.0500635951757431, 0.026153460144996643, -0.07097233086824417, 0.04085946083068848, -0.1781800538301468, -0.18666276335716248]
  OUT[0]: [1, 5, 576] | μ=-0.010791 σ=0.399692
    First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
    Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
  WEIGHT: [576] | μ=0.531379

206_model.layers.17.self_attn.q_proj: model.layers.17.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.010791 σ=0.399692
    First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
    Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
  OUT[0]: [1, 5, 576] | μ=-0.059662 σ=1.029027
    First 10: [0.17620018124580383, -0.005415042862296104, 0.49547407031059265, -0.017877716571092606, -0.32422691583633423, 0.18870772421360016, 0.1761510968208313, -0.13226057589054108, 0.038664113730192184, -0.301192969083786]
    Last 10:  [-3.7760860919952393, -0.6261692047119141, -0.1570054590702057, -0.7031912803649902, 2.3285138607025146, 2.1023638248443604, -1.1395542621612549, -1.1463192701339722, -1.2893290519714355, -2.6063854694366455]
  WEIGHT: [576, 576] | μ=0.000077

207_model.layers.17.self_attn.k_proj: model.layers.17.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.010791 σ=0.399692
    First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
    Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
  OUT[0]: [1, 5, 192] | μ=0.050232 σ=1.297941
    First 10: [0.009398944675922394, 0.014877453446388245, -0.004718877375125885, -0.010775990784168243, 0.016413161531090736, 0.004908842965960503, -6.683915853500366e-05, 0.011351363733410835, -0.004613161087036133, 0.004563651978969574]
    Last 10:  [-1.0040006637573242, -1.1255080699920654, -1.067479133605957, 4.940567493438721, 0.9456954002380371, -0.722148060798645, -1.3144100904464722, 1.9216970205307007, 2.6383395195007324, 0.9680902361869812]
  WEIGHT: [192, 576] | μ=0.000040

208_model.layers.17.self_attn.v_proj: model.layers.17.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.010791 σ=0.399692
    First 10: [-0.017566876485943794, -0.07852987200021744, -0.00843737181276083, 0.017096687108278275, -0.3197370171546936, -0.3199518918991089, 0.1554076373577118, 0.0005453436751849949, -0.019083136692643166, -0.044318363070487976]
    Last 10:  [0.12999695539474487, -0.300735205411911, 0.5048385858535767, -0.6953357458114624, 0.07003987580537796, 0.041115302592515945, -0.10998018831014633, 0.06461214274168015, -0.27752381563186646, -0.3227934241294861]
  OUT[0]: [1, 5, 192] | μ=0.007038 σ=0.230105
    First 10: [0.00912422128021717, 0.004715672228485346, 0.015031802468001842, 0.0020664026960730553, 0.004894088953733444, -0.006862367503345013, -0.007586793974041939, 0.008753946051001549, 0.0024590101093053818, 0.008908390067517757]
    Last 10:  [-0.21751290559768677, 0.25524118542671204, 0.692430853843689, 0.20677316188812256, -0.2521459460258484, 0.2303818166255951, 0.11270415782928467, 0.24939972162246704, -0.09557916224002838, -0.2958414852619171]
  WEIGHT: [192, 576] | μ=0.000092

209_model.layers.17.self_attn.o_proj: model.layers.17.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.004303 σ=0.047361
    First 10: [0.00912422128021717, 0.004715672228485346, 0.015031802468001842, 0.0020664026960730553, 0.004894088953733444, -0.006862367503345013, -0.007586793974041939, 0.008753946051001549, 0.0024590101093053818, 0.008908390067517757]
    Last 10:  [-0.034934308379888535, 0.023565512150526047, 0.02834794484078884, -0.0021806051954627037, -0.019562136381864548, 0.02060643769800663, -0.032415006309747696, 0.012397365644574165, -0.0006528444355353713, 0.024543169885873795]
  OUT[0]: [1, 5, 576] | μ=0.001037 σ=0.031296
    First 10: [0.024034274742007256, 0.009686790406703949, -0.011641263030469418, 0.03035816177725792, -0.055995676666498184, -0.005811348557472229, 0.001816493459045887, -0.00855021458119154, 0.008600113913416862, -0.03423628583550453]
    Last 10:  [0.021419771015644073, -0.01344669796526432, 0.037874288856983185, -0.006192639470100403, 0.05086163058876991, 0.03190179541707039, -0.006532394327223301, 0.048124026507139206, 0.008581060916185379, 0.011034717783331871]
  WEIGHT: [576, 576] | μ=-0.000028

210_model.layers.17.self_attn: model.layers.17.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.001037 σ=0.031296
    First 10: [0.024034274742007256, 0.009686790406703949, -0.011641263030469418, 0.03035816177725792, -0.055995676666498184, -0.005811348557472229, 0.001816493459045887, -0.00855021458119154, 0.008600113913416862, -0.03423628583550453]
    Last 10:  [0.021419771015644073, -0.01344669796526432, 0.037874288856983185, -0.006192639470100403, 0.05086163058876991, 0.03190179541707039, -0.006532394327223301, 0.048124026507139206, 0.008581060916185379, 0.011034717783331871]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.273551
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4997972249984741, 0.5002027750015259, 0.0, 0.0, 0.0]
    Last 10:  [0.4557938575744629, 0.45536404848098755, 0.050707828253507614, 0.038134243339300156, 0.0, 0.4610772132873535, 0.46151381731033325, 0.02490229345858097, 0.026277411729097366, 0.026229215785861015]

211_model.layers.17.post_attention_layernorm: model.layers.17.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.082614 σ=11.993440
    First 10: [-0.6398372054100037, -2.6995208263397217, -0.27852529287338257, 0.5495657324790955, -11.852059364318848, -7.44607400894165, 4.941699028015137, 0.007668622769415379, -0.6029971241950989, -2.0782768726348877]
    Last 10:  [0.10411088913679123, -0.20669318735599518, 0.42446237802505493, -0.5227195620536804, 0.10092522203922272, 0.05805525556206703, -0.0775047242641449, 0.08898349106311798, -0.1695989966392517, -0.17562805116176605]
  OUT[0]: [1, 5, 576] | μ=-0.000918 σ=0.344057
    First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
    Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
  WEIGHT: [576] | μ=0.406987

212_model.layers.17.mlp.gate_proj: model.layers.17.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000918 σ=0.344057
    First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
    Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
  OUT[0]: [1, 5, 1536] | μ=-0.128233 σ=0.383859
    First 10: [0.21205765008926392, -0.26580917835235596, -0.21272900700569153, -0.5500901341438293, -0.5428280830383301, 0.2676239609718323, -1.086139440536499, 0.16430847346782684, -0.12101077288389206, -0.6859495639801025]
    Last 10:  [-0.37082260847091675, -0.09694718569517136, 0.2986826002597809, 0.460945725440979, -0.17704330384731293, -1.3766473531723022, 0.519015908241272, 0.4678449332714081, -0.2165176421403885, -0.12170661985874176]
  WEIGHT: [1536, 576] | μ=0.000151

213_model.layers.17.mlp.act_fn: model.layers.17.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.128233 σ=0.383859
    First 10: [0.21205765008926392, -0.26580917835235596, -0.21272900700569153, -0.5500901341438293, -0.5428280830383301, 0.2676239609718323, -1.086139440536499, 0.16430847346782684, -0.12101077288389206, -0.6859495639801025]
    Last 10:  [-0.37082260847091675, -0.09694718569517136, 0.2986826002597809, 0.460945725440979, -0.17704330384731293, -1.3766473531723022, 0.519015908241272, 0.4678449332714081, -0.2165176421403885, -0.12170661985874176]
  OUT[0]: [1, 5, 1536] | μ=-0.025368 σ=0.172134
    First 10: [0.11722899973392487, -0.11534423381090164, -0.09509357064962387, -0.20124688744544983, -0.19950558245182037, 0.15161150693893433, -0.27408286929130554, 0.08888841420412064, -0.05684894695878029, -0.22974832355976105]
    Last 10:  [-0.15142254531383514, -0.04612573981285095, 0.17147979140281677, 0.28266966342926025, -0.08070597797632217, -0.277460515499115, 0.32538023591041565, 0.2876654863357544, -0.09658442437648773, -0.05715474858880043]

214_model.layers.17.mlp.up_proj: model.layers.17.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000918 σ=0.344057
    First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
    Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
  OUT[0]: [1, 5, 1536] | μ=0.006167 σ=0.289655
    First 10: [0.2331659495830536, -0.20070266723632812, 0.05258927494287491, -0.16163861751556396, -0.36914438009262085, -0.14993810653686523, -0.19461660087108612, -0.11860565841197968, 0.4599470794200897, 0.013090181164443493]
    Last 10:  [0.10169914364814758, 0.43642550706863403, -0.13328209519386292, -0.4647698700428009, -0.46214526891708374, 0.0015610717236995697, -0.42340147495269775, -0.6607855558395386, 0.028821490705013275, -0.2917616069316864]
  WEIGHT: [1536, 576] | μ=0.000002

215_model.layers.17.mlp.down_proj: model.layers.17.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.001624 σ=0.048779
    First 10: [0.027333810925483704, 0.023149894550442696, -0.005000901874154806, 0.03252926841378212, 0.07364636659622192, -0.022732341662049294, 0.05334107577800751, -0.010542669333517551, -0.0261475071310997, -0.003007447114214301]
    Last 10:  [-0.015399543568491936, -0.020130449905991554, -0.022855184972286224, -0.13137634098529816, 0.03729788586497307, -0.0004331357777118683, -0.13776646554470062, -0.19008520245552063, -0.002783707110211253, 0.016675561666488647]
  OUT[0]: [1, 5, 576] | μ=0.000992 σ=0.078279
    First 10: [-0.026727400720119476, 0.04562339559197426, -0.049648717045784, 0.03508767485618591, -0.018014345318078995, 0.0794500932097435, -0.08577456325292587, -0.07889021933078766, -0.021168801933526993, -0.014344832859933376]
    Last 10:  [0.24368222057819366, -0.011579791083931923, 0.223648801445961, -0.06548430025577545, 0.14418953657150269, 0.01916997879743576, 0.13177256286144257, 0.01455237902700901, 0.015568599104881287, -0.05008528381586075]
  WEIGHT: [576, 1536] | μ=-0.000000

216_model.layers.17.mlp: model.layers.17.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.000918 σ=0.344057
    First 10: [-0.01375545933842659, -0.05205443874001503, -0.005912499967962503, 0.012331533245742321, -0.2682346701622009, -0.1104915663599968, 0.09280750900506973, 0.00015755325148347765, -0.013592601753771305, -0.03782638907432556]
    Last 10:  [0.12650203704833984, -0.23278649151325226, 0.4455719292163849, -0.536175549030304, 0.11301181465387344, 0.06274930387735367, -0.09432443231344223, 0.09565898030996323, -0.19562798738479614, -0.18789255619049072]
  OUT[0]: [1, 5, 576] | μ=0.000992 σ=0.078279
    First 10: [-0.026727400720119476, 0.04562339559197426, -0.049648717045784, 0.03508767485618591, -0.018014345318078995, 0.0794500932097435, -0.08577456325292587, -0.07889021933078766, -0.021168801933526993, -0.014344832859933376]
    Last 10:  [0.24368222057819366, -0.011579791083931923, 0.223648801445961, -0.06548430025577545, 0.14418953657150269, 0.01916997879743576, 0.13177256286144257, 0.01455237902700901, 0.015568599104881287, -0.05008528381586075]

217_model.layers.18.input_layernorm: model.layers.18.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.081622 σ=12.001882
    First 10: [-0.6665645837783813, -2.653897523880005, -0.32817399501800537, 0.584653377532959, -11.870073318481445, -7.366623878479004, 4.855924606323242, -0.07122159749269485, -0.624165952205658, -2.0926218032836914]
    Last 10:  [0.3477931022644043, -0.21827298402786255, 0.6481111645698547, -0.5882038474082947, 0.2451147586107254, 0.07722523808479309, 0.05426783859729767, 0.10353586822748184, -0.15403039753437042, -0.2257133424282074]
  OUT[0]: [1, 5, 576] | μ=-0.004963 σ=0.364190
    First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
    Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
  WEIGHT: [576] | μ=0.496113

218_model.layers.18.self_attn.q_proj: model.layers.18.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004963 σ=0.364190
    First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
    Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
  OUT[0]: [1, 5, 576] | μ=0.042951 σ=1.120729
    First 10: [-0.8377794623374939, 0.8824523091316223, 0.2115900218486786, 0.32088714838027954, 0.35283565521240234, -0.4460007846355438, 0.3452777862548828, 0.3937453627586365, 0.15842032432556152, -0.05418047308921814]
    Last 10:  [-0.16999435424804688, 0.5271382927894592, 1.0528688430786133, -0.14003366231918335, 0.0482865646481514, 0.3681071996688843, -0.0046808719635009766, -1.1384668350219727, 2.352933883666992, 2.8094520568847656]
  WEIGHT: [576, 576] | μ=0.000061

219_model.layers.18.self_attn.k_proj: model.layers.18.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004963 σ=0.364190
    First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
    Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
  OUT[0]: [1, 5, 192] | μ=-0.080262 σ=1.316830
    First 10: [0.005962073802947998, 0.0027903690934181213, 0.0018459558486938477, -0.0059988126158714294, 0.009695593267679214, -0.011615023016929626, 0.007661469280719757, -0.0024742260575294495, -0.0025738850235939026, 0.01688353717327118]
    Last 10:  [0.7259094715118408, -8.153075218200684, 3.7973084449768066, 1.25607430934906, 0.051473021507263184, -2.106125593185425, -0.4597930908203125, 0.7789705991744995, -2.2604992389678955, -1.1115267276763916]
  WEIGHT: [192, 576] | μ=-0.000312

220_model.layers.18.self_attn.v_proj: model.layers.18.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004963 σ=0.364190
    First 10: [-0.016491353511810303, -0.07672812044620514, -0.009099354036152363, 0.016195762902498245, -0.2475307136774063, -0.26835867762565613, 0.1395168900489807, -0.0022938218899071217, -0.01809377409517765, -0.04746323451399803]
    Last 10:  [0.44091448187828064, -0.3205752372741699, 0.7024609446525574, -0.6944777965545654, 0.2910427153110504, 0.09479872137308121, 0.07606774568557739, 0.14147064089775085, -0.19733533263206482, -0.3089628219604492]
  OUT[0]: [1, 5, 192] | μ=-0.026012 σ=0.231529
    First 10: [-0.025765325874090195, -0.32301434874534607, 0.01865028589963913, -0.007378639653325081, -0.006604405120015144, -0.011182085610926151, 0.014351952821016312, -0.005257573910057545, -0.009171336889266968, 0.010653816163539886]
    Last 10:  [0.3005211055278778, -0.5478452444076538, 0.24131830036640167, 0.17114663124084473, -0.4464709162712097, 0.22537609934806824, -0.0653851181268692, 0.39671340584754944, 0.1975804567337036, -0.13870105147361755]
  WEIGHT: [192, 576] | μ=0.000073

221_model.layers.18.self_attn.o_proj: model.layers.18.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003647 σ=0.044973
    First 10: [-0.025765325874090195, -0.32301434874534607, 0.01865028589963913, -0.007378639653325081, -0.006604405120015144, -0.011182085610926151, 0.014351952821016312, -0.005257573910057545, -0.009171336889266968, 0.010653816163539886]
    Last 10:  [0.009222244843840599, -0.014640099368989468, 0.01489559281617403, 0.007551583927124739, -0.020604563876986504, -0.0031008003279566765, -0.0035804761573672295, 0.010456638410687447, 0.017065126448869705, -0.0016377160791307688]
  OUT[0]: [1, 5, 576] | μ=-0.002427 σ=0.044122
    First 10: [0.058850452303886414, -0.02426319569349289, -0.0020857914350926876, -0.04168229550123215, 0.05155494809150696, 0.08311007916927338, -0.0810142531991005, -0.017859669402241707, -0.0020665014162659645, 0.014075408689677715]
    Last 10:  [-0.006787922233343124, -0.03319380059838295, -0.006946871988475323, 0.023417925462126732, 0.0062087830156087875, -0.003955204039812088, -0.0059850080870091915, -0.050419051200151443, -0.04298160970211029, -0.000788138248026371]
  WEIGHT: [576, 576] | μ=-0.000025

222_model.layers.18.self_attn: model.layers.18.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.002427 σ=0.044122
    First 10: [0.058850452303886414, -0.02426319569349289, -0.0020857914350926876, -0.04168229550123215, 0.05155494809150696, 0.08311007916927338, -0.0810142531991005, -0.017859669402241707, -0.0020665014162659645, 0.014075408689677715]
    Last 10:  [-0.006787922233343124, -0.03319380059838295, -0.006946871988475323, 0.023417925462126732, 0.0062087830156087875, -0.003955204039812088, -0.0059850080870091915, -0.050419051200151443, -0.04298160970211029, -0.000788138248026371]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.272157
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001047849655151, 0.4998951852321625, 0.0, 0.0, 0.0]
    Last 10:  [0.48647257685661316, 0.48779740929603577, 0.013902934268116951, 0.0118270767852664, 0.0, 0.4796624481678009, 0.4806508421897888, 0.005016630049794912, 0.009345166385173798, 0.025324858725070953]

223_model.layers.18.post_attention_layernorm: model.layers.18.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.084049 σ=12.005649
    First 10: [-0.6077141165733337, -2.6781606674194336, -0.3302597999572754, 0.5429710745811462, -11.81851863861084, -7.283514022827148, 4.7749104499816895, -0.08908126503229141, -0.6262324452400208, -2.0785462856292725]
    Last 10:  [0.3410051763057709, -0.2514667809009552, 0.6411643028259277, -0.564785897731781, 0.25132355093955994, 0.0732700377702713, 0.04828283190727234, 0.0531168170273304, -0.1970120072364807, -0.2265014797449112]
  OUT[0]: [1, 5, 576] | μ=-0.000532 σ=0.362479
    First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
    Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
  WEIGHT: [576] | μ=0.433940

224_model.layers.18.mlp.gate_proj: model.layers.18.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000532 σ=0.362479
    First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
    Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
  OUT[0]: [1, 5, 1536] | μ=-0.175681 σ=0.419672
    First 10: [-0.2780911326408386, 0.5901973247528076, 0.25485217571258545, -0.3146638572216034, -0.18665599822998047, 0.024738967418670654, -0.018943332135677338, -0.1688479483127594, 0.26470211148262024, -0.04802200570702553]
    Last 10:  [-0.07905992120504379, -0.5438293218612671, -0.624908983707428, 0.278026282787323, 0.1272514909505844, -0.1826837807893753, -0.5444859266281128, -0.22942592203617096, 0.034127384424209595, -0.27837133407592773]
  WEIGHT: [1536, 576] | μ=0.000243

225_model.layers.18.mlp.act_fn: model.layers.18.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.175681 σ=0.419672
    First 10: [-0.2780911326408386, 0.5901973247528076, 0.25485217571258545, -0.3146638572216034, -0.18665599822998047, 0.024738967418670654, -0.018943332135677338, -0.1688479483127594, 0.26470211148262024, -0.04802200570702553]
    Last 10:  [-0.07905992120504379, -0.5438293218612671, -0.624908983707428, 0.278026282787323, 0.1272514909505844, -0.1826837807893753, -0.5444859266281128, -0.22942592203617096, 0.034127384424209595, -0.27837133407592773]
  OUT[0]: [1, 5, 1536] | μ=-0.039594 σ=0.181315
    First 10: [-0.11983554065227509, 0.37973910570144653, 0.14357617497444153, -0.13278083503246307, -0.08464308828115463, 0.012522480450570583, -0.009381955489516258, -0.07731346040964127, 0.14976628124713898, -0.023434584960341454]
    Last 10:  [-0.03796815499663353, -0.1997470110654831, -0.21788440644741058, 0.158214271068573, 0.06766852736473083, -0.08302167803049088, -0.19990509748458862, -0.10161130875349045, 0.01735483482480049, -0.11993715167045593]

226_model.layers.18.mlp.up_proj: model.layers.18.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000532 σ=0.362479
    First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
    Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
  OUT[0]: [1, 5, 1536] | μ=0.000021 σ=0.303260
    First 10: [0.2583382725715637, 0.06000901758670807, 0.2526709735393524, -0.38453179597854614, -0.027783101424574852, -0.05933084338903427, -0.03775890916585922, 0.41567912697792053, -0.08919097483158112, 0.3805236518383026]
    Last 10:  [0.48384466767311096, 0.5883837938308716, 0.97945237159729, 0.028534285724163055, 0.39286166429519653, -0.2969404458999634, -0.15566077828407288, -0.09004196524620056, 0.5595129132270813, -0.23826977610588074]
  WEIGHT: [1536, 576] | μ=0.000024

227_model.layers.18.mlp.down_proj: model.layers.18.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000844 σ=0.066806
    First 10: [-0.030958106741309166, 0.02278777025640011, 0.03627753257751465, 0.05105845257639885, 0.002351647475734353, -0.000742969335988164, 0.0003542524063959718, -0.032137591391801834, -0.013357800431549549, -0.008917413651943207]
    Last 10:  [-0.01837068982422352, -0.11752790212631226, -0.21340739727020264, 0.00451453123241663, 0.02658437006175518, 0.024652494117617607, 0.031117383390665054, 0.009149282239377499, 0.009710254147648811, 0.028577398508787155]
  OUT[0]: [1, 5, 576] | μ=0.000535 σ=0.097557
    First 10: [-0.019346637651324272, 0.05246886983513832, -0.018062008544802666, -0.010608246549963951, 0.033735714852809906, 0.06311169266700745, -0.09063869714736938, 0.05476351082324982, -0.028151072561740875, 0.03922653570771217]
    Last 10:  [-0.13474345207214355, 0.1326989233493805, -0.031219426542520523, -0.032321397215127945, -0.06690109521150589, 0.006207451224327087, -0.21834003925323486, 0.19003926217556, 0.23019525408744812, -0.05090992525219917]
  WEIGHT: [576, 1536] | μ=0.000052

228_model.layers.18.mlp: model.layers.18.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.000532 σ=0.362479
    First 10: [-0.014006655663251877, -0.05621214210987091, -0.007531109731644392, 0.012479514814913273, -0.2765008509159088, -0.11772538721561432, 0.09831612557172775, -0.00191215006634593, -0.015376358292996883, -0.04338604956865311]
    Last 10:  [0.3785823583602905, -0.2828429341316223, 0.6467670798301697, -0.5621469020843506, 0.2741823196411133, 0.07835347205400467, 0.05501102656126022, 0.05469595640897751, -0.22618868947029114, -0.22372625768184662]
  OUT[0]: [1, 5, 576] | μ=0.000535 σ=0.097557
    First 10: [-0.019346637651324272, 0.05246886983513832, -0.018062008544802666, -0.010608246549963951, 0.033735714852809906, 0.06311169266700745, -0.09063869714736938, 0.05476351082324982, -0.028151072561740875, 0.03922653570771217]
    Last 10:  [-0.13474345207214355, 0.1326989233493805, -0.031219426542520523, -0.032321397215127945, -0.06690109521150589, 0.006207451224327087, -0.21834003925323486, 0.19003926217556, 0.23019525408744812, -0.05090992525219917]

229_model.layers.19.input_layernorm: model.layers.19.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.083514 σ=12.008832
    First 10: [-0.6270607709884644, -2.6256918907165527, -0.348321795463562, 0.5323628187179565, -11.784783363342285, -7.220402240753174, 4.684271812438965, -0.034317754209041595, -0.6543835401535034, -2.0393197536468506]
    Last 10:  [0.20626172423362732, -0.11876785755157471, 0.6099448800086975, -0.5971072912216187, 0.18442246317863464, 0.07947748899459839, -0.17005720734596252, 0.2431560754776001, 0.03318324685096741, -0.27741140127182007]
  OUT[0]: [1, 5, 576] | μ=-0.007799 σ=0.419179
    First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
    Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
  WEIGHT: [576] | μ=0.526974

230_model.layers.19.self_attn.q_proj: model.layers.19.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007799 σ=0.419179
    First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
    Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
  OUT[0]: [1, 5, 576] | μ=0.013251 σ=1.160804
    First 10: [0.5734501481056213, 0.25922608375549316, 0.2998431324958801, -0.1883755326271057, -0.5414149165153503, 0.14954331517219543, 0.20416440069675446, 0.0311285313218832, 0.1654435396194458, -0.5489588379859924]
    Last 10:  [0.9710652232170105, -4.291116237640381, -1.591453194618225, 0.5787240862846375, 0.9696335196495056, 3.266785144805908, 5.116537094116211, -2.0767135620117188, -5.148368835449219, -2.04774808883667]
  WEIGHT: [576, 576] | μ=0.000013

231_model.layers.19.self_attn.k_proj: model.layers.19.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007799 σ=0.419179
    First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
    Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
  OUT[0]: [1, 5, 192] | μ=0.023870 σ=1.529393
    First 10: [-0.0066927894949913025, 0.003967344760894775, 0.010434471070766449, -0.013717111200094223, -0.013788595795631409, -0.015797004103660583, -0.015063539147377014, -0.03939242660999298, 0.0008821636438369751, 0.01291651837527752]
    Last 10:  [0.4805268347263336, 0.25657936930656433, -0.919264554977417, 1.3373773097991943, 1.3127280473709106, 1.02220618724823, -1.0766745805740356, 0.6707425713539124, 1.4604045152664185, 0.6491736769676208]
  WEIGHT: [192, 576] | μ=0.000294

232_model.layers.19.self_attn.v_proj: model.layers.19.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007799 σ=0.419179
    First 10: [-0.02042810432612896, -0.08351163566112518, -0.010146396234631538, 0.016575926914811134, -0.29233673214912415, -0.2861323058605194, 0.18129034340381622, -0.0012213096488267183, -0.018489258363842964, -0.05599309876561165]
    Last 10:  [0.2182561606168747, -0.164707213640213, 0.6392611265182495, -0.622319221496582, 0.1965177357196808, 0.08945827931165695, -0.21362413465976715, 0.32533156871795654, 0.0366104431450367, -0.3511325716972351]
  OUT[0]: [1, 5, 192] | μ=-0.018465 σ=0.236915
    First 10: [-0.0012876521795988083, 0.005803906358778477, -0.0031116995960474014, 0.0019115209579467773, 0.007089809514582157, -0.005595693364739418, 0.0014660404995083809, 0.0013264268636703491, -0.007427733391523361, -0.002674686722457409]
    Last 10:  [0.6521482467651367, -0.05542096495628357, -0.511698842048645, 0.13921010494232178, -0.7942172288894653, 0.1645403355360031, 0.045300811529159546, -0.18486541509628296, 0.1070793867111206, -0.09608727693557739]
  WEIGHT: [192, 576] | μ=0.000167

233_model.layers.19.self_attn.o_proj: model.layers.19.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004222 σ=0.068545
    First 10: [-0.0012876521795988083, 0.005803906358778477, -0.0031116995960474014, 0.0019115209579467773, 0.007089809514582157, -0.005595693364739418, 0.0014660404995083809, 0.0013264268636703491, -0.007427733391523361, -0.002674686722457409]
    Last 10:  [0.01340756006538868, -0.005559442564845085, -0.0501440092921257, 0.02397761307656765, 0.023086370900273323, -0.010344645008444786, -0.010802729055285454, 0.0032507518772035837, 0.0050797127187252045, 0.005924539640545845]
  OUT[0]: [1, 5, 576] | μ=-0.000955 σ=0.051300
    First 10: [0.01912875659763813, -0.044397465884685516, -0.00924941897392273, -0.07033971697092056, -0.039318837225437164, 0.022757938131690025, 0.08105392754077911, -0.005851780064404011, 0.027946799993515015, -0.01991271786391735]
    Last 10:  [0.0374181792140007, 0.05294226109981537, -0.013717522844672203, 0.014362967573106289, 0.031184270977973938, 0.04060351848602295, 0.012982919812202454, -0.0878903791308403, -0.024067485705018044, -0.03837566450238228]
  WEIGHT: [576, 576] | μ=0.000089

234_model.layers.19.self_attn: model.layers.19.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000955 σ=0.051300
    First 10: [0.01912875659763813, -0.044397465884685516, -0.00924941897392273, -0.07033971697092056, -0.039318837225437164, 0.022757938131690025, 0.08105392754077911, -0.005851780064404011, 0.027946799993515015, -0.01991271786391735]
    Last 10:  [0.0374181792140007, 0.05294226109981537, -0.013717522844672203, 0.014362967573106289, 0.031184270977973938, 0.04060351848602295, 0.012982919812202454, -0.0878903791308403, -0.024067485705018044, -0.03837566450238228]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.272694
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5002147555351257, 0.49978533387184143, 0.0, 0.0, 0.0]
    Last 10:  [0.49461960792541504, 0.4943769872188568, 0.0046204435639083385, 0.006382955238223076, 0.0, 0.48945072293281555, 0.4891625642776489, 0.005610514897853136, 0.008785570971667767, 0.0069905673153698444]

235_model.layers.19.post_attention_layernorm: model.layers.19.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.084470 σ=12.012135
    First 10: [-0.6079320311546326, -2.6700892448425293, -0.35757121443748474, 0.4620231091976166, -11.824102401733398, -7.197644233703613, 4.765325546264648, -0.04016953334212303, -0.626436710357666, -2.059232473373413]
    Last 10:  [0.24367991089820862, -0.06582559645175934, 0.5962273478507996, -0.582744300365448, 0.21560673415660858, 0.12008100748062134, -0.15707428753376007, 0.1552656888961792, 0.009115761145949364, -0.31578707695007324]
  OUT[0]: [1, 5, 576] | μ=-0.000201 σ=0.364422
    First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
    Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
  WEIGHT: [576] | μ=0.437132

236_model.layers.19.mlp.gate_proj: model.layers.19.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000201 σ=0.364422
    First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
    Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
  OUT[0]: [1, 5, 1536] | μ=-0.185284 σ=0.452420
    First 10: [-0.1248796135187149, -0.09433577209711075, 0.23153480887413025, 0.08005581796169281, 0.027399277314543724, -0.7590510845184326, 0.08980295807123184, -0.34578049182891846, -0.9712328910827637, 0.09294278919696808]
    Last 10:  [-0.005166793242096901, -0.36636078357696533, -1.0826754570007324, -1.2720487117767334, -0.5106630325317383, -0.3924185633659363, -0.9873599410057068, -0.02358730137348175, 0.4882686734199524, 0.08582982420921326]
  WEIGHT: [1536, 576] | μ=0.000195

237_model.layers.19.mlp.act_fn: model.layers.19.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.185284 σ=0.452420
    First 10: [-0.1248796135187149, -0.09433577209711075, 0.23153480887413025, 0.08005581796169281, 0.027399277314543724, -0.7590510845184326, 0.08980295807123184, -0.34578049182891846, -0.9712328910827637, 0.09294278919696808]
    Last 10:  [-0.005166793242096901, -0.36636078357696533, -1.0826754570007324, -1.2720487117767334, -0.5106630325317383, -0.3924185633659363, -0.9873599410057068, -0.02358730137348175, 0.4882686734199524, 0.08582982420921326]
  OUT[0]: [1, 5, 1536] | μ=-0.038270 σ=0.191010
    First 10: [-0.05854613706469536, -0.044944725930690765, 0.1291099488735199, 0.04162928834557533, 0.013887307606637478, -0.2420251965522766, 0.04691626876592636, -0.14329351484775543, -0.26673439145088196, 0.048629432916641235]
    Last 10:  [-0.002576722763478756, -0.14999568462371826, -0.27391692996025085, -0.2784600555896759, -0.19151809811592102, -0.15819774568080902, -0.2680029273033142, -0.0116545669734478, 0.3025793433189392, 0.0447554737329483]

238_model.layers.19.mlp.up_proj: model.layers.19.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000201 σ=0.364422
    First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
    Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
  OUT[0]: [1, 5, 1536] | μ=-0.009823 σ=0.310440
    First 10: [-0.011288153938949108, -0.206289604306221, -0.3903566896915436, 0.10192419588565826, 0.25222182273864746, -0.07980801165103912, -0.22571825981140137, -0.43774253129959106, -0.01914067752659321, 0.6913800835609436]
    Last 10:  [-0.08968715369701385, -0.05830085277557373, -0.2642518877983093, 0.04021197557449341, -0.1031121164560318, -1.0470170974731445, -0.11213459819555283, -0.08668024092912674, -0.25721248984336853, 0.1661846935749054]
  WEIGHT: [1536, 576] | μ=0.000010

239_model.layers.19.mlp.down_proj: model.layers.19.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.001336 σ=0.062591
    First 10: [0.0006608777912333608, 0.009271630086004734, -0.05039893090724945, 0.004243031609803438, 0.0035026820842176676, 0.019315550103783607, -0.010589858517050743, 0.06272566318511963, 0.005105476826429367, 0.033621422946453094]
    Last 10:  [0.00023109893663786352, 0.008744875900447369, 0.07238306850194931, -0.011197429150342941, 0.019747836515307426, 0.16563574969768524, 0.03005240112543106, 0.001010220730677247, -0.07782718539237976, 0.007437674794346094]
  OUT[0]: [1, 5, 576] | μ=-0.001719 σ=0.098107
    First 10: [-0.019145194441080093, 0.04688078910112381, -0.11684485524892807, 0.08168193697929382, 0.1020248532295227, 0.07961532473564148, -0.07758274674415588, -0.12421499192714691, 0.10597354173660278, 0.03329408913850784]
    Last 10:  [0.17151546478271484, 0.044858887791633606, -0.0059884823858737946, -0.09607034921646118, 0.15282511711120605, 0.20279699563980103, -0.21713387966156006, -0.10902567207813263, 0.02681189402937889, -0.1693846881389618]
  WEIGHT: [576, 1536] | μ=-0.000064

240_model.layers.19.mlp: model.layers.19.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.000201 σ=0.364422
    First 10: [-0.01411501131951809, -0.055605992674827576, -0.008035344071686268, 0.01059653889387846, -0.2874610424041748, -0.13397011160850525, 0.09825962036848068, -0.0008949395269155502, -0.015044249594211578, -0.04489769786596298]
    Last 10:  [0.23871587216854095, -0.06596628576517105, 0.5401610136032104, -0.5312249660491943, 0.19433946907520294, 0.11560768634080887, -0.15339238941669464, 0.14018869400024414, 0.009153896942734718, -0.2728455066680908]
  OUT[0]: [1, 5, 576] | μ=-0.001719 σ=0.098107
    First 10: [-0.019145194441080093, 0.04688078910112381, -0.11684485524892807, 0.08168193697929382, 0.1020248532295227, 0.07961532473564148, -0.07758274674415588, -0.12421499192714691, 0.10597354173660278, 0.03329408913850784]
    Last 10:  [0.17151546478271484, 0.044858887791633606, -0.0059884823858737946, -0.09607034921646118, 0.15282511711120605, 0.20279699563980103, -0.21713387966156006, -0.10902567207813263, 0.02681189402937889, -0.1693846881389618]

241_model.layers.20.input_layernorm: model.layers.20.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.086189 σ=12.014653
    First 10: [-0.6270772218704224, -2.623208522796631, -0.4744160771369934, 0.5437050461769104, -11.722077369689941, -7.1180291175842285, 4.687742710113525, -0.16438452899456024, -0.5204631686210632, -2.0259382724761963]
    Last 10:  [0.41519537568092346, -0.020966708660125732, 0.5902388691902161, -0.6788146495819092, 0.36843186616897583, 0.32287800312042236, -0.37420815229415894, 0.04624001681804657, 0.035927653312683105, -0.48517176508903503]
  OUT[0]: [1, 5, 576] | μ=-0.004985 σ=0.423120
    First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
    Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
  WEIGHT: [576] | μ=0.595183

242_model.layers.20.self_attn.q_proj: model.layers.20.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004985 σ=0.423120
    First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
    Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
  OUT[0]: [1, 5, 576] | μ=0.012340 σ=1.053530
    First 10: [0.23265299201011658, -0.31718090176582336, 0.05648844316601753, 0.37458574771881104, 0.18924075365066528, -0.056706465780735016, -0.1733427494764328, -0.042458172887563705, -0.2921498417854309, 0.0738520622253418]
    Last 10:  [-0.5507814288139343, 8.078080177307129, -0.040261536836624146, -0.5807448029518127, -0.18600089848041534, 0.8330078125, -0.8552126884460449, -0.18775293231010437, -2.929267406463623, -3.3478779792785645]
  WEIGHT: [576, 576] | μ=0.000116

243_model.layers.20.self_attn.k_proj: model.layers.20.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004985 σ=0.423120
    First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
    Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
  OUT[0]: [1, 5, 192] | μ=0.093698 σ=1.269144
    First 10: [0.0042570047080516815, 0.0051924362778663635, -0.0021750181913375854, -0.0070402175188064575, 0.002317999489605427, 0.00647111888974905, -0.005890600383281708, -0.01019047200679779, 0.007202997803688049, -0.0017686462961137295]
    Last 10:  [-1.1240390539169312, -0.44455721974372864, 2.5238137245178223, -1.7886484861373901, -0.07196877151727676, -1.8105428218841553, -1.6128242015838623, 1.778730869293213, 0.8856378197669983, 0.8938947319984436]
  WEIGHT: [192, 576] | μ=0.000064

244_model.layers.20.self_attn.v_proj: model.layers.20.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004985 σ=0.423120
    First 10: [-0.020324157550930977, -0.08933912962675095, -0.01597423292696476, 0.020992539823055267, -0.30137577652931213, -0.33891212940216064, 0.18617956340312958, -0.0066851903684437275, -0.016467060893774033, -0.06310900300741196]
    Last 10:  [0.40495988726615906, -0.027299823239445686, 0.6777445077896118, -0.7035819292068481, 0.36146092414855957, 0.3803078234195709, -0.4958193302154541, 0.055657315999269485, 0.041803233325481415, -0.552002489566803]
  OUT[0]: [1, 5, 192] | μ=-0.010529 σ=0.350973
    First 10: [-0.023633981123566628, -0.006339972838759422, 0.00045689381659030914, 0.0005622822791337967, 0.0064045824110507965, -0.011260639876127243, 0.012442640960216522, -0.005054449662566185, -0.010427902452647686, 0.004508761689066887]
    Last 10:  [0.045087650418281555, -0.5410366654396057, 0.4805414378643036, -0.19812090694904327, 0.31287962198257446, 0.25048092007637024, -0.45792415738105774, -0.2764427363872528, -0.12499526143074036, -0.4779697060585022]
  WEIGHT: [192, 576] | μ=0.000151

245_model.layers.20.self_attn.o_proj: model.layers.20.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.004341 σ=0.057772
    First 10: [-0.023633981123566628, -0.006339972838759422, 0.00045689381659030914, 0.0005622822791337967, 0.0064045824110507965, -0.011260639876127243, 0.012442640960216522, -0.005054449662566185, -0.010427902452647686, 0.004508761689066887]
    Last 10:  [-0.014603732153773308, -0.004075275268405676, 0.016180209815502167, -0.006033035460859537, 0.029000816866755486, 0.006662741303443909, -0.007959072478115559, -0.003721515880897641, 0.010445047169923782, -0.000668529886752367]
  OUT[0]: [1, 5, 576] | μ=-0.001363 σ=0.053707
    First 10: [0.012760322540998459, -0.00035351887345314026, 0.05874323099851608, -0.0026317350566387177, 0.0025421753525733948, 0.03461454436182976, -0.0007169647142291069, -0.028233088552951813, 0.007607476785778999, -0.011008314788341522]
    Last 10:  [0.001615370623767376, -0.10300357639789581, -0.0019391151145100594, -0.04226933419704437, -0.02460837922990322, 0.03137750178575516, 0.07179649919271469, 0.04638942331075668, -0.028725646436214447, -0.04564177617430687]
  WEIGHT: [576, 576] | μ=0.000067

246_model.layers.20.self_attn: model.layers.20.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.001363 σ=0.053707
    First 10: [0.012760322540998459, -0.00035351887345314026, 0.05874323099851608, -0.0026317350566387177, 0.0025421753525733948, 0.03461454436182976, -0.0007169647142291069, -0.028233088552951813, 0.007607476785778999, -0.011008314788341522]
    Last 10:  [0.001615370623767376, -0.10300357639789581, -0.0019391151145100594, -0.04226933419704437, -0.02460837922990322, 0.03137750178575516, 0.07179649919271469, 0.04638942331075668, -0.028725646436214447, -0.04564177617430687]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.272864
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.4997909367084503, 0.5002090334892273, 0.0, 0.0, 0.0]
    Last 10:  [0.49530720710754395, 0.4940468966960907, 0.003903933335095644, 0.006742018740624189, 0.0, 0.491840660572052, 0.49170583486557007, 0.0033464606385678053, 0.00603004964068532, 0.007077015936374664]

247_model.layers.20.post_attention_layernorm: model.layers.20.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.087552 σ=12.016282
    First 10: [-0.6143168807029724, -2.6235620975494385, -0.41567283868789673, 0.5410733222961426, -11.719534873962402, -7.083414554595947, 4.687025547027588, -0.19261762499809265, -0.5128557085990906, -2.0369465351104736]
    Last 10:  [0.4168107509613037, -0.12397028505802155, 0.5882997512817383, -0.7210839986801147, 0.34382349252700806, 0.3542554974555969, -0.30241164565086365, 0.09262944012880325, 0.0072020068764686584, -0.5308135151863098]
  OUT[0]: [1, 5, 576] | μ=-0.001203 σ=0.383795
    First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
    Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
  WEIGHT: [576] | μ=0.468847

248_model.layers.20.mlp.gate_proj: model.layers.20.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001203 σ=0.383795
    First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
    Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
  OUT[0]: [1, 5, 1536] | μ=-0.248543 σ=0.482246
    First 10: [-0.07429102808237076, -0.31238383054733276, -0.19426292181015015, -0.738709568977356, -0.08010382950305939, 0.8217036724090576, 0.05887015536427498, -0.26759710907936096, -0.32694217562675476, -0.5683956742286682]
    Last 10:  [-0.8872087001800537, -0.91950523853302, -0.6837807893753052, -0.6551245450973511, 0.28999125957489014, -0.4399922788143158, -0.21439611911773682, -0.3913847506046295, 0.7021210789680481, -0.26141926646232605]
  WEIGHT: [1536, 576] | μ=-0.000116

249_model.layers.20.mlp.act_fn: model.layers.20.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.248543 σ=0.482246
    First 10: [-0.07429102808237076, -0.31238383054733276, -0.19426292181015015, -0.738709568977356, -0.08010382950305939, 0.8217036724090576, 0.05887015536427498, -0.26759710907936096, -0.32694217562675476, -0.5683956742286682]
    Last 10:  [-0.8872087001800537, -0.91950523853302, -0.6837807893753052, -0.6551245450973511, 0.28999125957489014, -0.4399922788143158, -0.21439611911773682, -0.3913847506046295, 0.7021210789680481, -0.26141926646232605]
  OUT[0]: [1, 5, 1536] | μ=-0.058577 σ=0.185715
    First 10: [-0.0357663594186306, -0.13199247419834137, -0.08772649616003036, -0.23881474137306213, -0.03844861686229706, 0.5707535743713379, 0.030301252380013466, -0.11600257456302643, -0.13698381185531616, -0.2055359184741974]
    Last 10:  [-0.25878652930259705, -0.26211297512054443, -0.22935236990451813, -0.22394494712352753, 0.16587325930595398, -0.17236380279064178, -0.09575045108795166, -0.1578783392906189, 0.4694787859916687, -0.11372126638889313]

250_model.layers.20.mlp.up_proj: model.layers.20.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001203 σ=0.383795
    First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
    Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
  OUT[0]: [1, 5, 1536] | μ=-0.011350 σ=0.323297
    First 10: [0.1132466122508049, 0.004511840641498566, 0.06690139323472977, -0.05303361266851425, -0.15887556970119476, -0.6011021137237549, 0.3616740107536316, -0.27920079231262207, -0.10968785732984543, -0.3025050163269043]
    Last 10:  [-0.6118732690811157, 0.031049460172653198, 0.5534160137176514, -0.561439037322998, -0.0431094616651535, -0.19828274846076965, -0.20971760153770447, -0.2855392098426819, -0.25892388820648193, 0.5381672978401184]
  WEIGHT: [1536, 576] | μ=0.000034

251_model.layers.20.mlp.down_proj: model.layers.20.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000480 σ=0.061231
    First 10: [-0.004050419200211763, -0.0005955289816483855, -0.005869024898856878, 0.01266520842909813, 0.006108545698225498, -0.34308117628097534, 0.010959175415337086, 0.03238800913095474, 0.015025461092591286, 0.06217564642429352]
    Last 10:  [0.158344566822052, -0.008138466626405716, -0.12692727148532867, 0.1257314383983612, -0.007150706835091114, 0.034176766872406006, 0.020080555230379105, 0.045080456882715225, -0.1215592697262764, -0.0612010657787323]
  OUT[0]: [1, 5, 576] | μ=0.005363 σ=0.093906
    First 10: [-0.0224256981164217, 0.06704564392566681, 0.12639197707176208, 0.018606672063469887, 0.06070858612656593, 0.010594639927148819, -0.08867188543081284, 0.09653672575950623, -0.01186444517225027, -0.02915659174323082]
    Last 10:  [-0.05422423034906387, 0.0390188992023468, 0.010260045528411865, 0.2214052379131317, -0.06394590437412262, 0.10702542960643768, 0.036005109548568726, 0.0019258782267570496, -0.13415798544883728, 0.1235584169626236]
  WEIGHT: [576, 1536] | μ=0.000034

252_model.layers.20.mlp: model.layers.20.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.001203 σ=0.383795
    First 10: [-0.015705499798059464, -0.061641447246074677, -0.010365073569118977, 0.013192812912166119, -0.2741486132144928, -0.11933199316263199, 0.09879142045974731, -0.004260567016899586, -0.01328304409980774, -0.048723168671131134]
    Last 10:  [0.385272741317749, -0.11476325243711472, 0.5106383562088013, -0.6581286191940308, 0.3003566265106201, 0.32992517948150635, -0.290936142206192, 0.08204052597284317, 0.007243961561471224, -0.43627017736434937]
  OUT[0]: [1, 5, 576] | μ=0.005363 σ=0.093906
    First 10: [-0.0224256981164217, 0.06704564392566681, 0.12639197707176208, 0.018606672063469887, 0.06070858612656593, 0.010594639927148819, -0.08867188543081284, 0.09653672575950623, -0.01186444517225027, -0.02915659174323082]
    Last 10:  [-0.05422423034906387, 0.0390188992023468, 0.010260045528411865, 0.2214052379131317, -0.06394590437412262, 0.10702542960643768, 0.036005109548568726, 0.0019258782267570496, -0.13415798544883728, 0.1235584169626236]

253_model.layers.21.input_layernorm: model.layers.21.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.082189 σ=12.014915
    First 10: [-0.6367425918579102, -2.556516408920288, -0.28928086161613464, 0.5596799850463867, -11.658825874328613, -7.072819709777832, 4.598353862762451, -0.09608089923858643, -0.5247201323509216, -2.066103219985962]
    Last 10:  [0.36258652806282043, -0.08495138585567474, 0.5985597968101501, -0.49967876076698303, 0.27987760305404663, 0.4612809419631958, -0.2664065361022949, 0.0945553183555603, -0.12695598602294922, -0.4072551131248474]
  OUT[0]: [1, 5, 576] | μ=-0.003367 σ=0.376527
    First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
    Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
  WEIGHT: [576] | μ=0.519418

254_model.layers.21.self_attn.q_proj: model.layers.21.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003367 σ=0.376527
    First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
    Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
  OUT[0]: [1, 5, 576] | μ=-0.016057 σ=1.183110
    First 10: [0.22984200716018677, 0.15910829603672028, -0.003514103591442108, 0.2691241204738617, 0.35574468970298767, -0.22650867700576782, -0.025627970695495605, 0.02055741287767887, -0.39690181612968445, -0.21535637974739075]
    Last 10:  [-5.280608177185059, -0.1609506905078888, 6.974335193634033, -0.2305663526058197, -0.3093112111091614, -1.1503269672393799, 0.40631794929504395, -1.5499911308288574, -1.7624633312225342, -1.2647048234939575]
  WEIGHT: [576, 576] | μ=-0.000007

255_model.layers.21.self_attn.k_proj: model.layers.21.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003367 σ=0.376527
    First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
    Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
  OUT[0]: [1, 5, 192] | μ=0.127560 σ=1.394830
    First 10: [0.0027727633714675903, 0.01414744183421135, 0.007860049605369568, 0.003395169973373413, -0.006208799779415131, -0.004044666886329651, -0.0023282617330551147, 0.010892018675804138, -0.002220362424850464, 0.004994720220565796]
    Last 10:  [0.700076699256897, -0.922065019607544, -0.8861062526702881, -0.2740546762943268, 1.0790432691574097, 0.4173305332660675, -0.2509743273258209, -1.0885858535766602, 1.43439781665802, 1.6519968509674072]
  WEIGHT: [192, 576] | μ=0.000310

256_model.layers.21.self_attn.v_proj: model.layers.21.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003367 σ=0.376527
    First 10: [-0.015880528837442398, -0.08720579743385315, -0.008758896961808205, 0.01792513206601143, -0.25013479590415955, -0.3078550100326538, 0.12988470494747162, -0.0030747561249881983, -0.015280142426490784, -0.061441682279109955]
    Last 10:  [0.3068121075630188, -0.08590175211429596, 0.5515257120132446, -0.4516196846961975, 0.2511116862297058, 0.46644139289855957, -0.271028071641922, 0.09927455335855484, -0.12022008001804352, -0.4193377196788788]
  OUT[0]: [1, 5, 192] | μ=0.008311 σ=0.265292
    First 10: [0.0007532346062362194, -0.004728805273771286, 0.0024248436093330383, -0.005464533343911171, -0.005816289223730564, 0.0024835262447595596, 0.0019276496022939682, -0.001150958240032196, 0.2210206389427185, 0.006917043589055538]
    Last 10:  [0.4177420437335968, 0.4196409583091736, 0.04316980019211769, -0.04197131097316742, 0.2942720651626587, 0.23625461757183075, 0.2365373969078064, 0.26631057262420654, -0.35316744446754456, -0.12206064164638519]
  WEIGHT: [192, 576] | μ=-0.000143

257_model.layers.21.self_attn.o_proj: model.layers.21.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001630 σ=0.031378
    First 10: [0.0007532346062362194, -0.004728805273771286, 0.0024248436093330383, -0.005464533343911171, -0.005816289223730564, 0.0024835262447595596, 0.0019276496022939682, -0.001150958240032196, 0.2210206389427185, 0.006917043589055538]
    Last 10:  [0.011144109070301056, 0.0029293762054294348, -0.0017982112476602197, 0.0034878933802247047, 0.013988573104143143, 0.010967212729156017, -0.0008642916800454259, 0.006287422496825457, -0.023387420922517776, 0.004245796240866184]
  OUT[0]: [1, 5, 576] | μ=-0.001482 σ=0.031514
    First 10: [0.006397673394531012, -0.021988656371831894, 0.01723289117217064, -0.005464411340653896, 0.008898677304387093, 0.010687432251870632, 0.012334084138274193, 0.018378661945462227, 0.015047771856188774, -0.008032435551285744]
    Last 10:  [-0.0018701991066336632, -0.03900965303182602, 0.023705456405878067, -0.0012166635133326054, -0.03350949287414551, 0.02829616330564022, -0.009873336181044579, -0.002146713435649872, 0.03482506051659584, -0.023764245212078094]
  WEIGHT: [576, 576] | μ=-0.000071

258_model.layers.21.self_attn: model.layers.21.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.001482 σ=0.031514
    First 10: [0.006397673394531012, -0.021988656371831894, 0.01723289117217064, -0.005464411340653896, 0.008898677304387093, 0.010687432251870632, 0.012334084138274193, 0.018378661945462227, 0.015047771856188774, -0.008032435551285744]
    Last 10:  [-0.0018701991066336632, -0.03900965303182602, 0.023705456405878067, -0.0012166635133326054, -0.03350949287414551, 0.02829616330564022, -0.009873336181044579, -0.002146713435649872, 0.03482506051659584, -0.023764245212078094]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.272971
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49984100461006165, 0.5001590251922607, 0.0, 0.0, 0.0]
    Last 10:  [0.49036628007888794, 0.49185219407081604, 0.010607033967971802, 0.00717449514195323, 0.0, 0.47771021723747253, 0.4788278043270111, 0.015490761026740074, 0.01590866781771183, 0.012062611989676952]

259_model.layers.21.post_attention_layernorm: model.layers.21.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.083671 σ=12.016568
    First 10: [-0.6303449273109436, -2.578505039215088, -0.2720479667186737, 0.5542155504226685, -11.649927139282227, -7.062132358551025, 4.610687732696533, -0.07770223915576935, -0.5096723437309265, -2.0741355419158936]
    Last 10:  [0.3607163429260254, -0.12396103888750076, 0.6222652792930603, -0.5008954405784607, 0.24636811017990112, 0.48957711458206177, -0.27627986669540405, 0.09240860491991043, -0.09213092923164368, -0.4310193657875061]
  OUT[0]: [1, 5, 576] | μ=0.002642 σ=0.381286
    First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
    Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
  WEIGHT: [576] | μ=0.478825

260_model.layers.21.mlp.gate_proj: model.layers.21.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.002642 σ=0.381286
    First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
    Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
  OUT[0]: [1, 5, 1536] | μ=-0.237013 σ=0.460242
    First 10: [-0.22884830832481384, -0.5886576175689697, 0.03616829216480255, -0.0913294330239296, -0.0695096030831337, 0.4416232705116272, 0.12405756115913391, -0.5053423643112183, -0.6672689318656921, 0.08058266341686249]
    Last 10:  [-0.17080862820148468, -0.37617653608322144, -0.8496569395065308, 1.4870314598083496, 0.3391801118850708, -0.4760298728942871, -0.08977946639060974, -0.09948757290840149, 0.2536627948284149, -1.3234496116638184]
  WEIGHT: [1536, 576] | μ=-0.000094

261_model.layers.21.mlp.act_fn: model.layers.21.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.237013 σ=0.460242
    First 10: [-0.22884830832481384, -0.5886576175689697, 0.03616829216480255, -0.0913294330239296, -0.0695096030831337, 0.4416232705116272, 0.12405756115913391, -0.5053423643112183, -0.6672689318656921, 0.08058266341686249]
    Last 10:  [-0.17080862820148468, -0.37617653608322144, -0.8496569395065308, 1.4870314598083496, 0.3391801118850708, -0.4760298728942871, -0.08977946639060974, -0.09948757290840149, 0.2536627948284149, -1.3234496116638184]
  OUT[0]: [1, 5, 1536] | μ=-0.057293 σ=0.180461
    First 10: [-0.1013881117105484, -0.2101171612739563, 0.018411146476864815, -0.04358089715242386, -0.03354739025235176, 0.26879212260246277, 0.06587142497301102, -0.19015325605869293, -0.22627666592597961, 0.041913848370313644]
    Last 10:  [-0.07812810689210892, -0.1531224399805069, -0.2544763684272766, 1.2128709554672241, 0.1980782449245453, -0.18240991234779358, -0.0428759939968586, -0.04727138206362724, 0.14283189177513123, -0.2782485783100128]

262_model.layers.21.mlp.up_proj: model.layers.21.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.002642 σ=0.381286
    First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
    Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
  OUT[0]: [1, 5, 1536] | μ=-0.001538 σ=0.320853
    First 10: [0.3715323507785797, -0.2934679687023163, 0.5119953155517578, -0.1137034073472023, 0.11482010781764984, -0.27615001797676086, 1.0119760036468506, -0.15964308381080627, -0.055060748010873795, 0.06178927421569824]
    Last 10:  [-0.11325860768556595, -0.6864119172096252, 0.3015926778316498, -0.2248462736606598, 0.09742708504199982, -0.014056161046028137, 0.15402640402317047, 0.33341294527053833, 0.17754997313022614, 0.31568098068237305]
  WEIGHT: [1536, 576] | μ=0.000121

263_model.layers.21.mlp.down_proj: model.layers.21.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000617 σ=0.063210
    First 10: [-0.03766896203160286, 0.06166265532374382, 0.0094264205545187, 0.004955296404659748, -0.0038519150111824274, -0.07422695308923721, 0.06666029989719391, 0.030356653034687042, 0.012458962388336658, 0.002589826239272952]
    Last 10:  [0.008848680183291435, 0.10510506480932236, -0.07674820721149445, -0.27270951867103577, 0.019298186525702477, 0.002563983201980591, -0.006604035384953022, -0.015760891139507294, 0.025359798222780228, -0.08783778548240662]
  OUT[0]: [1, 5, 576] | μ=-0.002990 σ=0.098263
    First 10: [-0.07455923408269882, 0.005484214052557945, 0.12612131237983704, -0.04730931669473648, 0.009857622906565666, 0.02295755222439766, -0.10611513257026672, -0.07391823828220367, 0.08659259974956512, -0.028750231489539146]
    Last 10:  [-0.0646699070930481, 0.04960818588733673, 0.15533052384853363, -0.0018791472539305687, 0.07151983678340912, -0.1355585753917694, -0.08350910246372223, -0.08633317053318024, -0.025011468678712845, 0.03650433197617531]
  WEIGHT: [576, 1536] | μ=0.000027

264_model.layers.21.mlp: model.layers.21.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=0.002642 σ=0.381286
    First 10: [-0.016132771968841553, -0.06257740408182144, -0.006910194177180529, 0.014262756332755089, -0.2822810709476471, -0.1171664372086525, 0.0995028093457222, -0.0018267846899107099, -0.01343762781471014, -0.050817087292671204]
    Last 10:  [0.32387247681617737, -0.110552579164505, 0.5238878726959229, -0.43485745787620544, 0.20360086858272552, 0.4197632074356079, -0.25781160593032837, 0.07620822638273239, -0.08311714977025986, -0.34488165378570557]
  OUT[0]: [1, 5, 576] | μ=-0.002990 σ=0.098263
    First 10: [-0.07455923408269882, 0.005484214052557945, 0.12612131237983704, -0.04730931669473648, 0.009857622906565666, 0.02295755222439766, -0.10611513257026672, -0.07391823828220367, 0.08659259974956512, -0.028750231489539146]
    Last 10:  [-0.0646699070930481, 0.04960818588733673, 0.15533052384853363, -0.0018791472539305687, 0.07151983678340912, -0.1355585753917694, -0.08350910246372223, -0.08633317053318024, -0.025011468678712845, 0.03650433197617531]

265_model.layers.22.input_layernorm: model.layers.22.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.086661 σ=12.013081
    First 10: [-0.7049041390419006, -2.5730209350585938, -0.14592665433883667, 0.5069062113761902, -11.640069961547852, -7.039175033569336, 4.50457239151001, -0.15162047743797302, -0.4230797290802002, -2.1028857231140137]
    Last 10:  [0.2960464358329773, -0.07435285300016403, 0.7775958180427551, -0.5027745962142944, 0.31788796186447144, 0.35401853919029236, -0.3597889542579651, 0.006075434386730194, -0.11714239418506622, -0.3945150375366211]
  OUT[0]: [1, 5, 576] | μ=-0.007081 σ=0.402962
    First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
    Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
  WEIGHT: [576] | μ=0.550805

266_model.layers.22.self_attn.q_proj: model.layers.22.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007081 σ=0.402962
    First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
    Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
  OUT[0]: [1, 5, 576] | μ=-0.046878 σ=1.223713
    First 10: [-0.36074739694595337, -0.009188666939735413, -0.2593041956424713, -0.05504351109266281, 0.21059226989746094, 0.1324395388364792, 0.2643706798553467, 0.14050404727458954, 0.39174655079841614, -0.03540651127696037]
    Last 10:  [-0.5488568544387817, -1.5286352634429932, -1.4471421241760254, 1.2971949577331543, 2.5139453411102295, 0.323557585477829, -1.2507191896438599, -0.5471152067184448, 1.510875940322876, 5.33905029296875]
  WEIGHT: [576, 576] | μ=-0.000088

267_model.layers.22.self_attn.k_proj: model.layers.22.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007081 σ=0.402962
    First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
    Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
  OUT[0]: [1, 5, 192] | μ=-0.082793 σ=1.238845
    First 10: [0.012216828763484955, -0.023319728672504425, -0.023002414032816887, 0.010548101738095284, -0.004133984446525574, 0.013665445148944855, 0.0017748773097991943, 0.01568520814180374, 0.00408385694026947, 0.019474200904369354]
    Last 10:  [6.705820083618164, 0.47833529114723206, -0.9782044291496277, -0.7091226577758789, -0.4627963602542877, 1.531082034111023, 4.5554656982421875, 4.5525665283203125, -2.009323835372925, -0.33737683296203613]
  WEIGHT: [192, 576] | μ=0.000056

268_model.layers.22.self_attn.v_proj: model.layers.22.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.007081 σ=0.402962
    First 10: [-0.02227403223514557, -0.09070563316345215, -0.004945278633385897, 0.017087137326598167, -0.2219444066286087, -0.33165040612220764, 0.17062072455883026, -0.005266984459012747, -0.012922404333949089, -0.07121016085147858]
    Last 10:  [0.2531295120716095, -0.07952835410833359, 0.7232366800308228, -0.4327603578567505, 0.26311632990837097, 0.32927027344703674, -0.36193665862083435, 0.00652311323210597, -0.09805802255868912, -0.41328561305999756]
  OUT[0]: [1, 5, 192] | μ=-0.017767 σ=0.336811
    First 10: [0.01535709761083126, 0.002594597637653351, -0.0005996804684400558, -0.0014284218195825815, 0.0032699727453291416, -0.002136276103556156, -0.032770104706287384, 0.009553901851177216, -0.0060053253546357155, -0.010167919099330902]
    Last 10:  [1.1591767072677612, 0.6215489506721497, -0.9111776351928711, -0.451712042093277, 0.35181891918182373, 0.27174293994903564, 0.5214964747428894, -0.06354756653308868, -0.38733887672424316, -0.5985794067382812]
  WEIGHT: [192, 576] | μ=-0.000051

269_model.layers.22.self_attn.o_proj: model.layers.22.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.002043 σ=0.062767
    First 10: [0.01535709761083126, 0.002594597637653351, -0.0005996804684400558, -0.0014284218195825815, 0.0032699727453291416, -0.002136276103556156, -0.032770104706287384, 0.009553901851177216, -0.0060053253546357155, -0.010167919099330902]
    Last 10:  [0.04433678463101387, 0.013130789622664452, -0.04815905913710594, 0.0011169093195348978, 0.025027208030223846, 0.016393307596445084, 0.030104847624897957, 0.009956265799701214, -0.008847092278301716, -0.008229238912463188]
  OUT[0]: [1, 5, 576] | μ=0.000114 σ=0.068613
    First 10: [0.05316454917192459, -0.02331623062491417, -0.04013150930404663, -0.01521223969757557, 0.0427003875374794, -0.060108482837677, 0.008881362155079842, 0.013814093545079231, 0.002582073677331209, -0.017367791384458542]
    Last 10:  [-0.042863644659519196, -0.024657342582941055, -0.22159329056739807, 0.14144231379032135, -0.26581746339797974, -0.14955508708953857, -0.11467471718788147, 0.044134750962257385, 0.05820775777101517, -0.005239246413111687]
  WEIGHT: [576, 576] | μ=0.000074

270_model.layers.22.self_attn: model.layers.22.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.000114 σ=0.068613
    First 10: [0.05316454917192459, -0.02331623062491417, -0.04013150930404663, -0.01521223969757557, 0.0427003875374794, -0.060108482837677, 0.008881362155079842, 0.013814093545079231, 0.002582073677331209, -0.017367791384458542]
    Last 10:  [-0.042863644659519196, -0.024657342582941055, -0.22159329056739807, 0.14144231379032135, -0.26581746339797974, -0.14955508708953857, -0.11467471718788147, 0.044134750962257385, 0.05820775777101517, -0.005239246413111687]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.262732
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000764727592468, 0.4999235272407532, 0.0, 0.0, 0.0]
    Last 10:  [0.48176485300064087, 0.4822445213794708, 0.013340338133275509, 0.022650232538580894, 0.0, 0.4707629084587097, 0.4716876745223999, 0.012161492370069027, 0.012615066953003407, 0.03277279809117317]

271_model.layers.22.post_attention_layernorm: model.layers.22.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.086547 σ=12.018270
    First 10: [-0.6517395973205566, -2.596337080001831, -0.1860581636428833, 0.49169397354125977, -11.597369194030762, -7.099283695220947, 4.513453960418701, -0.13780638575553894, -0.4204976558685303, -2.120253562927246]
    Last 10:  [0.2531827986240387, -0.09901019930839539, 0.5560024976730347, -0.3613322973251343, 0.0520704984664917, 0.20446345210075378, -0.47446367144584656, 0.05021018534898758, -0.058934636414051056, -0.39975428581237793]
  OUT[0]: [1, 5, 576] | μ=0.000812 σ=0.365180
    First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
    Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
  WEIGHT: [576] | μ=0.470074

272_model.layers.22.mlp.gate_proj: model.layers.22.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.000812 σ=0.365180
    First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
    Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
  OUT[0]: [1, 5, 1536] | μ=-0.194071 σ=0.451186
    First 10: [0.12825141847133636, -0.1199914738535881, -0.33811861276626587, -0.9641696214675903, -0.453379362821579, -0.0516669787466526, -0.4012378454208374, -0.6591695547103882, -0.41465336084365845, -0.3029075860977173]
    Last 10:  [-1.0374350547790527, -0.2553068995475769, 0.3562857508659363, -0.8226343989372253, 0.6884980797767639, 0.16277694702148438, -0.20076915621757507, -1.2174700498580933, 0.46374034881591797, -0.11898411810398102]
  WEIGHT: [1536, 576] | μ=0.000001

273_model.layers.22.mlp.act_fn: model.layers.22.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.194071 σ=0.451186
    First 10: [0.12825141847133636, -0.1199914738535881, -0.33811861276626587, -0.9641696214675903, -0.453379362821579, -0.0516669787466526, -0.4012378454208374, -0.6591695547103882, -0.41465336084365845, -0.3029075860977173]
    Last 10:  [-1.0374350547790527, -0.2553068995475769, 0.3562857508659363, -0.8226343989372253, 0.6884980797767639, 0.16277694702148438, -0.20076915621757507, -1.2174700498580933, 0.46374034881591797, -0.11898411810398102]
  OUT[0]: [1, 5, 1536] | μ=-0.041171 σ=0.205181
    First 10: [0.06823218613862991, -0.056400563567876816, -0.14074747264385223, -0.26615336537361145, -0.17616400122642517, -0.02516626939177513, -0.16090239584445953, -0.22472815215587616, -0.16494780778884888, -0.12868933379650116]
    Last 10:  [-0.27143990993499756, -0.11144598573446274, 0.2095462530851364, -0.2510719299316406, 0.458286851644516, 0.08799797296524048, -0.0903412327170372, -0.27804821729660034, 0.2846907079219818, -0.055956922471523285]

274_model.layers.22.mlp.up_proj: model.layers.22.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.000812 σ=0.365180
    First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
    Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
  OUT[0]: [1, 5, 1536] | μ=0.002648 σ=0.323809
    First 10: [-0.3480209410190582, 0.2867393493652344, -0.26820698380470276, 0.332044780254364, 0.16729779541492462, 0.03233139216899872, -0.17248675227165222, 0.11719851940870285, 0.2806438207626343, 0.10768653452396393]
    Last 10:  [-0.4167974293231964, -0.3732747733592987, -0.10734303295612335, -0.2530028820037842, 0.1769219934940338, -0.1868470013141632, -0.2023690938949585, 0.19844475388526917, 0.002392515540122986, 0.11281710863113403]
  WEIGHT: [1536, 576] | μ=-0.000028

275_model.layers.22.mlp.down_proj: model.layers.22.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.001360 σ=0.080548
    First 10: [-0.023746229708194733, -0.01617226004600525, 0.037749454379081726, -0.08837483823299408, -0.029471848160028458, -0.0008136605028994381, 0.02775353193283081, -0.02633780613541603, -0.04629158228635788, -0.013858108781278133]
    Last 10:  [0.1131354570388794, 0.041599974036216736, -0.022493330761790276, 0.06352192163467407, 0.08108102530241013, -0.016442157328128815, 0.018282273784279823, -0.05517721176147461, 0.000681126955896616, -0.006312898360192776]
  OUT[0]: [1, 5, 576] | μ=0.004049 σ=0.123073
    First 10: [0.02610795758664608, 0.038270462304353714, -0.09737083315849304, -0.08154627680778503, 0.07211573421955109, 0.05738777294754982, 0.022405140101909637, -0.007602822966873646, 0.02503429725766182, -0.03890591487288475]
    Last 10:  [-0.44651857018470764, -0.23423157632350922, 0.2262629121541977, -0.2898651659488678, 0.022039901465177536, 0.1453578621149063, 0.04107243940234184, -0.07388395816087723, 0.1357453465461731, -0.12350669503211975]
  WEIGHT: [576, 1536] | μ=-0.000059

276_model.layers.22.mlp: model.layers.22.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=0.000812 σ=0.365180
    First 10: [-0.016479164361953735, -0.06260945647954941, -0.004534563980996609, 0.012729642912745476, -0.2678819000720978, -0.13038279116153717, 0.08950987458229065, -0.0032309747766703367, -0.010816111229360104, -0.05001090094447136]
    Last 10:  [0.20112648606300354, -0.07865294814109802, 0.4182995855808258, -0.2824947237968445, 0.03894926607608795, 0.1678088903427124, -0.39313557744026184, 0.036215681582689285, -0.04589061439037323, -0.29100659489631653]
  OUT[0]: [1, 5, 576] | μ=0.004049 σ=0.123073
    First 10: [0.02610795758664608, 0.038270462304353714, -0.09737083315849304, -0.08154627680778503, 0.07211573421955109, 0.05738777294754982, 0.022405140101909637, -0.007602822966873646, 0.02503429725766182, -0.03890591487288475]
    Last 10:  [-0.44651857018470764, -0.23423157632350922, 0.2262629121541977, -0.2898651659488678, 0.022039901465177536, 0.1453578621149063, 0.04107243940234184, -0.07388395816087723, 0.1357453465461731, -0.12350669503211975]

277_model.layers.23.input_layernorm: model.layers.23.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.082499 σ=12.007148
    First 10: [-0.6256316304206848, -2.5580666065216064, -0.28342899680137634, 0.41014769673347473, -11.525253295898438, -7.041895866394043, 4.535859107971191, -0.1454092115163803, -0.39546334743499756, -2.1591594219207764]
    Last 10:  [-0.19333577156066895, -0.3332417607307434, 0.7822654247283936, -0.6511974334716797, 0.07411040365695953, 0.3498213291168213, -0.4333912432193756, -0.02367377281188965, 0.07681071013212204, -0.5232609510421753]
  OUT[0]: [1, 5, 576] | μ=-0.000381 σ=0.404781
    First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
    Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
  WEIGHT: [576] | μ=0.582079

278_model.layers.23.self_attn.q_proj: model.layers.23.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000381 σ=0.404781
    First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
    Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
  OUT[0]: [1, 5, 576] | μ=0.016844 σ=1.013739
    First 10: [0.03882542997598648, -0.0109908077865839, 0.18332095444202423, 0.030024807900190353, 0.15492121875286102, -0.4824647903442383, -0.20886868238449097, -0.16639557480812073, -0.06901206076145172, -0.1604500412940979]
    Last 10:  [1.6566965579986572, 6.271746635437012, 0.6227735280990601, 0.7228705883026123, 1.2723091840744019, 0.09891939163208008, 0.24935078620910645, -1.673363447189331, -0.07355526089668274, -0.053754061460494995]
  WEIGHT: [576, 576] | μ=-0.000010

279_model.layers.23.self_attn.k_proj: model.layers.23.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000381 σ=0.404781
    First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
    Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
  OUT[0]: [1, 5, 192] | μ=-0.028193 σ=1.279865
    First 10: [0.0010672901989892125, -0.012887310236692429, -0.01687832549214363, -0.006928585469722748, -0.003949351608753204, -0.00751158595085144, -0.006269879639148712, 0.008447722531855106, 0.006100103259086609, 0.0034252703189849854]
    Last 10:  [0.8110791444778442, -0.46858835220336914, 0.9879734516143799, -0.5486447811126709, 2.29508900642395, -0.5165367722511292, 1.1878749132156372, 2.5627260208129883, 1.7888284921646118, -1.3071839809417725]
  WEIGHT: [192, 576] | μ=-0.000223

280_model.layers.23.self_attn.v_proj: model.layers.23.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000381 σ=0.404781
    First 10: [-0.018446121364831924, -0.09314126521348953, -0.010582618415355682, 0.01471201702952385, -0.24217064678668976, -0.37535378336906433, 0.16807426512241364, -0.006215568166226149, -0.013085496611893177, -0.08095187693834305]
    Last 10:  [-0.13374747335910797, -0.3006949722766876, 0.6616072058677673, -0.5465583205223083, 0.05065838247537613, 0.29335901141166687, -0.42830726504325867, -0.02578653022646904, 0.06056271120905876, -0.5156238675117493]
  OUT[0]: [1, 5, 192] | μ=0.028437 σ=0.393747
    First 10: [-0.0025939475744962692, 0.0065566785633563995, -0.005376212298870087, 0.007756686769425869, 0.00014266371726989746, -0.009234514087438583, 0.010661184787750244, 0.014128042384982109, 0.0024462398141622543, -0.0037422841414809227]
    Last 10:  [0.1966671645641327, 0.7185508608818054, 0.06158833205699921, 0.40007007122039795, -0.03582869470119476, -0.2765061855316162, 0.22976237535476685, -0.532347559928894, -0.33457404375076294, -0.2990342080593109]
  WEIGHT: [192, 576] | μ=0.000215

281_model.layers.23.self_attn.o_proj: model.layers.23.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.003804 σ=0.064498
    First 10: [-0.0025939475744962692, 0.0065566785633563995, -0.005376212298870087, 0.007756686769425869, 0.00014266371726989746, -0.009234514087438583, 0.010661184787750244, 0.014128042384982109, 0.0024462398141622543, -0.0037422841414809227]
    Last 10:  [0.005524877924472094, 0.022212868556380272, -0.0008092504576779902, 0.012956112623214722, -0.021154744550585747, -0.030004682019352913, 0.0014769032131880522, -0.009680322371423244, -0.01673680730164051, -0.013859687373042107]
  OUT[0]: [1, 5, 576] | μ=-0.001076 σ=0.068838
    First 10: [0.01896200142800808, -0.017869537696242332, 0.03212352097034454, 0.004578854888677597, -0.00672895135357976, -0.0015109654050320387, 0.012132639065384865, -0.008294843137264252, -0.008047648705542088, -0.015722960233688354]
    Last 10:  [0.004189356695860624, 0.04497360810637474, -0.11405371129512787, -0.11616293340921402, -0.14106105268001556, -0.09966025501489639, -0.1514626443386078, 0.017721250653266907, -0.12847736477851868, -0.3146999478340149]
  WEIGHT: [576, 576] | μ=0.000082

282_model.layers.23.self_attn: model.layers.23.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.001076 σ=0.068838
    First 10: [0.01896200142800808, -0.017869537696242332, 0.03212352097034454, 0.004578854888677597, -0.00672895135357976, -0.0015109654050320387, 0.012132639065384865, -0.008294843137264252, -0.008047648705542088, -0.015722960233688354]
    Last 10:  [0.004189356695860624, 0.04497360810637474, -0.11405371129512787, -0.11616293340921402, -0.14106105268001556, -0.09966025501489639, -0.1514626443386078, 0.017721250653266907, -0.12847736477851868, -0.3146999478340149]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.266964
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49996840953826904, 0.5000316500663757, 0.0, 0.0, 0.0]
    Last 10:  [0.4842434227466583, 0.4848483204841614, 0.013631118461489677, 0.017277171835303307, 0.0, 0.4552130699157715, 0.4570446312427521, 0.02106368914246559, 0.022347334772348404, 0.044331151992082596]

283_model.layers.23.post_attention_layernorm: model.layers.23.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.083575 σ=12.009032
    First 10: [-0.6066696047782898, -2.5759360790252686, -0.2513054609298706, 0.4147265553474426, -11.531982421875, -7.043406963348389, 4.547991752624512, -0.15370404720306396, -0.4035109877586365, -2.174882411956787]
    Last 10:  [-0.18914641439914703, -0.28826814889907837, 0.6682116985321045, -0.7673603892326355, -0.06695064902305603, 0.2501610815525055, -0.5848538875579834, -0.005952522158622742, -0.05166665464639664, -0.8379608988761902]
  OUT[0]: [1, 5, 576] | μ=0.003453 σ=0.355253
    First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
    Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
  WEIGHT: [576] | μ=0.478013

284_model.layers.23.mlp.gate_proj: model.layers.23.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.003453 σ=0.355253
    First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
    Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
  OUT[0]: [1, 5, 1536] | μ=-0.196718 σ=0.401999
    First 10: [-0.04783173277974129, 0.13219603896141052, 0.1513558179140091, -0.4377827048301697, -0.32963457703590393, -0.00872887670993805, 0.13432136178016663, 0.0012616664171218872, 0.03157933056354523, -0.4618930220603943]
    Last 10:  [-0.4584723114967346, -0.42884188890457153, -0.5990346670150757, -0.23482072353363037, -1.0211139917373657, -0.12887078523635864, 0.3433792293071747, -0.4555142819881439, -0.49022194743156433, 0.393224835395813]
  WEIGHT: [1536, 576] | μ=0.000056

285_model.layers.23.mlp.act_fn: model.layers.23.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.196718 σ=0.401999
    First 10: [-0.04783173277974129, 0.13219603896141052, 0.1513558179140091, -0.4377827048301697, -0.32963457703590393, -0.00872887670993805, 0.13432136178016663, 0.0012616664171218872, 0.03157933056354523, -0.4618930220603943]
    Last 10:  [-0.4584723114967346, -0.42884188890457153, -0.5990346670150757, -0.23482072353363037, -1.0211139917373657, -0.12887078523635864, 0.3433792293071747, -0.4555142819881439, -0.49022194743156433, 0.393224835395813]
  OUT[0]: [1, 5, 1536] | μ=-0.051588 σ=0.166738
    First 10: [-0.02334400825202465, 0.07046061754226685, 0.08139415085315704, -0.17172877490520477, -0.1378958821296692, -0.004345390014350414, 0.0716644674539566, 0.000631231174338609, 0.016038957983255386, -0.17853868007659912]
    Last 10:  [-0.17758846282958984, -0.16913649439811707, -0.21239647269248962, -0.10368816554546356, -0.27040165662765503, -0.060289207845926285, 0.20088067650794983, -0.17676255106925964, -0.18620653450489044, 0.23477834463119507]

286_model.layers.23.mlp.up_proj: model.layers.23.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.003453 σ=0.355253
    First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
    Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
  OUT[0]: [1, 5, 1536] | μ=-0.005359 σ=0.319773
    First 10: [-0.30181047320365906, -0.3488282859325409, 0.4746686816215515, -0.0848032608628273, 0.015756061300635338, 0.043790869414806366, 1.05527925491333, -0.9231560230255127, -0.2956707775592804, 0.12498873472213745]
    Last 10:  [-0.44497841596603394, -0.3743751049041748, -0.2455146610736847, 0.04781195521354675, 0.3584684431552887, 0.20256289839744568, 0.1163095235824585, -0.18338069319725037, -0.09849771857261658, -0.22898846864700317]
  WEIGHT: [1536, 576] | μ=0.000106

287_model.layers.23.mlp.down_proj: model.layers.23.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000852 σ=0.062536
    First 10: [0.007045465987175703, -0.024578657001256943, 0.03863525390625, 0.014563160017132759, -0.0021726959384977818, -0.0001902884105220437, 0.07562602311372757, -0.0005827248678542674, -0.004742251243442297, -0.02231532335281372]
    Last 10:  [0.07902303338050842, 0.06332049518823624, 0.05214644968509674, -0.004957533907145262, -0.09693045914173126, -0.012212356552481651, 0.023364335298538208, 0.032414838671684265, 0.01834091916680336, -0.05376153439283371]
  OUT[0]: [1, 5, 576] | μ=0.000066 σ=0.108549
    First 10: [0.0661521777510643, 0.062001004815101624, 0.031662195920944214, 0.026480158790946007, 0.05145028233528137, 0.24532906711101532, 0.03555052727460861, 0.008698836900293827, 0.0245942585170269, 0.04240277782082558]
    Last 10:  [0.0575750470161438, 0.11860759556293488, 0.25804969668388367, -0.12256695330142975, 0.21863412857055664, 0.1768510341644287, 0.20613552629947662, 0.059022217988967896, 0.02508348599076271, 0.224784255027771]
  WEIGHT: [576, 1536] | μ=-0.000036

288_model.layers.23.mlp: model.layers.23.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=0.003453 σ=0.355253
    First 10: [-0.01572173275053501, -0.06168114393949509, -0.006224599201232195, 0.010566034354269505, -0.24109843373298645, -0.10264401137828827, 0.09508458524942398, -0.003676507156342268, -0.010332241654396057, -0.05484974384307861]
    Last 10:  [-0.13120034337043762, -0.20342187583446503, 0.4497271180152893, -0.5212026238441467, -0.042874712496995926, 0.1772184520959854, -0.42959144711494446, -0.004034861456602812, -0.035270266234874725, -0.5403667092323303]
  OUT[0]: [1, 5, 576] | μ=0.000066 σ=0.108549
    First 10: [0.0661521777510643, 0.062001004815101624, 0.031662195920944214, 0.026480158790946007, 0.05145028233528137, 0.24532906711101532, 0.03555052727460861, 0.008698836900293827, 0.0245942585170269, 0.04240277782082558]
    Last 10:  [0.0575750470161438, 0.11860759556293488, 0.25804969668388367, -0.12256695330142975, 0.21863412857055664, 0.1768510341644287, 0.20613552629947662, 0.059022217988967896, 0.02508348599076271, 0.224784255027771]

289_model.layers.24.input_layernorm: model.layers.24.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.083509 σ=11.998659
    First 10: [-0.5405174493789673, -2.513935089111328, -0.2196432650089264, 0.4412067234516144, -11.480531692504883, -6.7980780601501465, 4.583542346954346, -0.1450052112340927, -0.37891674041748047, -2.132479667663574]
    Last 10:  [-0.13157136738300323, -0.1696605533361435, 0.9262614250183105, -0.889927327632904, 0.1516834795475006, 0.4270121157169342, -0.37871837615966797, 0.053069695830345154, -0.026583168655633926, -0.6131766438484192]
  OUT[0]: [1, 5, 576] | μ=-0.000504 σ=0.372201
    First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
    Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
  WEIGHT: [576] | μ=0.547602

290_model.layers.24.self_attn.q_proj: model.layers.24.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000504 σ=0.372201
    First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
    Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
  OUT[0]: [1, 5, 576] | μ=0.008843 σ=1.075763
    First 10: [-0.19828028976917267, 0.21985313296318054, -0.11545799672603607, -0.16001473367214203, 0.13438065350055695, -0.19457554817199707, -0.04535700008273125, -0.001008361577987671, -0.030495628714561462, -0.10290385782718658]
    Last 10:  [-0.47785496711730957, -1.8432698249816895, -1.4449994564056396, 0.6939730644226074, 1.4835745096206665, 0.27159950137138367, -0.9502145051956177, 0.9896942377090454, 0.8951778411865234, 0.4132666289806366]
  WEIGHT: [576, 576] | μ=-0.000309

291_model.layers.24.self_attn.k_proj: model.layers.24.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000504 σ=0.372201
    First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
    Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
  OUT[0]: [1, 5, 192] | μ=-0.010645 σ=1.181718
    First 10: [0.015383579768240452, -0.017470505088567734, -0.004446686245501041, 0.014349602162837982, 0.00897890329360962, 0.013567976653575897, 0.0016046389937400818, 0.009090833365917206, -0.0011606290936470032, -0.012900218367576599]
    Last 10:  [-6.41807746887207, -0.13302984833717346, -0.04055327549576759, -0.8856514692306519, 1.6969667673110962, 2.1277787685394287, 6.560985088348389, -2.6855905055999756, 0.024928748607635498, 0.02905094623565674]
  WEIGHT: [192, 576] | μ=-0.000118

292_model.layers.24.self_attn.v_proj: model.layers.24.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000504 σ=0.372201
    First 10: [-0.015477745793759823, -0.08837966620922089, -0.007976507768034935, 0.015272215940058231, -0.2627597749233246, -0.3602410554885864, 0.18382084369659424, -0.005564962513744831, -0.01125071756541729, -0.08805040270090103]
    Last 10:  [-0.07312458008527756, -0.11509934812784195, 0.6558325886726379, -0.6127126812934875, 0.0844937413930893, 0.2937275469303131, -0.2946532368659973, 0.04677711799740791, -0.016902944073081017, -0.5064499974250793]
  OUT[0]: [1, 5, 192] | μ=0.001971 σ=0.346682
    First 10: [-0.012662391178309917, 0.0010635113576427102, 0.001180274412035942, -0.004955079406499863, -0.013432323932647705, -0.0087968111038208, 0.025990135967731476, 0.0015112105756998062, 0.007911326363682747, 0.0035303710028529167]
    Last 10:  [0.11490588635206223, -0.29836198687553406, 0.44008973240852356, -0.009852483868598938, 0.008230469189584255, 0.46456316113471985, -0.16702018678188324, 0.3121829628944397, -0.8828476071357727, -0.1978970170021057]
  WEIGHT: [192, 576] | μ=-0.000072

293_model.layers.24.self_attn.o_proj: model.layers.24.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.001293 σ=0.041928
    First 10: [-0.012662391178309917, 0.0010635113576427102, 0.001180274412035942, -0.004955079406499863, -0.013432323932647705, -0.0087968111038208, 0.025990135967731476, 0.0015112105756998062, 0.007911326363682747, 0.0035303710028529167]
    Last 10:  [0.016313951462507248, -0.11438614130020142, -0.019928060472011566, -0.024368036538362503, 0.01062038354575634, 0.0442165732383728, -0.017899664118885994, 0.00136107939761132, -0.10151694715023041, -0.006969610694795847]
  OUT[0]: [1, 5, 576] | μ=0.001010 σ=0.035017
    First 10: [0.009150363504886627, 0.01459546759724617, 0.049958765506744385, 0.007941214367747307, -0.009602834470570087, -0.02898387424647808, 0.01245711650699377, -0.003556998446583748, 0.0050198836252093315, -0.02520291693508625]
    Last 10:  [-0.007110577076673508, -0.026778966188430786, -0.007922408170998096, -0.01079603098332882, 0.03251740336418152, 0.05992622300982475, -0.0009817946702241898, 0.026423141360282898, -0.0322178453207016, 0.00858977995812893]
  WEIGHT: [576, 576] | μ=-0.000100

294_model.layers.24.self_attn: model.layers.24.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.001010 σ=0.035017
    First 10: [0.009150363504886627, 0.01459546759724617, 0.049958765506744385, 0.007941214367747307, -0.009602834470570087, -0.02898387424647808, 0.01245711650699377, -0.003556998446583748, 0.0050198836252093315, -0.02520291693508625]
    Last 10:  [-0.007110577076673508, -0.026778966188430786, -0.007922408170998096, -0.01079603098332882, 0.03251740336418152, 0.05992622300982475, -0.0009817946702241898, 0.026423141360282898, -0.0322178453207016, 0.00858977995812893]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.265362
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001118183135986, 0.4998881220817566, 0.0, 0.0, 0.0]
    Last 10:  [0.4846850633621216, 0.48368605971336365, 0.017937304452061653, 0.013691598549485207, 0.0, 0.4358241558074951, 0.4360167384147644, 0.042052559554576874, 0.04222005605697632, 0.043886519968509674]

295_model.layers.24.post_attention_layernorm: model.layers.24.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.082499 σ=11.997826
    First 10: [-0.5313670635223389, -2.4993395805358887, -0.169684499502182, 0.44914793968200684, -11.490134239196777, -6.827062129974365, 4.595999240875244, -0.1485622078180313, -0.37389686703681946, -2.1576826572418213]
    Last 10:  [-0.13868194818496704, -0.19643951952457428, 0.9183390140533447, -0.9007233381271362, 0.18420088291168213, 0.48693832755088806, -0.3797001838684082, 0.07949283719062805, -0.058801013976335526, -0.6045868396759033]
  OUT[0]: [1, 5, 576] | μ=0.003819 σ=0.358905
    First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
    Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
  WEIGHT: [576] | μ=0.479659

296_model.layers.24.mlp.gate_proj: model.layers.24.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.003819 σ=0.358905
    First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
    Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
  OUT[0]: [1, 5, 1536] | μ=-0.202436 σ=0.393278
    First 10: [0.016141436994075775, 0.737815260887146, -0.49998190999031067, -1.0092025995254517, -0.5337592959403992, 0.5319071412086487, -0.3162623643875122, 0.09798599779605865, -0.02206818014383316, 0.4006209373474121]
    Last 10:  [-0.5592445731163025, -0.4574618935585022, -0.018152162432670593, -0.3723921775817871, -0.5067622661590576, -0.6970515847206116, -0.24927160143852234, -1.013108253479004, -0.02153243124485016, -0.8761065006256104]
  WEIGHT: [1536, 576] | μ=0.000015

297_model.layers.24.mlp.act_fn: model.layers.24.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.202436 σ=0.393278
    First 10: [0.016141436994075775, 0.737815260887146, -0.49998190999031067, -1.0092025995254517, -0.5337592959403992, 0.5319071412086487, -0.3162623643875122, 0.09798599779605865, -0.02206818014383316, 0.4006209373474121]
    Last 10:  [-0.5592445731163025, -0.4574618935585022, -0.018152162432670593, -0.3723921775817871, -0.5067622661590576, -0.6970515847206116, -0.24927160143852234, -1.013108253479004, -0.02153243124485016, -0.8761065006256104]
  OUT[0]: [1, 5, 1536] | μ=-0.055078 σ=0.165897
    First 10: [0.008135853335261345, 0.4991452693939209, -0.18876563012599945, -0.26959428191185, -0.197299063205719, 0.3350631296634674, -0.13333207368850708, 0.051391392946243286, -0.010912342928349972, 0.23990656435489655]
    Last 10:  [-0.20340970158576965, -0.17730678617954254, -0.008993708528578281, -0.15192227065563202, -0.19051869213581085, -0.2317461222410202, -0.10918164998292923, -0.26986366510391235, -0.010650308802723885, -0.25756239891052246]

298_model.layers.24.mlp.up_proj: model.layers.24.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.003819 σ=0.358905
    First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
    Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
  OUT[0]: [1, 5, 1536] | μ=-0.004803 σ=0.311171
    First 10: [-0.06243281811475754, -0.03320425748825073, 0.0640900582075119, -0.13079240918159485, -0.06173759326338768, 0.1487858146429062, 0.024105902761220932, 0.6352505087852478, -0.5493402481079102, -0.2756800353527069]
    Last 10:  [-0.32803279161453247, -0.5124449133872986, -0.04404153674840927, 0.3406355082988739, 0.23057328164577484, 0.20433661341667175, -0.0625760480761528, 0.38127774000167847, -0.13489696383476257, -0.18704092502593994]
  WEIGHT: [1536, 576] | μ=-0.000038

299_model.layers.24.mlp.down_proj: model.layers.24.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=-0.000543 σ=0.054245
    First 10: [-0.0005079442635178566, -0.016573747619986534, -0.012098000384867191, 0.0352608859539032, 0.012180768884718418, 0.0498526394367218, -0.003214089898392558, 0.032646410167217255, 0.005994589067995548, -0.06613744795322418]
    Last 10:  [0.06672505289316177, 0.09085995703935623, 0.0003960967587772757, -0.051750119775533676, -0.043928518891334534, -0.04735421761870384, 0.006832156330347061, -0.10289300978183746, 0.0014366942923516035, 0.04817470908164978]
  OUT[0]: [1, 5, 576] | μ=0.000277 σ=0.096120
    First 10: [0.035520002245903015, 0.022818995639681816, 0.044538166373968124, -0.04142330586910248, 0.05920391529798508, 0.16030186414718628, 0.05606876313686371, -0.09191669523715973, 0.010938156396150589, 0.020804349333047867]
    Last 10:  [0.025621481239795685, 0.0801917165517807, -0.13025996088981628, 0.15346160531044006, 0.07203134149312973, 0.0707186609506607, 0.1362040638923645, -0.0017171497456729412, 0.0033317506313323975, 0.06908836960792542]
  WEIGHT: [576, 1536] | μ=0.000046

300_model.layers.24.mlp: model.layers.24.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=0.003819 σ=0.358905
    First 10: [-0.0138742346316576, -0.05949316918849945, -0.0041353157721459866, 0.011455397121608257, -0.24803608655929565, -0.09599167108535767, 0.09192774444818497, -0.0036320402286946774, -0.009724067524075508, -0.05611560866236687]
    Last 10:  [-0.08867445588111877, -0.12529532611370087, 0.5729994177818298, -0.5676906704902649, 0.11324744671583176, 0.3081270158290863, -0.25176674127578735, 0.04919849708676338, -0.03730110824108124, -0.36312028765678406]
  OUT[0]: [1, 5, 576] | μ=0.000277 σ=0.096120
    First 10: [0.035520002245903015, 0.022818995639681816, 0.044538166373968124, -0.04142330586910248, 0.05920391529798508, 0.16030186414718628, 0.05606876313686371, -0.09191669523715973, 0.010938156396150589, 0.020804349333047867]
    Last 10:  [0.025621481239795685, 0.0801917165517807, -0.13025996088981628, 0.15346160531044006, 0.07203134149312973, 0.0707186609506607, 0.1362040638923645, -0.0017171497456729412, 0.0033317506313323975, 0.06908836960792542]

301_model.layers.25.input_layernorm: model.layers.25.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.082222 σ=11.978249
    First 10: [-0.49584704637527466, -2.476520538330078, -0.12514632940292358, 0.40772461891174316, -11.430930137634277, -6.666760444641113, 4.652068138122559, -0.24047890305519104, -0.362958699464798, -2.136878252029419]
    Last 10:  [-0.11306046694517136, -0.11624780297279358, 0.788079023361206, -0.7472617626190186, 0.25623223185539246, 0.55765700340271, -0.2434961199760437, 0.07777568697929382, -0.05546926334500313, -0.5354984998703003]
  OUT[0]: [1, 5, 576] | μ=-0.000666 σ=0.381757
    First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
    Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
  WEIGHT: [576] | μ=0.563047

302_model.layers.25.self_attn.q_proj: model.layers.25.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000666 σ=0.381757
    First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
    Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
  OUT[0]: [1, 5, 576] | μ=-0.001547 σ=1.141639
    First 10: [-0.4261331856250763, 0.022199584171175957, -0.12311774492263794, 0.3504631519317627, 0.17115673422813416, 0.8101059794425964, 0.9295114278793335, -0.560868501663208, 0.29431062936782837, -0.2072330117225647]
    Last 10:  [1.6453437805175781, -0.6023577451705933, 2.08278751373291, -2.7431681156158447, -0.6756157279014587, 0.6408123970031738, -0.4929950535297394, 0.16256083548069, -1.7509284019470215, 0.29147717356681824]
  WEIGHT: [576, 576] | μ=-0.000124

303_model.layers.25.self_attn.k_proj: model.layers.25.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000666 σ=0.381757
    First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
    Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
  OUT[0]: [1, 5, 192] | μ=-0.016736 σ=1.203004
    First 10: [0.007772316690534353, 0.01077929139137268, 0.0017339438199996948, 0.008605072274804115, -0.011484041810035706, 0.002602696418762207, 0.003750041127204895, 0.015081807971000671, -0.0021987035870552063, 0.020899023860692978]
    Last 10:  [-0.06104101240634918, -2.9472105503082275, 2.0914993286132812, -2.3398914337158203, -3.185286521911621, -1.6877696514129639, -0.6444486379623413, 2.022949457168579, 1.5233832597732544, -0.7443884611129761]
  WEIGHT: [192, 576] | μ=0.000140

304_model.layers.25.self_attn.v_proj: model.layers.25.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.000666 σ=0.381757
    First 10: [-0.01381408330053091, -0.09335717558860779, -0.004588379990309477, 0.013527668081223965, -0.3084258437156677, -0.39522066712379456, 0.19686925411224365, -0.00885420199483633, -0.010730392299592495, -0.08574004471302032]
    Last 10:  [-0.06424958258867264, -0.08271723985671997, 0.5899518132209778, -0.4854452311992645, 0.14895538985729218, 0.3822476267814636, -0.17252284288406372, 0.0644083172082901, -0.03546210378408432, -0.4603680968284607]
  OUT[0]: [1, 5, 192] | μ=0.017579 σ=0.277501
    First 10: [-0.0016400497406721115, 0.013595083728432655, -0.009687155485153198, -0.007029416039586067, -0.0005712909623980522, 0.013242900371551514, -0.005197100341320038, -0.008372807875275612, 0.007671581115573645, 0.0024570776149630547]
    Last 10:  [-0.020114701241254807, -0.29736828804016113, 0.45363229513168335, -0.1583147644996643, -0.20929983258247375, 0.6248188614845276, -0.08766117691993713, 0.15424302220344543, 0.5982751250267029, -0.13916878402233124]
  WEIGHT: [192, 576] | μ=0.000054

305_model.layers.25.self_attn.o_proj: model.layers.25.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.005277 σ=0.056906
    First 10: [-0.0016400497406721115, 0.013595083728432655, -0.009687155485153198, -0.007029416039586067, -0.0005712909623980522, 0.013242900371551514, -0.005197100341320038, -0.008372807875275612, 0.007671581115573645, 0.0024570776149630547]
    Last 10:  [-0.0016340846195816994, -0.006946050561964512, -0.01437340583652258, -0.04054759442806244, -0.017665838822722435, 0.036401551216840744, 0.027456242591142654, 0.01905916817486286, 0.03235683962702751, -0.002345101675018668]
  OUT[0]: [1, 5, 576] | μ=-0.000867 σ=0.054672
    First 10: [0.09278392791748047, -0.03881445527076721, -0.03408059850335121, -0.0062532005831599236, -0.05229702219367027, -0.05254851654171944, 0.03209899365901947, -0.05480077862739563, -0.040882449597120285, -0.015055214986205101]
    Last 10:  [-0.04049097001552582, -0.02880406752228737, 0.027155382558703423, -0.012123499996960163, 0.041778918355703354, -0.05336150527000427, 0.0019043106585741043, 0.03547871857881546, -0.012475385330617428, -0.0745677500963211]
  WEIGHT: [576, 576] | μ=-0.000045

306_model.layers.25.self_attn: model.layers.25.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000867 σ=0.054672
    First 10: [0.09278392791748047, -0.03881445527076721, -0.03408059850335121, -0.0062532005831599236, -0.05229702219367027, -0.05254851654171944, 0.03209899365901947, -0.05480077862739563, -0.040882449597120285, -0.015055214986205101]
    Last 10:  [-0.04049097001552582, -0.02880406752228737, 0.027155382558703423, -0.012123499996960163, 0.041778918355703354, -0.05336150527000427, 0.0019043106585741043, 0.03547871857881546, -0.012475385330617428, -0.0745677500963211]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.264015
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5001692771911621, 0.4998306930065155, 0.0, 0.0, 0.0]
    Last 10:  [0.47128477692604065, 0.4710506796836853, 0.04585563763976097, 0.011808880604803562, 0.0, 0.44682812690734863, 0.4461047351360321, 0.06272093206644058, 0.015955068171024323, 0.028391094878315926]

307_model.layers.25.post_attention_layernorm: model.layers.25.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.083089 σ=11.985290
    First 10: [-0.4030631184577942, -2.5153350830078125, -0.1592269241809845, 0.40147140622138977, -11.483226776123047, -6.719308853149414, 4.68416690826416, -0.29527968168258667, -0.40384113788604736, -2.151933431625366]
    Last 10:  [-0.15355142951011658, -0.14505186676979065, 0.8152344226837158, -0.7593852877616882, 0.2980111539363861, 0.5042954683303833, -0.24159181118011475, 0.11325440555810928, -0.06794464588165283, -0.6100662350654602]
  OUT[0]: [1, 5, 576] | μ=0.002872 σ=0.359445
    First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
    Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
  WEIGHT: [576] | μ=0.494041

308_model.layers.25.mlp.gate_proj: model.layers.25.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.002872 σ=0.359445
    First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
    Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
  OUT[0]: [1, 5, 1536] | μ=-0.219452 σ=0.402099
    First 10: [0.1971726268529892, -0.19388937950134277, -0.06389018893241882, 0.15800416469573975, -0.1179538369178772, -1.2225337028503418, -0.2434474527835846, -0.6076571941375732, -0.12232544273138046, 0.3323368430137634]
    Last 10:  [-0.5415199995040894, -0.08415818214416504, -0.11723829805850983, -0.40189647674560547, -0.21628065407276154, -0.48351824283599854, -0.7556737065315247, -0.6128374338150024, -0.4286273717880249, -0.14685137569904327]
  WEIGHT: [1536, 576] | μ=0.000084

309_model.layers.25.mlp.act_fn: model.layers.25.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.219452 σ=0.402099
    First 10: [0.1971726268529892, -0.19388937950134277, -0.06389018893241882, 0.15800416469573975, -0.1179538369178772, -1.2225337028503418, -0.2434474527835846, -0.6076571941375732, -0.12232544273138046, 0.3323368430137634]
    Last 10:  [-0.5415199995040894, -0.08415818214416504, -0.11723829805850983, -0.40189647674560547, -0.21628065407276154, -0.48351824283599854, -0.7556737065315247, -0.6128374338150024, -0.4286273717880249, -0.14685137569904327]
  OUT[0]: [1, 5, 1536] | μ=-0.060265 σ=0.170447
    First 10: [0.10827421396970749, -0.0875757485628128, -0.030924955382943153, 0.08523046225309372, -0.055502668023109436, -0.27811527252197266, -0.10697980970144272, -0.2142561674118042, -0.05742650479078293, 0.19352899491786957]
    Last 10:  [-0.19918949902057648, -0.04030948504805565, -0.05518687516450882, -0.16110292077064514, -0.09649136662483215, -0.18442434072494507, -0.24150294065475464, -0.21535855531692505, -0.16907384991645813, -0.0680440291762352]

310_model.layers.25.mlp.up_proj: model.layers.25.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.002872 σ=0.359445
    First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
    Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
  OUT[0]: [1, 5, 1536] | μ=0.000159 σ=0.317446
    First 10: [0.07091669738292694, -0.11660479009151459, 0.42214035987854004, -0.2542322874069214, 0.039477813988924026, -0.37491315603256226, 0.019831696525216103, 0.14064109325408936, -0.10694609582424164, -0.07383676618337631]
    Last 10:  [0.6308544874191284, -0.37563756108283997, -0.04934529960155487, -0.42159026861190796, 0.09808884561061859, -0.16135691106319427, -0.10801555216312408, -0.17016498744487762, -0.04450578987598419, 0.16235703229904175]
  WEIGHT: [1536, 576] | μ=-0.000052

311_model.layers.25.mlp.down_proj: model.layers.25.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000443 σ=0.061079
    First 10: [0.007678449619561434, 0.01021175179630518, -0.013054671697318554, -0.021668335422873497, -0.0021911240182816982, 0.10426907241344452, -0.0021215910091996193, -0.030133221298456192, 0.006141540594398975, -0.01428955513983965]
    Last 10:  [-0.12565958499908447, 0.015141756273806095, 0.002723212819546461, 0.06791942566633224, -0.009464726783335209, 0.029758142307400703, 0.026086073368787766, 0.036646485328674316, 0.007524765096604824, -0.011047426611185074]
  OUT[0]: [1, 5, 576] | μ=-0.002428 σ=0.110176
    First 10: [0.08096560090780258, 0.012979015707969666, 0.006341487169265747, -0.026800407096743584, 0.062232568860054016, 0.10063852369785309, 0.02401314117014408, -0.030360879376530647, 0.01913442835211754, 0.212269589304924]
    Last 10:  [-0.13092075288295746, -0.014723449945449829, -0.1526506543159485, -0.023363303393125534, -0.09001479297876358, 0.14186719059944153, -0.1713767647743225, -0.06416232138872147, -0.013386063277721405, 0.003587535582482815]
  WEIGHT: [576, 1536] | μ=0.000092

312_model.layers.25.mlp: model.layers.25.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=0.002872 σ=0.359445
    First 10: [-0.010776261799037457, -0.06354974955320358, -0.004133803304284811, 0.010837312787771225, -0.25470948219299316, -0.10369578003883362, 0.10178405791521072, -0.00740306731313467, -0.010536516085267067, -0.056228846311569214]
    Last 10:  [-0.09552460163831711, -0.08737501502037048, 0.4924927353858948, -0.46095705032348633, 0.17493027448654175, 0.3198687732219696, -0.14959366619586945, 0.06802397966384888, -0.041282739490270615, -0.3407566547393799]
  OUT[0]: [1, 5, 576] | μ=-0.002428 σ=0.110176
    First 10: [0.08096560090780258, 0.012979015707969666, 0.006341487169265747, -0.026800407096743584, 0.062232568860054016, 0.10063852369785309, 0.02401314117014408, -0.030360879376530647, 0.01913442835211754, 0.212269589304924]
    Last 10:  [-0.13092075288295746, -0.014723449945449829, -0.1526506543159485, -0.023363303393125534, -0.09001479297876358, 0.14186719059944153, -0.1713767647743225, -0.06416232138872147, -0.013386063277721405, 0.003587535582482815]

313_model.layers.26.input_layernorm: model.layers.26.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.085517 σ=11.965765
    First 10: [-0.322097510099411, -2.5023560523986816, -0.15288543701171875, 0.37467101216316223, -11.42099380493164, -6.618670463562012, 4.708179950714111, -0.32564055919647217, -0.3847067058086395, -1.9396638870239258]
    Last 10:  [-0.28447216749191284, -0.15977531671524048, 0.6625837683677673, -0.7827485799789429, 0.20799636840820312, 0.6461626291275024, -0.41296857595443726, 0.04909208416938782, -0.08133070915937424, -0.6064786911010742]
  OUT[0]: [1, 5, 576] | μ=-0.003127 σ=0.379457
    First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
    Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
  WEIGHT: [576] | μ=0.565114

314_model.layers.26.self_attn.q_proj: model.layers.26.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003127 σ=0.379457
    First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
    Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
  OUT[0]: [1, 5, 576] | μ=0.016609 σ=1.112635
    First 10: [-0.021311167627573013, 0.22213736176490784, -0.4895819425582886, 0.10150492191314697, -0.553885281085968, -0.7210612893104553, 0.008784223347902298, -0.3497462868690491, 0.2059897780418396, -0.9525598287582397]
    Last 10:  [-0.7325423955917358, -1.0361206531524658, -0.41487154364585876, 0.2520964443683624, -0.725663959980011, 1.922345757484436, 3.227874279022217, -2.7098734378814697, -4.9439287185668945, 1.5238721370697021]
  WEIGHT: [576, 576] | μ=-0.000033

315_model.layers.26.self_attn.k_proj: model.layers.26.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003127 σ=0.379457
    First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
    Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
  OUT[0]: [1, 5, 192] | μ=0.012143 σ=1.279046
    First 10: [0.005120024085044861, 0.0022451579570770264, -0.0007396042346954346, 0.0080175269395113, -0.010106161236763, 0.015942241996526718, -0.011179491877555847, 0.0036654025316238403, 0.0007702205330133438, -0.026198625564575195]
    Last 10:  [-1.2997297048568726, -0.8487428426742554, 8.742570877075195, 1.1087367534637451, -0.9479537010192871, 0.5693605542182922, -0.6856707334518433, -1.5405899286270142, 2.2890586853027344, -1.3717763423919678]
  WEIGHT: [192, 576] | μ=0.000123

316_model.layers.26.self_attn.v_proj: model.layers.26.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003127 σ=0.379457
    First 10: [-0.00880176480859518, -0.0933518260717392, -0.005881342105567455, 0.013105550780892372, -0.29733139276504517, -0.4065599739551544, 0.1936558485031128, -0.011861971579492092, -0.011815540492534637, -0.0776255801320076]
    Last 10:  [-0.1506700962781906, -0.10130252689123154, 0.4533992409706116, -0.43165725469589233, 0.12084820121526718, 0.39255744218826294, -0.2572733461856842, 0.037036579102277756, -0.04806257784366608, -0.47362393140792847]
  OUT[0]: [1, 5, 192] | μ=0.023115 σ=0.281833
    First 10: [-0.2879851460456848, -0.006393730640411377, 0.006837252527475357, 0.01450863853096962, -0.0017932765185832977, -0.017116744071245193, -0.014376552775502205, 0.0003307778388261795, 0.005851795896887779, -0.004778750240802765]
    Last 10:  [-0.005627982318401337, 0.44351285696029663, -0.35582131147384644, -0.0038735121488571167, 0.7844586372375488, 0.2800220847129822, 0.09418714791536331, -0.6016528010368347, 0.25983425974845886, 0.49603790044784546]
  WEIGHT: [192, 576] | μ=0.000039

317_model.layers.26.self_attn.o_proj: model.layers.26.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.005200 σ=0.095826
    First 10: [-0.2879851460456848, -0.006393730640411377, 0.006837252527475357, 0.01450863853096962, -0.0017932765185832977, -0.017116744071245193, -0.014376552775502205, 0.0003307778388261795, 0.005851795896887779, -0.004778750240802765]
    Last 10:  [-0.0035519334487617016, 0.02178470604121685, -0.0009402817231602967, 0.004095249343663454, 0.041731059551239014, 0.02264028787612915, 0.004501557908952236, -0.029926780611276627, 0.01218757126480341, 0.03811829909682274]
  OUT[0]: [1, 5, 576] | μ=0.003122 σ=0.097781
    First 10: [-0.013690924271941185, -0.0008350880816578865, 0.020384829491376877, -0.001758501399308443, -0.0020978241227567196, -0.061941634863615036, 0.007750852964818478, -0.014810938388109207, 0.009858159348368645, -0.031658802181482315]
    Last 10:  [0.05925004556775093, -0.07908536493778229, 0.3524857759475708, -0.07276730239391327, 0.08172174543142319, -0.037860944867134094, -0.009830759838223457, -0.0029500704258680344, -0.19947470724582672, -0.21566785871982574]
  WEIGHT: [576, 576] | μ=0.000072

318_model.layers.26.self_attn: model.layers.26.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.003122 σ=0.097781
    First 10: [-0.013690924271941185, -0.0008350880816578865, 0.020384829491376877, -0.001758501399308443, -0.0020978241227567196, -0.061941634863615036, 0.007750852964818478, -0.014810938388109207, 0.009858159348368645, -0.031658802181482315]
    Last 10:  [0.05925004556775093, -0.07908536493778229, 0.3524857759475708, -0.07276730239391327, 0.08172174543142319, -0.037860944867134094, -0.009830759838223457, -0.0029500704258680344, -0.19947470724582672, -0.21566785871982574]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.266404
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5000210404396057, 0.4999789893627167, 0.0, 0.0, 0.0]
    Last 10:  [0.4793320596218109, 0.47887924313545227, 0.026855159550905228, 0.01493354793637991, 0.0, 0.4678178131580353, 0.46807965636253357, 0.018439246341586113, 0.007677636109292507, 0.03798556327819824]

319_model.layers.26.post_attention_layernorm: model.layers.26.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.082395 σ=11.968935
    First 10: [-0.33578842878341675, -2.5031912326812744, -0.13250060379505157, 0.3729124963283539, -11.423091888427734, -6.680612087249756, 4.715930938720703, -0.34045150876045227, -0.37484854459762573, -1.9713226556777954]
    Last 10:  [-0.2252221256494522, -0.23886068165302277, 1.0150694847106934, -0.8555158972740173, 0.2897181212902069, 0.6083016991615295, -0.42279934883117676, 0.046142011880874634, -0.28080540895462036, -0.8221465349197388]
  OUT[0]: [1, 5, 576] | μ=0.001887 σ=0.389902
    First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
    Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
  WEIGHT: [576] | μ=0.510730

320_model.layers.26.mlp.gate_proj: model.layers.26.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.001887 σ=0.389902
    First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
    Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
  OUT[0]: [1, 5, 1536] | μ=-0.247667 σ=0.418645
    First 10: [-0.38888368010520935, -1.3792853355407715, 0.08825710415840149, 0.17754776775836945, -0.05420788750052452, -0.7204705476760864, 0.05992819741368294, 0.7282297015190125, -0.631008505821228, -0.9205172061920166]
    Last 10:  [-0.12042336165904999, -0.331166535615921, -0.9038124680519104, -0.23281939327716827, -0.5538129806518555, 0.6899538040161133, -0.7292208075523376, -0.5827463269233704, -0.10384413599967957, -0.9003647565841675]
  WEIGHT: [1536, 576] | μ=0.000069

321_model.layers.26.mlp.act_fn: model.layers.26.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.247667 σ=0.418645
    First 10: [-0.38888368010520935, -1.3792853355407715, 0.08825710415840149, 0.17754776775836945, -0.05420788750052452, -0.7204705476760864, 0.05992819741368294, 0.7282297015190125, -0.631008505821228, -0.9205172061920166]
    Last 10:  [-0.12042336165904999, -0.331166535615921, -0.9038124680519104, -0.23281939327716827, -0.5538129806518555, 0.6899538040161133, -0.7292208075523376, -0.5827463269233704, -0.10384413599967957, -0.9003647565841675]
  OUT[0]: [1, 5, 1536] | μ=-0.068820 σ=0.173378
    First 10: [-0.15710358321666718, -0.27740710973739624, 0.04607461765408516, 0.09663404524326324, -0.026369499042630196, -0.23580235242843628, 0.03086167760193348, 0.4911302626132965, -0.21913783252239227, -0.2622116506099701]
    Last 10:  [-0.05659060925245285, -0.13841331005096436, -0.2605399191379547, -0.10291935503482819, -0.20213079452514648, 0.45947930216789246, -0.23726347088813782, -0.20879852771759033, -0.04922858998179436, -0.2601833939552307]

322_model.layers.26.mlp.up_proj: model.layers.26.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.001887 σ=0.389902
    First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
    Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
  OUT[0]: [1, 5, 1536] | μ=-0.009516 σ=0.354890
    First 10: [-0.2236481010913849, 0.019144611433148384, -0.7300968170166016, 0.11007511615753174, -0.08696450293064117, -0.028518103063106537, 0.36674216389656067, -0.27103719115257263, -0.31252819299697876, -0.13719624280929565]
    Last 10:  [0.057018741965293884, -0.5966054201126099, -0.14395949244499207, 0.6542375683784485, -0.29571178555488586, -0.16868045926094055, -0.3908184766769409, -0.2766566276550293, -0.2813835144042969, 0.3547680377960205]
  WEIGHT: [1536, 576] | μ=-0.000038

323_model.layers.26.mlp.down_proj: model.layers.26.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000334 σ=0.066184
    First 10: [0.035135917365550995, -0.005310851149260998, -0.033638931810855865, 0.010637003928422928, 0.0022932104766368866, 0.0067246356047689915, 0.01131827849894762, -0.13311456143856049, 0.06848675012588501, 0.03597445413470268]
    Last 10:  [-0.00322672538459301, 0.0825781300663948, 0.03750719502568245, -0.06733370572328568, 0.05977245792746544, -0.07750517874956131, 0.0927269458770752, 0.05776549503207207, 0.013852113857865334, -0.09230475127696991]
  OUT[0]: [1, 5, 576] | μ=-0.003252 σ=0.111876
    First 10: [0.0575503334403038, 0.01184755191206932, -0.05154011398553848, -0.044469334185123444, 0.09567449241876602, 0.04824741557240486, -0.013508006930351257, -0.11702915281057358, -0.04365560784935951, -0.025028301402926445]
    Last 10:  [0.0873878002166748, -0.08558624982833862, -0.27438968420028687, 0.055550336837768555, 0.10353484004735947, -0.1279650330543518, -0.18676774203777313, 0.025006741285324097, 0.02349427342414856, -0.06937399506568909]
  WEIGHT: [576, 1536] | μ=0.000123

324_model.layers.26.mlp: model.layers.26.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=0.001887 σ=0.389902
    First 10: [-0.009140169247984886, -0.06664865463972092, -0.00350562809035182, 0.010218165814876556, -0.2579329013824463, -0.11829515546560287, 0.11178844422101974, -0.009240696206688881, -0.010339044034481049, -0.05371040850877762]
    Last 10:  [-0.12447389960289001, -0.1312635987997055, 0.5615299344062805, -0.4696940779685974, 0.1542222797870636, 0.3400006592273712, -0.23874419927597046, 0.025525465607643127, -0.15167608857154846, -0.4329240322113037]
  OUT[0]: [1, 5, 576] | μ=-0.003252 σ=0.111876
    First 10: [0.0575503334403038, 0.01184755191206932, -0.05154011398553848, -0.044469334185123444, 0.09567449241876602, 0.04824741557240486, -0.013508006930351257, -0.11702915281057358, -0.04365560784935951, -0.025028301402926445]
    Last 10:  [0.0873878002166748, -0.08558624982833862, -0.27438968420028687, 0.055550336837768555, 0.10353484004735947, -0.1279650330543518, -0.18676774203777313, 0.025006741285324097, 0.02349427342414856, -0.06937399506568909]

325_model.layers.27.input_layernorm: model.layers.27.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.085647 σ=11.952164
    First 10: [-0.27823808789253235, -2.4913437366485596, -0.18404072523117065, 0.32844316959381104, -11.327417373657227, -6.632364749908447, 4.702423095703125, -0.45748066902160645, -0.41850414872169495, -1.9963510036468506]
    Last 10:  [-0.1378343254327774, -0.3244469165802002, 0.7406798005104065, -0.7999655604362488, 0.393252968788147, 0.48033666610717773, -0.6095671057701111, 0.07114875316619873, -0.2573111355304718, -0.8915205001831055]
  OUT[0]: [1, 5, 576] | μ=-0.002023 σ=0.363523
    First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
    Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
  WEIGHT: [576] | μ=0.553649

326_model.layers.27.self_attn.q_proj: model.layers.27.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.002023 σ=0.363523
    First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
    Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
  OUT[0]: [1, 5, 576] | μ=0.114633 σ=1.183905
    First 10: [0.9597833752632141, -0.6753027439117432, -0.6546114683151245, 0.9736368060112, 0.7561737298965454, 1.0348362922668457, 0.9168868064880371, -0.47723090648651123, -0.8250890970230103, -0.5598512887954712]
    Last 10:  [0.3173826336860657, 0.7274330854415894, 0.44367632269859314, -0.8808079957962036, 0.9024897813796997, 0.23321092128753662, -1.631101131439209, -1.7187466621398926, -1.1417286396026611, 2.1076574325561523]
  WEIGHT: [576, 576] | μ=-0.000154

327_model.layers.27.self_attn.k_proj: model.layers.27.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.002023 σ=0.363523
    First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
    Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
  OUT[0]: [1, 5, 192] | μ=0.076987 σ=1.231408
    First 10: [0.007489114999771118, -0.030970722436904907, -0.009985923767089844, -0.00609879195690155, 0.015028825029730797, -0.003424830734729767, 0.012594595551490784, -0.01508418470621109, 0.0012328624725341797, 0.0043955594301223755]
    Last 10:  [1.3112461566925049, -8.007617950439453, 1.4307982921600342, 2.168120861053467, -0.22792783379554749, -0.983596920967102, -0.16879892349243164, 1.7389380931854248, -1.717474341392517, -0.2386631965637207]
  WEIGHT: [192, 576] | μ=-0.000147

328_model.layers.27.self_attn.v_proj: model.layers.27.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.002023 σ=0.363523
    First 10: [-0.007193147204816341, -0.08353292942047119, -0.006428063847124577, 0.010238610208034515, -0.2614607810974121, -0.36679545044898987, 0.18944606184959412, -0.014580925926566124, -0.011940861120820045, -0.07350058108568192]
    Last 10:  [-0.06994348019361496, -0.17329607903957367, 0.43843650817871094, -0.424121618270874, 0.20072048902511597, 0.27175065875053406, -0.34968188405036926, 0.04201018065214157, -0.13235144317150116, -0.5594412088394165]
  OUT[0]: [1, 5, 192] | μ=-0.018923 σ=0.294029
    First 10: [0.00789564661681652, -0.024062111973762512, -0.012917366810142994, -0.02527754381299019, 0.004070125054568052, 0.0023087509907782078, 0.002987094223499298, 0.0062078190967440605, -0.007357854396104813, -0.009961077012121677]
    Last 10:  [1.1650665998458862, 0.4366189241409302, -0.3104100525379181, 0.34271952509880066, -0.0005662739276885986, -0.018488243222236633, -0.4691067039966583, -0.0072876811027526855, 0.3638358414173126, -0.1827058345079422]
  WEIGHT: [192, 576] | μ=-0.000015

329_model.layers.27.self_attn.o_proj: model.layers.27.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.008217 σ=0.105770
    First 10: [0.00789564661681652, -0.024062111973762512, -0.012917366810142994, -0.02527754381299019, 0.004070125054568052, 0.0023087509907782078, 0.002987094223499298, 0.0062078190967440605, -0.007357854396104813, -0.009961077012121677]
    Last 10:  [0.06748168915510178, 0.01946718618273735, -0.016012661159038544, -0.00019510694255586714, -0.016799969598650932, 0.01139384601265192, -0.04447246715426445, -0.028189679607748985, -0.011893120594322681, -0.027149582281708717]
  OUT[0]: [1, 5, 576] | μ=-0.010562 σ=0.137762
    First 10: [0.007923866622149944, 0.011928820982575417, -0.05008391663432121, -0.09599346667528152, -0.11439596861600876, -0.15517084300518036, 0.039772916585206985, -0.01264312956482172, 0.04029567167162895, -0.02343432419002056]
    Last 10:  [0.07478094100952148, 0.07667869329452515, 0.31073683500289917, -0.016903679817914963, 0.006944466382265091, -0.13665689527988434, -0.17881113290786743, 0.2652333676815033, -0.05741901323199272, -0.08582337945699692]
  WEIGHT: [576, 576] | μ=0.000081

330_model.layers.27.self_attn: model.layers.27.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.010562 σ=0.137762
    First 10: [0.007923866622149944, 0.011928820982575417, -0.05008391663432121, -0.09599346667528152, -0.11439596861600876, -0.15517084300518036, 0.039772916585206985, -0.01264312956482172, 0.04029567167162895, -0.02343432419002056]
    Last 10:  [0.07478094100952148, 0.07667869329452515, 0.31073683500289917, -0.016903679817914963, 0.006944466382265091, -0.13665689527988434, -0.17881113290786743, 0.2652333676815033, -0.05741901323199272, -0.08582337945699692]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.252881
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49851638078689575, 0.5014836192131042, 0.0, 0.0, 0.0]
    Last 10:  [0.48121389746665955, 0.4798073172569275, 0.015549310483038425, 0.02342957817018032, 0.0, 0.4620146155357361, 0.4607367217540741, 0.011911962181329727, 0.024408554658293724, 0.040928225964307785]

331_model.layers.27.post_attention_layernorm: model.layers.27.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.096209 σ=11.960194
    First 10: [-0.27031421661376953, -2.479414939880371, -0.23412464559078217, 0.2324497103691101, -11.441813468933105, -6.787535667419434, 4.742196083068848, -0.4701237976551056, -0.3782084882259369, -2.0197854042053223]
    Last 10:  [-0.06305338442325592, -0.24776822328567505, 1.0514166355133057, -0.8168692588806152, 0.40019744634628296, 0.3436797857284546, -0.7883782386779785, 0.336382120847702, -0.31473013758659363, -0.9773438572883606]
  OUT[0]: [1, 5, 576] | μ=-0.003049 σ=0.384611
    First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
    Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
  WEIGHT: [576] | μ=0.515895

332_model.layers.27.mlp.gate_proj: model.layers.27.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003049 σ=0.384611
    First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
    Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
  OUT[0]: [1, 5, 1536] | μ=-0.223669 σ=0.447516
    First 10: [-0.06841742247343063, -0.39616748690605164, -0.32114291191101074, 0.06356251984834671, -0.3092915713787079, 0.09342837333679199, 0.15901362895965576, -0.22823604941368103, -0.612730860710144, 0.42952796816825867]
    Last 10:  [-0.3862904906272888, -0.2690432369709015, -1.252345085144043, -0.43416717648506165, -0.3731456995010376, -0.31602782011032104, -0.4834897220134735, -0.08001965284347534, -0.48378652334213257, -1.11398184299469]
  WEIGHT: [1536, 576] | μ=0.000123

333_model.layers.27.mlp.act_fn: model.layers.27.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.223669 σ=0.447516
    First 10: [-0.06841742247343063, -0.39616748690605164, -0.32114291191101074, 0.06356251984834671, -0.3092915713787079, 0.09342837333679199, 0.15901362895965576, -0.22823604941368103, -0.612730860710144, 0.42952796816825867]
    Last 10:  [-0.3862904906272888, -0.2690432369709015, -1.252345085144043, -0.43416717648506165, -0.3731456995010376, -0.31602782011032104, -0.4834897220134735, -0.08001965284347534, -0.48378652334213257, -1.11398184299469]
  OUT[0]: [1, 5, 1536] | μ=-0.054549 σ=0.214315
    First 10: [-0.03303893283009529, -0.15935182571411133, -0.1350075900554657, 0.03279096633195877, -0.13091930747032166, 0.048894815146923065, 0.08581486344337463, -0.10115133225917816, -0.21533599495887756, 0.26019126176834106]
    Last 10:  [-0.1562972366809845, -0.11653392016887665, -0.2783893644809723, -0.17068487405776978, -0.15216177701950073, -0.13325126469135284, -0.184416726231575, -0.038409896194934845, -0.18449606001377106, -0.2752975523471832]

334_model.layers.27.mlp.up_proj: model.layers.27.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003049 σ=0.384611
    First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
    Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
  OUT[0]: [1, 5, 1536] | μ=-0.007425 σ=0.371882
    First 10: [0.11251769959926605, 0.007027017883956432, -0.6513623595237732, 0.8430122137069702, 0.23220808804035187, -0.1367376446723938, -0.2646061182022095, -0.20783928036689758, 0.578169584274292, -0.2442985475063324]
    Last 10:  [-0.1232791543006897, 0.23765291273593903, -0.14259353280067444, -0.9280157685279846, 0.17850349843502045, -0.3006230294704437, -0.05015864968299866, -0.25989896059036255, -0.3430526852607727, 0.18115012347698212]
  WEIGHT: [1536, 576] | μ=0.000125

335_model.layers.27.mlp.down_proj: model.layers.27.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.000806 σ=0.082226
    First 10: [-0.003717464627698064, -0.0011197681305930018, 0.08793886005878448, 0.02764318510890007, -0.03040052205324173, -0.006685761734843254, -0.02270713821053505, 0.02102321945130825, -0.12450072169303894, -0.06356434524059296]
    Last 10:  [0.01926819048821926, -0.02769462577998638, 0.03969652205705643, 0.15839825570583344, -0.027161410078406334, 0.040058400481939316, 0.009250094182789326, 0.009982692077755928, 0.06329187005758286, -0.04987018555402756]
  OUT[0]: [1, 5, 576] | μ=0.001135 σ=0.135571
    First 10: [-0.061802081763744354, -0.14234863221645355, 0.040890883654356, -0.029381342232227325, -0.0033243298530578613, 0.12918423116207123, 0.06904754042625427, 0.1607809215784073, 0.004375232383608818, 0.1423788070678711]
    Last 10:  [0.014956608414649963, -0.00470835343003273, -0.11729138344526291, -0.18640510737895966, -0.08044008910655975, 0.05937131494283676, -0.16532473266124725, 0.034114912152290344, -0.06407684087753296, 0.0784221887588501]
  WEIGHT: [576, 1536] | μ=-0.000095

336_model.layers.27.mlp: model.layers.27.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.003049 σ=0.384611
    First 10: [-0.007248335517942905, -0.06552161276340485, -0.006380947306752205, 0.006485708523541689, -0.2598673105239868, -0.13316527009010315, 0.11015992611646652, -0.01273997500538826, -0.010611338540911674, -0.05368899926543236]
    Last 10:  [-0.030628196895122528, -0.12057780474424362, 0.508821964263916, -0.3890286386013031, 0.19059142470359802, 0.1714545041322708, -0.38438254594802856, 0.1617226004600525, -0.15416255593299866, -0.4579320251941681]
  OUT[0]: [1, 5, 576] | μ=0.001135 σ=0.135571
    First 10: [-0.061802081763744354, -0.14234863221645355, 0.040890883654356, -0.029381342232227325, -0.0033243298530578613, 0.12918423116207123, 0.06904754042625427, 0.1607809215784073, 0.004375232383608818, 0.1423788070678711]
    Last 10:  [0.014956608414649963, -0.00470835343003273, -0.11729138344526291, -0.18640510737895966, -0.08044008910655975, 0.05937131494283676, -0.16532473266124725, 0.034114912152290344, -0.06407684087753296, 0.0784221887588501]

337_model.layers.28.input_layernorm: model.layers.28.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.095075 σ=11.926068
    First 10: [-0.3321163058280945, -2.6217634677886963, -0.19323375821113586, 0.20306837558746338, -11.445137977600098, -6.658351421356201, 4.811243534088135, -0.3093428611755371, -0.3738332688808441, -1.8774065971374512]
    Last 10:  [-0.04809677600860596, -0.2524765729904175, 0.9341252446174622, -1.0032743215560913, 0.319757342338562, 0.40305110812187195, -0.953702986240387, 0.37049704790115356, -0.3788069784641266, -0.8989216685295105]
  OUT[0]: [1, 5, 576] | μ=-0.006960 σ=0.374844
    First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
    Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
  WEIGHT: [576] | μ=0.594132

338_model.layers.28.self_attn.q_proj: model.layers.28.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006960 σ=0.374844
    First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
    Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
  OUT[0]: [1, 5, 576] | μ=-0.073946 σ=1.154610
    First 10: [0.6149311065673828, -0.4805939495563507, -0.067977674305439, 0.4206625521183014, -0.7993888258934021, -0.2012237310409546, 0.18426552414894104, 0.26233819127082825, 0.4105304777622223, 0.9626890420913696]
    Last 10:  [0.2220304310321808, -1.7292859554290771, -4.643119812011719, -0.7658923268318176, -0.8189964294433594, -0.6285178065299988, 2.1022450923919678, -1.807969093322754, 0.10805679857730865, 1.7081286907196045]
  WEIGHT: [576, 576] | μ=0.000098

339_model.layers.28.self_attn.k_proj: model.layers.28.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006960 σ=0.374844
    First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
    Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
  OUT[0]: [1, 5, 192] | μ=-0.130582 σ=1.410843
    First 10: [0.0035872086882591248, -0.009303197264671326, 0.004126459360122681, -0.01750504970550537, -0.00853826105594635, 0.010820655152201653, 0.013142785057425499, 0.006411999464035034, 0.011238276958465576, -0.027142062783241272]
    Last 10:  [-2.664684772491455, -0.28530246019363403, 1.4246898889541626, -0.07578302919864655, -0.8041360378265381, -0.43867960572242737, -0.869949460029602, 0.8943350315093994, -0.9220335483551025, -1.4576408863067627]
  WEIGHT: [192, 576] | μ=0.000128

340_model.layers.28.self_attn.v_proj: model.layers.28.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.006960 σ=0.374844
    First 10: [-0.009105467237532139, -0.08624188601970673, -0.006933286786079407, 0.0075234039686620235, -0.3232942819595337, -0.2408052384853363, 0.16513416171073914, -0.010007054544985294, -0.012840617448091507, -0.06555846333503723]
    Last 10:  [-0.025299305096268654, -0.1243814155459404, 0.5468074083328247, -0.502517819404602, 0.16209900379180908, 0.19733880460262299, -0.487192839384079, 0.19713197648525238, -0.21090896427631378, -0.45842915773391724]
  OUT[0]: [1, 5, 192] | μ=-0.008993 σ=0.298576
    First 10: [0.01655512861907482, 0.010838732123374939, -0.005177323706448078, -0.02836153469979763, 0.0131099047139287, -0.029212556779384613, 0.0023323632776737213, -0.009417103603482246, 0.014474233612418175, 0.0006571020931005478]
    Last 10:  [0.43392443656921387, 0.65770024061203, -0.5855507850646973, 0.0868016853928566, -0.2170192301273346, -0.005132589489221573, 0.10735204070806503, -0.2543153166770935, -0.29783421754837036, -0.18187406659126282]
  WEIGHT: [192, 576] | μ=0.000065

341_model.layers.28.self_attn.o_proj: model.layers.28.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.002733 σ=0.095840
    First 10: [0.01655512861907482, 0.010838732123374939, -0.005177323706448078, -0.02836153469979763, 0.0131099047139287, -0.029212556779384613, 0.0023323632776737213, -0.009417103603482246, 0.014474233612418175, 0.0006571020931005478]
    Last 10:  [0.05649228394031525, 0.09885520488023758, -0.054442763328552246, 0.06785283982753754, -0.04014575108885765, -0.001224550069309771, -0.0007578872609883547, -0.04289951175451279, -0.06814179569482803, 0.00038398068863898516]
  OUT[0]: [1, 5, 576] | μ=-0.000712 σ=0.089222
    First 10: [-0.023951619863510132, -0.005104107316583395, 0.13757923245429993, -0.009691664017736912, 0.04752480238676071, -0.0004134867340326309, -0.07798007130622864, -0.029172945767641068, 0.04693637043237686, -0.08623510599136353]
    Last 10:  [0.05704334378242493, -0.024426601827144623, 0.05350762605667114, 0.020676258951425552, 0.05379326269030571, -0.24131079018115997, 0.020830443128943443, 0.12403851747512817, 0.10358887165784836, -0.07847173511981964]
  WEIGHT: [576, 576] | μ=-0.000039

342_model.layers.28.self_attn: model.layers.28.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=-0.000712 σ=0.089222
    First 10: [-0.023951619863510132, -0.005104107316583395, 0.13757923245429993, -0.009691664017736912, 0.04752480238676071, -0.0004134867340326309, -0.07798007130622864, -0.029172945767641068, 0.04693637043237686, -0.08623510599136353]
    Last 10:  [0.05704334378242493, -0.024426601827144623, 0.05350762605667114, 0.020676258951425552, 0.05379326269030571, -0.24131079018115997, 0.020830443128943443, 0.12403851747512817, 0.10358887165784836, -0.07847173511981964]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.252674
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.49974972009658813, 0.5002503395080566, 0.0, 0.0, 0.0]
    Last 10:  [0.4116165339946747, 0.41173818707466125, 0.06966163218021393, 0.10698362439870834, 0.0, 0.40154793858528137, 0.4018951952457428, 0.028445212170481682, 0.052452102303504944, 0.11565953493118286]

343_model.layers.28.post_attention_layernorm: model.layers.28.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.095786 σ=11.926423
    First 10: [-0.3560679256916046, -2.6268675327301025, -0.05565452575683594, 0.19337670505046844, -11.397613525390625, -6.658764839172363, 4.7332634925842285, -0.3385158181190491, -0.32689690589904785, -1.96364164352417]
    Last 10:  [0.00894656777381897, -0.2769031822681427, 0.9876328706741333, -0.982598066329956, 0.3735505938529968, 0.16174031794071198, -0.9328725337982178, 0.49453556537628174, -0.27521809935569763, -0.977393388748169]
  OUT[0]: [1, 5, 576] | μ=-0.001621 σ=0.398450
    First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
    Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
  WEIGHT: [576] | μ=0.532200

344_model.layers.28.mlp.gate_proj: model.layers.28.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001621 σ=0.398450
    First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
    Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
  OUT[0]: [1, 5, 1536] | μ=-0.264424 σ=0.506641
    First 10: [0.12897004187107086, -0.39608830213546753, -0.2803894579410553, -0.3247537910938263, 0.08042105287313461, -0.09660046547651291, 0.08362066000699997, 0.010287169367074966, 0.07964073121547699, 0.3445656895637512]
    Last 10:  [-0.35239189863204956, -0.29913103580474854, -0.09088311344385147, -0.14011387526988983, 0.3769327998161316, -0.9975695610046387, -0.5995365977287292, -0.6305609941482544, 0.6636433601379395, 0.3247942626476288]
  WEIGHT: [1536, 576] | μ=0.000336

345_model.layers.28.mlp.act_fn: model.layers.28.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.264424 σ=0.506641
    First 10: [0.12897004187107086, -0.39608830213546753, -0.2803894579410553, -0.3247537910938263, 0.08042105287313461, -0.09660046547651291, 0.08362066000699997, 0.010287169367074966, 0.07964073121547699, 0.3445656895637512]
    Last 10:  [-0.35239189863204956, -0.29913103580474854, -0.09088311344385147, -0.14011387526988983, 0.3769327998161316, -0.9975695610046387, -0.5995365977287292, -0.6305609941482544, 0.6636433601379395, 0.3247942626476288]
  OUT[0]: [1, 5, 1536] | μ=-0.061144 σ=0.256378
    First 10: [0.06863758713006973, -0.15932752192020416, -0.12066793441772461, -0.1362399458885193, 0.04182653874158859, -0.04596912860870361, 0.04355741664767265, 0.005170040763914585, 0.04140518978238106, 0.20167399942874908]
    Last 10:  [-0.14546826481819153, -0.127360999584198, -0.04337804391980171, -0.0651569738984108, 0.22357133030891418, -0.2687647342681885, -0.21250556409358978, -0.21904638409614563, 0.4380565881729126, 0.1885405331850052]

346_model.layers.28.mlp.up_proj: model.layers.28.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.001621 σ=0.398450
    First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
    Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
  OUT[0]: [1, 5, 1536] | μ=0.007951 σ=0.432298
    First 10: [0.05869648978114128, -0.018466472625732422, 0.22964343428611755, 0.10962295532226562, 0.25922828912734985, 0.12456062436103821, 0.05788126215338707, -0.36263954639434814, 0.7943211197853088, -0.31878596544265747]
    Last 10:  [0.7832505702972412, -0.24361921846866608, -0.44514787197113037, -1.2266592979431152, 0.3153582811355591, -0.12884919345378876, -0.302031934261322, -0.289600133895874, -0.30363649129867554, -0.3482217490673065]
  WEIGHT: [1536, 576] | μ=-0.000071

347_model.layers.28.mlp.down_proj: model.layers.28.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.018814 σ=1.276110
    First 10: [0.00402878550812602, 0.002942217281088233, -0.027710597962141037, -0.014935025945305824, 0.010842622257769108, -0.005725943483412266, 0.0025211581960320473, -0.0018748611910268664, 0.03288901597261429, -0.0642908439040184]
    Last 10:  [-0.1139381006360054, 0.03102758713066578, 0.019309643656015396, 0.07992541044950485, 0.0705050677061081, 0.03463011980056763, 0.06418346613645554, 0.063435859978199, -0.13300997018814087, -0.06565391272306442]
  OUT[0]: [1, 5, 576] | μ=0.052723 σ=1.842718
    First 10: [-1.5671627521514893, -2.2775650024414062, -1.4149456024169922, 1.237135887145996, -0.7577803134918213, -0.6648438572883606, 2.409255266189575, 0.5028420686721802, -0.9213648438453674, -0.6347841620445251]
    Last 10:  [0.47384971380233765, -0.034696534276008606, 0.3225051462650299, 0.49267566204071045, -0.16220074892044067, -0.31065380573272705, 0.3274558186531067, 0.05742955580353737, -0.03630128875374794, 0.24124374985694885]
  WEIGHT: [576, 1536] | μ=0.000030

348_model.layers.28.mlp: model.layers.28.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=-0.001621 σ=0.398450
    First 10: [-0.010244004428386688, -0.07325538992881775, -0.001546256011351943, 0.005598557647317648, -0.2968328595161438, -0.15630009770393372, 0.12548261880874634, -0.009325915947556496, -0.009133142419159412, -0.05562680587172508]
    Last 10:  [0.003943460527807474, -0.12823748588562012, 0.42825284600257874, -0.43683528900146484, 0.16669967770576477, 0.07674428075551987, -0.43005993962287903, 0.22631676495075226, -0.12861685454845428, -0.44687747955322266]
  OUT[0]: [1, 5, 576] | μ=0.052723 σ=1.842718
    First 10: [-1.5671627521514893, -2.2775650024414062, -1.4149456024169922, 1.237135887145996, -0.7577803134918213, -0.6648438572883606, 2.409255266189575, 0.5028420686721802, -0.9213648438453674, -0.6347841620445251]
    Last 10:  [0.47384971380233765, -0.034696534276008606, 0.3225051462650299, 0.49267566204071045, -0.16220074892044067, -0.31065380573272705, 0.3274558186531067, 0.05742955580353737, -0.03630128875374794, 0.24124374985694885]

349_model.layers.29.input_layernorm: model.layers.29.input_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.043063 σ=10.273553
    First 10: [-1.9232306480407715, -4.90443229675293, -1.4706001281738281, 1.430512547492981, -12.155393600463867, -7.323608875274658, 7.142518997192383, 0.1643262505531311, -1.2482616901397705, -2.59842586517334]
    Last 10:  [0.4827962815761566, -0.3115997314453125, 1.3101379871368408, -0.4899224042892456, 0.21134984493255615, -0.14891348779201508, -0.6054167151451111, 0.5519651174545288, -0.3115193843841553, -0.7361496686935425]
  OUT[0]: [1, 5, 576] | μ=0.000230 σ=0.341509
    First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
    Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
  WEIGHT: [576] | μ=0.584867

350_model.layers.29.self_attn.q_proj: model.layers.29.self_attn.q_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.000230 σ=0.341509
    First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
    Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
  OUT[0]: [1, 5, 576] | μ=-0.051633 σ=1.045315
    First 10: [0.2336878478527069, 0.3683914542198181, -0.26508623361587524, -0.531550407409668, 0.14748455584049225, -0.6213107109069824, -0.13117972016334534, 0.5304543972015381, 0.38900327682495117, 0.32231318950653076]
    Last 10:  [-0.2202601432800293, 0.446963906288147, -4.681027412414551, 0.31972789764404297, 1.186026930809021, 1.578558325767517, 0.9774452447891235, -1.9583154916763306, 1.9326210021972656, 0.23209038376808167]
  WEIGHT: [576, 576] | μ=0.000088

351_model.layers.29.self_attn.k_proj: model.layers.29.self_attn.k_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.000230 σ=0.341509
    First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
    Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
  OUT[0]: [1, 5, 192] | μ=0.002538 σ=1.012784
    First 10: [-0.00937582366168499, -0.038926564157009125, 0.005319289863109589, -0.0030039921402931213, 0.01790553703904152, 0.000886041671037674, -0.01605241373181343, -0.0011844933032989502, -0.027588024735450745, -0.006459102034568787]
    Last 10:  [-0.42949652671813965, 0.7413684725761414, 0.9240916967391968, -0.9947003126144409, -1.2200775146484375, 0.0010367855429649353, 1.6633961200714111, 1.2300827503204346, 0.3265140950679779, 1.193665862083435]
  WEIGHT: [192, 576] | μ=-0.000026

352_model.layers.29.self_attn.v_proj: model.layers.29.self_attn.v_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.000230 σ=0.341509
    First 10: [-0.056317634880542755, -0.20283859968185425, -0.059533946216106415, 0.0541108064353466, -0.2988831102848053, -0.3674493134021759, 0.2979893386363983, 0.005759442690759897, -0.04397621005773544, -0.11264348030090332]
    Last 10:  [0.20785894989967346, -0.1465705782175064, 0.5978374481201172, -0.21992294490337372, 0.08909362554550171, -0.06876615434885025, -0.2880878448486328, 0.24971430003643036, -0.14604595303535461, -0.32843953371047974]
  OUT[0]: [1, 5, 192] | μ=-0.003787 σ=0.249379
    First 10: [0.04828491434454918, -0.0006453683599829674, -0.003548063337802887, 0.012386120855808258, -0.019839972257614136, -0.004468521568924189, 0.004066263325512409, -0.0026262961328029633, -0.009701646864414215, -0.008686428889632225]
    Last 10:  [0.5079941749572754, -0.5518670082092285, -0.00813090056180954, 0.7327921390533447, 0.34959426522254944, 0.4754205644130707, 0.12692445516586304, 0.349627822637558, 0.1239483430981636, -0.4577659070491791]
  WEIGHT: [192, 576] | μ=0.000220

353_model.layers.29.self_attn.o_proj: model.layers.29.self_attn.o_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=-0.003314 σ=0.072479
    First 10: [0.04828491434454918, -0.0006453683599829674, -0.003548063337802887, 0.012386120855808258, -0.019839972257614136, -0.004468521568924189, 0.004066263325512409, -0.0026262961328029633, -0.009701646864414215, -0.008686428889632225]
    Last 10:  [0.01654069311916828, -0.17360030114650726, 0.014826632104814053, 0.1264973282814026, 0.04803786426782608, 0.032845038920640945, -0.029415013268589973, 0.0766880214214325, 0.06580642610788345, -0.04277350753545761]
  OUT[0]: [1, 5, 576] | μ=0.007071 σ=0.121768
    First 10: [-0.03164166212081909, -0.012140659615397453, -0.02638525702059269, -0.02672414854168892, -0.09785962104797363, -0.02832343615591526, -0.2260829359292984, 0.004762471653521061, -0.049237266182899475, -0.05203139781951904]
    Last 10:  [-0.06317279487848282, -0.03762347251176834, 0.04144430533051491, 0.038063451647758484, 0.021388234570622444, 0.04907140135765076, 0.13077114522457123, 0.06544061750173569, -0.09651215374469757, -0.013896534219384193]
  WEIGHT: [576, 576] | μ=0.000014

354_model.layers.29.self_attn: model.layers.29.self_attn (LlamaSdpaAttention)
  OUT[0]: [1, 5, 576] | μ=0.007071 σ=0.121768
    First 10: [-0.03164166212081909, -0.012140659615397453, -0.02638525702059269, -0.02672414854168892, -0.09785962104797363, -0.02832343615591526, -0.2260829359292984, 0.004762471653521061, -0.049237266182899475, -0.05203139781951904]
    Last 10:  [-0.06317279487848282, -0.03762347251176834, 0.04144430533051491, 0.038063451647758484, 0.021388234570622444, 0.04907140135765076, 0.13077114522457123, 0.06544061750173569, -0.09651215374469757, -0.013896534219384193]
  OUT[1]: [1, 9, 5, 5] | μ=0.200000 σ=0.254665
    First 10: [1.0, 0.0, 0.0, 0.0, 0.0, 0.5004143118858337, 0.49958574771881104, 0.0, 0.0, 0.0]
    Last 10:  [0.4367995262145996, 0.43619590997695923, 0.08444936573505402, 0.042555153369903564, 0.0, 0.38690683245658875, 0.38735440373420715, 0.10769648104906082, 0.0684686228632927, 0.04957360401749611]

355_model.layers.29.post_attention_layernorm: model.layers.29.post_attention_layernorm (LlamaRMSNorm)
  IN[0]:  [1, 5, 576] | μ=-0.035992 σ=10.270823
    First 10: [-1.9548723697662354, -4.9165730476379395, -1.4969854354858398, 1.403788447380066, -12.253252983093262, -7.351932525634766, 6.916436195373535, 0.1690887212753296, -1.2974989414215088, -2.6504573822021484]
    Last 10:  [0.4196234941482544, -0.34922319650650024, 1.3515822887420654, -0.4518589377403259, 0.23273807764053345, -0.09984208643436432, -0.47464555501937866, 0.6174057126045227, -0.40803152322769165, -0.7500461935997009]
  OUT[0]: [1, 5, 576] | μ=0.004375 σ=0.387368
    First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
    Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
  WEIGHT: [576] | μ=0.502549

356_model.layers.29.mlp.gate_proj: model.layers.29.mlp.gate_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.004375 σ=0.387368
    First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
    Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
  OUT[0]: [1, 5, 1536] | μ=-0.211077 σ=0.662286
    First 10: [-0.5671274065971375, -1.2409058809280396, -0.29080745577812195, -0.03479953855276108, 0.4953216016292572, -0.4618515968322754, -0.6326705813407898, 0.23212365806102753, -0.5857346653938293, 0.6488878130912781]
    Last 10:  [-0.3486367464065552, -0.030581653118133545, -1.0788029432296753, -0.7312259674072266, -0.34351158142089844, -0.14589543640613556, -0.04152501001954079, 0.43124714493751526, -0.5620436668395996, -0.9064484238624573]
  WEIGHT: [1536, 576] | μ=0.000235

357_model.layers.29.mlp.act_fn: model.layers.29.mlp.act_fn (SiLU)
  IN[0]:  [1, 5, 1536] | μ=-0.211077 σ=0.662286
    First 10: [-0.5671274065971375, -1.2409058809280396, -0.29080745577812195, -0.03479953855276108, 0.4953216016292572, -0.4618515968322754, -0.6326705813407898, 0.23212365806102753, -0.5857346653938293, 0.6488878130912781]
    Last 10:  [-0.3486367464065552, -0.030581653118133545, -1.0788029432296753, -0.7312259674072266, -0.34351158142089844, -0.14589543640613556, -0.04152501001954079, 0.43124714493751526, -0.5620436668395996, -0.9064484238624573]
  OUT[0]: [1, 5, 1536] | μ=-0.016233 σ=0.425680
    First 10: [-0.20524336397647858, -0.27830833196640015, -0.12440923601388931, -0.017097048461437225, 0.3077726662158966, -0.17852719128131866, -0.21947674453258514, 0.129472017288208, -0.20946697890758514, 0.4261634349822998]
    Last 10:  [-0.14423556625843048, -0.015057035721838474, -0.2737274765968323, -0.2375941425561905, -0.14254245162010193, -0.06763576716184616, -0.020331483334302902, 0.2614096999168396, -0.20406386256217957, -0.2608098089694977]

358_model.layers.29.mlp.up_proj: model.layers.29.mlp.up_proj (Linear)
  IN[0]:  [1, 5, 576] | μ=0.004375 σ=0.387368
    First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
    Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
  OUT[0]: [1, 5, 1536] | μ=0.007754 σ=0.650116
    First 10: [-0.6334491968154907, -0.4209849238395691, -0.22874568402767181, 0.2435075044631958, -0.22569306194782257, -0.4182506799697876, 0.3145217001438141, -1.746135950088501, -0.12157414853572845, -0.2710261344909668]
    Last 10:  [-0.5976570844650269, 0.575960636138916, 0.23476943373680115, -0.21857641637325287, -0.19871574640274048, -0.24691295623779297, -0.044295236468315125, -0.5169350504875183, -0.7826733589172363, -0.6185628771781921]
  WEIGHT: [1536, 576] | μ=0.000084

359_model.layers.29.mlp.down_proj: model.layers.29.mlp.down_proj (Linear)
  IN[0]:  [1, 5, 1536] | μ=0.015903 σ=3.356709
    First 10: [0.13001124560832977, 0.11716361343860626, 0.02845807559788227, -0.004163259640336037, -0.06946215778589249, 0.07466912269592285, -0.06903019547462463, -0.22607573866844177, 0.025465769693255424, -0.11550142616033554]
    Last 10:  [0.08620341122150421, -0.008672259747982025, -0.06426284462213516, 0.051932476460933685, 0.028325429186224937, 0.016700146719813347, 0.0009005878819152713, -0.1351318359375, 0.15971535444259644, 0.1613272726535797]
  OUT[0]: [1, 5, 576] | μ=0.048424 σ=10.158028
    First 10: [2.0837559700012207, 5.01254415512085, 1.7442330121994019, -1.349542498588562, 11.678293228149414, 7.412636756896973, -6.787923812866211, -0.03134290128946304, 1.1584196090698242, 2.5779271125793457]
    Last 10:  [-0.4681185781955719, 0.24617637693881989, 0.013099242001771927, 0.4093452990055084, -0.0956038311123848, -0.06480144709348679, 0.044930458068847656, -0.15281924605369568, -0.18177638947963715, 0.47355028986930847]
  WEIGHT: [576, 1536] | μ=-0.000026

360_model.layers.29.mlp: model.layers.29.mlp (LlamaMLP)
  IN[0]:  [1, 5, 576] | μ=0.004375 σ=0.387368
    First 10: [-0.04950914531946182, -0.14857426285743713, -0.04250200092792511, 0.04307837784290314, -0.35806870460510254, -0.20040692389011383, 0.213604137301445, 0.005035649985074997, -0.040365301072597504, -0.08621851354837418]
    Last 10:  [0.16959339380264282, -0.1395622193813324, 0.46326905488967896, -0.16560181975364685, 0.09230924397706985, -0.04042699933052063, -0.18172982335090637, 0.24859805405139923, -0.1546112596988678, -0.2926827669143677]
  OUT[0]: [1, 5, 576] | μ=0.048424 σ=10.158028
    First 10: [2.0837559700012207, 5.01254415512085, 1.7442330121994019, -1.349542498588562, 11.678293228149414, 7.412636756896973, -6.787923812866211, -0.03134290128946304, 1.1584196090698242, 2.5779271125793457]
    Last 10:  [-0.4681185781955719, 0.24617637693881989, 0.013099242001771927, 0.4093452990055084, -0.0956038311123848, -0.06480144709348679, 0.044930458068847656, -0.15281924605369568, -0.18177638947963715, 0.47355028986930847]
Debug data exported to hf_model_debug_operations.json and hf_model_debug_tensors.pkl
