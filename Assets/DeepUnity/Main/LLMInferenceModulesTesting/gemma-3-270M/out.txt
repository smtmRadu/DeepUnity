
================================================================================
000_model.embed_tokens: Gemma3TextScaledWordEmbedding (model.embed_tokens)
================================================================================

  → INPUT[0]: model.embed_tokens_input_0
     Shape: [1, 4]
     Dtype: torch.int64
     Mean: 98773.250000, Std: 113851.875000
     First 10: [2, 10979, 147224, 236888]
     Last 10:  [2, 10979, 147224, 236888]
     Zeros: 0, Total: 4

  → OUTPUT[0]: model.embed_tokens_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.011280, Std: 1.031319
     First 10: [-0.36131492257118225, 0.27021417021751404, 0.3381537199020386, 0.3149925172328949, 0.12352646887302399, -0.07913414388895035, 0.10345342010259628, 0.09496097266674042, 0.04844553768634796, -0.007479141931980848]
     Last 10:  [0.5991033911705017, -0.8770379424095154, -0.14514359831809998, -0.41999000310897827, -0.09882117807865143, -0.2856549620628357, -0.17448113858699799, -1.0437986850738525, 1.7664285898208618, 0.3072721064090729]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [262144, 640]
     Mean: -0.000165
     First 10: [-0.00732421875, 0.04296875, -0.0341796875, -0.02099609375, -0.011962890625, 0.0289306640625, 0.005096435546875, -0.0322265625, 0.0380859375, -0.01275634765625]
     Last 10:  [-0.025634765625, 0.0302734375, -0.003265380859375, 0.044921875, 0.02392578125, 0.05712890625, 0.01092529296875, 0.0125732421875, 6.198883056640625e-05, -0.01708984375]

================================================================================
001_model.layers.0.input_layernorm: Gemma3RMSNorm (model.layers.0.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.0.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.011280, Std: 1.031319
     First 10: [-0.36131492257118225, 0.27021417021751404, 0.3381537199020386, 0.3149925172328949, 0.12352646887302399, -0.07913414388895035, 0.10345342010259628, 0.09496097266674042, 0.04844553768634796, -0.007479141931980848]
     Last 10:  [0.5991033911705017, -0.8770379424095154, -0.14514359831809998, -0.41999000310897827, -0.09882117807865143, -0.2856549620628357, -0.17448113858699799, -1.0437986850738525, 1.7664285898208618, 0.3072721064090729]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.0.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.425515, Std: 16.641912
     First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
     Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 12.480810
     First 10: [15.0, 15.875, 9.8125, 40.5, 16.375, 9.375, 8.5, 16.25, 9.625, 10.9375]
     Last 10:  [14.8125, 8.25, 13.8125, 9.0625, 9.625, 10.0, 10.875, 14.0625, 9.4375, 9.5]

================================================================================
002_model.layers.0.self_attn.q_proj: Linear (model.layers.0.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.425515, Std: 16.641912
     First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
     Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.0.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 1.483598, Std: 22.160713
     First 10: [6.2979254722595215, 7.329462051391602, 9.404305458068848, -2.4617486000061035, 10.235593795776367, -4.497312068939209, 7.944532871246338, 1.0305190086364746, 2.898792266845703, 5.074529647827148]
     Last 10:  [5.745026588439941, -3.353013038635254, 1.8782310485839844, -3.236016273498535, 3.272514820098877, -6.085381984710693, 30.446889877319336, 1.4016046524047852, 14.366205215454102, 3.6530263423919678]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000076
     First 10: [-0.00457763671875, 0.0074462890625, -0.01507568359375, -0.006561279296875, 0.01116943359375, 0.00872802734375, 0.0003833770751953125, -0.0096435546875, 0.00421142578125, 0.0181884765625]
     Last 10:  [0.0030517578125, -0.000911712646484375, -0.10498046875, -0.01092529296875, 0.0262451171875, -0.0140380859375, -0.0107421875, -0.0027618408203125, -0.0118408203125, 0.0145263671875]

================================================================================
003_model.layers.0.self_attn.k_proj: Linear (model.layers.0.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.425515, Std: 16.641912
     First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
     Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.0.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.790188, Std: 21.040928
     First 10: [0.05973079800605774, -0.3500518798828125, 0.07768797874450684, -0.2811100482940674, -0.29856228828430176, -0.3946802616119385, 0.1314244270324707, -0.004389762878417969, 0.18865251541137695, -0.2531353235244751]
     Last 10:  [6.589937686920166, 33.81639862060547, -25.562625885009766, 42.109092712402344, -34.03094482421875, -0.6761453151702881, 41.62080764770508, -18.552766799926758, 16.03182601928711, -44.43439865112305]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000193
     First 10: [0.00164031982421875, -0.026123046875, 0.01068115234375, -0.026123046875, 0.031494140625, 0.01263427734375, -0.0223388671875, -0.0103759765625, -0.06298828125, 0.0576171875]
     Last 10:  [-0.01092529296875, -0.062255859375, 0.0186767578125, -0.0181884765625, -0.0174560546875, 0.01214599609375, -0.046875, -0.07470703125, -0.0111083984375, 0.022216796875]

================================================================================
004_model.layers.0.self_attn.v_proj: Linear (model.layers.0.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.425515, Std: 16.641912
     First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
     Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.0.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.047050, Std: 10.486319
     First 10: [-0.8908199071884155, 0.20305782556533813, -0.7873555421829224, -0.2379053831100464, -0.3717365264892578, -3.8010311126708984, -0.3445277214050293, 0.35037922859191895, 0.3936748504638672, -0.18098047375679016]
     Last 10:  [-6.451592445373535, -13.575428009033203, 10.863025665283203, -16.696815490722656, -5.791408538818359, 1.2365117073059082, 16.345947265625, -0.33417809009552, 0.9071155786514282, -2.2307324409484863]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000008
     First 10: [0.005859375, -0.03955078125, -0.031494140625, -0.05712890625, -0.042724609375, 0.054443359375, -0.06982421875, -0.047607421875, -0.0286865234375, -0.0277099609375]
     Last 10:  [0.0113525390625, 0.0252685546875, -0.0159912109375, -0.053955078125, -0.02685546875, 0.0108642578125, 0.025634765625, 0.004669189453125, 0.0517578125, -0.052978515625]

================================================================================
005_model.layers.0.self_attn.q_norm: Gemma3RMSNorm (model.layers.0.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 1.483598, Std: 22.160713
     First 10: [6.2979254722595215, 7.329462051391602, 9.404305458068848, -2.4617486000061035, 10.235593795776367, -4.497312068939209, 7.944532871246338, 1.0305190086364746, 2.898792266845703, 5.074529647827148]
     Last 10:  [5.745026588439941, -3.353013038635254, 1.8782310485839844, -3.236016273498535, 3.272514820098877, -6.085381984710693, 30.446889877319336, 1.4016046524047852, 14.366205215454102, 3.6530263423919678]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.0.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.084367, Std: 1.356608
     First 10: [1.1887798309326172, 1.2399744987487793, 2.0697598457336426, -0.5861437916755676, 1.595339059829712, -1.2152314186096191, 0.9924666881561279, 0.14730101823806763, 0.41718754172325134, 0.4930866062641144]
     Last 10:  [0.4484182894229889, -0.25760844349861145, 0.11411970853805542, -0.32984206080436707, 0.30250996351242065, -0.5327273607254028, 3.3829896450042725, 0.09996145218610764, 1.4335432052612305, 0.27059468626976013]
     Zeros: 16, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.965250
     First 10: [0.8828125, 0.6875, 1.1953125, 1.375, 0.5546875, 1.6953125, 0.24609375, 0.42578125, 0.435546875, -0.03076171875]
     Last 10:  [0.9921875, 0.9609375, 0.55078125, 1.6015625, 1.359375, 1.234375, 1.8359375, 0.8203125, 1.546875, 0.890625]

================================================================================
006_model.layers.0.self_attn.k_norm: Gemma3RMSNorm (model.layers.0.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.790188, Std: 21.040928
     First 10: [0.05973079800605774, -0.3500518798828125, 0.07768797874450684, -0.2811100482940674, -0.29856228828430176, -0.3946802616119385, 0.1314244270324707, -0.004389762878417969, 0.18865251541137695, -0.2531353235244751]
     Last 10:  [6.589937686920166, 33.81639862060547, -25.562625885009766, 42.109092712402344, -34.03094482421875, -0.6761453151702881, 41.62080764770508, -18.552766799926758, 16.03182601928711, -44.43439865112305]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.0.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.011500, Std: 1.486134
     First 10: [0.0053739422000944614, -0.04209119826555252, 0.008173738606274128, -0.02857976034283638, -0.02694612927734852, -0.0404619462788105, 0.011740312911570072, -0.0003451482334639877, 0.024021603167057037, -0.019759371876716614]
     Last 10:  [0.578190803527832, 2.612980842590332, -3.020167350769043, 2.802427291870117, -2.2902610301971436, -0.04904336482286453, 2.9255452156066895, -1.6139174699783325, 0.9750345945358276, -3.5995635986328125]
     Zeros: 12, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.973218
     First 10: [0.2392578125, 0.65625, 0.44921875, 0.400390625, 0.2431640625, 0.412109375, 0.23046875, 0.0830078125, 0.75390625, 0.0751953125]
     Last 10:  [1.75, 1.421875, 2.703125, 1.0859375, 1.109375, 1.2734375, 1.203125, 1.7265625, 0.90625, 1.5390625]

================================================================================
007_model.layers.0.self_attn.o_proj: Linear (model.layers.0.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.0.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.274047, Std: 4.680633
     First 10: [-0.8908199071884155, 0.20305782556533813, -0.7873555421829224, -0.2379053831100464, -0.3717365264892578, -3.8010311126708984, -0.3445277214050293, 0.35037922859191895, 0.3936748504638672, -0.18098047375679016]
     Last 10:  [-7.077987194061279, -1.1683422327041626, 6.293652534484863, -0.39658042788505554, -2.639127492904663, -1.5583627223968506, 5.5387773513793945, 1.0841830968856812, -2.716465711593628, 2.167811870574951]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.0.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.280524, Std: 7.449708
     First 10: [-2.8339426517486572, 9.01751708984375, -1.5068289041519165, 3.324098825454712, -0.17821688950061798, -4.301394939422607, 1.325487732887268, -1.3300023078918457, 5.322543621063232, 6.099685192108154]
     Last 10:  [2.1495187282562256, -1.1287586688995361, -4.557039260864258, 1.957016944885254, 6.339928150177002, 8.688642501831055, 4.975260257720947, -1.811781883239746, -4.587636947631836, -1.24509859085083]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000007
     First 10: [-0.020263671875, 0.048828125, -0.03662109375, 0.056396484375, -0.003265380859375, 0.0400390625, -0.0166015625, 0.030517578125, 0.04638671875, 0.01434326171875]
     Last 10:  [0.01556396484375, 0.00372314453125, 0.05810546875, -0.000637054443359375, 0.039306640625, -0.0017852783203125, -0.0576171875, 0.00147247314453125, 0.012451171875, -0.014892578125]

================================================================================
008_model.layers.0.self_attn: Gemma3Attention (model.layers.0.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.0.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.280524, Std: 7.449708
     First 10: [-2.8339426517486572, 9.01751708984375, -1.5068289041519165, 3.324098825454712, -0.17821688950061798, -4.301394939422607, 1.325487732887268, -1.3300023078918457, 5.322543621063232, 6.099685192108154]
     Last 10:  [2.1495187282562256, -1.1287586688995361, -4.557039260864258, 1.957016944885254, 6.339928150177002, 8.688642501831055, 4.975260257720947, -1.811781883239746, -4.587636947631836, -1.24509859085083]
     Zeros: 0, Total: 2560

================================================================================
009_model.layers.0.post_attention_layernorm: Gemma3RMSNorm (model.layers.0.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.0.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.280524, Std: 7.449708
     First 10: [-2.8339426517486572, 9.01751708984375, -1.5068289041519165, 3.324098825454712, -0.17821688950061798, -4.301394939422607, 1.325487732887268, -1.3300023078918457, 5.322543621063232, 6.099685192108154]
     Last 10:  [2.1495187282562256, -1.1287586688995361, -4.557039260864258, 1.957016944885254, 6.339928150177002, 8.688642501831055, 4.975260257720947, -1.811781883239746, -4.587636947631836, -1.24509859085083]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.0.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.216064, Std: 5.333917
     First 10: [-0.0490274615585804, 0.2558463513851166, -0.09280306845903397, 0.2116265445947647, -0.005303049925714731, -0.21729041635990143, 0.07062765955924988, -0.042336855083703995, 0.40147092938423157, 0.2701442837715149]
     Last 10:  [0.04692726209759712, -0.03696377947926521, -1.6453635692596436, 0.0714816302061081, 0.23689450323581696, 0.2553468942642212, 0.17128142714500427, -0.015973711386322975, -0.16371504962444305, -1.0831139087677002]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: -0.361111
     First 10: [-0.90234375, -0.83984375, -0.65234375, -0.640625, -0.83203125, -0.71484375, -0.69921875, -0.8203125, -0.57421875, -0.75]
     Last 10:  [-0.796875, -0.6953125, 2.359375, -0.66015625, -0.65234375, -0.7265625, -0.6796875, -0.91796875, -0.66796875, 7.09375]

================================================================================
010_model.layers.0.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.0.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.0.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.204784, Std: 5.431576
     First 10: [-0.41034239530563354, 0.5260605216026306, 0.2453506588935852, 0.5266190767288208, 0.11822342127561569, -0.2964245676994324, 0.17408108711242676, 0.05262411758303642, 0.4499164819717407, 0.26266515254974365]
     Last 10:  [0.6460306644439697, -0.9140017032623291, -1.790507197380066, -0.348508358001709, 0.13807332515716553, -0.030308067798614502, -0.0031997114419937134, -1.0597723722457886, 1.6027135848999023, -0.7758418321609497]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.0.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000537, Std: 1.386575
     First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
     Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 4.837134
     First 10: [4.78125, 4.96875, 4.46875, 13.875, 5.1875, 4.65625, 4.4375, 5.1875, 4.28125, 4.46875]
     Last 10:  [4.78125, 4.59375, 2.390625, 4.6875, 4.5, 4.90625, 5.0625, 4.40625, 4.53125, 0.7734375]

================================================================================
011_model.layers.0.mlp.gate_proj: Linear (model.layers.0.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.0.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000537, Std: 1.386575
     First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
     Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.0.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.667014, Std: 1.205215
     First 10: [-0.72669917345047, -1.898198127746582, 0.5486508011817932, -1.5706493854522705, -0.07239216566085815, -2.1983630657196045, -2.428624153137207, -0.8708949089050293, -1.4670095443725586, 0.9035918712615967]
     Last 10:  [1.127816915512085, -0.9413745999336243, -3.129889965057373, 1.5869991779327393, -0.5720880627632141, -1.2598730325698853, 0.22689089179039001, -0.9452670812606812, -0.8135976791381836, -0.32619887590408325]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000155
     First 10: [0.0211181640625, -0.0174560546875, 0.026123046875, 0.015625, -0.03076171875, 0.010498046875, -0.047119140625, 0.07958984375, 0.01318359375, -0.0250244140625]
     Last 10:  [0.007293701171875, -0.0069580078125, 0.01458740234375, 0.002471923828125, 0.00933837890625, -0.019775390625, -0.10498046875, -0.05078125, 0.062255859375, -0.05126953125]

================================================================================
012_model.layers.0.mlp.act_fn: PytorchGELUTanh (model.layers.0.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.0.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.667014, Std: 1.205215
     First 10: [-0.72669917345047, -1.898198127746582, 0.5486508011817932, -1.5706493854522705, -0.07239216566085815, -2.1983630657196045, -2.428624153137207, -0.8708949089050293, -1.4670095443725586, 0.9035918712615967]
     Last 10:  [1.127816915512085, -0.9413745999336243, -3.129889965057373, 1.5869991779327393, -0.5720880627632141, -1.2598730325698853, 0.22689089179039001, -0.9452670812606812, -0.8135976791381836, -0.32619887590408325]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.0.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.091257, Std: 0.491099
     First 10: [-0.1698956936597824, -0.0547233410179615, 0.3886277973651886, -0.09150096774101257, -0.034107208251953125, -0.030427876859903336, -0.01799318566918373, -0.1672370731830597, -0.10465618968009949, 0.7380202412605286]
     Last 10:  [0.9813459515571594, -0.16323119401931763, -0.0023764432407915592, 1.4975306987762451, -0.16228988766670227, -0.131072536110878, 0.13380709290504456, -0.1629662662744522, -0.1692659705877304, -0.12139410525560379]
     Zeros: 15, Total: 8192

================================================================================
013_model.layers.0.mlp.up_proj: Linear (model.layers.0.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.0.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000537, Std: 1.386575
     First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
     Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.0.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.010635, Std: 1.031289
     First 10: [0.011507511138916016, 1.0420186519622803, 0.839897632598877, -2.1196374893188477, -1.6762338876724243, 1.1423001289367676, -0.15204039216041565, -1.172397494316101, 0.0031760111451148987, -0.05712355673313141]
     Last 10:  [-1.1421189308166504, -0.577163815498352, 0.19567438960075378, -0.7690945863723755, -0.37523365020751953, -0.21140074729919434, -1.7868062257766724, -0.3990073800086975, 1.549891710281372, -0.8119871020317078]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000002
     First 10: [-0.015869140625, 0.0098876953125, 0.064453125, -0.072265625, 0.0267333984375, -0.01495361328125, 0.00061798095703125, -0.061279296875, 0.06787109375, 0.0400390625]
     Last 10:  [0.03564453125, 0.058349609375, -0.016845703125, 0.00506591796875, 0.033447265625, -0.01226806640625, 0.03564453125, 0.0033721923828125, 0.06787109375, 0.005462646484375]

================================================================================
014_model.layers.0.mlp.down_proj: Linear (model.layers.0.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.0.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.021364, Std: 1.561842
     First 10: [-0.0019550765864551067, -0.05702274292707443, 0.3264075815677643, 0.1939488798379898, 0.05717165768146515, -0.034757766872644424, 0.002735690912231803, 0.19606833159923553, -0.0003323892306070775, -0.042158342897892]
     Last 10:  [-1.1208138465881348, 0.0942111387848854, -0.00046500907046720386, -1.151742696762085, 0.060896627604961395, 0.027708832174539566, -0.23908734321594238, 0.06502474099397659, -0.2623439133167267, 0.09857045114040375]
     Zeros: 15, Total: 8192

  → OUTPUT[0]: model.layers.0.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.039222, Std: 1.005863
     First 10: [0.6804015040397644, -2.12420916557312, 0.6089076995849609, 0.8251789808273315, -0.2686074376106262, 1.042865514755249, -0.7763306498527527, -1.0072749853134155, -0.09280750155448914, 0.5881297588348389]
     Last 10:  [-0.7066440582275391, -0.32972604036331177, 0.15044572949409485, 0.05314284563064575, 0.4384809136390686, -0.34976643323898315, 0.6943364143371582, 0.4238384962081909, -0.35441577434539795, -2.0779619216918945]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000005
     First 10: [0.005645751953125, 0.0216064453125, -0.025146484375, 0.01397705078125, 0.027099609375, -0.0157470703125, 0.0101318359375, -0.00153350830078125, -0.02197265625, -0.00762939453125]
     Last 10:  [-0.02001953125, 0.005340576171875, 0.00982666015625, -0.001953125, 0.0111083984375, 0.011474609375, 0.01141357421875, 0.0162353515625, 0.00994873046875, -0.0089111328125]

================================================================================
015_model.layers.0.mlp: Gemma3MLP (model.layers.0.mlp)
================================================================================

  → INPUT[0]: model.layers.0.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000537, Std: 1.386575
     First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
     Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.0.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.039222, Std: 1.005863
     First 10: [0.6804015040397644, -2.12420916557312, 0.6089076995849609, 0.8251789808273315, -0.2686074376106262, 1.042865514755249, -0.7763306498527527, -1.0072749853134155, -0.09280750155448914, 0.5881297588348389]
     Last 10:  [-0.7066440582275391, -0.32972604036331177, 0.15044572949409485, 0.05314284563064575, 0.4384809136390686, -0.34976643323898315, 0.6943364143371582, 0.4238384962081909, -0.35441577434539795, -2.0779619216918945]
     Zeros: 0, Total: 2560

================================================================================
016_model.layers.0.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.0.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.0.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.039222, Std: 1.005863
     First 10: [0.6804015040397644, -2.12420916557312, 0.6089076995849609, 0.8251789808273315, -0.2686074376106262, 1.042865514755249, -0.7763306498527527, -1.0072749853134155, -0.09280750155448914, 0.5881297588348389]
     Last 10:  [-0.7066440582275391, -0.32972604036331177, 0.15044572949409485, 0.05314284563064575, 0.4384809136390686, -0.34976643323898315, 0.6943364143371582, 0.4238384962081909, -0.35441577434539795, -2.0779619216918945]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.0.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.953321, Std: 56.678665
     First 10: [0.16077499091625214, -0.6173266172409058, 0.4341249465942383, 0.4364752173423767, -0.11782149225473404, 0.5955226421356201, -0.5246955752372742, -0.4500366747379303, -0.06944462656974792, 0.31428369879722595]
     Last 10:  [-0.548168420791626, -0.31990712881088257, 1.8832556009292603, 0.05056198313832283, 0.4598255157470703, -0.3420562744140625, 0.6352964639663696, 0.11427898705005646, -0.38371118903160095, -67.89466857910156]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 0.805890
     First 10: [-0.66015625, -0.58203125, 0.025390625, -0.2392578125, -0.369140625, -0.1787109375, -0.0279541015625, -0.357421875, 0.076171875, -0.2314453125]
     Last 10:  [-0.314453125, -0.142578125, 10.0625, -0.1591796875, -0.0732421875, -0.1357421875, -0.19140625, -0.76171875, -0.043212890625, 27.875]

================================================================================
017_model.layers.1.input_layernorm: Gemma3RMSNorm (model.layers.1.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.1.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.158105, Std: 59.186935
     First 10: [-0.2495674043893814, -0.09126609563827515, 0.6794756054878235, 0.9630942940711975, 0.00040192902088165283, 0.29909807443618774, -0.3506144881248474, -0.39741256833076477, 0.3804718554019928, 0.576948881149292]
     Last 10:  [0.09786224365234375, -1.2339088916778564, 0.09274840354919434, -0.29794636368751526, 0.5978988409042358, -0.372364342212677, 0.6320967674255371, -0.9454934000968933, 1.219002366065979, -68.6705093383789]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.1.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.131824, Std: 7.272413
     First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
     Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 24.677223
     First 10: [31.125, 32.5, 22.75, 47.75, 30.25, 25.25, 22.375, 30.5, 23.25, 26.375]
     Last 10:  [29.875, 22.875, 4.53125, 23.125, 22.75, 24.5, 24.25, 33.5, 20.875, 1.2734375]

================================================================================
018_model.layers.1.self_attn.q_proj: Linear (model.layers.1.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.131824, Std: 7.272413
     First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
     Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.1.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.172085, Std: 17.123793
     First 10: [0.001455068588256836, 0.3045678734779358, 0.5205649733543396, 0.19726812839508057, 0.5817791819572449, -0.5869358777999878, 0.756453275680542, -0.10041530430316925, -0.03724992275238037, -0.0005526244640350342]
     Last 10:  [-0.07848024368286133, -3.776020050048828, -0.15801990032196045, -0.27927279472351074, -1.1166198253631592, -1.2101819515228271, 0.6111103296279907, -3.3597755432128906, 3.5492029190063477, 4.276642799377441]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000090
     First 10: [0.0033111572265625, 0.0059814453125, -0.1044921875, 0.02490234375, 0.034912109375, -0.022705078125, -0.03369140625, 0.00171661376953125, 0.0125732421875, -0.083984375]
     Last 10:  [-0.04248046875, 0.0947265625, -0.01324462890625, 0.00238037109375, -0.08984375, 0.01336669921875, 0.006500244140625, 0.023193359375, -0.01446533203125, -0.0191650390625]

================================================================================
019_model.layers.1.self_attn.k_proj: Linear (model.layers.1.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.131824, Std: 7.272413
     First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
     Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.1.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.449574, Std: 10.299052
     First 10: [-0.002749115228652954, -0.014828629791736603, -0.0026351213455200195, -0.012497276067733765, 0.006182482466101646, 0.009873345494270325, -0.005913525819778442, -0.012482799589633942, -0.01061733067035675, -0.00227385014295578]
     Last 10:  [-7.018460273742676, -15.688098907470703, -7.955350399017334, 4.832880020141602, -8.894634246826172, -13.616569519042969, 1.6221272945404053, -9.751404762268066, 6.787450790405273, -8.939099311828613]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000090
     First 10: [-0.03515625, 0.006134033203125, 0.035888671875, -0.050048828125, 0.046875, 0.04638671875, -0.00482177734375, 0.0263671875, -0.08349609375, -0.031982421875]
     Last 10:  [-0.06298828125, -0.004425048828125, -0.0286865234375, 0.034912109375, 0.0908203125, -0.02587890625, 0.01361083984375, 0.0103759765625, 0.0130615234375, 0.041748046875]

================================================================================
020_model.layers.1.self_attn.v_proj: Linear (model.layers.1.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.131824, Std: 7.272413
     First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
     Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.1.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.408808, Std: 6.869521
     First 10: [-0.1930898129940033, 0.703927755355835, -0.025813505053520203, -0.04328576475381851, 0.4046694040298462, 0.4346885681152344, -0.43850207328796387, -0.13784170150756836, 0.3647642433643341, 0.2999514639377594]
     Last 10:  [11.794339179992676, -1.5482444763183594, 4.548500061035156, 1.098741054534912, 0.5812458395957947, -4.630569934844971, 0.211034893989563, 3.003812789916992, -0.725727915763855, -1.6081442832946777]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000044
     First 10: [-0.007415771484375, -0.05712890625, 0.016357421875, 0.03173828125, 0.007293701171875, -0.01055908203125, 0.001953125, 0.072265625, 0.0235595703125, 0.003997802734375]
     Last 10:  [-0.011962890625, -0.054931640625, -0.0634765625, 0.0020294189453125, -0.09326171875, -0.06689453125, -0.0615234375, 0.060791015625, 0.03076171875, -0.00677490234375]

================================================================================
021_model.layers.1.self_attn.q_norm: Gemma3RMSNorm (model.layers.1.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.172085, Std: 17.123793
     First 10: [0.001455068588256836, 0.3045678734779358, 0.5205649733543396, 0.19726812839508057, 0.5817791819572449, -0.5869358777999878, 0.756453275680542, -0.10041530430316925, -0.03724992275238037, -0.0005526244640350342]
     Last 10:  [-0.07848024368286133, -3.776020050048828, -0.15801990032196045, -0.27927279472351074, -1.1166198253631592, -1.2101819515228271, 0.6111103296279907, -3.3597755432128906, 3.5492029190063477, 4.276642799377441]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.1.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.152854, Std: 3.089273
     First 10: [0.003626024816185236, 1.0341506004333496, 1.7830668687820435, 0.6071447134017944, 1.7270421981811523, -1.9229950904846191, 1.4982993602752686, -0.2891155779361725, -0.14793092012405396, -0.0022275615483522415]
     Last 10:  [-0.03866415098309517, -1.2068872451782227, -0.07613452523946762, -0.11029685288667679, -0.47434157133102417, -0.4795953929424286, 0.2687237858772278, -1.372518539428711, 1.4499026536941528, 1.8457444906234741]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.696144
     First 10: [0.9609375, 1.671875, 1.6953125, 1.421875, 1.3359375, 1.578125, 0.55859375, 1.265625, 2.125, 2.171875]
     Last 10:  [1.8359375, 0.83984375, 1.7734375, 1.2734375, 1.4453125, 1.28125, 1.53125, 1.3515625, 1.3515625, 1.484375]

================================================================================
022_model.layers.1.self_attn.k_norm: Gemma3RMSNorm (model.layers.1.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.449574, Std: 10.299052
     First 10: [-0.002749115228652954, -0.014828629791736603, -0.0026351213455200195, -0.012497276067733765, 0.006182482466101646, 0.009873345494270325, -0.005913525819778442, -0.012482799589633942, -0.01061733067035675, -0.00227385014295578]
     Last 10:  [-7.018460273742676, -15.688098907470703, -7.955350399017334, 4.832880020141602, -8.894634246826172, -13.616569519042969, 1.6221272945404053, -9.751404762268066, 6.787450790405273, -8.939099311828613]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.1.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.149437, Std: 2.266396
     First 10: [-0.0024325994309037924, -0.01985745318233967, -0.003616052446886897, -0.015493613667786121, 0.0073283580131828785, 0.014342959970235825, -0.005029808729887009, -0.016716081649065018, -0.013414135202765465, -0.002415540162473917]
     Last 10:  [-2.336909294128418, -5.823294162750244, -2.7769036293029785, 1.9543578624725342, -3.292668104171753, -4.739317893981934, 0.6037521362304688, -3.550976514816284, 2.799384593963623, -3.2371909618377686]
     Zeros: 4, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.660587
     First 10: [0.4609375, 1.2109375, 1.265625, 1.046875, 0.95703125, 1.3984375, 0.404296875, 1.2109375, 1.0859375, 0.75390625]
     Last 10:  [1.5859375, 1.8828125, 1.7109375, 2.140625, 1.875, 1.703125, 1.890625, 1.828125, 2.203125, 1.8125]

================================================================================
023_model.layers.1.self_attn.o_proj: Linear (model.layers.1.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.1.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.059337, Std: 0.904794
     First 10: [-0.1930898129940033, 0.703927755355835, -0.025813505053520203, -0.04328576475381851, 0.4046694040298462, 0.4346885681152344, -0.43850207328796387, -0.13784170150756836, 0.3647642433643341, 0.2999514639377594]
     Last 10:  [2.724358558654785, -0.04271163418889046, 0.23944006860256195, 0.05596053600311279, 0.2332592010498047, -0.7823140621185303, 0.36907970905303955, -0.10994230955839157, -0.051993563771247864, -0.4925673007965088]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.1.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.133937, Std: 3.610773
     First 10: [-0.032756257802248, 0.42673060297966003, 0.20813901722431183, -0.0850721225142479, 0.024427227675914764, 0.42045068740844727, -0.31599098443984985, -0.04048251360654831, 0.46051836013793945, 0.4792925715446472]
     Last 10:  [0.8076938390731812, 0.09550453722476959, -0.4400442838668823, 0.29874366521835327, 0.26817309856414795, 0.6036581993103027, 0.013121120631694794, 0.6340376734733582, -0.5236000418663025, 8.386391639709473]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000010
     First 10: [-0.00147247314453125, -0.0185546875, -0.00078582763671875, -0.014404296875, -0.00921630859375, 0.006072998046875, 0.0233154296875, 0.0010986328125, -0.0218505859375, 0.0233154296875]
     Last 10:  [0.0216064453125, 0.001800537109375, -0.003173828125, 0.000629425048828125, 0.0166015625, -0.01263427734375, 0.0234375, -0.01153564453125, 0.0033111572265625, -8.726119995117188e-05]

================================================================================
024_model.layers.1.self_attn: Gemma3Attention (model.layers.1.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.1.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.133937, Std: 3.610773
     First 10: [-0.032756257802248, 0.42673060297966003, 0.20813901722431183, -0.0850721225142479, 0.024427227675914764, 0.42045068740844727, -0.31599098443984985, -0.04048251360654831, 0.46051836013793945, 0.4792925715446472]
     Last 10:  [0.8076938390731812, 0.09550453722476959, -0.4400442838668823, 0.29874366521835327, 0.26817309856414795, 0.6036581993103027, 0.013121120631694794, 0.6340376734733582, -0.5236000418663025, 8.386391639709473]
     Zeros: 0, Total: 2560

================================================================================
025_model.layers.1.post_attention_layernorm: Gemma3RMSNorm (model.layers.1.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.1.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.133937, Std: 3.610773
     First 10: [-0.032756257802248, 0.42673060297966003, 0.20813901722431183, -0.0850721225142479, 0.024427227675914764, 0.42045068740844727, -0.31599098443984985, -0.04048251360654831, 0.46051836013793945, 0.4792925715446472]
     Last 10:  [0.8076938390731812, 0.09550453722476959, -0.4400442838668823, 0.29874366521835327, 0.26817309856414795, 0.6036581993103027, 0.013121120631694794, 0.6340376734733582, -0.5236000418663025, 8.386391639709473]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.1.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.137094, Std: 36.479416
     First 10: [-0.005863717291504145, 0.10485374927520752, 0.1501697152853012, -0.031876277178525925, 0.009240099228918552, 0.20948316156864166, -0.18926937878131866, -0.014359270222485065, 0.3322582542896271, 0.19834774732589722]
     Last 10:  [0.20410719513893127, 0.03427312150597572, -3.931753158569336, 0.10012345761060715, 0.10703735798597336, 0.20852768421173096, 0.004086349159479141, 0.05787626653909683, -0.19024401903152466, 123.92533874511719]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 2.747667
     First 10: [-0.486328125, -0.294921875, 1.0703125, 0.0751953125, 0.08544921875, 0.4296875, 0.71875, 0.017822265625, 1.0703125, 0.1875]
     Last 10:  [0.10302734375, 0.56640625, 38.0, 0.462890625, 0.7421875, 0.5078125, 0.359375, -0.6015625, 0.5859375, 63.5]

================================================================================
026_model.layers.1.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.1.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.1.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 2.295199, Std: 82.305878
     First 10: [-0.2554311156272888, 0.013587653636932373, 0.8296453356742859, 0.9312180280685425, 0.009642028249800205, 0.5085812211036682, -0.5398838520050049, -0.41177183389663696, 0.7127301096916199, 0.7752966284751892]
     Last 10:  [0.301969438791275, -1.1996357440948486, -3.8390047550201416, -0.19782289862632751, 0.7049362063407898, -0.16383665800094604, 0.6361831426620483, -0.8876171112060547, 1.0287582874298096, 55.25482940673828]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.1.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.019897, Std: 0.753058
     First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
     Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
     Zeros: 8, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 28.944311
     First 10: [34.25, 35.75, 29.0, 54.5, 33.5, 33.0, 29.875, 34.25, 28.875, 33.25]
     Last 10:  [33.0, 32.75, 2.71875, 33.25, 30.625, 32.75, 34.75, 31.375, 31.125, 1.734375]

================================================================================
027_model.layers.1.mlp.gate_proj: Linear (model.layers.1.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.1.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.019897, Std: 0.753058
     First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
     Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
     Zeros: 8, Total: 2560

  → OUTPUT[0]: model.layers.1.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.280730, Std: 0.806038
     First 10: [-0.1903638392686844, -0.1722165197134018, 0.004699863493442535, -0.7316675186157227, -0.015919163823127747, 0.3490030765533447, -0.31952163577079773, -0.38248202204704285, -0.10825420916080475, -0.2816339135169983]
     Last 10:  [-1.3022987842559814, 0.10207065939903259, -0.5640626549720764, -0.7606185078620911, 0.5842303037643433, -0.8458268046379089, -0.18360048532485962, 0.922817587852478, -0.8576288819313049, -0.40376460552215576]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000077
     First 10: [0.0128173828125, 0.023193359375, 0.021728515625, -0.0033111572265625, 0.00933837890625, 0.032470703125, -0.0244140625, -0.0303955078125, 0.0361328125, 0.00994873046875]
     Last 10:  [-0.0306396484375, 0.003509521484375, 0.00171661376953125, -0.052001953125, -0.01019287109375, -0.01348876953125, -0.0390625, 0.00457763671875, -0.0390625, 0.01068115234375]

================================================================================
028_model.layers.1.mlp.act_fn: PytorchGELUTanh (model.layers.1.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.1.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.280730, Std: 0.806038
     First 10: [-0.1903638392686844, -0.1722165197134018, 0.004699863493442535, -0.7316675186157227, -0.015919163823127747, 0.3490030765533447, -0.31952163577079773, -0.38248202204704285, -0.10825420916080475, -0.2816339135169983]
     Last 10:  [-1.3022987842559814, 0.10207065939903259, -0.5640626549720764, -0.7606185078620911, 0.5842303037643433, -0.8458268046379089, -0.18360048532485962, 0.922817587852478, -0.8576288819313049, -0.40376460552215576]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.1.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.067388, Std: 0.410792
     First 10: [-0.08081215620040894, -0.07433472573757172, 0.0023587439209222794, -0.16994653642177582, -0.007858486846089363, 0.22212080657482147, -0.11971691995859146, -0.13427741825580597, -0.04946107044816017, -0.10958912968635559]
     Last 10:  [-0.12578149139881134, 0.05518443509936333, -0.16154912114143372, -0.1700265109539032, 0.42088884115219116, -0.16827009618282318, -0.07842777669429779, 0.7583834528923035, -0.1678110510110855, -0.138576939702034]
     Zeros: 0, Total: 8192

================================================================================
029_model.layers.1.mlp.up_proj: Linear (model.layers.1.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.1.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.019897, Std: 0.753058
     First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
     Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
     Zeros: 8, Total: 2560

  → OUTPUT[0]: model.layers.1.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.001219, Std: 0.665121
     First 10: [0.007821436040103436, -0.19044017791748047, 0.09799472987651825, 0.13676457107067108, -0.15552160143852234, -0.11899124085903168, 0.07684407383203506, -0.013862053863704205, -0.3710987865924835, 0.2844812870025635]
     Last 10:  [0.9240207672119141, 0.512592077255249, -0.366669237613678, 0.09850356727838516, -0.019777856767177582, 0.7324585318565369, 0.23628492653369904, 0.27160388231277466, 0.3232074975967407, 0.18295952677726746]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000028
     First 10: [-0.0380859375, 0.00750732421875, 0.005035400390625, 0.01123046875, -0.00665283203125, -0.0031280517578125, -0.042236328125, 0.028076171875, -0.05859375, -0.01458740234375]
     Last 10:  [-0.0498046875, 0.002349853515625, -0.028564453125, -0.068359375, -0.01092529296875, -0.0294189453125, -0.01025390625, 0.0235595703125, -0.060546875, 0.01080322265625]

================================================================================
030_model.layers.1.mlp.down_proj: Linear (model.layers.1.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.1.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.000663, Std: 0.359935
     First 10: [-0.0006320670945569873, 0.01415631826967001, 0.00023114446958061308, -0.023242665454745293, 0.0012221644865348935, -0.02643042989075184, -0.009199535474181175, 0.0018613608554005623, 0.01835494302213192, -0.03117605671286583]
     Last 10:  [-0.11622471362352371, 0.02828710339963436, 0.059235092252492905, -0.0167482178658247, -0.008324279449880123, -0.12325086444616318, -0.018531301990151405, 0.20597988367080688, -0.054237790405750275, -0.025353971868753433]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.1.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.001554, Std: 0.288509
     First 10: [0.048990920186042786, -0.021225783973932266, -0.009275750257074833, -0.007051009684801102, -0.041074201464653015, 0.04439251124858856, -0.09249815344810486, -0.018146635964512825, -0.022141508758068085, 0.010069301351904869]
     Last 10:  [-0.0689162164926529, 0.14432072639465332, -0.04976645112037659, -0.18366581201553345, -0.07328064739704132, -0.25845474004745483, -0.042715590447187424, -0.21431973576545715, 0.14199259877204895, -0.3241061568260193]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000008
     First 10: [0.00689697265625, 0.003265380859375, -0.000797271728515625, -0.00482177734375, -0.010009765625, 0.0155029296875, 0.01519775390625, 0.0250244140625, 0.0034637451171875, 0.016845703125]
     Last 10:  [0.00112152099609375, -0.007598876953125, 0.004486083984375, -0.003448486328125, -0.007049560546875, -0.00469970703125, -0.001220703125, -0.0076904296875, 0.006439208984375, -0.004974365234375]

================================================================================
031_model.layers.1.mlp: Gemma3MLP (model.layers.1.mlp)
================================================================================

  → INPUT[0]: model.layers.1.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.019897, Std: 0.753058
     First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
     Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
     Zeros: 8, Total: 2560

  → OUTPUT[0]: model.layers.1.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.001554, Std: 0.288509
     First 10: [0.048990920186042786, -0.021225783973932266, -0.009275750257074833, -0.007051009684801102, -0.041074201464653015, 0.04439251124858856, -0.09249815344810486, -0.018146635964512825, -0.022141508758068085, 0.010069301351904869]
     Last 10:  [-0.0689162164926529, 0.14432072639465332, -0.04976645112037659, -0.18366581201553345, -0.07328064739704132, -0.25845474004745483, -0.042715590447187424, -0.21431973576545715, 0.14199259877204895, -0.3241061568260193]
     Zeros: 0, Total: 2560

================================================================================
032_model.layers.1.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.1.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.1.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.001554, Std: 0.288509
     First 10: [0.048990920186042786, -0.021225783973932266, -0.009275750257074833, -0.007051009684801102, -0.041074201464653015, 0.04439251124858856, -0.09249815344810486, -0.018146635964512825, -0.022141508758068085, 0.010069301351904869]
     Last 10:  [-0.0689162164926529, 0.14432072639465332, -0.04976645112037659, -0.18366581201553345, -0.07328064739704132, -0.25845474004745483, -0.042715590447187424, -0.21431973576545715, 0.14199259877204895, -0.3241061568260193]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.1.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.556356, Std: 27.924011
     First 10: [0.24489133059978485, -0.15261176228523254, -0.2940796911716461, -0.080872543156147, -0.48376286029815674, 0.8408840298652649, -2.2738535404205322, -0.25659677386283875, -0.7489781379699707, 0.162377268075943]
     Last 10:  [-0.17841671407222748, 0.5911997556686401, -4.173530101776123, -0.620212197303772, -0.3332844376564026, -0.9440721869468689, -0.161939337849617, -0.12814737856388092, 0.6250165104866028, -22.430341720581055]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 1.878360
     First 10: [-0.71484375, -0.58984375, 0.80859375, -0.345703125, -0.328125, 0.08056640625, 0.40234375, -0.193359375, 0.9296875, -0.080078125]
     Last 10:  [-0.205078125, 0.2578125, 24.75, 0.036865234375, 0.396484375, 0.12158203125, 0.1640625, -0.81640625, 0.3515625, 20.25]

================================================================================
033_model.layers.2.input_layernorm: Gemma3RMSNorm (model.layers.2.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.2.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 2.851555, Std: 107.769287
     First 10: [-0.010539785027503967, -0.13902410864830017, 0.5355656147003174, 0.8503454923629761, -0.4741208255290985, 1.349465250968933, -2.813737392425537, -0.6683685779571533, -0.03624802827835083, 0.9376739263534546]
     Last 10:  [0.12355272471904755, -0.6084359884262085, -8.012535095214844, -0.8180351257324219, 0.3716517686843872, -1.107908844947815, 0.47424381971359253, -1.0157644748687744, 1.6537747383117676, 32.824485778808594]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.2.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044388, Std: 1.581807
     First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
     Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 28.830969
     First 10: [48.75, 53.25, 21.75, 60.5, 37.5, 30.0, 24.0, 41.5, 19.25, 38.25]
     Last 10:  [39.75, 27.125, 0.5703125, 33.25, 24.875, 29.625, 29.875, 41.75, 24.75, 8.0625]

================================================================================
034_model.layers.2.self_attn.q_proj: Linear (model.layers.2.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044388, Std: 1.581807
     First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
     Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.2.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.034577, Std: 2.643258
     First 10: [0.03415389358997345, -0.2231147140264511, 0.1425134390592575, -0.15751111507415771, 0.5739494562149048, 0.4084988236427307, 0.02576206624507904, -0.061044082045555115, -0.29220426082611084, 0.05253136157989502]
     Last 10:  [0.24127504229545593, 0.07862727344036102, 0.17503884434700012, 0.6170549392700195, -0.6452509164810181, 0.049342215061187744, -0.5003542900085449, -0.003355562686920166, 0.20023030042648315, 0.6399357318878174]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000122
     First 10: [-0.00179290771484375, -0.0216064453125, 0.004241943359375, 0.0020599365234375, -0.000743865966796875, 0.056640625, -0.052001953125, 0.0177001953125, -0.027099609375, 0.025146484375]
     Last 10:  [-0.030517578125, -0.03759765625, 0.036376953125, 0.0012664794921875, -0.00604248046875, -0.037353515625, 0.01025390625, 0.037841796875, 0.01080322265625, -0.00909423828125]

================================================================================
035_model.layers.2.self_attn.k_proj: Linear (model.layers.2.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044388, Std: 1.581807
     First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
     Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.2.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.100959, Std: 1.934242
     First 10: [-0.04070580005645752, 0.02559863030910492, -0.16828960180282593, -0.02843756228685379, 0.006929993629455566, 0.03646424412727356, -0.006288886070251465, -0.15539485216140747, 0.030380934476852417, 0.056104954332113266]
     Last 10:  [5.311874866485596, -1.214493989944458, -9.939475059509277, 1.7636876106262207, 1.1449414491653442, -2.0439324378967285, 1.882590889930725, -2.367800235748291, -2.0842020511627197, -0.03061804175376892]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000123
     First 10: [0.046142578125, 0.01275634765625, 0.0106201171875, -0.07080078125, -0.006805419921875, 0.017333984375, 0.007232666015625, 0.01226806640625, -0.0458984375, -0.0267333984375]
     Last 10:  [0.09765625, -0.0224609375, -0.0015106201171875, 0.06005859375, -0.0191650390625, 0.004547119140625, 0.00653076171875, -0.049072265625, -0.05224609375, -0.00579833984375]

================================================================================
036_model.layers.2.self_attn.v_proj: Linear (model.layers.2.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044388, Std: 1.581807
     First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
     Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.2.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.022111, Std: 1.073037
     First 10: [-0.10747533291578293, -0.2006467878818512, -0.16723695397377014, -0.16673851013183594, 0.07862196862697601, -0.0007174760103225708, 0.0013808012008666992, -0.144599050283432, 0.04181528091430664, 0.06180395185947418]
     Last 10:  [0.6358954906463623, -0.13744869828224182, -0.10551083087921143, 0.3188331723213196, 0.36714404821395874, -0.3428195118904114, -0.4894849359989166, 0.7987401485443115, 0.253814160823822, -0.11407024413347244]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000002
     First 10: [-0.0184326171875, -0.02099609375, -0.00811767578125, 0.01104736328125, -0.007415771484375, -0.00162506103515625, -0.0537109375, -0.0242919921875, -0.047607421875, -0.0255126953125]
     Last 10:  [-0.05224609375, 0.004852294921875, 0.0123291015625, 0.0184326171875, 0.01446533203125, 0.04736328125, 0.04443359375, -0.00494384765625, 0.10400390625, 0.00064849853515625]

================================================================================
037_model.layers.2.self_attn.q_norm: Gemma3RMSNorm (model.layers.2.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.034577, Std: 2.643258
     First 10: [0.03415389358997345, -0.2231147140264511, 0.1425134390592575, -0.15751111507415771, 0.5739494562149048, 0.4084988236427307, 0.02576206624507904, -0.061044082045555115, -0.29220426082611084, 0.05253136157989502]
     Last 10:  [0.24127504229545593, 0.07862727344036102, 0.17503884434700012, 0.6170549392700195, -0.6452509164810181, 0.049342215061187744, -0.5003542900085449, -0.003355562686920166, 0.20023030042648315, 0.6399357318878174]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.2.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.023420, Std: 1.366390
     First 10: [0.058887600898742676, -0.64952152967453, 0.23641671240329742, -0.4916297197341919, 1.5433884859085083, 1.3044458627700806, 0.05713716894388199, -0.21618181467056274, -0.9225626587867737, 0.12738655507564545]
     Last 10:  [0.18464112281799316, 0.04977922514081001, 0.11395949125289917, 0.32571765780448914, -0.46431252360343933, 0.032285403460264206, -0.40086886286735535, -0.0019245706498622894, 0.13395436108112335, 0.4980786442756653]
     Zeros: 16, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.776192
     First 10: [0.1220703125, 0.89453125, 0.07958984375, 1.03125, 0.75, 1.078125, 0.443359375, 1.3046875, 1.0546875, 0.578125]
     Last 10:  [0.83203125, 0.515625, 0.55859375, 0.263671875, 0.72265625, 0.56640625, 0.91796875, 0.373046875, 0.6015625, 0.86328125]

================================================================================
038_model.layers.2.self_attn.k_norm: Gemma3RMSNorm (model.layers.2.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.100959, Std: 1.934242
     First 10: [-0.04070580005645752, 0.02559863030910492, -0.16828960180282593, -0.02843756228685379, 0.006929993629455566, 0.03646424412727356, -0.006288886070251465, -0.15539485216140747, 0.030380934476852417, 0.056104954332113266]
     Last 10:  [5.311874866485596, -1.214493989944458, -9.939475059509277, 1.7636876106262207, 1.1449414491653442, -2.0439324378967285, 1.882590889930725, -2.367800235748291, -2.0842020511627197, -0.03061804175376892]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.2.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.071700, Std: 3.341894
     First 10: [-0.03898235410451889, 0.02625017613172531, -0.22932708263397217, -0.036701835691928864, 0.009443454444408417, 0.06270648539066315, -0.013599465601146221, -0.10867804288864136, 0.043694090098142624, 0.08704568445682526]
     Last 10:  [3.7353341579437256, -1.5010356903076172, -5.9545392990112305, 2.3028018474578857, 1.1200815439224243, -2.209407091140747, 1.6338379383087158, -3.3300955295562744, -2.4386627674102783, -0.027402766048908234]
     Zeros: 4, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.072401
     First 10: [0.08984375, 0.1669921875, 0.55078125, 0.46875, 0.55078125, 0.95703125, 1.4609375, -0.2041015625, 0.63671875, 0.765625]
     Last 10:  [0.41796875, 1.4921875, 0.2080078125, 1.6328125, 0.97265625, 1.1796875, 0.75, 1.8359375, 1.359375, 0.8046875]

================================================================================
039_model.layers.2.self_attn.o_proj: Linear (model.layers.2.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.2.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.028122, Std: 0.576621
     First 10: [-0.10747533291578293, -0.2006467878818512, -0.16723695397377014, -0.16673851013183594, 0.07862196862697601, -0.0007174760103225708, 0.0013808012008666992, -0.144599050283432, 0.04181528091430664, 0.06180395185947418]
     Last 10:  [0.4177733361721039, 0.15704254806041718, -0.1114543229341507, 0.14303936064243317, -0.2807626724243164, 0.03595169633626938, -0.13535909354686737, 0.6102434396743774, 0.19527554512023926, 0.4993634819984436]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.2.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.021032, Std: 0.689930
     First 10: [0.14985978603363037, 0.5789029002189636, -0.15678927302360535, -0.3829214572906494, -0.1666674017906189, -0.13670817017555237, 0.8231567144393921, 0.04849042743444443, -0.5143294930458069, 0.5368988513946533]
     Last 10:  [0.8777404427528381, -0.5002163648605347, -0.5048267841339111, 0.4002346694469452, 0.5999507904052734, 0.02103293500840664, 0.8238273859024048, -0.17568057775497437, -0.36470288038253784, 0.966009795665741]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000049
     First 10: [-0.01397705078125, -0.0291748046875, -0.06494140625, 0.0101318359375, -0.0026702880859375, 0.037109375, -0.0322265625, -0.0164794921875, 0.037353515625, 0.04248046875]
     Last 10:  [-0.017333984375, -0.00537109375, 0.0213623046875, -0.001922607421875, -0.007080078125, 0.005950927734375, -0.006866455078125, -0.00107574462890625, 0.00897216796875, 0.00019931793212890625]

================================================================================
040_model.layers.2.self_attn: Gemma3Attention (model.layers.2.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.2.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.021032, Std: 0.689930
     First 10: [0.14985978603363037, 0.5789029002189636, -0.15678927302360535, -0.3829214572906494, -0.1666674017906189, -0.13670817017555237, 0.8231567144393921, 0.04849042743444443, -0.5143294930458069, 0.5368988513946533]
     Last 10:  [0.8777404427528381, -0.5002163648605347, -0.5048267841339111, 0.4002346694469452, 0.5999507904052734, 0.02103293500840664, 0.8238273859024048, -0.17568057775497437, -0.36470288038253784, 0.966009795665741]
     Zeros: 0, Total: 2560

================================================================================
041_model.layers.2.post_attention_layernorm: Gemma3RMSNorm (model.layers.2.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.2.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.021032, Std: 0.689930
     First 10: [0.14985978603363037, 0.5789029002189636, -0.15678927302360535, -0.3829214572906494, -0.1666674017906189, -0.13670817017555237, 0.8231567144393921, 0.04849042743444443, -0.5143294930458069, 0.5368988513946533]
     Last 10:  [0.8777404427528381, -0.5002163648605347, -0.5048267841339111, 0.4002346694469452, 0.5999507904052734, 0.02103293500840664, 0.8238273859024048, -0.17568057775497437, -0.36470288038253784, 0.966009795665741]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.2.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.432045, Std: 11.733462
     First 10: [0.048595644533634186, 0.23778271675109863, -0.286325603723526, -0.28697413206100464, -0.1128959208726883, -0.14678458869457245, 1.2990515232086182, 0.035990919917821884, -1.0025554895401, 0.5087658762931824]
     Last 10:  [0.4946292042732239, -0.5539035201072693, -9.604140281677246, 0.3755282461643219, 0.7395666837692261, 0.02109762281179428, 0.7242265939712524, -0.029700148850679398, -0.4387839138507843, 13.587505340576172]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 0.800125
     First 10: [-0.82421875, -0.77734375, -0.01007080078125, -0.59375, -0.6328125, -0.41796875, -0.14453125, -0.59765625, 0.056640625, -0.486328125]
     Last 10:  [-0.609375, -0.232421875, 12.1875, -0.349609375, -0.1455078125, -0.3046875, -0.390625, -0.8828125, -0.166015625, 8.75]

================================================================================
042_model.layers.2.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.2.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.2.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 3.283600, Std: 115.910568
     First 10: [0.03805585950613022, 0.09875860810279846, 0.24924001097679138, 0.5633713603019714, -0.587016761302948, 1.2026807069778442, -1.514685869216919, -0.6323776841163635, -1.0388035774230957, 1.4464397430419922]
     Last 10:  [0.6181819438934326, -1.162339448928833, -17.616676330566406, -0.4425068795681, 1.1112184524536133, -1.0868111848831177, 1.1984703540802002, -1.0454646348953247, 1.2149908542633057, 46.411991119384766]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.2.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006670, Std: 0.717162
     First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
     Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 22.039240
     First 10: [34.75, 35.25, 17.25, 46.25, 27.875, 26.25, 21.75, 28.0, 17.125, 29.625]
     Last 10:  [27.0, 23.625, 0.52734375, 28.0, 21.5, 26.125, 27.0, 31.0, 23.0, 1.7734375]

================================================================================
043_model.layers.2.mlp.gate_proj: Linear (model.layers.2.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.2.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006670, Std: 0.717162
     First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
     Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.2.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.410459, Std: 0.725702
     First 10: [-0.14806775748729706, -0.14914625883102417, 0.032802388072013855, 0.03333088755607605, 0.03008955344557762, -0.3349064886569977, -0.47867676615715027, 0.07680312544107437, -0.08538328111171722, -0.13642045855522156]
     Last 10:  [0.0741187334060669, 0.4117686152458191, -0.47728630900382996, 0.12611374258995056, -0.3743247985839844, 0.3294275999069214, 0.813797652721405, -0.0584227591753006, -2.4035611152648926, -0.04887950420379639]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000000
     First 10: [-0.031494140625, 0.034912109375, -0.0615234375, 0.0172119140625, -0.03955078125, 0.0166015625, 0.001434326171875, -0.0240478515625, -0.0162353515625, -0.00836181640625]
     Last 10:  [0.083984375, 0.01507568359375, -0.0235595703125, -0.057373046875, -0.0634765625, -0.0272216796875, -0.00396728515625, -0.032958984375, 0.00927734375, 0.0286865234375]

================================================================================
044_model.layers.2.mlp.act_fn: PytorchGELUTanh (model.layers.2.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.2.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.410459, Std: 0.725702
     First 10: [-0.14806775748729706, -0.14914625883102417, 0.032802388072013855, 0.03333088755607605, 0.03008955344557762, -0.3349064886569977, -0.47867676615715027, 0.07680312544107437, -0.08538328111171722, -0.13642045855522156]
     Last 10:  [0.0741187334060669, 0.4117686152458191, -0.47728630900382996, 0.12611374258995056, -0.3743247985839844, 0.3294275999069214, 0.813797652721405, -0.0584227591753006, -2.4035611152648926, -0.04887950420379639]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.2.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.005121, Std: 0.290989
     First 10: [-0.06531945616006851, -0.06573176383972168, 0.016830377280712128, 0.01710856519639492, 0.01540591660887003, -0.12353335320949554, -0.15131688117980957, 0.04075248911976814, -0.039786774665117264, -0.06080877408385277]
     Last 10:  [0.03924897685647011, 0.27165406942367554, -0.15111340582370758, 0.06938505917787552, -0.1325472891330719, 0.2072339504957199, 0.6445366740226746, -0.027850478887557983, -0.019113343209028244, -0.023486977443099022]
     Zeros: 0, Total: 8192

================================================================================
045_model.layers.2.mlp.up_proj: Linear (model.layers.2.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.2.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006670, Std: 0.717162
     First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
     Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.2.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.003614, Std: 0.595459
     First 10: [-0.12416496872901917, 0.04907379299402237, -0.14548656344413757, -0.29387781023979187, -0.06467214226722717, 0.13093741238117218, -0.2734249532222748, -0.005912665277719498, 0.16412962973117828, -0.0791151225566864]
     Last 10:  [0.05273684859275818, 0.35549306869506836, 0.8563618659973145, 0.2688782215118408, -0.7359681129455566, 0.7246559858322144, 0.45818108320236206, -0.5504655838012695, -0.10353678464889526, 0.3993242383003235]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000001
     First 10: [-0.049560546875, 0.006103515625, 0.05859375, -0.0078125, -0.050537109375, -0.009033203125, -0.0086669921875, -0.0062255859375, 0.035888671875, -0.0419921875]
     Last 10:  [-0.0230712890625, 0.04833984375, -0.03857421875, 0.045166015625, 0.054443359375, 0.01446533203125, 0.04248046875, -0.06396484375, -0.026123046875, 0.00439453125]

================================================================================
046_model.layers.2.mlp.down_proj: Linear (model.layers.2.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.2.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.001639, Std: 0.280641
     First 10: [0.008110388182103634, -0.003225706983357668, -0.002448593731969595, -0.0050278278067708015, -0.0009963336633518338, -0.01617513783276081, 0.04137381166219711, -0.0002409558219369501, -0.006530188489705324, 0.004810893442481756]
     Last 10:  [0.0020698674488812685, 0.09657113999128342, -0.12940776348114014, 0.018656130880117416, 0.09755057841539383, 0.15017332136631012, 0.2953145205974579, 0.015330730006098747, 0.001978934044018388, -0.009378919377923012]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.2.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002925, Std: 0.364637
     First 10: [0.0013268934562802315, -0.01318470761179924, -0.02286485582590103, 0.011747154407203197, 0.0012242470402270555, 0.038976557552814484, -0.027684427797794342, -0.0072929104790091515, -0.0064275446347892284, 0.0015067718923091888]
     Last 10:  [0.07402534782886505, 0.09985014796257019, 0.1316312998533249, -0.09039339423179626, 0.021122772246599197, -0.009412260726094246, -0.12378717958927155, -0.03229038417339325, -0.05691048875451088, -0.33706724643707275]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000007
     First 10: [-0.017333984375, -0.00445556640625, -0.00469970703125, -0.02099609375, 0.0196533203125, -0.038330078125, -5.6743621826171875e-05, 0.01226806640625, -0.004638671875, 0.000335693359375]
     Last 10:  [-0.00775146484375, 0.007568359375, -0.0230712890625, 0.00543212890625, -0.0211181640625, 0.0032501220703125, 0.00787353515625, -0.0028228759765625, -0.043701171875, 0.006622314453125]

================================================================================
047_model.layers.2.mlp: Gemma3MLP (model.layers.2.mlp)
================================================================================

  → INPUT[0]: model.layers.2.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006670, Std: 0.717162
     First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
     Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.2.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002925, Std: 0.364637
     First 10: [0.0013268934562802315, -0.01318470761179924, -0.02286485582590103, 0.011747154407203197, 0.0012242470402270555, 0.038976557552814484, -0.027684427797794342, -0.0072929104790091515, -0.0064275446347892284, 0.0015067718923091888]
     Last 10:  [0.07402534782886505, 0.09985014796257019, 0.1316312998533249, -0.09039339423179626, 0.021122772246599197, -0.009412260726094246, -0.12378717958927155, -0.03229038417339325, -0.05691048875451088, -0.33706724643707275]
     Zeros: 0, Total: 2560

================================================================================
048_model.layers.2.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.2.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.2.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002925, Std: 0.364637
     First 10: [0.0013268934562802315, -0.01318470761179924, -0.02286485582590103, 0.011747154407203197, 0.0012242470402270555, 0.038976557552814484, -0.027684427797794342, -0.0072929104790091515, -0.0064275446347892284, 0.0015067718923091888]
     Last 10:  [0.07402534782886505, 0.09985014796257019, 0.1316312998533249, -0.09039339423179626, 0.021122772246599197, -0.009412260726094246, -0.12378717958927155, -0.03229038417339325, -0.05691048875451088, -0.33706724643707275]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.2.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.128989, Std: 19.335550
     First 10: [0.008209633640944958, -0.11348165571689606, -1.1819368600845337, 0.15876971185207367, 0.01840187981724739, 1.0073938369750977, -0.9669403433799744, -0.13372859358787537, -0.3630426228046417, 0.031125325709581375]
     Last 10:  [0.41185635328292847, 0.9461872577667236, 27.30350685119629, -0.6319677829742432, 0.24449625611305237, -0.08425233513116837, -1.0507475137710571, -0.04186127707362175, -0.5726640224456787, -39.61899948120117]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 4.605221
     First 10: [-0.515625, -0.326171875, 3.046875, 0.05810546875, 0.1767578125, 1.0234375, 1.734375, 0.435546875, 3.421875, 0.6171875]
     Last 10:  [0.408203125, 1.3984375, 51.5, 0.76953125, 1.9296875, 1.265625, 1.1484375, -0.671875, 1.546875, 28.75]

================================================================================
049_model.layers.3.input_layernorm: Gemma3RMSNorm (model.layers.3.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.3.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 3.412589, Std: 130.642181
     First 10: [0.04626549407839775, -0.014723047614097595, -0.9326968193054199, 0.7221410870552063, -0.5686149001121521, 2.2100744247436523, -2.481626272201538, -0.7661062479019165, -1.401846170425415, 1.477565050125122]
     Last 10:  [1.0300383567810059, -0.21615219116210938, 9.686830520629883, -1.0744746923446655, 1.3557146787643433, -1.1710635423660278, 0.14772284030914307, -1.087325930595398, 0.642326831817627, 6.792991638183594]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.3.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.008138, Std: 1.227236
     First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
     Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 24.891054
     First 10: [50.25, 46.0, 14.125, 52.5, 40.5, 24.875, 20.0, 40.0, 13.4375, 30.375]
     Last 10:  [34.25, 23.25, 0.0260009765625, 29.5, 19.0, 22.875, 25.0, 50.0, 19.5, 2.78125]

================================================================================
050_model.layers.3.self_attn.q_proj: Linear (model.layers.3.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.008138, Std: 1.227236
     First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
     Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.3.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.062938, Std: 1.508438
     First 10: [-0.25074583292007446, -0.09411583840847015, 0.6649511456489563, 0.17810159921646118, 0.37636786699295044, -0.050610389560461044, -0.24055519700050354, 0.29217803478240967, -0.5770397186279297, -0.42095810174942017]
     Last 10:  [-1.995829701423645, 1.4491097927093506, 1.3992702960968018, 0.02862408757209778, 0.9588150978088379, -1.9767050743103027, -1.431427001953125, -0.3884492516517639, 0.8695794343948364, 0.9551999568939209]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: 0.000061
     First 10: [0.01422119140625, 0.01275634765625, -0.025634765625, -0.0247802734375, -0.01202392578125, 0.007659912109375, 0.00054168701171875, -0.01336669921875, 0.029296875, 0.01416015625]
     Last 10:  [0.03271484375, 0.0634765625, 0.0380859375, 0.0216064453125, 0.08935546875, -0.0086669921875, 0.0517578125, 0.007080078125, -0.033447265625, -0.02001953125]

================================================================================
051_model.layers.3.self_attn.k_proj: Linear (model.layers.3.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.008138, Std: 1.227236
     First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
     Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.3.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.056727, Std: 1.464699
     First 10: [0.0008884966373443604, 0.009633071720600128, -0.010251611471176147, -0.004567145369946957, 0.0030151456594467163, 0.006089635193347931, 0.016491279006004333, 0.01666705310344696, -0.024582069367170334, 0.0029260367155075073]
     Last 10:  [-5.540679454803467, 2.2137465476989746, 2.601306915283203, 2.405923843383789, 1.5562833547592163, -4.712998867034912, -2.8254618644714355, -1.679091453552246, 3.1283516883850098, -0.06130021810531616]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000066
     First 10: [-0.00921630859375, 0.001434326171875, -0.01055908203125, -0.01177978515625, 0.00732421875, -0.003662109375, -0.01116943359375, -0.01324462890625, -0.0162353515625, -0.0177001953125]
     Last 10:  [0.08642578125, 0.03271484375, 0.05810546875, 0.0247802734375, 0.07421875, -0.010986328125, 0.059326171875, -0.024169921875, 0.011474609375, -0.0174560546875]

================================================================================
052_model.layers.3.self_attn.v_proj: Linear (model.layers.3.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.008138, Std: 1.227236
     First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
     Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.3.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.028558, Std: 0.968695
     First 10: [-0.284274160861969, -0.08881960064172745, 0.11178702116012573, -0.00050383061170578, 0.2276776283979416, 0.02515965700149536, 0.03421882912516594, 0.10633990168571472, 0.0369911715388298, -0.03175998479127884]
     Last 10:  [-0.13296908140182495, 0.3866719603538513, -0.4184430241584778, -0.8877232074737549, -0.4196290671825409, 1.2389272451400757, -0.5988315343856812, 0.26545244455337524, 4.780564785003662, -0.6869760155677795]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000034
     First 10: [0.00109100341796875, 0.0322265625, -0.00885009765625, -0.058837890625, 2.562999725341797e-05, 0.0191650390625, -0.003662109375, 0.0223388671875, 0.023193359375, -0.01007080078125]
     Last 10:  [0.0211181640625, 0.017578125, -0.0057373046875, -0.0159912109375, 0.03125, 0.02978515625, -0.0164794921875, 0.0191650390625, 0.048583984375, -1.7881393432617188e-05]

================================================================================
053_model.layers.3.self_attn.q_norm: Gemma3RMSNorm (model.layers.3.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.062938, Std: 1.508438
     First 10: [-0.25074583292007446, -0.09411583840847015, 0.6649511456489563, 0.17810159921646118, 0.37636786699295044, -0.050610389560461044, -0.24055519700050354, 0.29217803478240967, -0.5770397186279297, -0.42095810174942017]
     Last 10:  [-1.995829701423645, 1.4491097927093506, 1.3992702960968018, 0.02862408757209778, 0.9588150978088379, -1.9767050743103027, -1.431427001953125, -0.3884492516517639, 0.8695794343948364, 0.9551999568939209]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.3.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.011989, Std: 1.193427
     First 10: [-1.2376598119735718, -0.421333909034729, 2.577834129333496, 0.7378442883491516, 1.6417055130004883, -0.20386064052581787, -0.7050732374191284, 1.25922691822052, -2.2882091999053955, -1.4342619180679321]
     Last 10:  [-3.796841621398926, 2.1623401641845703, 1.0315072536468506, 0.03525478020310402, 0.8253771662712097, -3.130582094192505, -1.6760005950927734, -0.6096593141555786, 1.8238403797149658, 1.3742276430130005]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.527080
     First 10: [0.84765625, 0.67578125, 0.451171875, 0.55078125, 0.6328125, 0.5078125, 0.09716796875, 0.61328125, 0.484375, 0.275390625]
     Last 10:  [0.5625, 0.2255859375, -0.39453125, 0.0115966796875, -0.29296875, 0.30078125, -0.038330078125, 0.2890625, 0.72265625, 0.181640625]

================================================================================
054_model.layers.3.self_attn.k_norm: Gemma3RMSNorm (model.layers.3.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.056727, Std: 1.464699
     First 10: [0.0008884966373443604, 0.009633071720600128, -0.010251611471176147, -0.004567145369946957, 0.0030151456594467163, 0.006089635193347931, 0.016491279006004333, 0.01666705310344696, -0.024582069367170334, 0.0029260367155075073]
     Last 10:  [-5.540679454803467, 2.2137465476989746, 2.601306915283203, 2.405923843383789, 1.5562833547592163, -4.712998867034912, -2.8254618644714355, -1.679091453552246, 3.1283516883850098, -0.06130021810531616]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.3.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.032657, Std: 1.912227
     First 10: [0.0016501785721629858, 0.02505628950893879, -0.022007884457707405, -0.007282273378223181, 0.005465649999678135, 0.0070645748637616634, 0.06816186010837555, 0.029099399223923683, -0.02096652053296566, 0.005551732145249844]
     Last 10:  [-4.393723487854004, 2.531566858291626, 7.053295612335205, 3.15594220161438, 3.0681941509246826, -4.88968563079834, -3.4577198028564453, -1.7811373472213745, 2.1929256916046143, -0.07184547930955887]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.747193
     First 10: [0.62890625, 1.28125, 0.8828125, 0.3984375, 0.58984375, 0.0174560546875, 2.625, 0.53125, -0.251953125, 0.6640625]
     Last 10:  [0.197265625, 0.7265625, 3.09375, 0.98046875, 1.9765625, 0.56640625, 0.84765625, 0.6015625, 0.058349609375, 0.76953125]

================================================================================
055_model.layers.3.self_attn.o_proj: Linear (model.layers.3.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.3.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.014414, Std: 0.528458
     First 10: [-0.284274160861969, -0.08881960064172745, 0.11178702116012573, -0.00050383061170578, 0.2276776283979416, 0.02515965700149536, 0.03421882912516594, 0.10633990168571472, 0.0369911715388298, -0.03175998479127884]
     Last 10:  [0.0789569690823555, 0.07366140186786652, -0.05541395768523216, 0.06708015501499176, 0.07130322605371475, 0.047814298421144485, -0.0368620865046978, 0.07006406784057617, 3.0681588649749756, -0.04629486799240112]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.3.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023480, Std: 0.587806
     First 10: [-0.09550150483846664, 0.12872424721717834, -0.040142640471458435, -0.013998221606016159, -0.08677597343921661, -0.4873543977737427, 0.0505685955286026, 0.00827064923942089, 0.15530873835086823, -0.01480042189359665]
     Last 10:  [-0.33203160762786865, -0.2759011387825012, -0.2136671394109726, -0.9174982309341431, 0.01963462308049202, -0.07284867018461227, 0.6251364350318909, 0.9127824306488037, 0.7735033631324768, 0.6552510261535645]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000012
     First 10: [0.0194091796875, -0.0081787109375, -0.0023193359375, -0.054931640625, -0.0172119140625, 0.012939453125, 0.0240478515625, -0.049560546875, 0.01025390625, 0.00921630859375]
     Last 10:  [-0.002471923828125, 4.172325134277344e-06, -0.0235595703125, -0.0135498046875, 0.00958251953125, -0.006317138671875, 0.0072021484375, 0.00106048583984375, 0.00872802734375, -0.0032958984375]

================================================================================
056_model.layers.3.self_attn: Gemma3Attention (model.layers.3.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.3.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023480, Std: 0.587806
     First 10: [-0.09550150483846664, 0.12872424721717834, -0.040142640471458435, -0.013998221606016159, -0.08677597343921661, -0.4873543977737427, 0.0505685955286026, 0.00827064923942089, 0.15530873835086823, -0.01480042189359665]
     Last 10:  [-0.33203160762786865, -0.2759011387825012, -0.2136671394109726, -0.9174982309341431, 0.01963462308049202, -0.07284867018461227, 0.6251364350318909, 0.9127824306488037, 0.7735033631324768, 0.6552510261535645]
     Zeros: 0, Total: 2560

================================================================================
057_model.layers.3.post_attention_layernorm: Gemma3RMSNorm (model.layers.3.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.3.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023480, Std: 0.587806
     First 10: [-0.09550150483846664, 0.12872424721717834, -0.040142640471458435, -0.013998221606016159, -0.08677597343921661, -0.4873543977737427, 0.0505685955286026, 0.00827064923942089, 0.15530873835086823, -0.01480042189359665]
     Last 10:  [-0.33203160762786865, -0.2759011387825012, -0.2136671394109726, -0.9174982309341431, 0.01963462308049202, -0.07284867018461227, 0.6251364350318909, 0.9127824306488037, 0.7735033631324768, 0.6552510261535645]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.3.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.836139, Std: 25.852365
     First 10: [-0.04815233126282692, 0.10227203369140625, -0.28949499130249023, -0.0247697401791811, -0.13258446753025055, -1.7833786010742188, 0.2163376361131668, 0.017414916306734085, 1.2766484022140503, -0.038951702415943146]
     Last 10:  [-0.4969940483570099, -0.9015001058578491, -12.054423332214355, -2.113629102706909, 0.07246356457471848, -0.21534118056297302, 1.678914189338684, 0.225296288728714, 2.6001338958740234, 17.128129959106445]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 4.098013
     First 10: [-0.7421875, -0.59375, 2.6875, -0.09521484375, -0.21875, 0.87109375, 1.1875, 0.07666015625, 3.203125, 0.345703125]
     Last 10:  [-0.00506591796875, 1.171875, 36.5, 0.53125, 1.453125, 0.96484375, 0.78515625, -0.8359375, 1.234375, 16.375]

================================================================================
058_model.layers.3.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.3.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.3.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 4.248727, Std: 148.026031
     First 10: [-0.0018868371844291687, 0.08754898607730865, -1.2221918106079102, 0.6973713636398315, -0.7011993527412415, 0.4266958236694336, -2.2652885913848877, -0.7486913204193115, -0.12519776821136475, 1.4386132955551147]
     Last 10:  [0.5330443382263184, -1.1176522970199585, -2.3675928115844727, -3.188103675842285, 1.4281781911849976, -1.3864047527313232, 1.8266370296478271, -0.8620296716690063, 3.2424607276916504, 23.92112159729004]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.3.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.018251, Std: 0.816628
     First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
     Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 24.323166
     First 10: [58.75, 51.5, 11.5625, 49.0, 31.25, 21.75, 17.375, 30.25, 10.3125, 27.875]
     Last 10:  [27.625, 19.25, 0.003173828125, 26.25, 16.0, 20.375, 22.125, 62.25, 18.25, 2.859375]

================================================================================
059_model.layers.3.mlp.gate_proj: Linear (model.layers.3.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.3.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.018251, Std: 0.816628
     First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
     Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.3.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.165914, Std: 0.967657
     First 10: [-0.19981473684310913, 0.15687692165374756, 0.10429394245147705, 0.25707221031188965, -0.17747117578983307, 0.219674214720726, -0.06518803536891937, 0.02652152068912983, -0.29083094000816345, -0.30567118525505066]
     Last 10:  [-0.625910222530365, -1.4436973333358765, -0.4778515100479126, -0.7620107531547546, 0.28050899505615234, 0.5467347502708435, -0.6734350323677063, -0.4590957462787628, -0.20380201935768127, -0.2578234076499939]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000237
     First 10: [-0.00518798828125, -0.04638671875, -0.09912109375, 0.006744384765625, -0.068359375, -0.0003032684326171875, 0.046142578125, 0.03466796875, -0.00933837890625, 0.0262451171875]
     Last 10:  [-0.0262451171875, 0.0081787109375, -0.03564453125, 0.05078125, 0.004058837890625, -0.007293701171875, -0.049072265625, -0.03271484375, -0.018310546875, -0.0152587890625]

================================================================================
060_model.layers.3.mlp.act_fn: PytorchGELUTanh (model.layers.3.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.3.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.165914, Std: 0.967657
     First 10: [-0.19981473684310913, 0.15687692165374756, 0.10429394245147705, 0.25707221031188965, -0.17747117578983307, 0.219674214720726, -0.06518803536891937, 0.02652152068912983, -0.29083094000816345, -0.30567118525505066]
     Last 10:  [-0.625910222530365, -1.4436973333358765, -0.4778515100479126, -0.7620107531547546, 0.28050899505615234, 0.5467347502708435, -0.6734350323677063, -0.4590957462787628, -0.20380201935768127, -0.2578234076499939]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.3.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.160577, Std: 0.624851
     First 10: [-0.08408509939908981, 0.08821625262498856, 0.05647846311330795, 0.1546117514371872, -0.07623646408319473, 0.12893430888652802, -0.030899925157427788, 0.013541338965296745, -0.11214381456375122, -0.11613558232784271]
     Last 10:  [-0.16633424162864685, -0.10765676945447922, -0.15119627118110657, -0.17002122104167938, 0.17123660445213318, 0.3869110941886902, -0.16863325238227844, -0.14833848178386688, -0.08544538170099258, -0.10268513113260269]
     Zeros: 3, Total: 8192

================================================================================
061_model.layers.3.mlp.up_proj: Linear (model.layers.3.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.3.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.018251, Std: 0.816628
     First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
     Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.3.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.012218, Std: 0.597607
     First 10: [0.09197308868169785, -0.13978458940982819, 0.09174202382564545, -0.07309075444936752, -0.01398362498730421, -0.1726754605770111, 0.1710388958454132, 0.16714079678058624, 0.0477653332054615, -0.03941812738776207]
     Last 10:  [1.0646008253097534, -0.8506368398666382, 0.22772520780563354, -0.07077012956142426, 0.6200867891311646, -1.0112954378128052, 0.17853108048439026, 0.3297814428806305, 0.28989270329475403, -0.2636798024177551]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000018
     First 10: [0.004302978515625, 0.1318359375, -0.08251953125, -0.000926971435546875, -0.004425048828125, 0.02783203125, -0.01220703125, -0.043701171875, 0.046630859375, 0.034912109375]
     Last 10:  [-0.044189453125, -0.002410888671875, 0.0306396484375, 0.10302734375, 0.050537109375, 0.041259765625, 0.00909423828125, -0.000560760498046875, -0.0361328125, 0.0341796875]

================================================================================
062_model.layers.3.mlp.down_proj: Linear (model.layers.3.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.3.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.006141, Std: 0.426529
     First 10: [-0.007733566220849752, -0.012331272475421429, 0.00518144853413105, -0.011300689540803432, 0.0010660621337592602, -0.022263791412115097, -0.005285088904201984, 0.002263310132548213, -0.005356586538255215, 0.004577847197651863]
     Last 10:  [-0.17707957327365875, 0.09157681465148926, -0.034431200474500656, 0.012032424099743366, 0.1061815544962883, -0.3912814259529114, -0.030106276273727417, -0.048919279128313065, -0.024769993498921394, 0.02707599475979805]
     Zeros: 3, Total: 8192

  → OUTPUT[0]: model.layers.3.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001021, Std: 0.283839
     First 10: [0.0129196522757411, 0.011055509559810162, -0.003166582202538848, 0.015654226765036583, -0.009001096710562706, 0.027541810646653175, -0.030250385403633118, 0.00432765856385231, -0.009179489687085152, -0.014656886458396912]
     Last 10:  [-0.20065413415431976, 0.05632609874010086, -0.09334658831357956, 0.25770553946495056, 0.03187824413180351, 0.11675828695297241, -0.15372350811958313, -0.04842228442430496, 0.14027810096740723, -0.271599143743515]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000011
     First 10: [-0.00689697265625, 0.00897216796875, -0.0020599365234375, 0.005859375, -0.006591796875, 0.0029296875, -0.00933837890625, -0.032958984375, -0.026611328125, 0.006683349609375]
     Last 10:  [0.001678466796875, -0.0023193359375, -0.00872802734375, 0.004425048828125, -0.005035400390625, -0.002288818359375, -0.00567626953125, -0.004730224609375, -0.0069580078125, -0.00014019012451171875]

================================================================================
063_model.layers.3.mlp: Gemma3MLP (model.layers.3.mlp)
================================================================================

  → INPUT[0]: model.layers.3.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.018251, Std: 0.816628
     First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
     Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.3.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001021, Std: 0.283839
     First 10: [0.0129196522757411, 0.011055509559810162, -0.003166582202538848, 0.015654226765036583, -0.009001096710562706, 0.027541810646653175, -0.030250385403633118, 0.00432765856385231, -0.009179489687085152, -0.014656886458396912]
     Last 10:  [-0.20065413415431976, 0.05632609874010086, -0.09334658831357956, 0.25770553946495056, 0.03187824413180351, 0.11675828695297241, -0.15372350811958313, -0.04842228442430496, 0.14027810096740723, -0.271599143743515]
     Zeros: 0, Total: 2560

================================================================================
064_model.layers.3.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.3.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.3.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001021, Std: 0.283839
     First 10: [0.0129196522757411, 0.011055509559810162, -0.003166582202538848, 0.015654226765036583, -0.009001096710562706, 0.027541810646653175, -0.030250385403633118, 0.00432765856385231, -0.009179489687085152, -0.014656886458396912]
     Last 10:  [-0.20065413415431976, 0.05632609874010086, -0.09334658831357956, 0.25770553946495056, 0.03187824413180351, 0.11675828695297241, -0.15372350811958313, -0.04842228442430496, 0.14027810096740723, -0.271599143743515]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.3.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 4.277775, Std: 213.252136
     First 10: [0.5105754137039185, 0.5711506009101868, -0.98434978723526, 1.4723032712936401, -0.5842500925064087, 3.125443458557129, -3.8602442741394043, 0.4089336097240448, -2.934561014175415, -1.3267271518707275]
     Last 10:  [-3.0965893268585205, 1.326377511024475, -50.243263244628906, 3.2726540565490723, 0.9919631481170654, 2.611973762512207, -3.128636360168457, -0.34309518337249756, 3.704409122467041, -81.49906921386719]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 12.406023
     First 10: [0.3984375, 0.828125, 10.0, 2.328125, 1.296875, 3.015625, 3.515625, 2.34375, 10.3125, 2.203125]
     Last 10:  [1.8671875, 3.375, 99.0, 1.359375, 4.78125, 3.15625, 2.78125, 0.31640625, 3.90625, 54.75]

================================================================================
065_model.layers.4.input_layernorm: Gemma3RMSNorm (model.layers.4.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.4.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 8.526503, Std: 282.475952
     First 10: [0.5086885690689087, 0.6586995720863342, -2.2065415382385254, 2.1696746349334717, -1.285449504852295, 3.5521392822265625, -6.125533103942871, -0.3397577106952667, -3.0597586631774902, 0.11188614368438721]
     Last 10:  [-2.563544988632202, 0.2087252140045166, -52.61085510253906, 0.08455038070678711, 2.4201412200927734, 1.2255690097808838, -1.3019993305206299, -1.205124855041504, 6.946869850158691, -57.57794952392578]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.4.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007862, Std: 1.382002
     First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
     Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 34.187679
     First 10: [102.0, 63.25, 5.75, 38.0, 26.25, 27.375, 20.5, 28.0, 7.4375, 32.75]
     Last 10:  [23.5, 21.875, -0.57421875, 51.0, 19.0, 24.625, 24.5, 91.0, 20.75, 2.90625]

================================================================================
066_model.layers.4.self_attn.q_proj: Linear (model.layers.4.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007862, Std: 1.382002
     First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
     Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.4.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.013430, Std: 1.545442
     First 10: [-0.47012460231781006, 0.27169206738471985, 0.35053449869155884, 0.1959158182144165, -0.04061535745859146, 0.5272778868675232, -0.1352287232875824, 0.11941427737474442, -0.3794046938419342, -0.06725836545228958]
     Last 10:  [0.5097470283508301, -1.4366296529769897, -0.5288398265838623, 1.1322662830352783, 0.511859118938446, -1.3537980318069458, -0.7387264966964722, 0.4904572367668152, 3.2224183082580566, -2.416114568710327]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: 0.000030
     First 10: [0.01080322265625, -0.0247802734375, -0.00958251953125, -0.01470947265625, 0.00390625, -0.0240478515625, 0.0072021484375, -0.0035552978515625, 0.01519775390625, 0.0086669921875]
     Last 10:  [0.00086212158203125, 0.0025634765625, 0.01483154296875, -0.043701171875, -0.015869140625, 0.0306396484375, -0.00274658203125, 0.0296630859375, -0.04150390625, -0.0174560546875]

================================================================================
067_model.layers.4.self_attn.k_proj: Linear (model.layers.4.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007862, Std: 1.382002
     First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
     Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.4.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.125669, Std: 1.655496
     First 10: [0.012314993888139725, -0.013491928577423096, -0.015415927395224571, -0.008481740951538086, -0.005269467830657959, -0.02654258906841278, 0.0031440891325473785, 0.017254464328289032, -0.04134698957204819, 0.0066382139921188354]
     Last 10:  [0.8974237442016602, -2.876885175704956, -2.4859397411346436, 4.968405246734619, 2.090125560760498, -1.0859206914901733, -2.987549066543579, 1.7265398502349854, 0.718114972114563, -6.725058078765869]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000031
     First 10: [-0.0240478515625, -0.029541015625, -0.00628662109375, -0.03515625, -0.010498046875, -0.032958984375, 0.0198974609375, -0.040283203125, 0.03173828125, -0.0018768310546875]
     Last 10:  [0.0673828125, -0.1025390625, 0.0233154296875, 0.00482177734375, 0.0751953125, 0.0022430419921875, -0.130859375, 0.07763671875, -0.046630859375, 0.045166015625]

================================================================================
068_model.layers.4.self_attn.v_proj: Linear (model.layers.4.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007862, Std: 1.382002
     First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
     Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.4.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.041414, Std: 0.990852
     First 10: [0.17033590376377106, 0.05051655322313309, -0.3656069338321686, -0.04745890572667122, 0.050696223974227905, -0.10772868245840073, 0.08719030022621155, -0.09973283857107162, -0.0852748230099678, -0.2342635691165924]
     Last 10:  [-0.4386290907859802, 0.8114195466041565, -0.7877724766731262, -0.5426243543624878, 1.1548233032226562, 0.08396440744400024, 0.23847681283950806, 0.8766622543334961, 0.12181052565574646, -1.042970895767212]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000018
     First 10: [-0.05419921875, 0.11083984375, 0.03369140625, -0.021240234375, 0.09130859375, -0.0419921875, -0.042236328125, -0.0091552734375, 0.03759765625, -0.040771484375]
     Last 10:  [0.0272216796875, -0.0361328125, 0.029296875, -0.0166015625, -0.02294921875, -0.001708984375, 0.00872802734375, -0.01806640625, 0.049560546875, 0.0038299560546875]

================================================================================
069_model.layers.4.self_attn.q_norm: Gemma3RMSNorm (model.layers.4.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.013430, Std: 1.545442
     First 10: [-0.47012460231781006, 0.27169206738471985, 0.35053449869155884, 0.1959158182144165, -0.04061535745859146, 0.5272778868675232, -0.1352287232875824, 0.11941427737474442, -0.3794046938419342, -0.06725836545228958]
     Last 10:  [0.5097470283508301, -1.4366296529769897, -0.5288398265838623, 1.1322662830352783, 0.511859118938446, -1.3537980318069458, -0.7387264966964722, 0.4904572367668152, 3.2224183082580566, -2.416114568710327]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.4.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.072409, Std: 1.290268
     First 10: [-1.2681182622909546, 1.14251708984375, 1.100699543952942, 0.642288327217102, -0.10042653232812881, 0.607205867767334, -0.3053755760192871, 0.3468867242336273, -0.9459978342056274, -0.12019293755292892]
     Last 10:  [0.48473209142684937, -0.8378839492797852, -0.2729683816432953, 0.5095483064651489, 0.3404342830181122, -0.7808080315589905, -0.40487968921661377, 0.2808314561843872, 1.701303482055664, -1.151113748550415]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.551699
     First 10: [0.5234375, 1.375, 0.7734375, 0.8515625, 0.396484375, -0.349609375, 0.275390625, 0.640625, 0.408203125, 0.00927734375]
     Last 10:  [1.0078125, 0.2314453125, 0.08984375, -0.0498046875, 0.404296875, 0.2177734375, 0.1572265625, 0.208984375, 0.11474609375, 0.005950927734375]

================================================================================
070_model.layers.4.self_attn.k_norm: Gemma3RMSNorm (model.layers.4.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.125669, Std: 1.655496
     First 10: [0.012314993888139725, -0.013491928577423096, -0.015415927395224571, -0.008481740951538086, -0.005269467830657959, -0.02654258906841278, 0.0031440891325473785, 0.017254464328289032, -0.04134698957204819, 0.0066382139921188354]
     Last 10:  [0.8974237442016602, -2.876885175704956, -2.4859397411346436, 4.968405246734619, 2.090125560760498, -1.0859206914901733, -2.987549066543579, 1.7265398502349854, 0.718114972114563, -6.725058078765869]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.4.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.012677, Std: 2.899725
     First 10: [0.01264932006597519, -0.031221136450767517, -0.03175630792975426, -0.016086656600236893, -0.010137646459043026, -0.056242506951093674, 0.006918964441865683, 0.029515309259295464, -0.073166623711586, 0.016325069591403008]
     Last 10:  [0.7501626014709473, -1.7800770998001099, -1.8761016130447388, 3.371260404586792, 1.2316701412200928, -0.9506036043167114, -1.6067391633987427, 1.5620344877243042, 0.40281760692596436, -4.028378009796143]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.060109
     First 10: [-0.11572265625, 0.9921875, 0.7734375, 0.6328125, 0.65625, 0.82421875, 0.89453125, 0.47265625, 0.5234375, 1.1171875]
     Last 10:  [0.447265625, 0.0712890625, 0.306640625, 0.1748046875, 0.020263671875, 0.515625, -0.06884765625, 0.56640625, -0.02880859375, 0.037109375]

================================================================================
071_model.layers.4.self_attn.o_proj: Linear (model.layers.4.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.4.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.022439, Std: 0.505021
     First 10: [0.17033590376377106, 0.05051655322313309, -0.3656069338321686, -0.04745890572667122, 0.050696223974227905, -0.10772868245840073, 0.08719030022621155, -0.09973283857107162, -0.0852748230099678, -0.2342635691165924]
     Last 10:  [0.0033289827406406403, 0.02252907305955887, -0.12790189683437347, 0.016578562557697296, 0.11266755312681198, 0.029179105535149574, 0.18660880625247955, 0.022074971348047256, 0.023214150220155716, 0.08293838053941727]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.4.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.007702, Std: 0.440621
     First 10: [0.014555143192410469, -0.09736758470535278, 0.16419917345046997, -0.18441641330718994, -0.0365825891494751, -0.21398252248764038, 0.3922446370124817, -0.03422272205352783, -0.1021280288696289, -0.03742174804210663]
     Last 10:  [0.37619465589523315, 0.21999165415763855, 2.294915199279785, 0.2593448758125305, 0.20895904302597046, -0.20583750307559967, 0.13486140966415405, 0.18887373805046082, 0.29951274394989014, 0.2394901067018509]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000009
     First 10: [0.009765625, 0.0086669921875, 0.01080322265625, 0.01025390625, -0.02734375, -0.00408935546875, 0.0245361328125, 0.0067138671875, -0.010498046875, -0.0098876953125]
     Last 10:  [-0.0054931640625, 0.00286865234375, 0.06689453125, 0.027099609375, -0.018798828125, -0.02294921875, -0.0208740234375, -0.0264892578125, -0.007080078125, -0.01165771484375]

================================================================================
072_model.layers.4.self_attn: Gemma3Attention (model.layers.4.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.4.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.007702, Std: 0.440621
     First 10: [0.014555143192410469, -0.09736758470535278, 0.16419917345046997, -0.18441641330718994, -0.0365825891494751, -0.21398252248764038, 0.3922446370124817, -0.03422272205352783, -0.1021280288696289, -0.03742174804210663]
     Last 10:  [0.37619465589523315, 0.21999165415763855, 2.294915199279785, 0.2593448758125305, 0.20895904302597046, -0.20583750307559967, 0.13486140966415405, 0.18887373805046082, 0.29951274394989014, 0.2394901067018509]
     Zeros: 0, Total: 2560

================================================================================
073_model.layers.4.post_attention_layernorm: Gemma3RMSNorm (model.layers.4.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.4.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.007702, Std: 0.440621
     First 10: [0.014555143192410469, -0.09736758470535278, 0.16419917345046997, -0.18441641330718994, -0.0365825891494751, -0.21398252248764038, 0.3922446370124817, -0.03422272205352783, -0.1021280288696289, -0.03742174804210663]
     Last 10:  [0.37619465589523315, 0.21999165415763855, 2.294915199279785, 0.2593448758125305, 0.20895904302597046, -0.20583750307559967, 0.13486140966415405, 0.18887373805046082, 0.29951274394989014, 0.2394901067018509]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.4.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.473291, Std: 27.055550
     First 10: [0.127812460064888, -0.41483813524246216, 0.5580591559410095, -0.5135622620582581, -0.03599090874195099, -1.116982340812683, 2.0475053787231445, -0.06017327308654785, -0.5563559532165527, -0.15152572095394135]
     Last 10:  [0.6357937455177307, 1.333924412727356, 0.8210874795913696, 2.2757928371429443, 1.5109899044036865, -1.2481005191802979, 0.7542465329170227, 0.8642652034759521, 2.0191450119018555, 6.566260814666748]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 3.181015
     First 10: [3.21875, 1.046875, 0.6328125, 0.337890625, -0.52734375, 1.5078125, 1.5078125, -0.1552734375, 1.6171875, 0.9453125]
     Last 10:  [-0.298828125, 1.515625, -0.8515625, 2.640625, 2.0, 1.515625, 1.3203125, 0.8984375, 1.796875, 10.375]

================================================================================
074_model.layers.4.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.4.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.4.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 9.999792, Std: 293.349976
     First 10: [0.6365010142326355, 0.24386143684387207, -1.648482322692871, 1.6561124324798584, -1.3214404582977295, 2.43515682220459, -4.078027725219727, -0.3999309837818146, -3.616114616394043, -0.03963957726955414]
     Last 10:  [-1.9277513027191162, 1.5426496267318726, -51.78976821899414, 2.3603432178497314, 3.93113112449646, -0.022531509399414062, -0.5477527976036072, -0.34085965156555176, 8.966014862060547, -51.011688232421875]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.4.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.017291, Std: 0.834793
     First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
     Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 12.549014
     First 10: [13.0, 26.75, 2.9375, 22.125, 19.0, 15.125, 13.25, 16.75, 4.21875, 19.5]
     Last 10:  [15.5, 14.0625, -0.6875, 17.0, 10.3125, 14.6875, 15.625, 35.25, 13.125, 1.640625]

================================================================================
075_model.layers.4.mlp.gate_proj: Linear (model.layers.4.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.4.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.017291, Std: 0.834793
     First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
     Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.4.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.295693, Std: 0.736810
     First 10: [-0.19264259934425354, 0.19654880464076996, 0.052477043122053146, 0.3934393525123596, -0.04491916298866272, 0.09266506880521774, -0.36228710412979126, 0.013112090528011322, 0.17016193270683289, 0.13246354460716248]
     Last 10:  [-0.35891932249069214, -0.613654375076294, -0.14002388715744019, -3.3150644302368164, -0.7791419625282288, -1.9079103469848633, -1.279184103012085, 0.570376992225647, 0.014779150485992432, -1.9315078258514404]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000163
     First 10: [-0.00872802734375, -0.054443359375, 0.0185546875, 0.06201171875, 0.050048828125, -0.0693359375, -0.009765625, 0.0093994140625, 0.003692626953125, 0.0038299560546875]
     Last 10:  [-0.12353515625, 0.056396484375, -0.0001277923583984375, -0.015380859375, 0.0250244140625, -0.11376953125, -0.11572265625, -0.04833984375, -0.11279296875, -0.03515625]

================================================================================
076_model.layers.4.mlp.act_fn: PytorchGELUTanh (model.layers.4.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.4.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.295693, Std: 0.736810
     First 10: [-0.19264259934425354, 0.19654880464076996, 0.052477043122053146, 0.3934393525123596, -0.04491916298866272, 0.09266506880521774, -0.36228710412979126, 0.013112090528011322, 0.17016193270683289, 0.13246354460716248]
     Last 10:  [-0.35891932249069214, -0.613654375076294, -0.14002388715744019, -3.3150644302368164, -0.7791419625282288, -1.9079103469848633, -1.279184103012085, 0.570376992225647, 0.014779150485992432, -1.9315078258514404]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.4.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.022001, Std: 0.293080
     First 10: [-0.08160758763551712, 0.11358698457479477, 0.027336636558175087, 0.2569097876548767, -0.021654896438121796, 0.04975325986742973, -0.12990999221801758, 0.006624632515013218, 0.09657660126686096, 0.07321132719516754]
     Last 10:  [-0.12915411591529846, -0.1655515879392624, -0.062215618789196014, -0.00124137953389436, -0.16989000141620636, -0.053786467760801315, -0.12867867946624756, 0.40824198722839355, 0.007476710248738527, -0.05155172571539879]
     Zeros: 7, Total: 8192

================================================================================
077_model.layers.4.mlp.up_proj: Linear (model.layers.4.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.4.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.017291, Std: 0.834793
     First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
     Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.4.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.009083, Std: 0.565768
     First 10: [0.1912337988615036, -0.054631222039461136, -0.11619031429290771, 0.02423485368490219, -0.15307192504405975, -0.07235513627529144, 0.9513946771621704, -0.02000294253230095, 0.27787795662879944, 0.3723047375679016]
     Last 10:  [0.15171466767787933, -0.20338484644889832, -0.1197502464056015, -0.19090571999549866, 1.0182992219924927, 0.23071669042110443, -0.10596521198749542, -0.05943194776773453, 0.9068560600280762, -0.03951413929462433]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000034
     First 10: [-0.00677490234375, -0.03564453125, 0.036376953125, -0.0245361328125, -0.00124359130859375, 0.0272216796875, -0.01025390625, 0.00946044921875, -0.006317138671875, -0.037841796875]
     Last 10:  [-0.041259765625, 0.019287109375, 0.054443359375, 0.037109375, -0.0791015625, 0.035400390625, 0.037353515625, 0.0289306640625, -0.0196533203125, -0.016357421875]

================================================================================
078_model.layers.4.mlp.down_proj: Linear (model.layers.4.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.4.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.000240, Std: 0.224250
     First 10: [-0.015606128610670567, -0.006205395795404911, -0.003176252357661724, 0.006226171273738146, 0.003314756788313389, -0.0035999040119349957, -0.12359567731618881, -0.00013251214113552123, 0.026836508885025978, 0.027256924659013748]
     Last 10:  [-0.019594574347138405, 0.033670682460069656, 0.007450335659086704, 0.00023698645236436278, -0.17299886047840118, -0.012409435585141182, 0.013635464012622833, -0.02426261641085148, 0.006780299823731184, 0.002037022029981017]
     Zeros: 7, Total: 8192

  → OUTPUT[0]: model.layers.4.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001320, Std: 0.120257
     First 10: [0.02510225772857666, 0.029887359589338303, -0.012600895017385483, 0.010254492983222008, -0.039633609354496, -0.014592068269848824, 0.007261150050908327, 0.024423446506261826, -0.03147883713245392, 0.013053882867097855]
     Last 10:  [-0.03207657113671303, 0.011519163846969604, 0.10274564474821091, -0.07696116715669632, -0.0006829332560300827, -0.1984306275844574, 0.012257646769285202, 0.03350318223237991, 0.1248776763677597, -0.07188175618648529]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000012
     First 10: [0.0220947265625, -0.01007080078125, 0.0140380859375, 0.006805419921875, -0.00689697265625, -0.006866455078125, 0.006622314453125, -0.0023651123046875, 0.016357421875, -0.0123291015625]
     Last 10:  [-0.003509521484375, -0.003875732421875, 0.015380859375, -0.048095703125, -0.0135498046875, -0.01611328125, -0.00084686279296875, 6.914138793945312e-05, 0.007537841796875, 0.005767822265625]

================================================================================
079_model.layers.4.mlp: Gemma3MLP (model.layers.4.mlp)
================================================================================

  → INPUT[0]: model.layers.4.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.017291, Std: 0.834793
     First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
     Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.4.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001320, Std: 0.120257
     First 10: [0.02510225772857666, 0.029887359589338303, -0.012600895017385483, 0.010254492983222008, -0.039633609354496, -0.014592068269848824, 0.007261150050908327, 0.024423446506261826, -0.03147883713245392, 0.013053882867097855]
     Last 10:  [-0.03207657113671303, 0.011519163846969604, 0.10274564474821091, -0.07696116715669632, -0.0006829332560300827, -0.1984306275844574, 0.012257646769285202, 0.03350318223237991, 0.1248776763677597, -0.07188175618648529]
     Zeros: 0, Total: 2560

================================================================================
080_model.layers.4.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.4.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.4.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001320, Std: 0.120257
     First 10: [0.02510225772857666, 0.029887359589338303, -0.012600895017385483, 0.010254492983222008, -0.039633609354496, -0.014592068269848824, 0.007261150050908327, 0.024423446506261826, -0.03147883713245392, 0.013053882867097855]
     Last 10:  [-0.03207657113671303, 0.011519163846969604, 0.10274564474821091, -0.07696116715669632, -0.0006829332560300827, -0.1984306275844574, 0.012257646769285202, 0.03350318223237991, 0.1248776763677597, -0.07188175618648529]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.4.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 5.266237, Std: 271.957733
     First 10: [0.6422917246818542, 1.3990375995635986, -0.6423391103744507, 0.3681481182575226, -0.6318506598472595, -1.0245895385742188, 0.5242478251457214, 0.6709443926811218, -2.2602529525756836, 0.7508748173713684]
     Last 10:  [-0.4026384949684143, 0.5052456855773926, 18.121112823486328, -2.3629329204559326, -0.031530898064374924, -8.428584098815918, 0.4980212152004242, 0.6303354501724243, 5.70792293548584, -11.881185531616211]
     Zeros: 28, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 6.994090
     First 10: [1.015625, 2.6875, 3.015625, 1.828125, 0.255859375, 4.53125, 4.6875, 1.1640625, 4.65625, 3.53125]
     Last 10:  [0.69921875, 4.9375, 22.875, 3.15625, 5.25, 4.75, 4.5, 1.546875, 5.1875, 21.375]

================================================================================
081_model.layers.5.input_layernorm: Gemma3RMSNorm (model.layers.5.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.5.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 15.266031, Std: 527.773193
     First 10: [1.2787927389144897, 1.6428990364074707, -2.2908215522766113, 2.0242605209350586, -1.9532911777496338, 1.410567283630371, -3.5537798404693604, 0.27101340889930725, -5.876367568969727, 0.7112352252006531]
     Last 10:  [-2.3303897380828857, 2.0478954315185547, -33.66865539550781, -0.002589702606201172, 3.8996002674102783, -8.451115608215332, -0.04973158240318298, 0.28947579860687256, 14.673937797546387, -62.89287567138672]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.5.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002493, Std: 1.256721
     First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
     Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 13.077600
     First 10: [27.125, 23.25, 6.21875, 18.375, 16.125, 11.25, 9.625, 17.125, 4.75, 15.375]
     Last 10:  [15.5625, 11.0, -0.69140625, 25.125, 9.0625, 11.25, 11.5625, 33.0, 9.5, 1.1328125]

================================================================================
082_model.layers.5.self_attn.q_proj: Linear (model.layers.5.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002493, Std: 1.256721
     First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
     Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.5.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.001296, Std: 1.526073
     First 10: [0.15951862931251526, 0.0761331170797348, 0.3921384811401367, -0.16702556610107422, -0.303239643573761, -0.29362058639526367, -0.12124329805374146, -0.13774695992469788, 0.8641344308853149, 0.17867092788219452]
     Last 10:  [-1.8424277305603027, -0.708545982837677, 0.208127960562706, -0.2384515106678009, 0.16396889090538025, 0.26922449469566345, 0.044506654143333435, -0.5637449622154236, 0.2543957829475403, -0.4659898579120636]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: 0.000024
     First 10: [0.11669921875, 0.158203125, 0.0303955078125, 0.036376953125, -0.1396484375, 0.0498046875, 0.034912109375, 0.2138671875, 0.08740234375, -0.01556396484375]
     Last 10:  [0.032470703125, -0.0087890625, 0.020751953125, 0.01007080078125, 0.01202392578125, -0.015869140625, -0.00762939453125, 0.0167236328125, 0.00152587890625, -0.0014801025390625]

================================================================================
083_model.layers.5.self_attn.k_proj: Linear (model.layers.5.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002493, Std: 1.256721
     First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
     Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.5.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.026624, Std: 1.072770
     First 10: [0.0052673304453492165, -0.6102632880210876, 0.0014539677649736404, 0.07531185448169708, -0.031955406069755554, -0.00020262273028492928, -0.07609817385673523, -0.0008615069091320038, -0.023195087909698486, 1.6033649444580078e-05]
     Last 10:  [-2.0749192237854004, -3.9119951725006104, 3.5476362705230713, 1.3466933965682983, 0.11295855045318604, 0.95540452003479, -1.5809605121612549, -2.5169272422790527, 4.610164642333984, -5.188906192779541]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000002
     First 10: [0.0947265625, 0.11279296875, 0.0198974609375, 0.01495361328125, -0.006988525390625, 0.06640625, -0.0093994140625, 0.0004634857177734375, 0.0038604736328125, -0.004791259765625]
     Last 10:  [0.000957489013671875, 0.04248046875, -0.078125, 0.0556640625, -0.0284423828125, -0.0186767578125, 0.034423828125, 0.07861328125, -0.036865234375, -0.0206298828125]

================================================================================
084_model.layers.5.self_attn.v_proj: Linear (model.layers.5.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002493, Std: 1.256721
     First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
     Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.5.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.000814, Std: 0.839529
     First 10: [-0.03275470435619354, -0.02581084705889225, -0.0416322723031044, 0.054272882640361786, -0.024547144770622253, 0.005798856727778912, -0.06249213218688965, -0.0034544747322797775, -0.04761842265725136, 0.09810389578342438]
     Last 10:  [-0.1880067139863968, 0.9147224426269531, -0.10676532983779907, -0.6548353433609009, -0.020590394735336304, 1.4306422472000122, -0.7914350032806396, -0.4583362340927124, -0.256325364112854, -0.2087811827659607]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000052
     First 10: [-0.0791015625, -0.00714111328125, -0.01202392578125, 0.06884765625, 0.000774383544921875, -0.0240478515625, 0.046630859375, -0.007781982421875, 0.02734375, 0.107421875]
     Last 10:  [0.005950927734375, 0.09375, -0.004913330078125, 0.031494140625, -0.04443359375, -0.00860595703125, -0.08154296875, 0.03955078125, -0.1044921875, 0.003448486328125]

================================================================================
085_model.layers.5.self_attn.q_norm: Gemma3RMSNorm (model.layers.5.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.001296, Std: 1.526073
     First 10: [0.15951862931251526, 0.0761331170797348, 0.3921384811401367, -0.16702556610107422, -0.303239643573761, -0.29362058639526367, -0.12124329805374146, -0.13774695992469788, 0.8641344308853149, 0.17867092788219452]
     Last 10:  [-1.8424277305603027, -0.708545982837677, 0.208127960562706, -0.2384515106678009, 0.16396889090538025, 0.26922449469566345, 0.044506654143333435, -0.5637449622154236, 0.2543957829475403, -0.4659898579120636]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.5.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.058599, Std: 1.411086
     First 10: [0.03264807537198067, -0.006232751067727804, -0.11771100759506226, -0.0, 0.03310022130608559, -3.0287485122680664, -0.0, -0.8589208722114563, 0.24760279059410095, 1.2481844425201416]
     Last 10:  [-0.7923656702041626, -0.38339516520500183, 0.1996859312057495, -0.3475509583950043, 0.12855012714862823, 0.14062532782554626, 0.028502393513917923, -0.9027866125106812, 0.11219190061092377, -0.35171329975128174]
     Zeros: 64, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.001856
     First 10: [-0.94140625, -1.0234375, -1.0859375, -1.0, -1.03125, 1.953125, -1.0, 0.78515625, -0.91796875, 1.0]
     Last 10:  [-0.462890625, -0.32421875, 0.1982421875, 0.8203125, -0.0208740234375, -0.34765625, -0.2001953125, 1.0, -0.44921875, -0.057373046875]

================================================================================
086_model.layers.5.self_attn.k_norm: Gemma3RMSNorm (model.layers.5.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.026624, Std: 1.072770
     First 10: [0.0052673304453492165, -0.6102632880210876, 0.0014539677649736404, 0.07531185448169708, -0.031955406069755554, -0.00020262273028492928, -0.07609817385673523, -0.0008615069091320038, -0.023195087909698486, 1.6033649444580078e-05]
     Last 10:  [-2.0749192237854004, -3.9119951725006104, 3.5476362705230713, 1.3466933965682983, 0.11295855045318604, 0.95540452003479, -1.5809605121612549, -2.5169272422790527, 4.610164642333984, -5.188906192779541]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.5.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.178844, Std: 6.838080
     First 10: [0.00024941281299106777, -0.036120641976594925, 0.005296891089528799, 0.0, -0.0, -0.0013384142657741904, -0.0, -0.005588657688349485, 0.008786465041339397, 9.015590330818668e-05]
     Last 10:  [-12.835454940795898, -27.23606300354004, 14.769530296325684, 3.801056385040283, 0.5738880634307861, 6.651710510253906, -10.188867568969727, -8.702466011047363, 35.783653259277344, -27.460765838623047]
     Zeros: 20, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.930750
     First 10: [-0.984375, -0.98046875, 0.2021484375, -1.0, -1.0, 1.1796875, -1.0, 1.140625, -1.125, 0.85546875]
     Last 10:  [7.21875, 8.25, 4.53125, 2.75, 5.75, 8.25, 7.5625, 3.59375, 9.3125, 6.03125]

================================================================================
087_model.layers.5.self_attn.o_proj: Linear (model.layers.5.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.5.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.023490, Std: 0.338652
     First 10: [-0.03275470435619354, -0.02581084705889225, -0.0416322723031044, 0.054272882640361786, -0.024547144770622253, 0.005798856727778912, -0.06249213218688965, -0.0034544747322797775, -0.04761842265725136, 0.09810389578342438]
     Last 10:  [-0.31355229020118713, 0.142403244972229, 0.0572931170463562, -0.2752956449985504, 0.250468909740448, 0.36510512232780457, -0.46638888120651245, -0.31967693567276, 0.34967198967933655, -0.04819459095597267]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.5.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.010112, Std: 0.687828
     First 10: [-0.30982542037963867, -0.06090114265680313, 0.09604249149560928, 0.004388913512229919, -0.023113245144486427, -0.1547355204820633, 0.1662079095840454, 0.05067374184727669, -0.014496896415948868, -0.07123209536075592]
     Last 10:  [-0.004512708634138107, -0.04602029174566269, -2.4307641983032227, -2.854137897491455, -0.08219689875841141, -0.021359354257583618, -0.16004371643066406, -0.05735214054584503, 0.22727367281913757, 0.7559831738471985]
     Zeros: 1, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000068
     First 10: [0.0106201171875, 0.0146484375, 0.0205078125, 0.060302734375, -0.02685546875, 0.04345703125, 0.020263671875, -0.04296875, -0.017578125, -0.042236328125]
     Last 10:  [-0.0013427734375, 0.011474609375, -0.0069580078125, 0.0189208984375, 0.00157928466796875, -0.0012664794921875, -0.006805419921875, 0.0040283203125, 0.0009918212890625, 0.0098876953125]

================================================================================
088_model.layers.5.self_attn: Gemma3Attention (model.layers.5.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.5.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.010112, Std: 0.687828
     First 10: [-0.30982542037963867, -0.06090114265680313, 0.09604249149560928, 0.004388913512229919, -0.023113245144486427, -0.1547355204820633, 0.1662079095840454, 0.05067374184727669, -0.014496896415948868, -0.07123209536075592]
     Last 10:  [-0.004512708634138107, -0.04602029174566269, -2.4307641983032227, -2.854137897491455, -0.08219689875841141, -0.021359354257583618, -0.16004371643066406, -0.05735214054584503, 0.22727367281913757, 0.7559831738471985]
     Zeros: 1, Total: 2560

================================================================================
089_model.layers.5.post_attention_layernorm: Gemma3RMSNorm (model.layers.5.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.5.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.010112, Std: 0.687828
     First 10: [-0.30982542037963867, -0.06090114265680313, 0.09604249149560928, 0.004388913512229919, -0.023113245144486427, -0.1547355204820633, 0.1662079095840454, 0.05067374184727669, -0.014496896415948868, -0.07123209536075592]
     Last 10:  [-0.004512708634138107, -0.04602029174566269, -2.4307641983032227, -2.854137897491455, -0.08219689875841141, -0.021359354257583618, -0.16004371643066406, -0.05735214054584503, 0.22727367281913757, 0.7559831738471985]
     Zeros: 1, Total: 2560

  → OUTPUT[0]: model.layers.5.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 7.332312, Std: 127.745911
     First 10: [-6.5338053703308105, -0.430414617061615, 0.611114501953125, 0.03002096898853779, -0.037653516978025436, -1.1392951011657715, 1.1104530096054077, 0.1992187649011612, -0.13177596032619476, -0.4516282379627228]
     Last 10:  [-0.011925594881176949, -0.3543650209903717, -2.069706678390503, -174.77890014648438, -0.8126993775367737, -0.16933710873126984, -1.239659309387207, -0.2678478956222534, 1.977867841720581, 37.75178146362305]
     Zeros: 1, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 9.760829
     First 10: [13.5, 3.859375, 3.375, 3.703125, 0.1201171875, 4.0625, 3.59375, 1.703125, 5.25, 3.359375]
     Last 10:  [0.8125, 4.28125, -0.416015625, 41.0, 5.78125, 4.4375, 4.3125, 2.203125, 4.96875, 33.25]

================================================================================
090_model.layers.5.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.5.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.5.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 22.598341, Std: 589.394226
     First 10: [-5.255012512207031, 1.212484359741211, -1.6797070503234863, 2.05428147315979, -1.990944743156433, 0.2712721824645996, -2.443326950073242, 0.47023218870162964, -6.008143424987793, 0.2596069872379303]
     Last 10:  [-2.342315435409546, 1.6935304403305054, -35.73836135864258, -174.781494140625, 3.0869009494781494, -8.620452880859375, -1.2893909215927124, 0.02162790298461914, 16.651805877685547, -25.141094207763672]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.5.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002586, Std: 0.504907
     First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
     Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
     Zeros: 8, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 11.425471
     First 10: [8.875, 19.625, 4.21875, 24.0, 31.125, 13.9375, 13.25, 21.125, 5.75, 17.625]
     Last 10:  [23.625, 13.5625, -0.57421875, 3.109375, 10.875, 12.9375, 14.3125, 28.625, 12.125, 2.140625]

================================================================================
091_model.layers.5.mlp.gate_proj: Linear (model.layers.5.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.5.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002586, Std: 0.504907
     First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
     Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
     Zeros: 8, Total: 2560

  → OUTPUT[0]: model.layers.5.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.281477, Std: 0.603702
     First 10: [-0.036246344447135925, -0.008793199434876442, -0.009725552052259445, -0.0047374144196510315, 0.0344904363155365, -0.05973176658153534, -0.03852725028991699, -0.029716668650507927, -0.06765959411859512, 0.027868853881955147]
     Last 10:  [-0.3453226685523987, -0.10118064284324646, -0.2203465700149536, -1.264524221420288, 0.051933228969573975, -0.14620453119277954, -0.20661771297454834, -1.612801432609558, -0.978604793548584, -0.16097387671470642]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000070
     First 10: [-0.0067138671875, 0.0052490234375, -0.0322265625, -0.029052734375, 0.03173828125, -0.068359375, 0.0029144287109375, -0.0013580322265625, -0.01214599609375, 0.01025390625]
     Last 10:  [-0.032958984375, 0.0286865234375, 0.056396484375, 0.0830078125, 0.029052734375, -0.054931640625, -0.00836181640625, -0.08642578125, -0.07666015625, 0.025146484375]

================================================================================
092_model.layers.5.mlp.act_fn: PytorchGELUTanh (model.layers.5.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.5.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.281477, Std: 0.603702
     First 10: [-0.036246344447135925, -0.008793199434876442, -0.009725552052259445, -0.0047374144196510315, 0.0344904363155365, -0.05973176658153534, -0.03852725028991699, -0.029716668650507927, -0.06765959411859512, 0.027868853881955147]
     Last 10:  [-0.3453226685523987, -0.10118064284324646, -0.2203465700149536, -1.264524221420288, 0.051933228969573975, -0.14620453119277954, -0.20661771297454834, -1.612801432609558, -0.978604793548584, -0.16097387671470642]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.5.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.006933, Std: 0.228916
     First 10: [-0.017599157989025116, -0.004365753848105669, -0.0048250420950353146, -0.002359753707423806, 0.017719700932502747, -0.02844335325062275, -0.018671603873372078, -0.014506088569760323, -0.032004911452531815, 0.014244234189391136]
     Last 10:  [-0.12602148950099945, -0.04651313275098801, -0.09095995128154755, -0.13049839437007904, 0.027042098343372345, -0.0646049976348877, -0.0863986536860466, -0.08629217743873596, -0.16052642464637756, -0.07019399851560593]
     Zeros: 0, Total: 8192

================================================================================
093_model.layers.5.mlp.up_proj: Linear (model.layers.5.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.5.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002586, Std: 0.504907
     First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
     Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
     Zeros: 8, Total: 2560

  → OUTPUT[0]: model.layers.5.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.015976, Std: 0.463782
     First 10: [0.03354082629084587, -0.047890804708004, -0.04949067160487175, -0.01926650106906891, -0.025972086936235428, 0.021167386323213577, 0.0427279993891716, 0.03771296143531799, -0.011903483420610428, -0.007411304861307144]
     Last 10:  [-0.44587942957878113, 0.10131098330020905, 0.4576833248138428, -0.10138928890228271, 0.000503448536619544, 0.09858541190624237, -0.20070575177669525, 0.8169538378715515, -0.27038806676864624, 0.04573577642440796]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000013
     First 10: [0.02197265625, -0.00823974609375, 0.0057373046875, 0.00640869140625, -0.044189453125, -0.01409912109375, -0.0267333984375, 0.01531982421875, -0.08447265625, -0.0244140625]
     Last 10:  [-0.07470703125, -0.045166015625, 0.01275634765625, 0.00421142578125, 0.052734375, -0.043212890625, -0.059326171875, -0.01104736328125, 0.07568359375, -0.006866455078125]

================================================================================
094_model.layers.5.mlp.down_proj: Linear (model.layers.5.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.5.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.001273, Std: 0.120330
     First 10: [-0.0005902902921661735, 0.00020907945872750133, 0.00023879457148723304, 4.546419586404227e-05, -0.0004602176195476204, -0.0006020714645273983, -0.0007978002540767193, -0.0005470675532706082, 0.0003809699264820665, -0.00010556836059549823]
     Last 10:  [0.05619039013981819, -0.004712291061878204, -0.04163085296750069, 0.01323113963007927, 1.3614304407383315e-05, -0.006369110196828842, 0.017340706661343575, -0.07049672305583954, 0.04340443015098572, -0.00321037694811821]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.5.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000349, Std: 0.132691
     First 10: [-0.002620988292619586, 0.00015947571955621243, 0.00011993785301456228, -0.0002842181420419365, 0.0017182312440127134, -0.00039728463161736727, -0.0024894720409065485, -0.0003424918104428798, 0.0012876114342361689, 0.00023883699032012373]
     Last 10:  [-0.09005661308765411, 0.026190225034952164, 0.07193337380886078, 0.12247564643621445, -0.0817975327372551, 0.03725900128483772, 0.04256312549114227, 0.05825946107506752, -0.10471723973751068, -0.09494146704673767]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000007
     First 10: [0.0054931640625, 0.031494140625, -0.027099609375, -0.006744384765625, -0.01708984375, -0.01324462890625, 0.000560760498046875, -0.01385498046875, -0.00958251953125, -0.006439208984375]
     Last 10:  [9.775161743164062e-05, 0.00982666015625, -0.031494140625, 0.0064697265625, 0.0068359375, 0.0152587890625, 0.004638671875, 0.002105712890625, -0.00982666015625, 0.0067138671875]

================================================================================
095_model.layers.5.mlp: Gemma3MLP (model.layers.5.mlp)
================================================================================

  → INPUT[0]: model.layers.5.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002586, Std: 0.504907
     First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
     Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
     Zeros: 8, Total: 2560

  → OUTPUT[0]: model.layers.5.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000349, Std: 0.132691
     First 10: [-0.002620988292619586, 0.00015947571955621243, 0.00011993785301456228, -0.0002842181420419365, 0.0017182312440127134, -0.00039728463161736727, -0.0024894720409065485, -0.0003424918104428798, 0.0012876114342361689, 0.00023883699032012373]
     Last 10:  [-0.09005661308765411, 0.026190225034952164, 0.07193337380886078, 0.12247564643621445, -0.0817975327372551, 0.03725900128483772, 0.04256312549114227, 0.05825946107506752, -0.10471723973751068, -0.09494146704673767]
     Zeros: 0, Total: 2560

================================================================================
096_model.layers.5.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.5.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.5.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000349, Std: 0.132691
     First 10: [-0.002620988292619586, 0.00015947571955621243, 0.00011993785301456228, -0.0002842181420419365, 0.0017182312440127134, -0.00039728463161736727, -0.0024894720409065485, -0.0003424918104428798, 0.0012876114342361689, 0.00023883699032012373]
     Last 10:  [-0.09005661308765411, 0.026190225034952164, 0.07193337380886078, 0.12247564643621445, -0.0817975327372551, 0.03725900128483772, 0.04256312549114227, 0.05825946107506752, -0.10471723973751068, -0.09494146704673767]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.5.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 4.475021, Std: 291.647949
     First 10: [-9.993051528930664, 0.3081437349319458, 0.153636172413826, -0.3027816414833069, 0.7355173230171204, -0.8567469120025635, -5.2611918449401855, -0.2518577575683594, 2.4324264526367188, 0.4408858120441437]
     Last 10:  [-1.7385258674621582, 2.0523505210876465, 15.470702171325684, 19.755617141723633, -5.988829135894775, 2.6533362865448, 2.945849895477295, 2.0577621459960938, -7.547106742858887, -22.048229217529297]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 10.625275
     First 10: [12.8125, 6.0, 3.640625, 2.859375, 0.55078125, 6.8125, 6.65625, 1.6640625, 5.84375, 5.6875]
     Last 10:  [1.109375, 7.5625, 22.5, 16.625, 7.0, 6.78125, 6.5625, 2.859375, 6.875, 24.375]

================================================================================
097_model.layers.6.input_layernorm: Gemma3RMSNorm (model.layers.6.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.6.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 27.073360, Std: 860.292725
     First 10: [-15.248064041137695, 1.5206280946731567, -1.5260708332061768, 1.751499891281128, -1.255427360534668, -0.5854747295379639, -7.704518795013428, 0.21837443113327026, -3.575716972351074, 0.700492799282074]
     Last 10:  [-4.080841064453125, 3.7458810806274414, -20.267658233642578, -155.02587890625, -2.901928186416626, -5.967116355895996, 1.6564589738845825, 2.079390048980713, 9.10469913482666, -47.18932342529297]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.6.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005187, Std: 1.405668
     First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
     Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 20.808052
     First 10: [14.6875, 37.0, 6.65625, 42.5, 57.75, 23.875, 22.75, 41.25, 10.75, 29.375]
     Last 10:  [44.0, 24.0, -0.30078125, 17.75, 18.625, 22.5, 24.375, 53.5, 20.375, 3.5625]

================================================================================
098_model.layers.6.self_attn.q_proj: Linear (model.layers.6.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005187, Std: 1.405668
     First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
     Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.6.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.072990, Std: 2.141624
     First 10: [-0.11726157367229462, -0.17229698598384857, 2.3283441066741943, -0.17828825116157532, 0.18344822525978088, -0.8387015461921692, -0.1320740282535553, -0.12437295913696289, -0.4533459544181824, 0.09086351096630096]
     Last 10:  [-2.0913147926330566, -1.2477000951766968, 0.7917398810386658, 0.7085075378417969, -1.003490686416626, 3.7832610607147217, 0.5595666170120239, 0.24933119118213654, -0.47622621059417725, 1.0557661056518555]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: 0.000035
     First 10: [0.011962890625, 0.0152587890625, -0.001922607421875, -0.0238037109375, -0.00360107421875, 0.007110595703125, 0.0028076171875, -0.01300048828125, -0.0257568359375, 0.02294921875]
     Last 10:  [-0.06298828125, 0.0185546875, 0.07568359375, -0.045654296875, -0.07080078125, 0.033935546875, -0.0213623046875, 0.039306640625, -0.033935546875, 0.03173828125]

================================================================================
099_model.layers.6.self_attn.k_proj: Linear (model.layers.6.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005187, Std: 1.405668
     First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
     Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.6.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.023396, Std: 1.857035
     First 10: [-0.004414652474224567, 0.00013446807861328125, -0.630158543586731, -1.7858089208602905, 2.456576108932495, -0.03904158994555473, -2.072155475616455, 0.0025050006806850433, -0.0065973661839962006, -0.002747509628534317]
     Last 10:  [-5.209957599639893, -4.453174591064453, 3.201387405395508, -0.8471319675445557, -3.2109005451202393, 2.397477149963379, 3.8828206062316895, 1.761521816253662, -2.1639976501464844, 1.8167604207992554]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000060
     First 10: [-0.0224609375, 0.011474609375, -0.0054931640625, -0.00665283203125, -0.00396728515625, -0.0031890869140625, -0.0022735595703125, 0.003326416015625, -0.00531005859375, -0.00347900390625]
     Last 10:  [-0.039306640625, 0.03759765625, -0.02880859375, -0.060791015625, 0.03857421875, -0.00970458984375, -0.06884765625, -0.037109375, 0.047119140625, -0.0126953125]

================================================================================
100_model.layers.6.self_attn.v_proj: Linear (model.layers.6.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005187, Std: 1.405668
     First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
     Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.6.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.003487, Std: 0.971380
     First 10: [0.16002824902534485, -0.021760500967502594, 0.030557751655578613, 0.03504304215312004, -0.017011435702443123, -0.045198749750852585, -0.03244747221469879, 0.021938040852546692, 0.10085819661617279, -0.09421645849943161]
     Last 10:  [-0.05353561043739319, -0.6082468032836914, 0.18518412113189697, -0.25615179538726807, -0.4734961688518524, -0.1703626662492752, -0.1812119483947754, -0.5853073596954346, -0.48112261295318604, 0.3949637711048126]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000199
     First 10: [-0.030029296875, -0.0025482177734375, 0.03662109375, -0.04443359375, -0.039794921875, 2.4318695068359375e-05, 0.044677734375, 0.0107421875, -0.0322265625, -0.02734375]
     Last 10:  [0.031982421875, -0.060791015625, 0.0213623046875, -0.007110595703125, -0.0380859375, -0.0220947265625, -0.0086669921875, 0.0191650390625, 0.0189208984375, 0.009033203125]

================================================================================
101_model.layers.6.self_attn.q_norm: Gemma3RMSNorm (model.layers.6.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.072990, Std: 2.141624
     First 10: [-0.11726157367229462, -0.17229698598384857, 2.3283441066741943, -0.17828825116157532, 0.18344822525978088, -0.8387015461921692, -0.1320740282535553, -0.12437295913696289, -0.4533459544181824, 0.09086351096630096]
     Last 10:  [-2.0913147926330566, -1.2477000951766968, 0.7917398810386658, 0.7085075378417969, -1.003490686416626, 3.7832610607147217, 0.5595666170120239, 0.24933119118213654, -0.47622621059417725, 1.0557661056518555]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.6.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.058286, Std: 1.042379
     First 10: [-0.6014613509178162, -0.7614458203315735, 0.8263858556747437, -0.6042107343673706, 0.6043699979782104, 0.05761462077498436, -0.42509981989860535, -0.7974216341972351, -3.1142609119415283, 0.25663191080093384]
     Last 10:  [-1.4753828048706055, -0.6252681612968445, 0.6235255599021912, 0.46850427985191345, -0.5549523830413818, 3.6850197315216064, 0.4126531779766083, 0.12734036147594452, -0.3740314245223999, 0.7459479570388794]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.460508
     First 10: [0.75, 0.5078125, -0.87890625, 0.15625, 0.1240234375, -1.0234375, 0.09814453125, 1.1875, 1.34375, -0.036376953125]
     Last 10:  [0.29296875, -0.08154296875, 0.443359375, 0.2119140625, 0.0135498046875, 0.78515625, 0.3515625, -0.06396484375, 0.439453125, 0.294921875]

================================================================================
102_model.layers.6.self_attn.k_norm: Gemma3RMSNorm (model.layers.6.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.023396, Std: 1.857035
     First 10: [-0.004414652474224567, 0.00013446807861328125, -0.630158543586731, -1.7858089208602905, 2.456576108932495, -0.03904158994555473, -2.072155475616455, 0.0025050006806850433, -0.0065973661839962006, -0.002747509628534317]
     Last 10:  [-5.209957599639893, -4.453174591064453, 3.201387405395508, -0.8471319675445557, -3.2109005451202393, 2.397477149963379, 3.8828206062316895, 1.761521816253662, -2.1639976501464844, 1.8167604207992554]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.6.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.026852, Std: 1.826840
     First 10: [-0.010893047787249088, 3.8865557144163176e-05, -0.013491547666490078, 0.019116876646876335, 0.0, -0.010030455887317657, -0.01109109167009592, 0.004987727850675583, -0.011900150217115879, -0.008382354862987995]
     Last 10:  [-2.5537240505218506, -2.9274468421936035, 1.7809685468673706, -0.45804303884506226, -3.0606534481048584, 3.1836488246917725, 1.9319273233413696, 1.2911664247512817, -1.4545860290527344, 1.0017272233963013]
     Zeros: 12, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.674679
     First 10: [0.80078125, -0.7890625, -0.984375, -1.0078125, -1.0, -0.8125, -0.99609375, 0.453125, 0.31640625, 1.2265625]
     Last 10:  [0.1650390625, 0.5625, 0.322265625, 0.28515625, 1.265625, 2.15625, 0.1826171875, 0.7421875, 0.59765625, 0.310546875]

================================================================================
103_model.layers.6.self_attn.o_proj: Linear (model.layers.6.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.6.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.015269, Std: 0.259612
     First 10: [0.16002824902534485, -0.021760500967502594, 0.030557751655578613, 0.03504304215312004, -0.017011435702443123, -0.045198749750852585, -0.03244747221469879, 0.021938040852546692, 0.10085819661617279, -0.09421645849943161]
     Last 10:  [-0.18812942504882812, 0.0840262919664383, 0.02867027372121811, 0.1367187201976776, 0.2910793125629425, 0.06432031840085983, 0.42376574873924255, -0.2333696335554123, -0.044937193393707275, 0.16788272559642792]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.6.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.009015, Std: 0.331181
     First 10: [0.2307932823896408, -0.033523980528116226, -0.07949678599834442, 0.02954702079296112, -0.018485646694898605, 0.0454990416765213, -0.025795429944992065, -0.06685611605644226, -0.10976025462150574, -0.0029549933969974518]
     Last 10:  [0.35093921422958374, 0.28944385051727295, 0.22064557671546936, 0.7390665411949158, -0.25340667366981506, 0.3190060555934906, 0.3734707832336426, -0.15005674958229065, 0.03949996456503868, 0.39223021268844604]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000019
     First 10: [-0.0086669921875, -0.00897216796875, 0.006683349609375, -0.0096435546875, -0.0181884765625, -0.006256103515625, 0.005523681640625, 0.0166015625, 0.0291748046875, -0.0205078125]
     Last 10:  [0.0159912109375, 0.004425048828125, 0.01507568359375, 0.0234375, -0.009033203125, 0.0191650390625, 0.003936767578125, 0.01263427734375, -0.00933837890625, -0.0098876953125]

================================================================================
104_model.layers.6.self_attn: Gemma3Attention (model.layers.6.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.6.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.009015, Std: 0.331181
     First 10: [0.2307932823896408, -0.033523980528116226, -0.07949678599834442, 0.02954702079296112, -0.018485646694898605, 0.0454990416765213, -0.025795429944992065, -0.06685611605644226, -0.10976025462150574, -0.0029549933969974518]
     Last 10:  [0.35093921422958374, 0.28944385051727295, 0.22064557671546936, 0.7390665411949158, -0.25340667366981506, 0.3190060555934906, 0.3734707832336426, -0.15005674958229065, 0.03949996456503868, 0.39223021268844604]
     Zeros: 0, Total: 2560

================================================================================
105_model.layers.6.post_attention_layernorm: Gemma3RMSNorm (model.layers.6.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.6.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.009015, Std: 0.331181
     First 10: [0.2307932823896408, -0.033523980528116226, -0.07949678599834442, 0.02954702079296112, -0.018485646694898605, 0.0454990416765213, -0.025795429944992065, -0.06685611605644226, -0.10976025462150574, -0.0029549933969974518]
     Last 10:  [0.35093921422958374, 0.28944385051727295, 0.22064557671546936, 0.7390665411949158, -0.25340667366981506, 0.3190060555934906, 0.3734707832336426, -0.15005674958229065, 0.03949996456503868, 0.39223021268844604]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.6.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.680589, Std: 14.775277
     First 10: [10.979560852050781, -0.9313870668411255, -0.5635048151016235, 0.3204869329929352, -0.05084148049354553, 1.2640868425369263, -0.6234021186828613, -0.3371388614177704, -1.34196138381958, -0.06916452944278717]
     Last 10:  [1.2219395637512207, 7.662448883056641, 0.16867250204086304, 94.6069564819336, -4.946410179138184, 6.627760410308838, 8.69796371459961, -1.8856581449508667, 0.7875711917877197, 16.101016998291016]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 8.017050
     First 10: [14.625, 8.125, 1.328125, 2.5625, -0.0966796875, 8.125, 6.9375, 0.65625, 3.015625, 6.6875]
     Last 10:  [0.298828125, 8.875, -0.71484375, 46.75, 6.28125, 6.75, 7.6875, 3.6875, 6.4375, 14.3125]

================================================================================
106_model.layers.6.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.6.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.6.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 27.753948, Std: 857.399353
     First 10: [-4.268503189086914, 0.5892410278320312, -2.08957576751709, 2.0719869136810303, -1.3062688112258911, 0.6786121129989624, -8.327920913696289, -0.11876443028450012, -4.917678356170654, 0.631328284740448]
     Last 10:  [-2.8589015007019043, 11.408329963684082, -20.09898567199707, -60.418922424316406, -7.8483381271362305, 0.6606440544128418, 10.354422569274902, 0.1937319040298462, 9.8922700881958, -31.088306427001953]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.6.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006409, Std: 0.610577
     First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
     Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 7.150157
     First 10: [4.15625, 8.5, 3.59375, 17.5, 23.375, 7.84375, 8.0625, 17.125, 4.78125, 9.8125]
     Last 10:  [19.25, 7.0625, -0.578125, 12.25, 7.3125, 7.78125, 7.6875, 16.5, 7.8125, 1.8828125]

================================================================================
107_model.layers.6.mlp.gate_proj: Linear (model.layers.6.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.6.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006409, Std: 0.610577
     First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
     Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.6.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.419021, Std: 0.716203
     First 10: [0.010462353006005287, -0.007480908185243607, -0.09548194706439972, -0.02929212711751461, 0.06714053452014923, -0.005614057183265686, -0.1212737187743187, 0.11738111823797226, -0.008310857228934765, -0.02469593472778797]
     Last 10:  [-0.278298556804657, -0.6951957941055298, -0.11705362051725388, -0.5295006036758423, 0.41540366411209106, -0.2696792483329773, -1.5020802021026611, -0.7568891644477844, -0.7355124354362488, -0.7378833889961243]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000132
     First 10: [0.0076904296875, 0.0014801025390625, -0.0272216796875, 0.03271484375, 0.00144195556640625, -0.01214599609375, 0.0286865234375, -0.0179443359375, 0.005218505859375, -0.01068115234375]
     Last 10:  [-0.032470703125, 0.052978515625, 0.03857421875, 0.0322265625, -0.059814453125, -0.060302734375, -0.0693359375, -0.0001049041748046875, -0.0189208984375, -0.043701171875]

================================================================================
108_model.layers.6.mlp.act_fn: PytorchGELUTanh (model.layers.6.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.6.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.419021, Std: 0.716203
     First 10: [0.010462353006005287, -0.007480908185243607, -0.09548194706439972, -0.02929212711751461, 0.06714053452014923, -0.005614057183265686, -0.1212737187743187, 0.11738111823797226, -0.008310857228934765, -0.02469593472778797]
     Last 10:  [-0.278298556804657, -0.6951957941055298, -0.11705362051725388, -0.5295006036758423, 0.41540366411209106, -0.2696792483329773, -1.5020802021026611, -0.7568891644477844, -0.7355124354362488, -0.7378833889961243]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.6.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.024126, Std: 0.240961
     First 10: [0.005274844355881214, -0.00371812772937119, -0.044109441339969635, -0.014303809031844139, 0.03536728397011757, -0.002794455038383603, -0.054783910512924194, 0.06417465955018997, -0.004127874039113522, -0.012104681693017483]
     Last 10:  [-0.10864728689193726, -0.1693108081817627, -0.053073201328516006, -0.15793368220329285, 0.27460506558418274, -0.10617532581090927, -0.10016284883022308, -0.17003653943538666, -0.1699783056974411, -0.16999460756778717]
     Zeros: 4, Total: 8192

================================================================================
109_model.layers.6.mlp.up_proj: Linear (model.layers.6.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.6.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006409, Std: 0.610577
     First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
     Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.6.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.015625, Std: 0.514268
     First 10: [0.09926675260066986, -0.08424293249845505, 0.020935427397489548, 0.16355112195014954, 0.10422397404909134, 0.009894141927361488, 0.03576844930648804, 0.02773936092853546, 0.010855473577976227, 0.11359960585832596]
     Last 10:  [-0.09728927165269852, 0.7029109001159668, 0.7152701020240784, 0.16923286020755768, -0.6261065602302551, 0.518202543258667, -0.7293983697891235, -0.01047445833683014, -0.1029612272977829, -0.03194883465766907]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000054
     First 10: [-0.05810546875, 0.005462646484375, 0.0341796875, 0.039794921875, -0.019775390625, -0.02587890625, -0.1044921875, 0.049560546875, -0.0546875, -0.0004787445068359375]
     Last 10:  [-0.00799560546875, -0.05615234375, 0.016357421875, 0.0012969970703125, -0.0198974609375, 0.0250244140625, 0.021484375, 0.0115966796875, -0.01324462890625, 0.026611328125]

================================================================================
110_model.layers.6.mlp.down_proj: Linear (model.layers.6.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.6.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.003705, Std: 0.141725
     First 10: [0.0005236166762188077, 0.0003132259880658239, -0.000923449988476932, -0.0023394040763378143, 0.003686118870973587, -2.7648735340335406e-05, -0.001959535526111722, 0.001780164078809321, -4.4810029066866264e-05, -0.001375087071210146]
     Last 10:  [0.010570215061306953, -0.11901041120290756, -0.037961672991514206, -0.026727568358182907, -0.17193202674388885, -0.055020324885845184, 0.07305862009525299, 0.0017810406861826777, 0.01750117540359497, 0.005431129597127438]
     Zeros: 4, Total: 8192

  → OUTPUT[0]: model.layers.6.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004191, Std: 0.106109
     First 10: [-0.0004089747089892626, -0.002305450616404414, 4.928826820105314e-05, -0.0062036216259002686, -0.0016609018202871084, 0.002725818660110235, -0.005698984023183584, 0.002841054229065776, -0.0001353949774056673, -0.002037203637883067]
     Last 10:  [0.050114359706640244, -0.010155703872442245, -0.07401532679796219, 0.22497253119945526, 0.010645193979144096, 0.10378660261631012, -0.04157324135303497, 0.06269757449626923, -0.1329967975616455, 0.11691781133413315]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000006
     First 10: [0.001953125, -0.00885009765625, -0.0035858154296875, 0.0017242431640625, -0.004852294921875, -0.0022125244140625, 0.0026702880859375, 0.006011962890625, -0.0181884765625, -0.004241943359375]
     Last 10:  [-0.0013885498046875, -0.015869140625, 0.000537872314453125, 0.0025482177734375, -0.01263427734375, -0.002166748046875, -0.0111083984375, -0.01336669921875, 0.00579833984375, 0.00750732421875]

================================================================================
111_model.layers.6.mlp: Gemma3MLP (model.layers.6.mlp)
================================================================================

  → INPUT[0]: model.layers.6.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006409, Std: 0.610577
     First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
     Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.6.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004191, Std: 0.106109
     First 10: [-0.0004089747089892626, -0.002305450616404414, 4.928826820105314e-05, -0.0062036216259002686, -0.0016609018202871084, 0.002725818660110235, -0.005698984023183584, 0.002841054229065776, -0.0001353949774056673, -0.002037203637883067]
     Last 10:  [0.050114359706640244, -0.010155703872442245, -0.07401532679796219, 0.22497253119945526, 0.010645193979144096, 0.10378660261631012, -0.04157324135303497, 0.06269757449626923, -0.1329967975616455, 0.11691781133413315]
     Zeros: 0, Total: 2560

================================================================================
112_model.layers.6.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.6.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.6.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004191, Std: 0.106109
     First 10: [-0.0004089747089892626, -0.002305450616404414, 4.928826820105314e-05, -0.0062036216259002686, -0.0016609018202871084, 0.002725818660110235, -0.005698984023183584, 0.002841054229065776, -0.0001353949774056673, -0.002037203637883067]
     Last 10:  [0.050114359706640244, -0.010155703872442245, -0.07401532679796219, 0.22497253119945526, 0.010645193979144096, 0.10378660261631012, -0.04157324135303497, 0.06269757449626923, -0.1329967975616455, 0.11691781133413315]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.6.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 5.007032, Std: 263.156586
     First 10: [-1.8638916015625, -6.594838619232178, 0.04450780525803566, -6.617048263549805, -0.5397810339927673, 7.499963760375977, -14.506180763244629, 1.7218126058578491, -0.19118985533714294, -4.765715599060059]
     Last 10:  [1.139736294746399, -1.5634770393371582, -16.484041213989258, 42.711524963378906, 1.2177900075912476, 12.820303916931152, -5.565414905548096, 3.4717869758605957, -16.50943946838379, 25.469749450683594]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 13.812119
     First 10: [22.5, 13.75, 3.65625, 4.5, 0.67578125, 13.1875, 12.125, 2.125, 6.28125, 11.0625]
     Last 10:  [1.3359375, 14.8125, 21.875, 18.5, 10.75, 11.6875, 12.75, 4.6875, 11.75, 21.375]

================================================================================
113_model.layers.7.input_layernorm: Gemma3RMSNorm (model.layers.7.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.7.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 32.760986, Std: 1110.876953
     First 10: [-6.132394790649414, -6.0055975914001465, -2.0450680255889893, -4.545061111450195, -1.8460497856140137, 8.17857551574707, -22.834102630615234, 1.6030482053756714, -5.10886812210083, -4.134387493133545]
     Last 10:  [-1.7191652059555054, 9.844852447509766, -36.58302688598633, -17.7073974609375, -6.630548000335693, 13.480947494506836, 4.789007663726807, 3.6655187606811523, -6.617169380187988, -5.618556976318359]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.7.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.067677, Std: 2.138058
     First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
     Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 20.039852
     First 10: [11.6875, 19.0, 10.4375, 50.75, 69.5, 17.375, 18.625, 60.25, 15.625, 23.875]
     Last 10:  [62.0, 17.25, -0.1708984375, 41.25, 23.25, 20.5, 19.125, 46.25, 18.875, 7.9375]

================================================================================
114_model.layers.7.self_attn.q_proj: Linear (model.layers.7.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.067677, Std: 2.138058
     First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
     Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.7.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.050716, Std: 2.782881
     First 10: [0.24431437253952026, 0.5895841121673584, -0.12216698378324509, -0.11651404201984406, -0.32702210545539856, -0.03953804820775986, -3.5625674724578857, -0.03614358976483345, -0.07490988820791245, -0.015936069190502167]
     Last 10:  [-1.6367186307907104, -0.6669793128967285, 0.028153032064437866, -1.376692771911621, 0.3988007605075836, -0.31629204750061035, -0.018605593591928482, -0.8642804026603699, 0.7570029497146606, 0.875950813293457]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000049
     First 10: [-0.00286865234375, -0.007354736328125, -0.0289306640625, -0.00142669677734375, -0.020263671875, -0.00982666015625, 0.00127410888671875, 0.006591796875, -0.00439453125, 0.01495361328125]
     Last 10:  [-0.01318359375, -0.01409912109375, 0.01043701171875, 0.022216796875, 0.0022125244140625, -0.023193359375, 0.057373046875, 0.004302978515625, 0.01068115234375, 0.000591278076171875]

================================================================================
115_model.layers.7.self_attn.k_proj: Linear (model.layers.7.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.067677, Std: 2.138058
     First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
     Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.7.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.195011, Std: 2.468267
     First 10: [0.03433772921562195, -0.008914414793252945, 2.665456771850586, 0.0015624389052391052, -0.002694077789783478, 1.466333270072937, 0.2729759216308594, 0.004630564711987972, 0.0040669627487659454, 2.3641457557678223]
     Last 10:  [1.7067885398864746, 5.15956974029541, -3.6862330436706543, 5.445260047912598, 1.0766410827636719, -4.450538635253906, -1.3090513944625854, -5.713581085205078, 2.5216875076293945, -0.4549422264099121]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000023
     First 10: [-0.046630859375, 0.0771484375, 0.052490234375, -0.0537109375, 0.02197265625, -0.003997802734375, -0.1025390625, 0.020263671875, -0.0311279296875, 0.0517578125]
     Last 10:  [-0.00173187255859375, 0.054931640625, 0.03564453125, -0.04248046875, 0.025146484375, -0.0228271484375, -0.019775390625, -0.020751953125, 0.0035400390625, 4.792213439941406e-05]

================================================================================
116_model.layers.7.self_attn.v_proj: Linear (model.layers.7.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.067677, Std: 2.138058
     First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
     Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.7.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.006311, Std: 1.410400
     First 10: [0.0021059364080429077, -0.060208164155483246, 0.12020843476057053, 0.010753985494375229, 0.3733794093132019, 0.259493887424469, 0.3000595271587372, 0.003366784192621708, 0.020796898752450943, 0.06453762948513031]
     Last 10:  [-0.7692805528640747, -0.31057482957839966, -0.04113595932722092, -3.161863327026367, -1.126147985458374, 0.13373272120952606, -0.4966580271720886, -0.09629499912261963, -0.441213995218277, 1.758063554763794]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000136
     First 10: [0.029541015625, -0.0166015625, 0.033447265625, -0.01312255859375, -0.11279296875, 0.05810546875, 0.11865234375, 0.026611328125, 0.052001953125, -0.044677734375]
     Last 10:  [0.0830078125, 0.031982421875, 0.02685546875, -0.02294921875, -0.03271484375, -0.0260009765625, -0.049072265625, 0.057861328125, 0.050537109375, -0.00165557861328125]

================================================================================
117_model.layers.7.self_attn.q_norm: Gemma3RMSNorm (model.layers.7.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.050716, Std: 2.782881
     First 10: [0.24431437253952026, 0.5895841121673584, -0.12216698378324509, -0.11651404201984406, -0.32702210545539856, -0.03953804820775986, -3.5625674724578857, -0.03614358976483345, -0.07490988820791245, -0.015936069190502167]
     Last 10:  [-1.6367186307907104, -0.6669793128967285, 0.028153032064437866, -1.376692771911621, 0.3988007605075836, -0.31629204750061035, -0.018605593591928482, -0.8642804026603699, 0.7570029497146606, 0.875950813293457]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.7.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.035273, Std: 1.107397
     First 10: [0.6284729838371277, 1.3573774099349976, -0.29251089692115784, -0.2600197196006775, -1.2929643392562866, -0.09369709342718124, 0.26246291399002075, -0.09785735607147217, -0.2138531655073166, -0.026318253949284554]
     Last 10:  [-1.3133049011230469, -0.3256201446056366, 0.05516459420323372, -0.8160979747772217, 0.3986717760562897, -0.23766811192035675, -0.015197214670479298, -0.5823863744735718, 0.8070967793464661, 1.1455516815185547]
     Zeros: 16, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.792706
     First 10: [0.63671875, 0.46484375, 0.5234375, 0.419921875, 1.515625, 0.5078125, -1.046875, 0.72265625, 0.81640625, 0.05078125]
     Last 10:  [0.4140625, -0.1396484375, 2.453125, 0.044677734375, 0.76171875, 0.32421875, 0.439453125, 0.1875, 0.87890625, 1.3046875]

================================================================================
118_model.layers.7.self_attn.k_norm: Gemma3RMSNorm (model.layers.7.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.195011, Std: 2.468267
     First 10: [0.03433772921562195, -0.008914414793252945, 2.665456771850586, 0.0015624389052391052, -0.002694077789783478, 1.466333270072937, 0.2729759216308594, 0.004630564711987972, 0.0040669627487659454, 2.3641457557678223]
     Last 10:  [1.7067885398864746, 5.15956974029541, -3.6862330436706543, 5.445260047912598, 1.0766410827636719, -4.450538635253906, -1.3090513944625854, -5.713581085205078, 2.5216875076293945, -0.4549422264099121]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.7.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.193346, Std: 2.207407
     First 10: [0.008690139278769493, -0.020341401919722557, 0.011058518663048744, 0.0030272286385297775, -0.0030639669857919216, 0.0, 0.01698794774711132, 0.009086997248232365, 0.007795383222401142, 0.009808431379497051]
     Last 10:  [1.591261625289917, 6.954370021820068, -1.075203537940979, 4.032341003417969, 0.5836186408996582, -3.189030170440674, -0.7601634860038757, -4.931138515472412, 1.450904369354248, -0.24176473915576935]
     Zeros: 16, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.029377
     First 10: [-0.76171875, 1.1484375, -0.99609375, 0.82421875, 0.07080078125, -1.0, -0.94140625, 0.84765625, 0.8046875, -0.99609375]
     Last 10:  [1.734375, 2.953125, -0.14453125, 1.171875, 0.58984375, 1.1015625, 0.703125, 1.53125, 0.6875, 0.55859375]

================================================================================
119_model.layers.7.self_attn.o_proj: Linear (model.layers.7.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.7.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.000988, Std: 0.737207
     First 10: [0.0021059364080429077, -0.060208164155483246, 0.12020843476057053, 0.010753985494375229, 0.3733794093132019, 0.259493887424469, 0.3000595271587372, 0.003366784192621708, 0.020796898752450943, 0.06453762948513031]
     Last 10:  [0.16163520514965057, -0.325320303440094, 0.019032606855034828, -0.5384095907211304, -0.11045888811349869, 0.1203482523560524, -0.11890638619661331, -1.359337568283081, -0.6725009679794312, 0.5041097402572632]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.7.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.063095, Std: 1.067218
     First 10: [0.6702954769134521, 0.01799793541431427, -0.1798892468214035, -0.003955386579036713, 0.021287810057401657, -0.15691615641117096, -0.003825247287750244, -0.04575778916478157, 0.4134564995765686, 0.09486135840415955]
     Last 10:  [0.3193379342556, -0.03784262388944626, 0.17737647891044617, 1.494147539138794, 0.32569509744644165, 0.4048355221748352, -0.5796380639076233, -0.5703885555267334, 0.19646018743515015, 0.34248271584510803]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000078
     First 10: [0.0216064453125, 0.003662109375, -0.0079345703125, 0.000762939453125, 0.0115966796875, -0.03173828125, -0.0024261474609375, 0.020751953125, -0.01708984375, 0.0194091796875]
     Last 10:  [0.01202392578125, -0.0133056640625, -0.00946044921875, 0.0028228759765625, -0.016845703125, -0.0120849609375, 0.0157470703125, 0.02978515625, -0.00311279296875, -0.011474609375]

================================================================================
120_model.layers.7.self_attn: Gemma3Attention (model.layers.7.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.7.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.063095, Std: 1.067218
     First 10: [0.6702954769134521, 0.01799793541431427, -0.1798892468214035, -0.003955386579036713, 0.021287810057401657, -0.15691615641117096, -0.003825247287750244, -0.04575778916478157, 0.4134564995765686, 0.09486135840415955]
     Last 10:  [0.3193379342556, -0.03784262388944626, 0.17737647891044617, 1.494147539138794, 0.32569509744644165, 0.4048355221748352, -0.5796380639076233, -0.5703885555267334, 0.19646018743515015, 0.34248271584510803]
     Zeros: 0, Total: 2560

================================================================================
121_model.layers.7.post_attention_layernorm: Gemma3RMSNorm (model.layers.7.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.7.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.063095, Std: 1.067218
     First 10: [0.6702954769134521, 0.01799793541431427, -0.1798892468214035, -0.003955386579036713, 0.021287810057401657, -0.15691615641117096, -0.003825247287750244, -0.04575778916478157, 0.4134564995765686, 0.09486135840415955]
     Last 10:  [0.3193379342556, -0.03784262388944626, 0.17737647891044617, 1.494147539138794, 0.32569509744644165, 0.4048355221748352, -0.5796380639076233, -0.5703885555267334, 0.19646018743515015, 0.34248271584510803]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.7.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.753366, Std: 36.837067
     First 10: [11.900168418884277, 0.19371411204338074, -0.611291229724884, -0.02161533758044243, 0.031519241631031036, -1.6366747617721558, -0.034380435943603516, -0.1256629079580307, 2.4085516929626465, 0.8631177544593811]
     Last 10:  [1.062107801437378, -0.7601159811019897, 3.192662477493286, 23.775585174560547, 4.474607467651367, 5.984311580657959, -8.870665550231934, -4.0669732093811035, 2.6820130348205566, 6.134688377380371]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 10.429689
     First 10: [19.0, 11.125, 2.828125, 5.15625, 0.66796875, 10.75, 9.125, 2.09375, 5.5625, 9.25]
     Last 10:  [1.390625, 13.4375, 11.9375, 10.4375, 8.875, 9.625, 10.0, 4.125, 8.8125, 11.875]

================================================================================
122_model.layers.7.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.7.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.7.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 34.514355, Std: 1121.432617
     First 10: [5.767773628234863, -5.811883449554443, -2.6563591957092285, -4.566676616668701, -1.8145304918289185, 6.541900634765625, -22.86848258972168, 1.4773852825164795, -2.7003164291381836, -3.2712697982788086]
     Last 10:  [-0.6570574045181274, 9.084736824035645, -33.39036560058594, 6.068187713623047, -2.155940532684326, 19.465259552001953, -4.081657886505127, -0.40145444869995117, -3.9351563453674316, 0.5161314010620117]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.7.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009573, Std: 0.531535
     First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
     Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 6.327952
     First 10: [2.78125, 5.53125, 4.3125, 16.375, 27.0, 5.65625, 6.625, 19.625, 5.59375, 7.28125]
     Last 10:  [22.25, 4.78125, -0.53515625, 10.1875, 6.9375, 6.46875, 6.1875, 14.8125, 6.65625, 2.375]

================================================================================
123_model.layers.7.mlp.gate_proj: Linear (model.layers.7.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.7.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009573, Std: 0.531535
     First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
     Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.7.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.209894, Std: 0.635020
     First 10: [-0.041100479662418365, -0.020136309787631035, -0.03717191517353058, -0.005653719417750835, 0.054934289306402206, -0.028081879019737244, 0.03883669897913933, -0.0863206535577774, -0.06271971017122269, 0.0029556734953075647]
     Last 10:  [0.8696548938751221, 0.3737149238586426, 0.4447750747203827, -0.9978352785110474, 0.31425875425338745, -0.4645819067955017, -0.19631743431091309, -0.4586385488510132, -0.8861578702926636, -0.2358107566833496]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000139
     First 10: [-0.034912109375, 0.0654296875, 0.0546875, 0.006103515625, 0.044677734375, 0.0186767578125, -0.005401611328125, 0.06689453125, -0.020751953125, 0.027099609375]
     Last 10:  [-0.018798828125, 0.01025390625, 0.01239013671875, -0.004241943359375, 0.021728515625, 0.00494384765625, -0.01287841796875, 0.045166015625, 0.016357421875, -0.0517578125]

================================================================================
124_model.layers.7.mlp.act_fn: PytorchGELUTanh (model.layers.7.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.7.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.209894, Std: 0.635020
     First 10: [-0.041100479662418365, -0.020136309787631035, -0.03717191517353058, -0.005653719417750835, 0.054934289306402206, -0.028081879019737244, 0.03883669897913933, -0.0863206535577774, -0.06271971017122269, 0.0029556734953075647]
     Last 10:  [0.8696548938751221, 0.3737149238586426, 0.4447750747203827, -0.9978352785110474, 0.31425875425338745, -0.4645819067955017, -0.19631743431091309, -0.4586385488510132, -0.8861578702926636, -0.2358107566833496]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.7.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.022117, Std: 0.293096
     First 10: [-0.019876517355442047, -0.0099064065143466, -0.01803484559059143, -0.002814107807353139, 0.028670454397797585, -0.013726378791034222, 0.02001991868019104, -0.0401914119720459, -0.029791545122861862, 0.0014813219895586371]
     Last 10:  [0.7023615837097168, 0.2412988543510437, 0.29877039790153503, -0.15898704528808594, 0.19588634371757507, -0.14919757843017578, -0.08288193494081497, -0.1482660174369812, -0.16650310158729553, -0.09592638164758682]
     Zeros: 3, Total: 8192

================================================================================
125_model.layers.7.mlp.up_proj: Linear (model.layers.7.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.7.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009573, Std: 0.531535
     First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
     Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.7.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.000177, Std: 0.486977
     First 10: [-0.05950107425451279, 0.040578581392765045, 0.027255475521087646, 0.04677848145365715, -0.001685265451669693, -0.030796855688095093, 0.006998740136623383, -0.05120092257857323, -0.04394146427512169, 0.0010621780529618263]
     Last 10:  [0.20231947302818298, -0.519631564617157, 1.0311037302017212, -1.3209960460662842, 0.2371266931295395, -0.16631898283958435, -1.0834150314331055, 0.12040401995182037, 1.0218755006790161, -0.2611122131347656]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000006
     First 10: [0.06689453125, 0.001007080078125, 0.10205078125, -0.015380859375, -0.00811767578125, -0.0546875, 0.0142822265625, 0.029052734375, -0.06396484375, -0.01092529296875]
     Last 10:  [0.11572265625, -0.08056640625, 0.041748046875, -0.024169921875, 0.0010528564453125, -0.000499725341796875, -0.06884765625, -0.00390625, 0.0233154296875, 0.0152587890625]

================================================================================
126_model.layers.7.mlp.down_proj: Linear (model.layers.7.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.7.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.000408, Std: 0.178466
     First 10: [0.0011826740810647607, -0.00040198792703449726, -0.0004915482713840902, -0.00013163969560991973, -4.8317328037228435e-05, 0.00042272929567843676, 0.00014011420716997236, 0.0020578373223543167, 0.0013090841239318252, 1.5734276530565694e-06]
     Last 10:  [0.14210142195224762, -0.12538650631904602, 0.308063268661499, 0.2100212574005127, 0.04644988104701042, 0.024814389646053314, 0.0897955372929573, -0.017851823940873146, -0.1701454371213913, 0.025047549977898598]
     Zeros: 3, Total: 8192

  → OUTPUT[0]: model.layers.7.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001097, Std: 0.130703
     First 10: [0.005046057049185038, -0.004904364235699177, 0.000976785086095333, -0.003465132787823677, -0.0002820085792336613, -0.0024692232254892588, 0.002825802657753229, 0.0004924096865579486, 0.001484330277889967, -0.00023496626818086952]
     Last 10:  [-0.010282468050718307, -0.0758746862411499, -0.13669851422309875, -0.05427517741918564, -0.09429673850536346, 0.1394980102777481, 0.03237081319093704, 0.034539345651865005, -0.12881425023078918, 0.02473367564380169]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000014
     First 10: [-0.00136566162109375, -0.00433349609375, -0.005950927734375, -0.0255126953125, 0.0159912109375, 0.00213623046875, 0.0517578125, -0.0120849609375, -0.0022125244140625, 0.001861572265625]
     Last 10:  [0.005615234375, -0.000579833984375, -0.0162353515625, 0.002349853515625, -1.0907649993896484e-05, -0.0186767578125, -0.01031494140625, -0.0027923583984375, 0.00762939453125, -0.0194091796875]

================================================================================
127_model.layers.7.mlp: Gemma3MLP (model.layers.7.mlp)
================================================================================

  → INPUT[0]: model.layers.7.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009573, Std: 0.531535
     First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
     Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.7.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001097, Std: 0.130703
     First 10: [0.005046057049185038, -0.004904364235699177, 0.000976785086095333, -0.003465132787823677, -0.0002820085792336613, -0.0024692232254892588, 0.002825802657753229, 0.0004924096865579486, 0.001484330277889967, -0.00023496626818086952]
     Last 10:  [-0.010282468050718307, -0.0758746862411499, -0.13669851422309875, -0.05427517741918564, -0.09429673850536346, 0.1394980102777481, 0.03237081319093704, 0.034539345651865005, -0.12881425023078918, 0.02473367564380169]
     Zeros: 0, Total: 2560

================================================================================
128_model.layers.7.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.7.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.7.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.001097, Std: 0.130703
     First 10: [0.005046057049185038, -0.004904364235699177, 0.000976785086095333, -0.003465132787823677, -0.0002820085792336613, -0.0024692232254892588, 0.002825802657753229, 0.0004924096865579486, 0.001484330277889967, -0.00023496626818086952]
     Last 10:  [-0.010282468050718307, -0.0758746862411499, -0.13669851422309875, -0.05427517741918564, -0.09429673850536346, 0.1394980102777481, 0.03237081319093704, 0.034539345651865005, -0.12881425023078918, 0.02473367564380169]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.7.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 5.195705, Std: 269.921295
     First 10: [28.188772201538086, -18.884523391723633, 0.6966924071311951, -2.8171727657318115, -0.06584612280130386, -8.818188667297363, 8.174775123596191, 0.21060405671596527, 1.710206389427185, -0.555507481098175]
     Last 10:  [-0.20901812613010406, -17.84382438659668, -31.29306411743164, -9.708910942077637, -13.329330444335938, 26.17534828186035, 6.478981971740723, 2.5707762241363525, -23.687240600585938, 5.6929802894592285]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 16.627895
     First 10: [34.0, 23.125, 3.46875, 4.09375, 0.462890625, 21.375, 17.125, 1.6796875, 6.21875, 13.8125]
     Last 10:  [1.03125, 22.5, 21.875, 16.875, 13.125, 17.75, 19.0, 6.4375, 17.375, 22.0]

================================================================================
129_model.layers.8.input_layernorm: Gemma3RMSNorm (model.layers.8.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.8.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 39.710060, Std: 1385.020020
     First 10: [33.956546783447266, -24.696407318115234, -1.9596667289733887, -7.383849143981934, -1.8803765773773193, -2.2762880325317383, -14.693707466125488, 1.687989354133606, -0.9901100397109985, -3.826777219772339]
     Last 10:  [-0.8660755157470703, -8.759087562561035, -64.68342590332031, -3.64072322845459, -15.485271453857422, 45.64060974121094, 2.3973240852355957, 2.1693217754364014, -27.62239646911621, 6.20911169052124]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.8.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.050684, Std: 2.506804
     First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
     Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 19.921200
     First 10: [9.0, 15.1875, 13.0625, 49.0, 63.0, 15.4375, 17.375, 62.75, 17.875, 21.375]
     Last 10:  [57.75, 14.75, 0.06787109375, 28.625, 23.375, 18.625, 16.75, 46.75, 18.0, 13.5]

================================================================================
130_model.layers.8.self_attn.q_proj: Linear (model.layers.8.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.050684, Std: 2.506804
     First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
     Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.8.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.033616, Std: 2.241807
     First 10: [-0.20592297613620758, 0.1215912327170372, -3.636873483657837, 0.4720737934112549, -0.3297586739063263, -0.04457107186317444, 0.30267030000686646, -0.42607206106185913, -3.6061618328094482, -0.15356773138046265]
     Last 10:  [1.1324197053909302, 1.2875440120697021, 4.043564796447754, 1.1188546419143677, -0.38098055124282837, 3.2240681648254395, -1.9775700569152832, -0.6778863668441772, 1.2239301204681396, -2.886838674545288]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: 0.000092
     First 10: [0.02978515625, -0.017578125, -0.003936767578125, 0.007293701171875, 0.0264892578125, 0.0185546875, 0.00653076171875, 0.005218505859375, 0.03173828125, -0.0206298828125]
     Last 10:  [0.0191650390625, 0.0224609375, -0.06640625, 0.07177734375, -0.0220947265625, -0.0301513671875, -0.055908203125, 0.03466796875, -0.059326171875, 0.007476806640625]

================================================================================
131_model.layers.8.self_attn.k_proj: Linear (model.layers.8.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.050684, Std: 2.506804
     First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
     Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.8.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.038035, Std: 2.222081
     First 10: [0.000681266188621521, -0.0008283713832497597, 0.001509547233581543, 0.0006539206951856613, -0.00836212933063507, -0.007144542410969734, -0.0020429715514183044, -0.004627332091331482, 0.0010004937648773193, 0.000714537687599659]
     Last 10:  [5.0788421630859375, 1.1583746671676636, 7.681475639343262, 7.742763042449951, -2.562598466873169, 7.324058532714844, 3.7039167881011963, -3.3107681274414062, 4.2640228271484375, -1.4356439113616943]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000161
     First 10: [0.0211181640625, 0.0115966796875, 0.021240234375, 0.00836181640625, 0.01055908203125, -0.01397705078125, -0.0034332275390625, 0.0017547607421875, -0.0247802734375, 0.0269775390625]
     Last 10:  [-0.013916015625, -0.031494140625, -0.0196533203125, 0.00396728515625, -0.01068115234375, 0.0079345703125, -0.027099609375, 0.0194091796875, -0.00372314453125, -0.00537109375]

================================================================================
132_model.layers.8.self_attn.v_proj: Linear (model.layers.8.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.050684, Std: 2.506804
     First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
     Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.8.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.047027, Std: 1.579868
     First 10: [-0.0016240105032920837, -0.05041125416755676, 0.563729465007782, -0.1687164157629013, 0.23277026414871216, -0.46235352754592896, 0.44956421852111816, -0.5913743376731873, 0.4182305335998535, -0.7496524453163147]
     Last 10:  [0.7890281081199646, 0.3912888765335083, 0.08059287071228027, -0.6172995567321777, -1.8521642684936523, -2.1216602325439453, 0.15696388483047485, 3.0107362270355225, -2.226724624633789, -0.9957597255706787]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000041
     First 10: [0.002960205078125, -0.0301513671875, -0.01190185546875, 0.0281982421875, 0.0498046875, -0.01513671875, 0.059814453125, -0.03759765625, 0.01226806640625, -0.00677490234375]
     Last 10:  [0.0771484375, -0.005157470703125, -0.0084228515625, -0.0177001953125, -0.00750732421875, -0.0595703125, -0.001373291015625, -0.0576171875, -0.0194091796875, -0.0009918212890625]

================================================================================
133_model.layers.8.self_attn.q_norm: Gemma3RMSNorm (model.layers.8.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.033616, Std: 2.241807
     First 10: [-0.20592297613620758, 0.1215912327170372, -3.636873483657837, 0.4720737934112549, -0.3297586739063263, -0.04457107186317444, 0.30267030000686646, -0.42607206106185913, -3.6061618328094482, -0.15356773138046265]
     Last 10:  [1.1324197053909302, 1.2875440120697021, 4.043564796447754, 1.1188546419143677, -0.38098055124282837, 3.2240681648254395, -1.9775700569152832, -0.6778863668441772, 1.2239301204681396, -2.886838674545288]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.8.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.131853, Std: 2.182832
     First 10: [-1.1646696329116821, 0.467898428440094, -0.5858433842658997, 1.8799692392349243, -1.2896103858947754, -0.25128957629203796, 0.6155375242233276, -1.3688578605651855, -0.6131682991981506, -0.6885231137275696]
     Last 10:  [0.3508286476135254, 0.4712815284729004, 0.8499079346656799, 0.18231385946273804, -0.09207602590322495, 0.6600008606910706, -1.1779319047927856, -0.2148854285478592, 0.4847625494003296, -1.6562870740890503]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.649311
     First 10: [1.46875, 0.6796875, -0.9296875, 0.73828125, 0.70703125, 1.4609375, -0.1123046875, 0.40234375, -0.92578125, 0.95703125]
     Last 10:  [-0.1162109375, 0.044189453125, -0.400390625, -0.53515625, -0.310546875, -0.416015625, 0.69921875, -0.095703125, 0.1298828125, 0.63671875]

================================================================================
134_model.layers.8.self_attn.k_norm: Gemma3RMSNorm (model.layers.8.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.038035, Std: 2.222081
     First 10: [0.000681266188621521, -0.0008283713832497597, 0.001509547233581543, 0.0006539206951856613, -0.00836212933063507, -0.007144542410969734, -0.0020429715514183044, -0.004627332091331482, 0.0010004937648773193, 0.000714537687599659]
     Last 10:  [5.0788421630859375, 1.1583746671676636, 7.681475639343262, 7.742763042449951, -2.562598466873169, 7.324058532714844, 3.7039167881011963, -3.3107681274414062, 4.2640228271484375, -1.4356439113616943]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.8.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.011741, Std: 2.187317
     First 10: [0.001158080529421568, -0.0010838708840310574, 0.001208157860673964, 0.0007276220712810755, -0.007600624114274979, -0.0062353103421628475, -0.00393295893445611, -0.004937946796417236, 0.0006314041092991829, 0.0008985260501503944]
     Last 10:  [3.083397388458252, 0.8997129201889038, 8.332574844360352, 12.890395164489746, -2.0659613609313965, 8.49692153930664, 1.8177518844604492, -2.0506763458251953, 2.6131701469421387, -0.8292441368103027]
     Zeros: 4, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.914341
     First 10: [1.4765625, 0.90625, 0.166015625, 0.62109375, 0.32421875, 0.271484375, 1.8046875, 0.5546875, -0.08056640625, 0.83203125]
     Last 10:  [0.447265625, 0.8515625, 1.5859375, 2.96875, 0.921875, 1.765625, 0.169921875, 0.4765625, 0.4609375, 0.376953125]

================================================================================
135_model.layers.8.self_attn.o_proj: Linear (model.layers.8.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.8.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.032283, Std: 0.876659
     First 10: [-0.0016240105032920837, -0.05041125416755676, 0.563729465007782, -0.1687164157629013, 0.23277026414871216, -0.46235352754592896, 0.44956421852111816, -0.5913743376731873, 0.4182305335998535, -0.7496524453163147]
     Last 10:  [-0.14225426316261292, 0.12044709175825119, 0.1741883009672165, -0.5136483907699585, -0.278732568025589, -0.5724631547927856, 0.12035499513149261, 0.8774973750114441, -0.678219199180603, -0.4439106583595276]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.8.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004267, Std: 0.962400
     First 10: [0.5100055932998657, -0.024546053260564804, 0.09209299087524414, 0.4076889455318451, -0.22412195801734924, 0.041946008801460266, -0.2892238199710846, 0.038145676255226135, -0.41832679510116577, -0.18366047739982605]
     Last 10:  [-0.21824318170547485, 0.17788654565811157, -3.678676128387451, 0.6267490386962891, 0.3044055998325348, 0.21921947598457336, 0.7549707889556885, 0.39805445075035095, 0.35519930720329285, 0.12530586123466492]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000009
     First 10: [0.01043701171875, -0.00958251953125, 0.01092529296875, 0.0267333984375, 0.029052734375, -0.0262451171875, -0.021728515625, -0.00439453125, 0.041015625, -0.041015625]
     Last 10:  [-0.00567626953125, 0.0223388671875, -0.0181884765625, -0.0033416748046875, -0.01153564453125, 0.003631591796875, -0.0223388671875, -0.004608154296875, -0.00726318359375, -0.004119873046875]

================================================================================
136_model.layers.8.self_attn: Gemma3Attention (model.layers.8.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.8.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004267, Std: 0.962400
     First 10: [0.5100055932998657, -0.024546053260564804, 0.09209299087524414, 0.4076889455318451, -0.22412195801734924, 0.041946008801460266, -0.2892238199710846, 0.038145676255226135, -0.41832679510116577, -0.18366047739982605]
     Last 10:  [-0.21824318170547485, 0.17788654565811157, -3.678676128387451, 0.6267490386962891, 0.3044055998325348, 0.21921947598457336, 0.7549707889556885, 0.39805445075035095, 0.35519930720329285, 0.12530586123466492]
     Zeros: 0, Total: 2560

================================================================================
137_model.layers.8.post_attention_layernorm: Gemma3RMSNorm (model.layers.8.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.8.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004267, Std: 0.962400
     First 10: [0.5100055932998657, -0.024546053260564804, 0.09209299087524414, 0.4076889455318451, -0.22412195801734924, 0.041946008801460266, -0.2892238199710846, 0.038145676255226135, -0.41832679510116577, -0.18366047739982605]
     Last 10:  [-0.21824318170547485, 0.17788654565811157, -3.678676128387451, 0.6267490386962891, 0.3044055998325348, 0.21921947598457336, 0.7549707889556885, 0.39805445075035095, 0.35519930720329285, 0.12530586123466492]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.8.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.164990, Std: 26.675339
     First 10: [34.708866119384766, -1.252876877784729, 0.3928047716617584, 2.7938294410705566, -0.29426538944244385, 1.9348328113555908, -9.732335090637207, 0.097802072763443, -2.9853525161743164, -4.617749214172363]
     Last 10:  [-0.26583927869796753, 5.822103023529053, -3.0774121284484863, 15.263654708862305, 4.530411720275879, 5.5365376472473145, 19.359149932861328, 5.513837814331055, 8.284259796142578, 2.187830686569214]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 18.394093
     First 10: [44.0, 32.75, 1.8203125, 3.53125, -0.1318359375, 29.5, 21.25, 0.6953125, 3.71875, 15.625]
     Last 10:  [0.181640625, 30.75, -0.1884765625, 22.625, 13.4375, 23.5, 23.875, 12.4375, 21.625, 15.9375]

================================================================================
138_model.layers.8.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.8.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.8.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 39.875053, Std: 1377.840210
     First 10: [68.66541290283203, -25.949283599853516, -1.5668619871139526, -4.590019702911377, -2.1746420860290527, -0.34145522117614746, -24.426042556762695, 1.7857913970947266, -3.9754624366760254, -8.444526672363281]
     Last 10:  [-1.1319148540496826, -2.9369845390319824, -67.7608413696289, 11.622931480407715, -10.954859733581543, 51.177146911621094, 21.756473541259766, 7.683159828186035, -19.338136672973633, 8.396942138671875]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.8.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009105, Std: 0.584839
     First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
     Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 5.790224
     First 10: [2.328125, 3.90625, 4.21875, 16.75, 23.5, 4.0625, 5.5, 19.875, 5.34375, 6.3125]
     Last 10:  [19.375, 3.890625, -0.546875, 8.75, 6.4375, 5.15625, 5.09375, 12.0625, 5.71875, 3.265625]

================================================================================
139_model.layers.8.mlp.gate_proj: Linear (model.layers.8.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.8.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009105, Std: 0.584839
     First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
     Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.8.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.225966, Std: 0.643881
     First 10: [-0.1339147835969925, -0.06319180130958557, -0.08838286250829697, 0.0020267926156520844, -0.006209836341440678, -0.020326368510723114, -0.007588140666484833, 0.02135574445128441, 0.014291232451796532, 0.1522691547870636]
     Last 10:  [-0.1036207377910614, -0.08026671409606934, -2.0917277336120605, 0.24701295793056488, -2.812556743621826, 0.7814949750900269, -1.3370630741119385, -0.335783988237381, -0.07243295013904572, -1.0749191045761108]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000145
     First 10: [0.044189453125, -0.1142578125, -0.039794921875, 0.018310546875, 0.0196533203125, 0.08447265625, -0.01104736328125, -0.01068115234375, 0.0218505859375, 0.011474609375]
     Last 10:  [-0.032470703125, -0.0045166015625, -0.020263671875, 0.024169921875, -0.07763671875, -0.01324462890625, 0.1337890625, 0.004638671875, 0.052490234375, -0.016357421875]

================================================================================
140_model.layers.8.mlp.act_fn: PytorchGELUTanh (model.layers.8.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.8.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.225966, Std: 0.643881
     First 10: [-0.1339147835969925, -0.06319180130958557, -0.08838286250829697, 0.0020267926156520844, -0.006209836341440678, -0.020326368510723114, -0.007588140666484833, 0.02135574445128441, 0.014291232451796532, 0.1522691547870636]
     Last 10:  [-0.1036207377910614, -0.08026671409606934, -2.0917277336120605, 0.24701295793056488, -2.812556743621826, 0.7814949750900269, -1.3370630741119385, -0.335783988237381, -0.07243295013904572, -1.0749191045761108]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.8.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.019959, Std: 0.253419
     First 10: [-0.05982452258467674, -0.030003907158970833, -0.04107915237545967, 0.0010150352027267218, -0.0030895343516021967, -0.009998368099331856, -0.0037710994947701693, 0.010859803296625614, 0.007227092981338501, 0.08534861356019974]
     Last 10:  [-0.0475345142185688, -0.037565842270851135, -0.037957943975925446, 0.1476016491651535, -0.006448244210332632, 0.6116324663162231, -0.12137235701084137, -0.12374593317508698, -0.034125249832868576, -0.1519627571105957]
     Zeros: 1, Total: 8192

================================================================================
141_model.layers.8.mlp.up_proj: Linear (model.layers.8.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.8.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009105, Std: 0.584839
     First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
     Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.8.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.011525, Std: 0.511516
     First 10: [0.0069982074201107025, -0.0018233507871627808, -0.02581140398979187, 0.04949656501412392, 0.05429309979081154, 0.03644245117902756, -0.06297539174556732, 0.0073771290481090546, -0.07512594759464264, -0.0323769748210907]
     Last 10:  [-0.3077259063720703, 0.8118508458137512, -0.4542738199234009, -0.467755526304245, 0.3847951292991638, -0.054115667939186096, -0.3261362314224243, 0.1422538459300995, -0.22436045110225677, 1.3267168998718262]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000069
     First 10: [0.06201171875, -0.01251220703125, -0.037353515625, -0.021484375, 0.02001953125, 0.0181884765625, 0.00885009765625, -0.00151824951171875, 0.0225830078125, -0.037109375]
     Last 10:  [-0.01275634765625, -0.045166015625, 0.033935546875, 0.01312255859375, -0.0341796875, -0.000911712646484375, -0.0849609375, 0.0498046875, 0.0181884765625, 0.03271484375]

================================================================================
142_model.layers.8.mlp.down_proj: Linear (model.layers.8.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.8.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.001944, Std: 0.132516
     First 10: [-0.0004186644218862057, 5.470764881465584e-05, 0.0010603106347844005, 5.024075653636828e-05, -0.000167740392498672, -0.00036436502705328166, 0.0002374864707235247, 8.011417230591178e-05, -0.000542942201718688, -0.00276333000510931]
     Last 10:  [0.014627601020038128, -0.03049786016345024, 0.01724329963326454, -0.06904149055480957, -0.0024812528863549232, -0.03309889882802963, 0.03958392143249512, -0.017603334039449692, 0.007656356319785118, -0.20161156356334686]
     Zeros: 1, Total: 8192

  → OUTPUT[0]: model.layers.8.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003363, Std: 0.078347
     First 10: [0.002752384403720498, -0.0028649973683059216, 0.0008750008419156075, 0.0019868325907737017, 0.0019726967439055443, 0.0018472479423508048, -0.0012814633082598448, -0.0007704828749410808, 0.0017381240613758564, 0.0009488607756793499]
     Last 10:  [-0.03431360796093941, -0.05467401444911957, 0.08141867816448212, 0.06532250344753265, -0.0410686619579792, -0.06537225842475891, 0.03702259063720703, 0.08513130247592926, 0.03497286140918732, 0.017679432407021523]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000003
     First 10: [-0.011962890625, -0.006591796875, 0.0206298828125, -0.00677490234375, -0.03662109375, 0.006866455078125, 0.012451171875, -0.000423431396484375, -0.00616455078125, -0.01275634765625]
     Last 10:  [0.0186767578125, -0.00811767578125, -0.0015411376953125, 0.0014190673828125, -0.053466796875, -0.0223388671875, -0.0108642578125, 0.0111083984375, 0.006988525390625, 0.008056640625]

================================================================================
143_model.layers.8.mlp: Gemma3MLP (model.layers.8.mlp)
================================================================================

  → INPUT[0]: model.layers.8.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009105, Std: 0.584839
     First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
     Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.8.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003363, Std: 0.078347
     First 10: [0.002752384403720498, -0.0028649973683059216, 0.0008750008419156075, 0.0019868325907737017, 0.0019726967439055443, 0.0018472479423508048, -0.0012814633082598448, -0.0007704828749410808, 0.0017381240613758564, 0.0009488607756793499]
     Last 10:  [-0.03431360796093941, -0.05467401444911957, 0.08141867816448212, 0.06532250344753265, -0.0410686619579792, -0.06537225842475891, 0.03702259063720703, 0.08513130247592926, 0.03497286140918732, 0.017679432407021523]
     Zeros: 0, Total: 2560

================================================================================
144_model.layers.8.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.8.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.8.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003363, Std: 0.078347
     First 10: [0.002752384403720498, -0.0028649973683059216, 0.0008750008419156075, 0.0019868325907737017, 0.0019726967439055443, 0.0018472479423508048, -0.0012814633082598448, -0.0007704828749410808, 0.0017381240613758564, 0.0009488607756793499]
     Last 10:  [-0.03431360796093941, -0.05467401444911957, 0.08141867816448212, 0.06532250344753265, -0.0410686619579792, -0.06537225842475891, 0.03702259063720703, 0.08513130247592926, 0.03497286140918732, 0.017679432407021523]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.8.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 4.812262, Std: 297.411346
     First 10: [32.48869705200195, -25.36347198486328, 0.9016271829605103, 2.7562763690948486, 0.5583570599555969, 15.760944366455078, -7.93302583694458, -0.46029332280158997, 2.7457590103149414, 4.687039375305176]
     Last 10:  [-0.6746705770492554, -20.607059478759766, 15.527055740356445, 19.42179298400879, -7.924557209014893, -19.043928146362305, 11.285587310791016, 10.993830680847168, 9.76799488067627, 4.539685249328613]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 20.080070
     First 10: [45.0, 33.5, 3.015625, 4.40625, 0.10302734375, 32.25, 23.125, 1.328125, 5.15625, 18.25]
     Last 10:  [0.63671875, 30.375, 14.875, 23.75, 15.0625, 23.25, 24.375, 9.75, 22.25, 20.375]

================================================================================
145_model.layers.9.input_layernorm: Gemma3RMSNorm (model.layers.9.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.9.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 44.687309, Std: 1667.299927
     First 10: [101.15411376953125, -51.3127555847168, -0.6652348041534424, -1.8337433338165283, -1.6162850856781006, 15.419488906860352, -32.35906982421875, 1.325498104095459, -1.229703426361084, -3.7574872970581055]
     Last 10:  [-1.806585431098938, -23.544044494628906, -52.233787536621094, 31.044723510742188, -18.879417419433594, 32.133216857910156, 33.04206085205078, 18.676990509033203, -9.570141792297363, 12.936627388000488]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.9.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.056097, Std: 2.281155
     First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
     Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 16.710205
     First 10: [6.96875, 10.4375, 12.0, 45.5, 53.25, 10.375, 14.1875, 61.5, 17.125, 17.375]
     Last 10:  [53.75, 10.75, -0.08056640625, 20.0, 19.0, 13.6875, 13.375, 43.25, 14.4375, 13.0625]

================================================================================
146_model.layers.9.self_attn.q_proj: Linear (model.layers.9.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.056097, Std: 2.281155
     First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
     Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.9.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.166783, Std: 2.789583
     First 10: [0.02467852458357811, -0.026729445904493332, -0.05607378110289574, 0.1246224045753479, 0.18721359968185425, -0.06746292114257812, 0.010742666199803352, 0.016207100823521614, -0.08036583662033081, 1.1960859298706055]
     Last 10:  [-1.2501798868179321, -1.1280786991119385, -1.1663203239440918, -6.917331218719482, 1.7192314863204956, 2.2820048332214355, -0.8183374404907227, -3.1023542881011963, 6.532040596008301, -5.053793907165527]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000029
     First 10: [0.0263671875, 0.00933837890625, -0.007659912109375, 0.00927734375, 0.0123291015625, -0.01336669921875, 0.0126953125, -0.060546875, -0.0791015625, 0.006011962890625]
     Last 10:  [-0.060302734375, -0.0255126953125, 0.057373046875, -0.04931640625, -0.03271484375, -0.032958984375, 0.07568359375, -0.01092529296875, -0.10888671875, 0.0126953125]

================================================================================
147_model.layers.9.self_attn.k_proj: Linear (model.layers.9.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.056097, Std: 2.281155
     First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
     Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.9.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.125679, Std: 2.435753
     First 10: [-0.005412565544247627, 0.0009385999292135239, 0.00040969252586364746, -1.9190289974212646, 0.012705106288194656, 0.0020060623064637184, -0.0035470612347126007, -0.0073425909504294395, 0.0016313884407281876, -0.1791614592075348]
     Last 10:  [-1.3604893684387207, -5.823982238769531, -0.28018632531166077, -5.998579502105713, -1.6269752979278564, -0.4232148826122284, -1.4390008449554443, -3.0131161212921143, 11.10321044921875, -4.0316667556762695]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000128
     First 10: [-0.002471923828125, -0.0004558563232421875, -0.0135498046875, -0.0208740234375, 0.023193359375, -0.00167083740234375, 0.01416015625, 0.00067901611328125, 0.01708984375, -0.0128173828125]
     Last 10:  [-0.024658203125, -0.01513671875, 0.011474609375, 0.001434326171875, 0.036376953125, -0.03564453125, -0.002197265625, -0.00457763671875, -0.087890625, -0.046142578125]

================================================================================
148_model.layers.9.self_attn.v_proj: Linear (model.layers.9.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.056097, Std: 2.281155
     First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
     Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.9.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.064794, Std: 1.284427
     First 10: [0.0455450564622879, 0.08494949340820312, -0.1192307248711586, -0.0823470801115036, -0.23964686691761017, 0.07151760160923004, 0.06058730185031891, 0.2196996510028839, -0.1093779131770134, -0.13625472784042358]
     Last 10:  [1.2232604026794434, -0.4954414367675781, 1.7907648086547852, 1.1888755559921265, 0.13683170080184937, -0.012479547411203384, 0.003463447093963623, -1.7538161277770996, -0.3638496398925781, -0.520622193813324]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000083
     First 10: [-0.0654296875, -0.0140380859375, 0.0888671875, 0.051513671875, 0.059814453125, -0.0712890625, 0.038330078125, 0.052490234375, 0.0177001953125, -0.0751953125]
     Last 10:  [-0.0086669921875, 0.01129150390625, -0.000518798828125, -0.01336669921875, -0.01904296875, 0.0303955078125, 0.03271484375, -0.0133056640625, 0.08251953125, -0.003662109375]

================================================================================
149_model.layers.9.self_attn.q_norm: Gemma3RMSNorm (model.layers.9.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.166783, Std: 2.789583
     First 10: [0.02467852458357811, -0.026729445904493332, -0.05607378110289574, 0.1246224045753479, 0.18721359968185425, -0.06746292114257812, 0.010742666199803352, 0.016207100823521614, -0.08036583662033081, 1.1960859298706055]
     Last 10:  [-1.2501798868179321, -1.1280786991119385, -1.1663203239440918, -6.917331218719482, 1.7192314863204956, 2.2820048332214355, -0.8183374404907227, -3.1023542881011963, 6.532040596008301, -5.053793907165527]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.9.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.048057, Std: 1.091970
     First 10: [0.19254617393016815, -0.10917513817548752, -0.39707162976264954, 0.5999670028686523, 0.6902206540107727, -0.693884015083313, 0.06273304671049118, 0.057837534695863724, -0.5253132581710815, 0.11497420072555542]
     Last 10:  [-0.7786942720413208, -0.28496021032333374, -0.4475806951522827, -3.7436676025390625, 1.3278546333312988, 1.7814663648605347, -0.28243133425712585, -5.359051704406738, 1.8692885637283325, -1.5957776308059692]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.828014
     First 10: [0.90234375, -0.004119873046875, 0.7265625, 0.173828125, -0.10107421875, 1.5078125, 0.423828125, -0.1298828125, 0.59375, -0.9765625]
     Last 10:  [0.7578125, -0.287109375, 0.0830078125, 0.52734375, 1.1796875, 1.203125, -0.0260009765625, 3.875, -0.1923828125, -0.10888671875]

================================================================================
150_model.layers.9.self_attn.k_norm: Gemma3RMSNorm (model.layers.9.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.125679, Std: 2.435753
     First 10: [-0.005412565544247627, 0.0009385999292135239, 0.00040969252586364746, -1.9190289974212646, 0.012705106288194656, 0.0020060623064637184, -0.0035470612347126007, -0.0073425909504294395, 0.0016313884407281876, -0.1791614592075348]
     Last 10:  [-1.3604893684387207, -5.823982238769531, -0.28018632531166077, -5.998579502105713, -1.6269752979278564, -0.4232148826122284, -1.4390008449554443, -3.0131161212921143, 11.10321044921875, -4.0316667556762695]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.9.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.029138, Std: 2.373605
     First 10: [-0.008878723718225956, 0.0031822719611227512, 0.0008405378321185708, -0.008768684230744839, 0.03901220113039017, 0.003721545683220029, -0.007763491477817297, -0.021807987242937088, 0.003488639835268259, -0.014735673554241657]
     Last 10:  [-0.9166615605354309, -6.881414413452148, -0.42879942059516907, -6.851455211639404, -1.7484478950500488, -0.42147573828697205, -1.2833014726638794, -0.6569403409957886, 13.61894416809082, -5.5803070068359375]
     Zeros: 16, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.036833
     First 10: [0.40234375, 1.8984375, 0.75390625, -0.99609375, 1.625, 0.5859375, 0.87109375, 1.5390625, 0.828125, -0.9296875]
     Last 10:  [0.87109375, 2.28125, 3.25, 2.171875, 1.984375, 1.765625, 1.4765625, -0.39453125, 2.40625, 2.84375]

================================================================================
151_model.layers.9.self_attn.o_proj: Linear (model.layers.9.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.9.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.043312, Std: 0.331764
     First 10: [0.0455450564622879, 0.08494949340820312, -0.1192307248711586, -0.0823470801115036, -0.23964686691761017, 0.07151760160923004, 0.06058730185031891, 0.2196996510028839, -0.1093779131770134, -0.13625472784042358]
     Last 10:  [0.15846066176891327, -0.06593266874551773, 0.1664915531873703, 0.2848215401172638, 0.0798199400305748, -0.30302372574806213, -0.020718175917863846, -0.29387107491493225, -0.06467234343290329, 0.024669818580150604]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.9.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000331, Std: 0.693421
     First 10: [0.12421678006649017, -0.3773933947086334, 0.06552499532699585, -0.016443565487861633, 0.014863226562738419, -0.08929777145385742, -0.11459504812955856, -0.021725639700889587, 0.004523247480392456, -0.13275565207004547]
     Last 10:  [-0.062127433717250824, 0.1397394835948944, 0.12208817154169083, -0.050935082137584686, -0.035726457834243774, 0.3017229437828064, -0.1014072373509407, 0.015057563781738281, -0.0320780947804451, -0.07806655764579773]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000006
     First 10: [-0.002044677734375, 0.007232666015625, 0.0181884765625, 0.01953125, 0.0108642578125, 0.0301513671875, -0.0303955078125, -0.0225830078125, -0.0595703125, -0.0264892578125]
     Last 10:  [0.0052490234375, -0.019775390625, 0.03955078125, -0.0126953125, -0.0238037109375, -0.0034027099609375, -0.0223388671875, -0.034423828125, -0.0023193359375, 0.00160980224609375]

================================================================================
152_model.layers.9.self_attn: Gemma3Attention (model.layers.9.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.9.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000331, Std: 0.693421
     First 10: [0.12421678006649017, -0.3773933947086334, 0.06552499532699585, -0.016443565487861633, 0.014863226562738419, -0.08929777145385742, -0.11459504812955856, -0.021725639700889587, 0.004523247480392456, -0.13275565207004547]
     Last 10:  [-0.062127433717250824, 0.1397394835948944, 0.12208817154169083, -0.050935082137584686, -0.035726457834243774, 0.3017229437828064, -0.1014072373509407, 0.015057563781738281, -0.0320780947804451, -0.07806655764579773]
     Zeros: 0, Total: 2560

================================================================================
153_model.layers.9.post_attention_layernorm: Gemma3RMSNorm (model.layers.9.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.9.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000331, Std: 0.693421
     First 10: [0.12421678006649017, -0.3773933947086334, 0.06552499532699585, -0.016443565487861633, 0.014863226562738419, -0.08929777145385742, -0.11459504812955856, -0.021725639700889587, 0.004523247480392456, -0.13275565207004547]
     Last 10:  [-0.062127433717250824, 0.1397394835948944, 0.12208817154169083, -0.050935082137584686, -0.035726457834243774, 0.3017229437828064, -0.1014072373509407, 0.015057563781738281, -0.0320780947804451, -0.07806655764579773]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.9.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.377312, Std: 18.516899
     First 10: [10.098784446716309, -25.69425392150879, 0.31244587898254395, -0.20332732796669006, 0.02286282368004322, -5.650547504425049, -4.497640132904053, -0.09625396877527237, 0.03713615983724594, -4.731902599334717]
     Last 10:  [-0.14627674221992493, 7.923409461975098, 2.0506889820098877, -1.9286869764328003, -1.019805669784546, 13.651311874389648, -4.469973087310791, 0.36841335892677307, -1.3330070972442627, -1.6826668977737427]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 21.472797
     First 10: [49.75, 41.5, 1.9765625, 6.71875, -0.039794921875, 38.5, 23.5, 1.765625, 4.125, 21.25]
     Last 10:  [0.515625, 35.5, 9.8125, 23.375, 17.375, 28.125, 27.375, 14.75, 25.75, 12.875]

================================================================================
154_model.layers.9.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.9.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.9.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 45.064625, Std: 1674.910156
     First 10: [111.25289916992188, -77.00701141357422, -0.35278892517089844, -2.0370707511901855, -1.593422293663025, 9.768941879272461, -36.85670852661133, 1.2292441129684448, -1.192567229270935, -8.489389419555664]
     Last 10:  [-1.9528621435165405, -15.620635032653809, -50.18309783935547, 29.116037368774414, -19.89922332763672, 45.78453063964844, 28.57208824157715, 19.0454044342041, -10.903148651123047, 11.253960609436035]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.9.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003974, Std: 0.403407
     First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
     Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 3.108285
     First 10: [0.6328125, 1.0859375, 3.515625, 11.9375, 19.375, 1.265625, 2.34375, 16.0, 4.15625, 3.09375]
     Last 10:  [15.125, 1.359375, -0.6171875, 3.21875, 3.40625, 2.015625, 1.8828125, 5.21875, 2.171875, 2.59375]

================================================================================
155_model.layers.9.mlp.gate_proj: Linear (model.layers.9.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.9.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003974, Std: 0.403407
     First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
     Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.9.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.082205, Std: 0.430109
     First 10: [-0.03826673701405525, -0.011213406920433044, 0.009805092588067055, 0.04224740341305733, 0.021607521921396255, -0.028274821117520332, 0.013320150785148144, -0.007507678121328354, 0.018904097378253937, -0.017052382230758667]
     Last 10:  [-0.11123155057430267, 0.08569207787513733, -0.04644787684082985, -0.8346119523048401, -0.15960519015789032, -0.9508532285690308, -0.010694049298763275, -0.7839912176132202, -0.3340357542037964, 0.3532089591026306]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000209
     First 10: [-0.02294921875, 0.0218505859375, -0.016357421875, 0.0084228515625, 0.0181884765625, -0.04150390625, 0.0242919921875, 0.0206298828125, -0.017578125, 0.0084228515625]
     Last 10:  [0.037353515625, 0.01153564453125, 0.07275390625, -0.006011962890625, 0.0181884765625, 0.0296630859375, 0.06396484375, -0.01806640625, -0.018310546875, -0.0179443359375]

================================================================================
156_model.layers.9.mlp.act_fn: PytorchGELUTanh (model.layers.9.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.9.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.082205, Std: 0.430109
     First 10: [-0.03826673701405525, -0.011213406920433044, 0.009805092588067055, 0.04224740341305733, 0.021607521921396255, -0.028274821117520332, 0.013320150785148144, -0.007507678121328354, 0.018904097378253937, -0.017052382230758667]
     Last 10:  [-0.11123155057430267, 0.08569207787513733, -0.04644787684082985, -0.8346119523048401, -0.15960519015789032, -0.9508532285690308, -0.010694049298763275, -0.7839912176132202, -0.3340357542037964, 0.3532089591026306]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.9.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.022524, Std: 0.206507
     First 10: [-0.018549323081970215, -0.005556541029363871, 0.004940900020301342, 0.021835537627339363, 0.010990006849169731, -0.013818512670695782, 0.0067308563739061356, -0.0037313527427613735, 0.009594608098268509, -0.008410191163420677]
     Last 10:  [-0.05069008842110634, 0.045771922916173935, -0.0223635695874691, -0.16866013407707214, -0.06968320161104202, -0.16257856786251068, -0.005301401484757662, -0.16983085870742798, -0.12332186847925186, 0.22535467147827148]
     Zeros: 0, Total: 8192

================================================================================
157_model.layers.9.mlp.up_proj: Linear (model.layers.9.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.9.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003974, Std: 0.403407
     First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
     Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.9.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.006251, Std: 0.338116
     First 10: [0.05998493358492851, 0.04723457992076874, -0.03580149635672569, 0.011865541338920593, 0.008993403054773808, 0.0197139885276556, 0.04562540352344513, 0.013179447501897812, -0.039749689400196075, -0.019456282258033752]
     Last 10:  [1.8716135025024414, -0.06909598410129547, 0.029802411794662476, -0.013266190886497498, 0.09007932245731354, -0.08685919642448425, 0.38390839099884033, -0.33110395073890686, 0.02308119647204876, 0.03661017119884491]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000016
     First 10: [-0.08056640625, 0.036376953125, 0.08154296875, -0.044189453125, -0.01214599609375, -0.0223388671875, -0.03564453125, 0.0634765625, -0.053466796875, 0.019287109375]
     Last 10:  [-0.003509521484375, -0.01708984375, 0.051513671875, 0.0003337860107421875, -0.052734375, 0.00628662109375, -0.04248046875, -0.0028076171875, 0.01397705078125, -0.00958251953125]

================================================================================
158_model.layers.9.mlp.down_proj: Linear (model.layers.9.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.9.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.000056, Std: 0.077191
     First 10: [-0.0011126799508929253, -0.0002624608750920743, -0.00017689161177258939, 0.00025909047690220177, 9.883756138151512e-05, -0.00027241799398325384, 0.00030709803104400635, -4.917716796626337e-05, -0.00038138270610943437, 0.00016363104805350304]
     Last 10:  [-0.09487225115299225, -0.0031626559793949127, -0.0006664883112534881, 0.002237477572634816, -0.006277015432715416, 0.014121443964540958, -0.0020352525170892477, 0.05623167008161545, -0.0028464163187891245, 0.008250272832810879]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.9.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000846, Std: 0.065763
     First 10: [0.0005802260129712522, -0.0010055621387436986, -0.0003045388148166239, -0.00030722582596354187, 0.0008253133855760098, -0.0002181680902140215, 0.0007850056863389909, 0.0006270183948799968, 0.0002043709100689739, -0.0001967987627722323]
     Last 10:  [0.00511874258518219, -0.028077974915504456, 0.11696401238441467, 0.010940564796328545, -0.020559977740049362, -0.08478829264640808, -0.14219513535499573, -0.020652109757065773, 0.025808138772845268, -0.022718943655490875]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000007
     First 10: [-0.0035400390625, 0.005950927734375, 0.000713348388671875, -0.000354766845703125, -0.0089111328125, 0.014892578125, -0.0012054443359375, 0.0079345703125, 0.005035400390625, 0.00579833984375]
     Last 10:  [0.006072998046875, -0.011474609375, 0.0234375, 0.0291748046875, 0.0028839111328125, 0.0250244140625, 0.0169677734375, 0.007476806640625, 0.01324462890625, 0.0021514892578125]

================================================================================
159_model.layers.9.mlp: Gemma3MLP (model.layers.9.mlp)
================================================================================

  → INPUT[0]: model.layers.9.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003974, Std: 0.403407
     First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
     Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.9.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000846, Std: 0.065763
     First 10: [0.0005802260129712522, -0.0010055621387436986, -0.0003045388148166239, -0.00030722582596354187, 0.0008253133855760098, -0.0002181680902140215, 0.0007850056863389909, 0.0006270183948799968, 0.0002043709100689739, -0.0001967987627722323]
     Last 10:  [0.00511874258518219, -0.028077974915504456, 0.11696401238441467, 0.010940564796328545, -0.020559977740049362, -0.08478829264640808, -0.14219513535499573, -0.020652109757065773, 0.025808138772845268, -0.022718943655490875]
     Zeros: 0, Total: 2560

================================================================================
160_model.layers.9.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.9.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.9.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000846, Std: 0.065763
     First 10: [0.0005802260129712522, -0.0010055621387436986, -0.0003045388148166239, -0.00030722582596354187, 0.0008253133855760098, -0.0002181680902140215, 0.0007850056863389909, 0.0006270183948799968, 0.0002043709100689739, -0.0001967987627722323]
     Last 10:  [0.00511874258518219, -0.028077974915504456, 0.11696401238441467, 0.010940564796328545, -0.020559977740049362, -0.08478829264640808, -0.14219513535499573, -0.020652109757065773, 0.025808138772845268, -0.022718943655490875]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.9.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.331436, Std: 103.208344
     First 10: [31.50017738342285, -44.134002685546875, -1.0937179327011108, -1.5636688470840454, 0.7751011252403259, -8.152524948120117, 20.478567123413086, 1.2779006958007812, 1.1302326917648315, -4.0412278175354]
     Last 10:  [0.13835592567920685, -18.577091217041016, 28.818344116210938, 5.315811634063721, -5.9867353439331055, -45.28749465942383, -81.82987976074219, -4.803733825683594, 14.051558494567871, -8.57260799407959]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 30.176754
     First 10: [76.0, 61.25, 4.09375, 6.21875, 0.33203125, 52.0, 36.0, 1.890625, 6.84375, 28.125]
     Last 10:  [0.9609375, 47.0, 16.875, 34.25, 20.125, 37.75, 40.75, 15.875, 38.5, 26.375]

================================================================================
161_model.layers.10.input_layernorm: Gemma3RMSNorm (model.layers.10.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.10.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 46.396053, Std: 1768.300903
     First 10: [142.75308227539062, -121.1410140991211, -1.4465068578720093, -3.6007394790649414, -0.818321168422699, 1.6164169311523438, -16.378141403198242, 2.5071449279785156, -0.062334537506103516, -12.530616760253906]
     Last 10:  [-1.81450617313385, -34.19772720336914, -21.36475372314453, 34.43185043334961, -25.88595962524414, 0.4970359802246094, -53.257789611816406, 14.241670608520508, 3.148409843444824, 2.6813526153564453]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.10.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.094682, Std: 2.859625
     First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
     Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 16.907379
     First 10: [5.15625, 6.46875, 20.5, 74.0, 91.0, 6.71875, 10.5, 96.0, 23.375, 15.125]
     Last 10:  [81.5, 8.125, 0.181640625, 12.5625, 18.25, 10.75, 9.375, 29.5, 9.4375, 15.125]

================================================================================
162_model.layers.10.self_attn.q_proj: Linear (model.layers.10.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.094682, Std: 2.859625
     First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
     Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.10.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.190860, Std: 3.272703
     First 10: [-0.0828784927725792, 0.00469246506690979, -0.036376941949129105, -0.014535492286086082, 0.18007519841194153, 0.23827996850013733, 0.04098324477672577, -0.21951580047607422, 1.7257674932479858, 0.006941231898963451]
     Last 10:  [2.5783162117004395, -0.4095580577850342, -1.7054486274719238, -0.5070802569389343, 2.8596291542053223, 0.2969181537628174, -2.062356472015381, -1.1114716529846191, 0.864229142665863, 1.78420090675354]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000038
     First 10: [0.0084228515625, 0.005950927734375, 0.00193023681640625, 0.006988525390625, -0.00421142578125, 0.002838134765625, -0.004730224609375, -0.0037384033203125, 0.00049591064453125, -0.005706787109375]
     Last 10:  [0.03662109375, -0.024658203125, -0.0145263671875, -0.00421142578125, 0.0169677734375, 0.013916015625, 0.005767822265625, 0.06787109375, 0.001678466796875, 0.01312255859375]

================================================================================
163_model.layers.10.self_attn.k_proj: Linear (model.layers.10.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.094682, Std: 2.859625
     First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
     Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.10.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.140907, Std: 2.868601
     First 10: [0.008754164911806583, 0.006492519751191139, 0.006138555705547333, 2.6632561683654785, -0.0065263547003269196, -0.003983864560723305, 0.009535718709230423, 0.005177866201847792, -0.0042121317237615585, -0.012321025133132935]
     Last 10:  [2.639613151550293, 3.187025785446167, -4.759749412536621, -6.3840155601501465, 1.4393818378448486, 13.828171730041504, 2.076052665710449, -5.20221471786499, 7.615484714508057, -3.292590856552124]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000004
     First 10: [0.0093994140625, -0.01708984375, 0.01470947265625, -0.004730224609375, -0.01611328125, -0.0162353515625, 0.000514984130859375, -0.0022430419921875, 0.006866455078125, 0.043701171875]
     Last 10:  [0.013916015625, 0.025390625, -0.000865936279296875, -0.031982421875, -0.015869140625, -0.017578125, 0.003326416015625, 0.007354736328125, 0.00335693359375, 0.06689453125]

================================================================================
164_model.layers.10.self_attn.v_proj: Linear (model.layers.10.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.094682, Std: 2.859625
     First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
     Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.10.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.091402, Std: 1.538940
     First 10: [0.11426025629043579, -0.021701347082853317, -0.14884179830551147, -0.08573133498430252, 0.07151083648204803, 0.09273259341716766, -0.10236135125160217, -0.12006890028715134, -0.08674372732639313, 0.012592658400535583]
     Last 10:  [3.297092914581299, -1.8792952299118042, -1.078461766242981, 2.0830676555633545, -1.9314913749694824, 0.3150622844696045, 0.9158605933189392, 1.11196768283844, -1.8287694454193115, -0.6908064484596252]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000024
     First 10: [0.00193023681640625, 0.01373291015625, 0.130859375, 0.02685546875, 0.05810546875, -0.017333984375, -0.033203125, 0.019287109375, -0.0830078125, -0.052490234375]
     Last 10:  [0.0634765625, -0.01904296875, -0.056884765625, -0.018798828125, -0.05322265625, -0.0267333984375, 0.0439453125, 0.0198974609375, -0.0078125, -0.002532958984375]

================================================================================
165_model.layers.10.self_attn.q_norm: Gemma3RMSNorm (model.layers.10.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.190860, Std: 3.272703
     First 10: [-0.0828784927725792, 0.00469246506690979, -0.036376941949129105, -0.014535492286086082, 0.18007519841194153, 0.23827996850013733, 0.04098324477672577, -0.21951580047607422, 1.7257674932479858, 0.006941231898963451]
     Last 10:  [2.5783162117004395, -0.4095580577850342, -1.7054486274719238, -0.5070802569389343, 2.8596291542053223, 0.2969181537628174, -2.062356472015381, -1.1114716529846191, 0.864229142665863, 1.78420090675354]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.10.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.055874, Std: 1.145066
     First 10: [-0.8290181159973145, 0.030477747321128845, -0.2700224816799164, -0.06940840929746628, 1.0880906581878662, 0.6856436729431152, 0.24578315019607544, -0.4862251877784729, 1.1716649532318115, 0.032359637320041656]
     Last 10:  [0.3373975157737732, -0.042468246072530746, -0.5925214290618896, -0.3073346018791199, 0.6559616923332214, 0.04470551386475563, -1.054265022277832, -0.5358565449714661, 0.09854276478290558, 0.9175349473953247]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.608988
     First 10: [2.453125, 1.2421875, 1.5625, 0.6484375, 1.0859375, -0.00665283203125, 1.0703125, -0.2353515625, -0.765625, 0.609375]
     Last 10:  [-0.33203125, -0.470703125, 0.7734375, 2.09375, 0.1708984375, -0.2314453125, 1.609375, 1.4609375, -0.41796875, 1.625]

================================================================================
166_model.layers.10.self_attn.k_norm: Gemma3RMSNorm (model.layers.10.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.140907, Std: 2.868601
     First 10: [0.008754164911806583, 0.006492519751191139, 0.006138555705547333, 2.6632561683654785, -0.0065263547003269196, -0.003983864560723305, 0.009535718709230423, 0.005177866201847792, -0.0042121317237615585, -0.012321025133132935]
     Last 10:  [2.639613151550293, 3.187025785446167, -4.759749412536621, -6.3840155601501465, 1.4393818378448486, 13.828171730041504, 2.076052665710449, -5.20221471786499, 7.615484714508057, -3.292590856552124]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.10.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.220218, Std: 3.583435
     First 10: [0.019340284168720245, 0.00747619429603219, 0.0065431478433310986, 0.0, -0.012795715592801571, -0.011302190832793713, 0.012515787035226822, 0.014520718716084957, -0.0030389532912522554, -0.0023102210834622383]
     Last 10:  [3.158682346343994, 5.162819862365723, -2.397439479827881, -1.6778192520141602, 2.2262704372406006, 15.252899169921875, 0.8661249876022339, -1.7574570178985596, 13.204607009887695, -1.8762216567993164]
     Zeros: 28, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.687784
     First 10: [1.1171875, 0.103515625, 0.021484375, -1.0, 0.87890625, 1.71875, 0.2578125, 1.6875, -0.30859375, -0.8203125]
     Last 10:  [3.59375, 5.21875, 0.93359375, 0.0089111328125, 4.9375, 3.234375, 0.6015625, 0.296875, 5.65625, 1.1875]

================================================================================
167_model.layers.10.self_attn.o_proj: Linear (model.layers.10.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.10.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.038541, Std: 0.492254
     First 10: [0.11426025629043579, -0.021701347082853317, -0.14884179830551147, -0.08573133498430252, 0.07151083648204803, 0.09273259341716766, -0.10236135125160217, -0.12006890028715134, -0.08674372732639313, 0.012592658400535583]
     Last 10:  [0.8527554869651794, -0.3474169969558716, -0.3893723785877228, 0.4288720488548279, -0.3392183482646942, -0.09674204140901566, 0.11644220352172852, 0.22969727218151093, -0.41802793741226196, -0.27048882842063904]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.10.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000173, Std: 1.198253
     First 10: [0.08139391243457794, 0.3662101626396179, -0.17216959595680237, 0.20862720906734467, 0.1282903254032135, 0.1841568499803543, 0.13195739686489105, -0.46017882227897644, -0.24284854531288147, 0.39935487508773804]
     Last 10:  [0.09876037389039993, 1.2092902660369873, -4.387842655181885, 0.7453374862670898, -0.4389907717704773, -0.1877751350402832, 0.35236942768096924, 0.18171849846839905, -0.8472557067871094, -0.6854297518730164]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000029
     First 10: [0.037841796875, 0.05126953125, 0.00116729736328125, 0.061767578125, -0.0177001953125, 0.028564453125, -0.004852294921875, -0.021484375, 0.03466796875, -0.047607421875]
     Last 10:  [-0.0026702880859375, -0.0038909912109375, 0.044921875, -0.009521484375, 0.0712890625, -0.0286865234375, -0.027099609375, -0.01458740234375, 0.051025390625, 0.0196533203125]

================================================================================
168_model.layers.10.self_attn: Gemma3Attention (model.layers.10.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.10.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000173, Std: 1.198253
     First 10: [0.08139391243457794, 0.3662101626396179, -0.17216959595680237, 0.20862720906734467, 0.1282903254032135, 0.1841568499803543, 0.13195739686489105, -0.46017882227897644, -0.24284854531288147, 0.39935487508773804]
     Last 10:  [0.09876037389039993, 1.2092902660369873, -4.387842655181885, 0.7453374862670898, -0.4389907717704773, -0.1877751350402832, 0.35236942768096924, 0.18171849846839905, -0.8472557067871094, -0.6854297518730164]
     Zeros: 0, Total: 2560

================================================================================
169_model.layers.10.post_attention_layernorm: Gemma3RMSNorm (model.layers.10.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.10.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000173, Std: 1.198253
     First 10: [0.08139391243457794, 0.3662101626396179, -0.17216959595680237, 0.20862720906734467, 0.1282903254032135, 0.1841568499803543, 0.13195739686489105, -0.46017882227897644, -0.24284854531288147, 0.39935487508773804]
     Last 10:  [0.09876037389039993, 1.2092902660369873, -4.387842655181885, 0.7453374862670898, -0.4389907717704773, -0.1877751350402832, 0.35236942768096924, 0.18171849846839905, -0.8472557067871094, -0.6854297518730164]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.10.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 3.082355, Std: 75.407829
     First 10: [7.217288494110107, 27.952926635742188, -0.6049535274505615, 2.1097676753997803, 0.12769657373428345, 10.858203887939453, 5.126643180847168, -1.2817156314849854, -1.220980167388916, 14.146242141723633]
     Last 10:  [0.14265073835849762, 55.39801025390625, -2.1548707485198975, 25.722963333129883, -9.874798774719238, -7.521966934204102, 16.069780349731445, 13.140156745910645, -32.199188232421875, -10.067641258239746]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 42.107410
     First 10: [96.0, 82.5, 2.84375, 10.0625, 0.0888671875, 63.5, 41.5, 2.046875, 4.5, 37.75]
     Last 10:  [0.7578125, 54.75, -0.40234375, 41.0, 26.375, 47.75, 54.5, 87.0, 45.25, 16.875]

================================================================================
170_model.layers.10.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.10.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.10.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 49.478416, Std: 1801.616699
     First 10: [149.97036743164062, -93.1880874633789, -2.0514602661132812, -1.4909718036651611, -0.6906245946884155, 12.474620819091797, -11.251498222351074, 1.2254292964935303, -1.2833147048950195, 1.6156253814697266]
     Last 10:  [-1.6718554496765137, 21.20028305053711, -23.519624710083008, 60.154815673828125, -35.76075744628906, -7.024930953979492, -37.188011169433594, 27.38182830810547, -29.050777435302734, -7.386288642883301]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.10.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.016517, Std: 0.574370
     First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
     Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 3.507970
     First 10: [0.4375, 0.6796875, 6.0, 12.125, 32.5, 1.0, 2.015625, 23.25, 6.5, 2.671875]
     Last 10:  [25.5, 1.296875, -0.32421875, 2.515625, 3.484375, 1.8125, 1.3671875, 0.40234375, 1.828125, 5.34375]

================================================================================
171_model.layers.10.mlp.gate_proj: Linear (model.layers.10.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.10.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.016517, Std: 0.574370
     First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
     Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.10.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.124841, Std: 0.583820
     First 10: [0.017862141132354736, -0.041865307837724686, 0.06635323166847229, -0.019236870110034943, -0.03254745528101921, 0.020630240440368652, -0.009831612929701805, -0.033729176968336105, -0.04689794406294823, 0.022470058873295784]
     Last 10:  [-0.5746508836746216, 0.030100315809249878, 0.4244081974029541, -0.12016615271568298, -0.08698640018701553, -0.1862708330154419, -0.632300615310669, -0.30126065015792847, -0.5871347188949585, -0.792340874671936]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000101
     First 10: [0.0262451171875, -0.0010986328125, -0.031982421875, -0.02001953125, -0.005096435546875, -0.010986328125, 0.00135040283203125, -0.01202392578125, -0.0050048828125, 0.0078125]
     Last 10:  [-0.054443359375, 0.03857421875, 0.007781982421875, -0.049560546875, 0.002410888671875, -0.022705078125, 0.04638671875, 0.017822265625, -0.0556640625, -0.055419921875]

================================================================================
172_model.layers.10.mlp.act_fn: PytorchGELUTanh (model.layers.10.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.10.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.124841, Std: 0.583820
     First 10: [0.017862141132354736, -0.041865307837724686, 0.06635323166847229, -0.019236870110034943, -0.03254745528101921, 0.020630240440368652, -0.009831612929701805, -0.033729176968336105, -0.04689794406294823, 0.022470058873295784]
     Last 10:  [-0.5746508836746216, 0.030100315809249878, 0.4244081974029541, -0.12016615271568298, -0.08698640018701553, -0.1862708330154419, -0.632300615310669, -0.30126065015792847, -0.5871347188949585, -0.792340874671936]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.10.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.042985, Std: 0.289494
     First 10: [0.009058347903192043, -0.020233631134033203, 0.034931764006614685, -0.009470812976360321, -0.015851188451051712, 0.010484900325536728, -0.004877245053648949, -0.016410816460847855, -0.022571854293346405, 0.01143644005060196]
     Last 10:  [-0.16251875460147858, 0.015411555767059326, 0.28195294737815857, -0.054336290806531906, -0.04047837108373642, -0.07937340438365936, -0.16671083867549896, -0.11496598273515701, -0.1635807901620865, -0.16970670223236084]
     Zeros: 0, Total: 8192

================================================================================
173_model.layers.10.mlp.up_proj: Linear (model.layers.10.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.10.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.016517, Std: 0.574370
     First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
     Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.10.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.011889, Std: 0.442743
     First 10: [-0.01862650364637375, 0.029584746807813644, -0.048310358077287674, 0.03148055076599121, -0.00691143237054348, -0.04060946777462959, -0.08718965202569962, -0.024562746286392212, 0.06944655627012253, 0.05524211749434471]
     Last 10:  [-0.5216392278671265, 0.18255525827407837, 0.34192290902137756, -0.7979309558868408, 0.5336673259735107, -1.235034465789795, -0.3385167717933655, 0.4499995708465576, 0.5553113222122192, -0.2656688690185547]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000046
     First 10: [-0.02783203125, 0.013427734375, -0.04296875, 0.020751953125, 0.0152587890625, -0.006011962890625, 0.0067138671875, -0.0157470703125, 0.00299072265625, -0.00457763671875]
     Last 10:  [0.10888671875, -0.0301513671875, -0.01953125, 0.0201416015625, -0.10595703125, 0.0203857421875, -0.017333984375, -0.033203125, -0.01373291015625, 0.025390625]

================================================================================
174_model.layers.10.mlp.down_proj: Linear (model.layers.10.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.10.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.002149, Std: 0.134331
     First 10: [-0.00016872535343281925, -0.0005986068281345069, -0.0016875660512596369, -0.0002981464203912765, 0.00010955441393889487, -0.0004257862165104598, 0.00042524529271759093, 0.0004030947166029364, -0.0015675375470891595, 0.0006317731458693743]
     Last 10:  [0.08477615565061569, 0.002813460538163781, 0.09640616923570633, 0.04335660859942436, -0.021601984277367592, 0.09802889078855515, 0.05643441528081894, -0.05173464119434357, -0.09083826839923859, 0.045085787773132324]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.10.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.002034, Std: 0.094973
     First 10: [-0.0017821466317400336, -0.0003282146353740245, 0.0012456298572942615, 0.001877032220363617, 0.0007391266408376396, -0.0003862714802380651, 6.981969636399299e-05, 0.0002841940149664879, 0.001292796223424375, -0.0012813075445592403]
     Last 10:  [0.004799460992217064, 0.17821981012821198, 0.07964351773262024, -0.15814101696014404, -0.16745144128799438, 0.0569375678896904, 0.051184915006160736, 0.06220697611570358, 0.05926840752363205, -0.004520704969763756]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000008
     First 10: [-0.00165557861328125, -0.0016021728515625, 0.0045166015625, 0.0128173828125, 0.012939453125, 0.004852294921875, -0.016845703125, -0.00144195556640625, 0.0135498046875, 0.006561279296875]
     Last 10:  [-0.000518798828125, -0.000766754150390625, 0.006378173828125, 0.00081634521484375, -0.0047607421875, 0.0164794921875, -0.01226806640625, -0.00909423828125, 0.00848388671875, -0.007293701171875]

================================================================================
175_model.layers.10.mlp: Gemma3MLP (model.layers.10.mlp)
================================================================================

  → INPUT[0]: model.layers.10.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.016517, Std: 0.574370
     First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
     Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.10.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.002034, Std: 0.094973
     First 10: [-0.0017821466317400336, -0.0003282146353740245, 0.0012456298572942615, 0.001877032220363617, 0.0007391266408376396, -0.0003862714802380651, 6.981969636399299e-05, 0.0002841940149664879, 0.001292796223424375, -0.0012813075445592403]
     Last 10:  [0.004799460992217064, 0.17821981012821198, 0.07964351773262024, -0.15814101696014404, -0.16745144128799438, 0.0569375678896904, 0.051184915006160736, 0.06220697611570358, 0.05926840752363205, -0.004520704969763756]
     Zeros: 0, Total: 2560

================================================================================
176_model.layers.10.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.10.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.10.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.002034, Std: 0.094973
     First 10: [-0.0017821466317400336, -0.0003282146353740245, 0.0012456298572942615, 0.001877032220363617, 0.0007391266408376396, -0.0003862714802380651, 6.981969636399299e-05, 0.0002841940149664879, 0.001292796223424375, -0.0012813075445592403]
     Last 10:  [0.004799460992217064, 0.17821981012821198, 0.07964351773262024, -0.15814101696014404, -0.16745144128799438, 0.0569375678896904, 0.051184915006160736, 0.06220697611570358, 0.05926840752363205, -0.004520704969763756]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.10.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.354804, Std: 105.579086
     First 10: [-117.80230712890625, -19.375795364379883, 3.495473623275757, 10.778473854064941, 0.6409636735916138, -15.89794921875, 2.046351909637451, 0.5464367866516113, 5.777657508850098, -30.36277961730957]
     Last 10:  [0.13558687269687653, 135.1779022216797, 20.809425354003906, -91.35601043701172, -58.705299377441406, 36.90945053100586, 38.59751892089844, 20.025495529174805, 38.6817626953125, -1.6048089265823364]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 55.949501
     First 10: [158.0, 141.0, 5.75, 12.8125, 1.0859375, 98.0, 69.5, 3.625, 9.75, 56.0]
     Last 10:  [2.203125, 85.0, 28.625, 64.5, 38.75, 72.5, 84.5, 35.5, 73.0, 39.25]

================================================================================
177_model.layers.11.input_layernorm: Gemma3RMSNorm (model.layers.11.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.11.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 49.123611, Std: 1837.341187
     First 10: [32.168060302734375, -112.56388092041016, 1.4440133571624756, 9.28750228881836, -0.04966092109680176, -3.423328399658203, -9.205146789550781, 1.7718660831451416, 4.494342803955078, -28.747154235839844]
     Last 10:  [-1.5362685918807983, 156.37818908691406, -2.7101993560791016, -31.201194763183594, -94.46605682373047, 29.884519577026367, 1.4095077514648438, 47.407325744628906, 9.630985260009766, -8.991097450256348]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.11.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014602, Std: 0.654240
     First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
     Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 4.293710
     First 10: [0.1748046875, 0.04638671875, 10.75, 9.375, 48.5, 0.283203125, 1.1015625, 28.0, 8.9375, 1.2890625]
     Last 10:  [36.5, 0.6640625, 0.5078125, 2.140625, 2.375, 1.0546875, 0.80078125, 2.34375, 1.4765625, 4.8125]

================================================================================
178_model.layers.11.self_attn.q_proj: Linear (model.layers.11.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014602, Std: 0.654240
     First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
     Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.11.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.007995, Std: 0.924809
     First 10: [0.01365344412624836, -0.11098835617303848, 0.06696315109729767, 0.10959981381893158, 0.055122341960668564, 0.29333293437957764, -0.1122264489531517, 0.0718197450041771, 0.07852256298065186, 0.010455025359988213]
     Last 10:  [-0.44298988580703735, -0.45652490854263306, -0.24241551756858826, -0.5209836959838867, 0.06410010904073715, -0.321913480758667, 0.06172740459442139, -1.0597397089004517, 0.6172072887420654, 0.03667899966239929]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000012
     First 10: [0.00762939453125, -0.04345703125, -0.007415771484375, -0.01287841796875, -0.0003795623779296875, 0.008544921875, 0.0147705078125, 0.007568359375, 0.0150146484375, 0.0113525390625]
     Last 10:  [0.0115966796875, -0.049560546875, -0.0059814453125, -0.0263671875, 0.01123046875, 0.0244140625, 0.026123046875, 0.03173828125, 0.006561279296875, 0.01531982421875]

================================================================================
179_model.layers.11.self_attn.k_proj: Linear (model.layers.11.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014602, Std: 0.654240
     First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
     Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.11.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.026486, Std: 0.587689
     First 10: [-0.002469163853675127, -0.00035997433587908745, 0.24178196489810944, 0.001079668290913105, -0.27047163248062134, -0.00366036593914032, 0.0005518437828868628, 0.0009618494659662247, 0.0008495703223161399, 0.00015368242748081684]
     Last 10:  [-1.6532350778579712, -0.46099531650543213, 0.036373320966959, -2.6158387660980225, -0.32249176502227783, -1.019518494606018, -0.31767696142196655, -3.3103952407836914, -1.6283916234970093, 0.9027724266052246]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000080
     First 10: [0.0159912109375, 0.00787353515625, -0.01025390625, -0.00714111328125, 0.003753662109375, -0.00921630859375, 0.006988525390625, -0.000732421875, 0.00160980224609375, -0.00162506103515625]
     Last 10:  [0.00897216796875, -0.0145263671875, -0.01446533203125, 0.068359375, -0.01080322265625, 0.050537109375, 0.017333984375, 0.171875, -0.0240478515625, 0.04833984375]

================================================================================
180_model.layers.11.self_attn.v_proj: Linear (model.layers.11.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014602, Std: 0.654240
     First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
     Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.11.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.011266, Std: 0.384210
     First 10: [-0.0047807153314352036, 0.014551674015820026, -0.06743748486042023, 0.009456025436520576, 0.014122195541858673, 0.002366265282034874, -0.006114509887993336, -0.010338650085031986, 0.012651808559894562, 0.0396566279232502]
     Last 10:  [-0.4023992717266083, -0.1231396347284317, -0.02763575315475464, 0.33686989545822144, 0.49059170484542847, -0.10026510059833527, -0.2628980278968811, 0.2927035689353943, -0.12586474418640137, -0.0731324553489685]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000228
     First 10: [-0.060302734375, 0.01275634765625, -0.0732421875, 0.05224609375, 0.02783203125, -0.0184326171875, 0.06982421875, -0.01239013671875, -0.029296875, -0.0032196044921875]
     Last 10:  [-0.060791015625, -0.01025390625, -0.03662109375, -0.00726318359375, -0.01263427734375, 0.0289306640625, 0.01068115234375, -0.00176239013671875, 0.013916015625, 0.00469970703125]

================================================================================
181_model.layers.11.self_attn.q_norm: Gemma3RMSNorm (model.layers.11.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.007995, Std: 0.924809
     First 10: [0.01365344412624836, -0.11098835617303848, 0.06696315109729767, 0.10959981381893158, 0.055122341960668564, 0.29333293437957764, -0.1122264489531517, 0.0718197450041771, 0.07852256298065186, 0.010455025359988213]
     Last 10:  [-0.44298988580703735, -0.45652490854263306, -0.24241551756858826, -0.5209836959838867, 0.06410010904073715, -0.321913480758667, 0.06172740459442139, -1.0597397089004517, 0.6172072887420654, 0.03667899966239929]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.11.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.007903, Std: 1.376288
     First 10: [0.13516652584075928, -1.280019760131836, 0.8400245308876038, 1.340036153793335, 0.5512772798538208, -0.016957318410277367, -3.451462984085083, 0.7089260220527649, 1.9791436195373535, 0.08982832729816437]
     Last 10:  [-0.12306046485900879, -0.1152913048863411, -0.0839586928486824, -0.11277410387992859, 0.020813029259443283, -0.12891283631324768, 0.014697923325002193, -0.6136323809623718, 0.3618415594100952, 0.0071457079611718655]
     Zeros: 16, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.544234
     First 10: [0.337890625, 0.55859375, 0.6953125, 0.65234375, 0.3515625, -1.0078125, 3.15625, 0.333984375, 2.40625, 0.1611328125]
     Last 10:  [-0.69921875, -0.7265625, -0.625, -0.765625, -0.6484375, -0.56640625, -0.7421875, -0.373046875, -0.365234375, -0.7890625]

================================================================================
182_model.layers.11.self_attn.k_norm: Gemma3RMSNorm (model.layers.11.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.026486, Std: 0.587689
     First 10: [-0.002469163853675127, -0.00035997433587908745, 0.24178196489810944, 0.001079668290913105, -0.27047163248062134, -0.00366036593914032, 0.0005518437828868628, 0.0009618494659662247, 0.0008495703223161399, 0.00015368242748081684]
     Last 10:  [-1.6532350778579712, -0.46099531650543213, 0.036373320966959, -2.6158387660980225, -0.32249176502227783, -1.019518494606018, -0.31767696142196655, -3.3103952407836914, -1.6283916234970093, 0.9027724266052246]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.11.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.514531, Std: 8.139625
     First 10: [-0.04140526056289673, -0.005979708395898342, -0.009517434984445572, 0.009838691912591457, -0.0, -0.0015849412884563208, 0.004154450725764036, 0.01245658565312624, 0.005752064753323793, 0.003000557189807296]
     Last 10:  [-19.985376358032227, -6.603254318237305, 0.34512636065483093, -53.93631362915039, -3.6336863040924072, -9.022537231445312, -6.173439025878906, -27.786224365234375, -71.60902404785156, 16.555267333984375]
     Zeros: 20, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 2.486512
     First 10: [2.328125, 2.296875, -1.0078125, 0.80859375, -1.0, -0.9140625, 0.494140625, 1.5703125, 0.34375, 2.875]
     Last 10:  [7.28125, 8.8125, 5.5, 13.125, 6.71875, 5.0625, 12.3125, 4.75, 29.125, 11.5625]

================================================================================
183_model.layers.11.self_attn.o_proj: Linear (model.layers.11.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.11.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.005071, Std: 0.116178
     First 10: [-0.0047807153314352036, 0.014551674015820026, -0.06743748486042023, 0.009456025436520576, 0.014122195541858673, 0.002366265282034874, -0.006114509887993336, -0.010338650085031986, 0.012651808559894562, 0.0396566279232502]
     Last 10:  [-0.029682962223887444, -0.0044352649711072445, 0.0013895579613745213, 0.06387273967266083, 0.052462149411439896, 0.022754201665520668, -0.015520865097641945, -0.014873801730573177, 0.01218338217586279, -0.03349282965064049]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.11.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000749, Std: 0.181747
     First 10: [-0.010984765365719795, 0.01847337745130062, -0.019123654812574387, -0.036313023418188095, -0.005859001073986292, 0.01295386254787445, -0.022312497720122337, -0.006964103318750858, 0.04751380532979965, -0.029459405690431595]
     Last 10:  [0.00819660909473896, -0.09836061298847198, 0.025486791506409645, 0.0856885090470314, -0.05444877967238426, -0.012221388518810272, -0.11825872957706451, -0.004064660053700209, -0.06401020288467407, -0.042394526302814484]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000008
     First 10: [0.0128173828125, -0.00860595703125, 0.007293701171875, 0.005126953125, -0.021484375, 0.0296630859375, -0.000499725341796875, 0.02294921875, 0.01336669921875, -0.0031280517578125]
     Last 10:  [-0.00030517578125, 0.07958984375, 0.030029296875, 0.0289306640625, -0.012939453125, 0.036865234375, -0.015625, 0.0262451171875, 0.00078582763671875, -0.052001953125]

================================================================================
184_model.layers.11.self_attn: Gemma3Attention (model.layers.11.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.11.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000749, Std: 0.181747
     First 10: [-0.010984765365719795, 0.01847337745130062, -0.019123654812574387, -0.036313023418188095, -0.005859001073986292, 0.01295386254787445, -0.022312497720122337, -0.006964103318750858, 0.04751380532979965, -0.029459405690431595]
     Last 10:  [0.00819660909473896, -0.09836061298847198, 0.025486791506409645, 0.0856885090470314, -0.05444877967238426, -0.012221388518810272, -0.11825872957706451, -0.004064660053700209, -0.06401020288467407, -0.042394526302814484]
     Zeros: 0, Total: 2560

================================================================================
185_model.layers.11.post_attention_layernorm: Gemma3RMSNorm (model.layers.11.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.11.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.000749, Std: 0.181747
     First 10: [-0.010984765365719795, 0.01847337745130062, -0.019123654812574387, -0.036313023418188095, -0.005859001073986292, 0.01295386254787445, -0.022312497720122337, -0.006964103318750858, 0.04751380532979965, -0.029459405690431595]
     Last 10:  [0.00819660909473896, -0.09836061298847198, 0.025486791506409645, 0.0856885090470314, -0.05444877967238426, -0.012221388518810272, -0.11825872957706451, -0.004064660053700209, -0.06401020288467407, -0.042394526302814484]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.11.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 2.086629, Std: 52.864017
     First 10: [-31.27505874633789, 30.821630477905273, -1.974318504333496, -5.576315879821777, -0.22645039856433868, 7.741854667663574, -9.491947174072266, -0.39019545912742615, 4.806707382202148, -15.03877067565918]
     Last 10:  [0.3318438231945038, -38.16423034667969, 15.001043319702148, 32.261260986328125, -10.697426795959473, -4.420450210571289, -48.60673904418945, -12.108927726745605, -23.047117233276367, -8.677656173706055]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 110.148514
     First 10: [342.0, 200.0, 11.4375, 17.5, 3.65625, 71.0, 50.25, 5.75, 11.1875, 60.5]
     Last 10:  [5.15625, 58.0, 88.5, 56.25, 28.875, 54.0, 61.5, 452.0, 53.75, 30.125]

================================================================================
186_model.layers.11.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.11.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.11.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 51.210228, Std: 1853.544312
     First 10: [0.8930015563964844, -81.74224853515625, -0.5303051471710205, 3.711186408996582, -0.27611130475997925, 4.318526268005371, -18.697093963623047, 1.381670594215393, 9.301050186157227, -43.785926818847656]
     Last 10:  [-1.2044247388839722, 118.21395874023438, 12.290843963623047, 1.0600662231445312, -105.16348266601562, 25.464069366455078, -47.19723129272461, 35.298397064208984, -13.416131973266602, -17.66875457763672]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.11.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003438, Std: 0.681187
     First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
     Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 4.350527
     First 10: [-0.5546875, -0.1240234375, 9.25, 14.5625, 39.25, 0.91796875, 2.203125, 28.25, 9.6875, 2.140625]
     Last 10:  [30.125, 1.40625, 0.1416015625, 2.75, 3.890625, 1.859375, 1.421875, -0.81640625, 1.9765625, 8.1875]

================================================================================
187_model.layers.11.mlp.gate_proj: Linear (model.layers.11.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.11.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003438, Std: 0.681187
     First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
     Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.11.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.235493, Std: 0.750462
     First 10: [0.020761700347065926, 0.011689679697155952, -0.1729077398777008, 0.0051129050552845, 0.01828872039914131, -0.0018476257100701332, 0.07961639761924744, -0.0219118595123291, 0.04736284911632538, -0.022017337381839752]
     Last 10:  [0.6845484972000122, 0.0765204206109047, 0.09464208036661148, 0.6403622031211853, -1.0202829837799072, -0.6993950009346008, -0.06335951387882233, -3.185143232345581, -0.7923871278762817, 2.0591392517089844]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000025
     First 10: [-0.0264892578125, 0.025146484375, -0.0194091796875, 0.005035400390625, 0.017822265625, -0.00897216796875, -0.06884765625, 0.0185546875, -0.0021209716796875, -0.00170135498046875]
     Last 10:  [0.0218505859375, 0.0162353515625, -0.0250244140625, 0.050537109375, 0.039306640625, 0.0218505859375, -0.03369140625, 0.01904296875, -0.0311279296875, -0.080078125]

================================================================================
188_model.layers.11.mlp.act_fn: PytorchGELUTanh (model.layers.11.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.11.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.235493, Std: 0.750462
     First 10: [0.020761700347065926, 0.011689679697155952, -0.1729077398777008, 0.0051129050552845, 0.01828872039914131, -0.0018476257100701332, 0.07961639761924744, -0.0219118595123291, 0.04736284911632538, -0.022017337381839752]
     Last 10:  [0.6845484972000122, 0.0765204206109047, 0.09464208036661148, 0.6403622031211853, -1.0202829837799072, -0.6993950009346008, -0.06335951387882233, -3.185143232345581, -0.7923871278762817, 2.0591392517089844]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.11.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.046215, Std: 0.343515
     First 10: [0.010552801191806793, 0.005899353418499231, -0.07458611577749252, 0.002566881477832794, 0.00927778985351324, -0.0009224509703926742, 0.04233432188630104, -0.010764401406049728, 0.024576010182499886, -0.010815291665494442]
     Last 10:  [0.5155406594276428, 0.04059387743473053, 0.050889063626527786, 0.47320663928985596, -0.15707643330097198, -0.16941547393798828, -0.03007930889725685, -0.0019682629499584436, -0.16970594227313995, 2.0186400413513184]
     Zeros: 6, Total: 8192

================================================================================
189_model.layers.11.mlp.up_proj: Linear (model.layers.11.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.11.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003438, Std: 0.681187
     First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
     Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.11.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.003224, Std: 0.536166
     First 10: [0.0007078398484736681, 0.007055371999740601, -0.014281325042247772, -0.006269518751651049, 0.07185100018978119, 0.027219807729125023, -0.08948952704668045, 0.05133172497153282, 0.0224746223539114, 0.08964072167873383]
     Last 10:  [0.7427328824996948, 0.4149007499217987, -0.11974874138832092, -0.3620299696922302, 0.12279263138771057, -0.5135499238967896, -0.740970253944397, -0.8525660037994385, -0.09037508815526962, -0.5341997146606445]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000010
     First 10: [0.0098876953125, -0.0064697265625, -0.0032958984375, -0.00469970703125, 0.0869140625, -0.00634765625, 0.0556640625, 0.0252685546875, -0.02685546875, 0.0177001953125]
     Last 10:  [-0.054443359375, -0.017333984375, -0.05419921875, 0.00994873046875, 0.0225830078125, 0.01806640625, 0.0024871826171875, -0.032470703125, -0.003936767578125, 0.00921630859375]

================================================================================
190_model.layers.11.mlp.down_proj: Linear (model.layers.11.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.11.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.002809, Std: 0.219256
     First 10: [7.469693173334235e-06, 4.162213372183032e-05, 0.0010651885531842709, -1.6093112208181992e-05, 0.0006666184635832906, -2.510893864382524e-05, -0.0037884784396737814, -0.000552555313333869, 0.0005523365689441562, -0.0009694905602373183]
     Last 10:  [0.38290899991989136, 0.0168424304574728, -0.006093901116400957, -0.17131498456001282, -0.019287828356027603, 0.08700330555438995, 0.022287873551249504, 0.0016780741279944777, 0.01533718965947628, -1.0783569812774658]
     Zeros: 6, Total: 8192

  → OUTPUT[0]: model.layers.11.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.004118, Std: 0.211647
     First 10: [-0.00044058426283299923, -0.0012573034036904573, -0.000139257637783885, 0.0012291735038161278, 0.0008612052770331502, -0.0016133333556354046, -0.00015306833665817976, -0.0006767365266568959, 0.0015748918522149324, 0.0005461339605972171]
     Last 10:  [-0.018172457814216614, 0.12842777371406555, -0.035472191870212555, 0.066526859998703, 0.10862579941749573, 0.015471599996089935, -0.09423895925283432, -0.17461246252059937, 0.16095152497291565, 0.024173054844141006]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000002
     First 10: [0.0201416015625, 0.0074462890625, -0.010498046875, 0.023681640625, 0.017333984375, 0.037841796875, 0.01019287109375, -0.056640625, 0.01263427734375, -0.0595703125]
     Last 10:  [-0.01171875, -0.01458740234375, -0.0177001953125, 0.00518798828125, -0.002105712890625, -0.004730224609375, 0.0126953125, 0.00057220458984375, 0.00156402587890625, 0.004119873046875]

================================================================================
191_model.layers.11.mlp: Gemma3MLP (model.layers.11.mlp)
================================================================================

  → INPUT[0]: model.layers.11.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.003438, Std: 0.681187
     First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
     Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.11.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.004118, Std: 0.211647
     First 10: [-0.00044058426283299923, -0.0012573034036904573, -0.000139257637783885, 0.0012291735038161278, 0.0008612052770331502, -0.0016133333556354046, -0.00015306833665817976, -0.0006767365266568959, 0.0015748918522149324, 0.0005461339605972171]
     Last 10:  [-0.018172457814216614, 0.12842777371406555, -0.035472191870212555, 0.066526859998703, 0.10862579941749573, 0.015471599996089935, -0.09423895925283432, -0.17461246252059937, 0.16095152497291565, 0.024173054844141006]
     Zeros: 0, Total: 2560

================================================================================
192_model.layers.11.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.11.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.11.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.004118, Std: 0.211647
     First 10: [-0.00044058426283299923, -0.0012573034036904573, -0.000139257637783885, 0.0012291735038161278, 0.0008612052770331502, -0.0016133333556354046, -0.00015306833665817976, -0.0006767365266568959, 0.0015748918522149324, 0.0005461339605972171]
     Last 10:  [-0.018172457814216614, 0.12842777371406555, -0.035472191870212555, 0.066526859998703, 0.10862579941749573, 0.015471599996089935, -0.09423895925283432, -0.17461246252059937, 0.16095152497291565, 0.024173054844141006]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.11.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 2.406040, Std: 174.350357
     First 10: [-38.334495544433594, -78.37940216064453, -0.5744926333427429, 8.605036735534668, 1.0496941804885864, -84.43917846679688, -6.557053089141846, -1.5933027267456055, 9.712760925292969, 14.564963340759277]
     Last 10:  [-0.549500584602356, 94.41680908203125, -13.518502235412598, 40.81730651855469, 38.021060943603516, 10.328889846801758, -73.86690521240234, -272.78753662109375, 112.237060546875, 6.860287666320801]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 112.834496
     First 10: [260.0, 186.0, 11.375, 20.0, 2.65625, 156.0, 127.5, 6.0625, 17.5, 79.0]
     Last 10:  [4.59375, 135.0, 69.5, 112.5, 63.75, 122.5, 144.0, 288.0, 128.0, 51.5]

================================================================================
193_model.layers.12.input_layernorm: Gemma3RMSNorm (model.layers.12.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.12.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 53.616283, Std: 1991.643799
     First 10: [-37.44149398803711, -160.12164306640625, -1.1047978401184082, 12.31622314453125, 0.7735828757286072, -80.12065124511719, -25.254146575927734, -0.2116321325302124, 19.013811111450195, -29.220962524414062]
     Last 10:  [-1.7539253234863281, 212.63076782226562, -1.2276582717895508, 41.87737274169922, -67.14242553710938, 35.79296112060547, -121.06413269042969, -237.4891357421875, 98.82093048095703, -10.808466911315918]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.12.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023097, Std: 2.113587
     First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
     Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 14.549128
     First 10: [0.060791015625, 1.234375, 36.25, 32.75, 155.0, 2.796875, 5.0625, 91.0, 32.75, 5.46875]
     Last 10:  [107.5, 3.96875, 3.734375, 8.1875, 9.4375, 4.6875, 3.8125, -0.494140625, 4.59375, 18.0]

================================================================================
194_model.layers.12.self_attn.q_proj: Linear (model.layers.12.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023097, Std: 2.113587
     First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
     Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.12.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.045301, Std: 2.664215
     First 10: [-0.06358160078525543, -0.2240535318851471, -0.039401229470968246, 0.2656867206096649, 0.009820550680160522, -0.015561763197183609, -0.04221489280462265, 0.03709699213504791, -0.03146548569202423, 0.7696008086204529]
     Last 10:  [-0.975795567035675, -0.050007522106170654, -0.6796281933784485, -0.043120525777339935, -1.231899619102478, -0.07505464553833008, -3.2128655910491943, -0.7609623670578003, -1.1725811958312988, 0.5478821396827698]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000030
     First 10: [0.1708984375, -0.01068115234375, -0.08740234375, 0.044189453125, -0.11083984375, -0.146484375, -0.051025390625, 0.04541015625, 0.07763671875, -0.03955078125]
     Last 10:  [-0.00168609619140625, -0.033935546875, 0.0001087188720703125, 0.048095703125, 0.007415771484375, 0.00836181640625, -0.046630859375, -0.00616455078125, 0.005584716796875, 0.00144195556640625]

================================================================================
195_model.layers.12.self_attn.k_proj: Linear (model.layers.12.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023097, Std: 2.113587
     First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
     Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.12.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.089197, Std: 2.426896
     First 10: [-0.8677104711532593, -0.009737581014633179, 1.256080150604248, -0.3138313591480255, 0.4551379084587097, 0.004774325527250767, 0.6414008140563965, -0.6928508877754211, 0.0025312090292572975, 0.8959704637527466]
     Last 10:  [0.7062675356864929, 1.3677343130111694, 3.0223946571350098, -3.2614693641662598, -6.981871604919434, -2.9776196479797363, 1.636380910873413, -3.199155807495117, -9.21869945526123, 0.8566823601722717]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000031
     First 10: [0.051025390625, -0.0712890625, 0.031982421875, -0.031494140625, -0.004241943359375, 0.1171875, 0.1416015625, -0.15234375, -0.018310546875, -0.0027008056640625]
     Last 10:  [0.0205078125, 0.0184326171875, -0.000690460205078125, 0.0284423828125, 0.00433349609375, 0.0390625, 0.04345703125, 0.060546875, -0.0201416015625, -0.0279541015625]

================================================================================
196_model.layers.12.self_attn.v_proj: Linear (model.layers.12.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023097, Std: 2.113587
     First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
     Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.12.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.107021, Std: 1.453971
     First 10: [-0.16694942116737366, -0.16491203010082245, 0.004787806421518326, -0.07429400831460953, 0.0851186066865921, -0.1673336625099182, 0.147842139005661, -0.1453549861907959, 0.005079973489046097, 0.020989656448364258]
     Last 10:  [-1.0213571786880493, 0.07794189453125, 0.8037082552909851, 0.10917162895202637, -0.7665489912033081, 0.25748157501220703, -0.5276670455932617, 2.0314958095550537, 1.360581874847412, 0.7988132834434509]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000004
     First 10: [0.0174560546875, -0.053955078125, -0.017333984375, -0.0205078125, -0.020263671875, -0.00408935546875, 0.0224609375, 0.00732421875, 0.0196533203125, 0.00023365020751953125]
     Last 10:  [-0.00897216796875, -0.0048828125, -0.028564453125, 0.013427734375, -0.01470947265625, -0.0081787109375, -0.0302734375, 0.0184326171875, 0.00176239013671875, -0.00592041015625]

================================================================================
197_model.layers.12.self_attn.q_norm: Gemma3RMSNorm (model.layers.12.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.045301, Std: 2.664215
     First 10: [-0.06358160078525543, -0.2240535318851471, -0.039401229470968246, 0.2656867206096649, 0.009820550680160522, -0.015561763197183609, -0.04221489280462265, 0.03709699213504791, -0.03146548569202423, 0.7696008086204529]
     Last 10:  [-0.975795567035675, -0.050007522106170654, -0.6796281933784485, -0.043120525777339935, -1.231899619102478, -0.07505464553833008, -3.2128655910491943, -0.7609623670578003, -1.1725811958312988, 0.5478821396827698]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.12.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.033059, Std: 0.882198
     First 10: [-0.0, -0.06432300806045532, -0.0, 0.0, -0.0008674950804561377, -0.16976843774318695, -0.0, 0.0, -0.5155959725379944, 0.0]
     Last 10:  [-0.37094971537590027, -0.020429814234375954, -0.6866517066955566, -0.02420341596007347, -0.8171816468238831, -0.08017074316740036, -2.5083255767822266, -0.41806554794311523, -0.3001636564731598, 0.3690294325351715]
     Zeros: 224, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.781188
     First 10: [-1.0, -0.94921875, -1.0, -1.0, -1.015625, 0.9296875, -1.0, -1.0, 1.8984375, -1.0]
     Last 10:  [-0.126953125, -0.061767578125, 1.3203125, 0.2890625, 0.5234375, 1.453125, 0.79296875, 0.26171875, -0.412109375, 0.546875]

================================================================================
198_model.layers.12.self_attn.k_norm: Gemma3RMSNorm (model.layers.12.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.089197, Std: 2.426896
     First 10: [-0.8677104711532593, -0.009737581014633179, 1.256080150604248, -0.3138313591480255, 0.4551379084587097, 0.004774325527250767, 0.6414008140563965, -0.6928508877754211, 0.0025312090292572975, 0.8959704637527466]
     Last 10:  [0.7062675356864929, 1.3677343130111694, 3.0223946571350098, -3.2614693641662598, -6.981871604919434, -2.9776196479797363, 1.636380910873413, -3.199155807495117, -9.21869945526123, 0.8566823601722717]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.12.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.069580, Std: 3.169030
     First 10: [-0.0, -0.0028694800566881895, 0.0, -0.0, -0.042674701660871506, 0.01777813769876957, 0.0, -0.0, 0.007747179828584194, 0.0]
     Last 10:  [1.1483677625656128, 1.85691237449646, 2.148998975753784, -4.988009452819824, -7.1935319900512695, -2.0612375736236572, 1.7474573850631714, -6.317603588104248, -21.568763732910156, 0.8780562281608582]
     Zeros: 60, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.218889
     First 10: [-1.0, -0.828125, -1.0, -1.0, -1.0546875, 1.171875, -1.0, -1.0, 0.78515625, -1.0]
     Last 10:  [3.734375, 2.953125, 1.0703125, 3.453125, 2.0, 1.015625, 2.109375, 4.75, 5.8125, 1.984375]

================================================================================
199_model.layers.12.self_attn.o_proj: Linear (model.layers.12.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.12.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.029761, Std: 0.172917
     First 10: [-0.16694942116737366, -0.16491203010082245, 0.004787806421518326, -0.07429400831460953, 0.0851186066865921, -0.1673336625099182, 0.147842139005661, -0.1453549861907959, 0.005079973489046097, 0.020989656448364258]
     Last 10:  [-0.08664517849683762, 0.3716493844985962, 0.008566631004214287, -0.1938609629869461, -0.04727737605571747, -0.058128852397203445, -0.003436692524701357, -0.06921088695526123, 0.3843255639076233, -0.005756429396569729]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.12.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006972, Std: 0.737182
     First 10: [-0.04924732446670532, -0.20066849887371063, -0.12120641767978668, -0.1883285492658615, -0.07893367111682892, 0.27536875009536743, -0.1806802600622177, 0.06752364337444305, 0.006717264652252197, -0.11898058652877808]
     Last 10:  [0.05691812187433243, 0.19453245401382446, 0.01863277703523636, 0.6745097041130066, 0.27898094058036804, -0.20499172806739807, -0.11778879165649414, 0.0917646512389183, 0.06570817530155182, -1.2257717847824097]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000039
     First 10: [-0.03955078125, 0.052001953125, 0.032958984375, 0.0032806396484375, 0.000308990478515625, -0.0279541015625, 0.014404296875, -0.015625, 0.024169921875, 0.01397705078125]
     Last 10:  [0.006683349609375, 0.00531005859375, -0.005584716796875, 0.00885009765625, 0.0174560546875, -0.0135498046875, 0.03564453125, -0.0322265625, -0.013916015625, 0.03759765625]

================================================================================
200_model.layers.12.self_attn: Gemma3Attention (model.layers.12.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.12.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006972, Std: 0.737182
     First 10: [-0.04924732446670532, -0.20066849887371063, -0.12120641767978668, -0.1883285492658615, -0.07893367111682892, 0.27536875009536743, -0.1806802600622177, 0.06752364337444305, 0.006717264652252197, -0.11898058652877808]
     Last 10:  [0.05691812187433243, 0.19453245401382446, 0.01863277703523636, 0.6745097041130066, 0.27898094058036804, -0.20499172806739807, -0.11778879165649414, 0.0917646512389183, 0.06570817530155182, -1.2257717847824097]
     Zeros: 0, Total: 2560

================================================================================
201_model.layers.12.post_attention_layernorm: Gemma3RMSNorm (model.layers.12.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.12.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.006972, Std: 0.737182
     First 10: [-0.04924732446670532, -0.20066849887371063, -0.12120641767978668, -0.1883285492658615, -0.07893367111682892, 0.27536875009536743, -0.1806802600622177, 0.06752364337444305, 0.006717264652252197, -0.11898058652877808]
     Last 10:  [0.05691812187433243, 0.19453245401382446, 0.01863277703523636, 0.6745097041130066, 0.27898094058036804, -0.20499172806739807, -0.11778879165649414, 0.0917646512389183, 0.06570817530155182, -1.2257717847824097]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.12.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.488417, Std: 85.229561
     First 10: [-24.665367126464844, -63.00262451171875, -2.0726065635681152, -6.827913761138916, -0.7560078501701355, 52.696861267089844, -24.041460037231445, 1.0032130479812622, 0.18202482163906097, -13.697047233581543]
     Last 10:  [0.602192223072052, 26.04939842224121, 1.2181133031845093, 114.18071746826172, 17.621557235717773, -25.378278732299805, -16.74003028869629, 54.83228302001953, 7.678225994110107, -45.09987258911133]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 130.185471
     First 10: [334.0, 209.0, 10.4375, 23.25, 5.40625, 127.0, 88.0, 8.9375, 17.125, 76.0]
     Last 10:  [7.375, 105.0, 50.75, 133.0, 49.0, 97.0, 111.5, 472.0, 91.5, 28.125]

================================================================================
202_model.layers.12.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.12.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.12.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 55.104698, Std: 2034.907104
     First 10: [-62.10686111450195, -223.124267578125, -3.1774044036865234, 5.488309383392334, 0.01757502555847168, -27.423789978027344, -49.29560852050781, 0.7915809154510498, 19.19583511352539, -42.91801071166992]
     Last 10:  [-1.151733160018921, 238.68016052246094, -0.009544968605041504, 156.05809020996094, -49.52086639404297, 10.414682388305664, -137.80416870117188, -182.6568603515625, 106.49915313720703, -55.90834045410156]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.12.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004842, Std: 0.458555
     First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
     Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 2.400574
     First 10: [-0.7421875, -0.51171875, 6.9375, 8.3125, 21.375, -0.0274658203125, 0.51953125, 16.125, 5.90625, 0.7265625]
     Last 10:  [17.625, 0.234375, 0.1474609375, 0.85546875, 1.53125, 0.443359375, 0.2353515625, -0.84765625, 0.384765625, 4.84375]

================================================================================
203_model.layers.12.mlp.gate_proj: Linear (model.layers.12.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.12.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004842, Std: 0.458555
     First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
     Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.12.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.130874, Std: 0.495230
     First 10: [-0.028002562001347542, -0.03427787870168686, -0.019265055656433105, 0.02978655695915222, -0.014230594038963318, 0.008247947320342064, 0.01807716116309166, -0.004126193001866341, -0.03685946762561798, -0.04242347925901413]
     Last 10:  [-0.6921693682670593, -0.7171858549118042, -0.3842568099498749, 0.10186724364757538, 0.1851509064435959, -0.4304801821708679, -0.35439372062683105, -0.7095569968223572, 0.748548686504364, -0.25060468912124634]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000209
     First 10: [-0.0030975341796875, 0.041015625, 0.0162353515625, 0.07080078125, 0.049560546875, 0.01806640625, -0.020263671875, -0.00775146484375, 0.048583984375, 0.0201416015625]
     Last 10:  [-0.0439453125, -0.01220703125, 0.0037078857421875, -0.01953125, -0.02392578125, 0.00933837890625, -0.039794921875, 0.0206298828125, 0.007537841796875, -0.047607421875]

================================================================================
204_model.layers.12.mlp.act_fn: PytorchGELUTanh (model.layers.12.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.12.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.130874, Std: 0.495230
     First 10: [-0.028002562001347542, -0.03427787870168686, -0.019265055656433105, 0.02978655695915222, -0.014230594038963318, 0.008247947320342064, 0.01807716116309166, -0.004126193001866341, -0.03685946762561798, -0.04242347925901413]
     Last 10:  [-0.6921693682670593, -0.7171858549118042, -0.3842568099498749, 0.10186724364757538, 0.1851509064435959, -0.4304801821708679, -0.35439372062683105, -0.7095569968223572, 0.748548686504364, -0.25060468912124634]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.12.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.017502, Std: 0.261244
     First 10: [-0.013688494451344013, -0.016670284792780876, -0.009484472684562206, 0.015247182920575142, -0.007034510374069214, 0.004151112865656614, 0.009168940596282482, -0.0020563043653964996, -0.017887845635414124, -0.02049395814538002]
     Last 10:  [-0.16923022270202637, -0.16976729035377502, -0.13464777171611786, 0.05506623908877373, 0.10617342591285706, -0.14354197680950165, -0.1281258463859558, -0.16963443160057068, 0.5785112380981445, -0.10050871968269348]
     Zeros: 1, Total: 8192

================================================================================
205_model.layers.12.mlp.up_proj: Linear (model.layers.12.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.12.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004842, Std: 0.458555
     First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
     Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.12.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.008261, Std: 0.387421
     First 10: [0.014339018613100052, 0.003976594191044569, 0.00027756672352552414, -0.0006734868511557579, 0.026239831000566483, -0.011569378897547722, 0.022204114124178886, 0.024107232689857483, 0.004735920578241348, 0.0035862582735717297]
     Last 10:  [-0.24145108461380005, -0.0487213209271431, -0.6878368258476257, -0.36254680156707764, -0.25074443221092224, 0.5296995639801025, -0.2416045218706131, 0.13893000781536102, -0.29465484619140625, 0.44786354899406433]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000002
     First 10: [0.0341796875, -0.000453948974609375, -0.05419921875, 0.0267333984375, 0.01068115234375, 0.00115203857421875, -0.04052734375, -0.030517578125, -0.01611328125, 0.044677734375]
     Last 10:  [-0.05078125, 0.0296630859375, 0.059326171875, -0.00775146484375, -0.004058837890625, -0.0130615234375, -0.04345703125, 0.034423828125, -0.004302978515625, -0.026123046875]

================================================================================
206_model.layers.12.mlp.down_proj: Linear (model.layers.12.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.12.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.001761, Std: 0.143187
     First 10: [-0.00019627957954071462, -6.629095878452063e-05, -2.632574023664347e-06, -1.0268777259625494e-05, -0.0001845843653427437, -4.802579860552214e-05, 0.00020358820620458573, -4.9571808631299064e-05, -8.471541514154524e-05, -7.349662337219343e-05]
     Last 10:  [0.040860820561647415, 0.008271286264061928, 0.09261569380760193, -0.01996408961713314, -0.02662239596247673, -0.07603412121534348, 0.030955784022808075, -0.023567313328385353, -0.17046113312244415, -0.045014191418886185]
     Zeros: 1, Total: 8192

  → OUTPUT[0]: model.layers.12.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.004615, Std: 0.154089
     First 10: [5.988735210848972e-05, 0.0005273829447105527, -0.0003281553217675537, -0.0016731981886550784, -0.0008336306782439351, -0.0005642225150950253, -8.936149242799729e-05, 2.3320109903579578e-05, -0.0006813560030423105, 0.0008459886885248125]
     Last 10:  [-0.0208803191781044, -0.03335457295179367, -0.02766421064734459, -0.025533266365528107, -0.02498169057071209, 0.13545185327529907, -0.09248585999011993, -0.00780028011649847, 0.03345503658056259, -0.024605419486761093]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000005
     First 10: [-0.034423828125, 0.0184326171875, -0.018310546875, 0.00787353515625, -0.01123046875, 0.047607421875, 0.0289306640625, 0.015869140625, -0.0196533203125, -0.006195068359375]
     Last 10:  [-0.0238037109375, 0.004119873046875, -0.02587890625, 0.01092529296875, -0.04150390625, 0.01324462890625, -0.0025634765625, 0.002716064453125, 0.00147247314453125, -0.043212890625]

================================================================================
207_model.layers.12.mlp: Gemma3MLP (model.layers.12.mlp)
================================================================================

  → INPUT[0]: model.layers.12.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.004842, Std: 0.458555
     First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
     Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.12.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.004615, Std: 0.154089
     First 10: [5.988735210848972e-05, 0.0005273829447105527, -0.0003281553217675537, -0.0016731981886550784, -0.0008336306782439351, -0.0005642225150950253, -8.936149242799729e-05, 2.3320109903579578e-05, -0.0006813560030423105, 0.0008459886885248125]
     Last 10:  [-0.0208803191781044, -0.03335457295179367, -0.02766421064734459, -0.025533266365528107, -0.02498169057071209, 0.13545185327529907, -0.09248585999011993, -0.00780028011649847, 0.03345503658056259, -0.024605419486761093]
     Zeros: 0, Total: 2560

================================================================================
208_model.layers.12.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.12.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.12.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.004615, Std: 0.154089
     First 10: [5.988735210848972e-05, 0.0005273829447105527, -0.0003281553217675537, -0.0016731981886550784, -0.0008336306782439351, -0.0005642225150950253, -8.936149242799729e-05, 2.3320109903579578e-05, -0.0006813560030423105, 0.0008459886885248125]
     Last 10:  [-0.0208803191781044, -0.03335457295179367, -0.02766421064734459, -0.025533266365528107, -0.02498169057071209, 0.13545185327529907, -0.09248585999011993, -0.00780028011649847, 0.03345503658056259, -0.024605419486761093]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.12.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.285441, Std: 109.526093
     First 10: [10.229170799255371, 61.50520324707031, -3.8312931060791016, -22.6649112701416, -2.9574930667877197, -58.81378173828125, -8.346537590026855, 0.12485215067863464, -10.987565040588379, 44.747188568115234]
     Last 10:  [-1.3894346952438354, -46.02555465698242, -22.284019470214844, -37.55805206298828, -16.186052322387695, 169.83038330078125, -117.90280151367188, -22.34657096862793, 42.64914321899414, -11.978239059448242]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 162.501419
     First 10: [330.0, 225.0, 21.625, 25.25, 5.875, 201.0, 180.0, 9.375, 30.25, 101.5]
     Last 10:  [8.5, 196.0, 114.0, 209.0, 91.5, 178.0, 181.0, 408.0, 181.0, 68.5]

================================================================================
209_model.layers.13.input_layernorm: Gemma3RMSNorm (model.layers.13.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.13.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 56.390137, Std: 2027.257568
     First 10: [-51.877689361572266, -161.6190643310547, -7.008697509765625, -17.17660140991211, -2.939918041229248, -86.2375717163086, -57.642147064208984, 0.9164330959320068, 8.208270072937012, 1.8291778564453125]
     Last 10:  [-2.541167736053467, 192.65460205078125, -22.293563842773438, 118.50003814697266, -65.70691680908203, 180.2450714111328, -255.70697021484375, -205.00343322753906, 149.14830017089844, -67.88658142089844]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.127432, Std: 3.706955
     First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
     Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 11.329188
     First 10: [0.234375, 0.8671875, 37.25, 25.0, 67.0, 2.0625, 3.296875, 57.75, 27.75, 4.4375]
     Last 10:  [52.5, 3.1875, 5.71875, 3.546875, 6.71875, 3.75, 3.21875, -0.05859375, 3.28125, 11.5625]

================================================================================
210_model.layers.13.self_attn.q_proj: Linear (model.layers.13.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.127432, Std: 3.706955
     First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
     Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.011610, Std: 2.330506
     First 10: [0.3612440824508667, 0.12880228459835052, 0.2316921502351761, -0.16731218993663788, -0.19560670852661133, 0.008259095251560211, -3.1987829208374023, 0.03870191425085068, -0.4358840584754944, -0.21641740202903748]
     Last 10:  [0.5578280091285706, 1.0190964937210083, 1.664643406867981, -1.571599006652832, 1.6427546739578247, -1.2382352352142334, -0.09782607853412628, -2.3036954402923584, 0.18529659509658813, 0.5044712424278259]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000054
     First 10: [-0.0020904541015625, 0.02392578125, 0.01904296875, -0.01129150390625, 0.0016632080078125, -0.0267333984375, -0.0013275146484375, 0.005950927734375, 0.00665283203125, 0.03857421875]
     Last 10:  [-0.0133056640625, 0.013671875, -0.00665283203125, -0.01416015625, -0.051513671875, 0.013427734375, 0.030517578125, 0.016845703125, 0.01043701171875, -0.0172119140625]

================================================================================
211_model.layers.13.self_attn.k_proj: Linear (model.layers.13.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.127432, Std: 3.706955
     First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
     Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.011416, Std: 2.053910
     First 10: [-0.0020686164498329163, 0.006217023357748985, 4.091761112213135, 0.0024563726037740707, -3.9802873134613037, -0.0017994018271565437, -0.12375594675540924, 1.24365234375, -0.0009178835898637772, -4.232999801635742]
     Last 10:  [1.1919364929199219, 2.0350561141967773, 4.547500133514404, 0.6425000429153442, 2.003917932510376, -2.9013776779174805, -2.4789624214172363, -2.9661006927490234, 3.473496198654175, -2.4101815223693848]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000086
     First 10: [0.0203857421875, 0.018798828125, -0.0172119140625, 0.01171875, -0.05078125, -0.0223388671875, -0.01141357421875, -0.01458740234375, -0.004119873046875, -0.01165771484375]
     Last 10:  [0.0169677734375, 0.01324462890625, 0.0206298828125, 0.006500244140625, -0.024169921875, 0.00469970703125, 0.0233154296875, -0.0038604736328125, 0.003082275390625, 0.0191650390625]

================================================================================
212_model.layers.13.self_attn.v_proj: Linear (model.layers.13.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.127432, Std: 3.706955
     First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
     Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.050492, Std: 1.432910
     First 10: [-0.046511050313711166, -0.020304512232542038, -0.026450205594301224, -0.04027312994003296, 0.004717771429568529, -0.18583889305591583, 0.05088362842798233, 0.06272327899932861, 0.0631592720746994, 0.043709319084882736]
     Last 10:  [1.0932190418243408, -0.8873891830444336, 0.487265944480896, 1.277878999710083, 0.08657288551330566, 1.2546601295471191, -1.3693642616271973, -0.5918368101119995, 0.7305423021316528, 0.6351077556610107]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000082
     First 10: [-0.02587890625, -0.0400390625, 0.02392578125, 0.0196533203125, -0.027099609375, -0.006591796875, -0.02001953125, 0.022705078125, -0.07177734375, -0.03271484375]
     Last 10:  [0.0164794921875, 0.06396484375, 0.01409912109375, 0.02392578125, 0.00225830078125, 0.0235595703125, 0.01141357421875, -0.000804901123046875, -0.039306640625, 0.000507354736328125]

================================================================================
213_model.layers.13.self_attn.q_norm: Gemma3RMSNorm (model.layers.13.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.011610, Std: 2.330506
     First 10: [0.3612440824508667, 0.12880228459835052, 0.2316921502351761, -0.16731218993663788, -0.19560670852661133, 0.008259095251560211, -3.1987829208374023, 0.03870191425085068, -0.4358840584754944, -0.21641740202903748]
     Last 10:  [0.5578280091285706, 1.0190964937210083, 1.664643406867981, -1.571599006652832, 1.6427546739578247, -1.2382352352142334, -0.09782607853412628, -2.3036954402923584, 0.18529659509658813, 0.5044712424278259]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.13.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.030150, Std: 1.478100
     First 10: [1.4899060726165771, 0.4347286820411682, 0.5620079040527344, -0.5361603498458862, -0.4073679447174072, 0.029775021597743034, 0.0474567636847496, 0.12115135043859482, -1.3838789463043213, -0.41221046447753906]
     Last 10:  [0.2117370367050171, 0.4798690676689148, 0.9413792490959167, -0.6158853769302368, 1.1914803981781006, -0.9018956422805786, -0.041046179831027985, -1.6094108819961548, 0.0780324712395668, 0.8860025405883789]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.572764
     First 10: [1.171875, 0.77734375, 0.27734375, 0.6875, 0.0966796875, 0.8984375, -1.0078125, 0.6484375, 0.671875, 0.0030059814453125]
     Last 10:  [-0.27734375, -0.103515625, 0.07666015625, -0.25390625, 0.380859375, 0.38671875, -0.201171875, 0.330078125, -0.1982421875, 2.34375]

================================================================================
214_model.layers.13.self_attn.k_norm: Gemma3RMSNorm (model.layers.13.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.011416, Std: 2.053910
     First 10: [-0.0020686164498329163, 0.006217023357748985, 4.091761112213135, 0.0024563726037740707, -3.9802873134613037, -0.0017994018271565437, -0.12375594675540924, 1.24365234375, -0.0009178835898637772, -4.232999801635742]
     Last 10:  [1.1919364929199219, 2.0350561141967773, 4.547500133514404, 0.6425000429153442, 2.003917932510376, -2.9013776779174805, -2.4789624214172363, -2.9661006927490234, 3.473496198654175, -2.4101815223693848]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.13.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.097803, Std: 1.928435
     First 10: [-0.00532769039273262, 0.017870904877781868, 0.0, 0.005283806007355452, -0.0, -0.005102972965687513, -0.004775004927068949, 0.005998142529278994, -0.0014144129818305373, -0.020415782928466797]
     Last 10:  [0.9455217123031616, 1.3490456342697144, 2.8842170238494873, 0.479971706867218, 1.1931551694869995, -1.5974127054214478, -1.8152073621749878, -2.599712610244751, 2.7361350059509277, -3.957526206970215]
     Zeros: 12, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.041344
     First 10: [1.0859375, 1.328125, -1.0, 0.7421875, -1.0, 1.296875, -0.96875, -0.99609375, 0.248046875, -0.99609375]
     Last 10:  [0.67578125, 0.400390625, 0.33984375, 0.578125, 0.2578125, 0.1630859375, 0.546875, 0.8515625, 0.6640625, 2.46875]

================================================================================
215_model.layers.13.self_attn.o_proj: Linear (model.layers.13.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.13.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.013005, Std: 0.407037
     First 10: [-0.046511050313711166, -0.020304512232542038, -0.026450205594301224, -0.04027312994003296, 0.004717771429568529, -0.18583889305591583, 0.05088362842798233, 0.06272327899932861, 0.0631592720746994, 0.043709319084882736]
     Last 10:  [0.07559388130903244, -0.05784866213798523, -0.13291990756988525, 0.28604987263679504, -0.030800852924585342, 0.23273949325084686, -0.21756193041801453, -0.09324836730957031, 0.5016294121742249, 0.04357055202126503]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.13.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009107, Std: 1.110920
     First 10: [-0.06809940934181213, -0.24627932906150818, 0.044035181403160095, -0.4424225687980652, -0.04004305973649025, 0.07336953282356262, 0.1429828256368637, 0.4113996624946594, -0.014029568061232567, 0.019683316349983215]
     Last 10:  [0.38772135972976685, 0.6424385905265808, -0.0994315892457962, -0.056463561952114105, 0.09680076688528061, 0.036679089069366455, -0.616761326789856, 0.10469985008239746, -0.10467465966939926, 0.5417362451553345]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000032
     First 10: [0.0625, 0.030029296875, 0.0322265625, 0.006744384765625, -0.00130462646484375, 0.018310546875, 0.0240478515625, 0.01409912109375, -0.0439453125, -0.01483154296875]
     Last 10:  [-0.012939453125, -0.019775390625, -0.020751953125, -0.000286102294921875, -0.0174560546875, 0.00799560546875, 0.01953125, 0.00634765625, 0.011474609375, -0.006134033203125]

================================================================================
216_model.layers.13.self_attn: Gemma3Attention (model.layers.13.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.13.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009107, Std: 1.110920
     First 10: [-0.06809940934181213, -0.24627932906150818, 0.044035181403160095, -0.4424225687980652, -0.04004305973649025, 0.07336953282356262, 0.1429828256368637, 0.4113996624946594, -0.014029568061232567, 0.019683316349983215]
     Last 10:  [0.38772135972976685, 0.6424385905265808, -0.0994315892457962, -0.056463561952114105, 0.09680076688528061, 0.036679089069366455, -0.616761326789856, 0.10469985008239746, -0.10467465966939926, 0.5417362451553345]
     Zeros: 0, Total: 2560

================================================================================
217_model.layers.13.post_attention_layernorm: Gemma3RMSNorm (model.layers.13.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.13.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.009107, Std: 1.110920
     First 10: [-0.06809940934181213, -0.24627932906150818, 0.044035181403160095, -0.4424225687980652, -0.04004305973649025, 0.07336953282356262, 0.1429828256368637, 0.4113996624946594, -0.014029568061232567, 0.019683316349983215]
     Last 10:  [0.38772135972976685, 0.6424385905265808, -0.0994315892457962, -0.056463561952114105, 0.09680076688528061, 0.036679089069366455, -0.616761326789856, 0.10469985008239746, -0.10467465966939926, 0.5417362451553345]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 7.244958, Std: 140.101364
     First 10: [-17.297237396240234, -39.80766296386719, 0.7219362258911133, -7.2533111572265625, -0.16758917272090912, 9.893940925598145, 14.527043342590332, 2.695507287979126, -0.2931794822216034, 1.1998939514160156]
     Last 10:  [1.9934537410736084, 76.92179870605469, -6.466576099395752, -7.587450981140137, 5.670100212097168, 3.7598328590393066, -68.00332641601562, 32.01676559448242, -10.504383087158203, 19.94927215576172]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 124.406898
     First 10: [274.0, 174.0, 16.75, 16.75, 3.53125, 145.0, 109.0, 6.09375, 21.625, 65.0]
     Last 10:  [4.96875, 138.0, 74.5, 155.0, 67.0, 118.0, 127.0, 354.0, 115.5, 41.75]

================================================================================
218_model.layers.13.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.13.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.13.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 63.635101, Std: 2113.373291
     First 10: [-69.1749267578125, -201.42672729492188, -6.286761283874512, -24.429912567138672, -3.1075072288513184, -76.3436279296875, -43.11510467529297, 3.611940383911133, 7.915090560913086, 3.029071807861328]
     Last 10:  [-0.5477139949798584, 269.576416015625, -28.76013946533203, 110.91259002685547, -60.03681564331055, 184.00489807128906, -323.7102966308594, -172.98666381835938, 138.6439208984375, -47.93730926513672]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003589, Std: 0.349925
     First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
     Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 2.165846
     First 10: [-0.6875, -0.41796875, 6.84375, 8.25, 23.625, -0.07177734375, 0.310546875, 17.25, 5.46875, 0.78515625]
     Last 10:  [18.125, 0.12060546875, 0.1455078125, 0.376953125, 1.265625, 0.3359375, 0.1474609375, -0.82421875, 0.26171875, 3.671875]

================================================================================
219_model.layers.13.mlp.gate_proj: Linear (model.layers.13.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.13.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003589, Std: 0.349925
     First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
     Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.102763, Std: 0.371830
     First 10: [-0.01274721696972847, -0.0031851953826844692, -0.001140909269452095, 0.0010472651338204741, -0.003474391996860504, 0.006230496801435947, 0.026194987818598747, -0.0018616141751408577, -0.021434715017676353, 0.004909653682261705]
     Last 10:  [-0.3608516454696655, -0.07765829563140869, -0.4888724982738495, -0.33890244364738464, -0.10866956412792206, 1.145201563835144, 0.11459872871637344, -0.2678413987159729, 0.7660181522369385, -0.30814293026924133]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000189
     First 10: [0.00408935546875, -0.01434326171875, -0.020751953125, -0.003692626953125, 0.01055908203125, -0.0030517578125, 0.018310546875, -0.01336669921875, -0.0115966796875, -0.0216064453125]
     Last 10:  [-0.00396728515625, 0.049560546875, 0.01953125, -0.03564453125, 0.0211181640625, -0.002166748046875, 0.015869140625, -0.05126953125, -0.0654296875, 0.01275634765625]

================================================================================
220_model.layers.13.mlp.act_fn: PytorchGELUTanh (model.layers.13.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.13.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.102763, Std: 0.371830
     First 10: [-0.01274721696972847, -0.0031851953826844692, -0.001140909269452095, 0.0010472651338204741, -0.003474391996860504, 0.006230496801435947, 0.026194987818598747, -0.0018616141751408577, -0.021434715017676353, 0.004909653682261705]
     Last 10:  [-0.3608516454696655, -0.07765829563140869, -0.4888724982738495, -0.33890244364738464, -0.10866956412792206, 1.145201563835144, 0.11459872871637344, -0.2678413987159729, 0.7660181522369385, -0.30814293026924133]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.13.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.000218, Std: 0.189299
     First 10: [-0.006308785639703274, -0.0015885502798482776, -0.0005699353059753776, 0.0005240700556896627, -0.0017323802458122373, 0.0031307346653193235, 0.013371208682656288, -0.0009294245392084122, -0.010534078814089298, 0.002464443212375045]
     Last 10:  [-0.129588782787323, -0.03642563149333, -0.15277190506458282, -0.12449698150157928, -0.04963294416666031, 1.0006345510482788, 0.0625271201133728, -0.10564112663269043, 0.5960166454315186, -0.11678487062454224]
     Zeros: 0, Total: 8192

================================================================================
221_model.layers.13.mlp.up_proj: Linear (model.layers.13.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.13.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003589, Std: 0.349925
     First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
     Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.007174, Std: 0.318259
     First 10: [-0.0016608042642474174, 0.0066913217306137085, 0.01708505116403103, 0.008786545135080814, 0.008463648147881031, 0.014211398549377918, -0.009687371551990509, -0.0016897639725357294, -0.012761260382831097, -0.0016886701341718435]
     Last 10:  [-0.20554886758327484, 0.4232608675956726, 0.21830642223358154, -0.5607266426086426, 0.512967586517334, 0.35627755522727966, 0.38990873098373413, -0.05410391837358475, 0.04851941019296646, 0.2783465087413788]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000058
     First 10: [-0.04736328125, -0.062255859375, -0.0250244140625, -0.01611328125, -0.034912109375, 0.0478515625, -0.0634765625, 0.039306640625, -0.034912109375, -0.01092529296875]
     Last 10:  [-0.0732421875, -0.08544921875, -0.10498046875, 0.051513671875, -0.00213623046875, -0.11376953125, -0.008056640625, 0.03515625, 0.018798828125, -0.00421142578125]

================================================================================
222_model.layers.13.mlp.down_proj: Linear (model.layers.13.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.13.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.002321, Std: 0.104542
     First 10: [1.0477658179297578e-05, -1.0629501048242673e-05, -9.737374057294801e-06, 4.604765308613423e-06, -1.4662256944575347e-05, 4.449211701285094e-05, -0.00012953186524100602, 1.5705080613770406e-06, 0.00013442811905406415, -4.1616317503212485e-06]
     Last 10:  [0.026636827737092972, -0.015417544171214104, -0.03335108980536461, 0.06980877369642258, -0.02546009235084057, 0.3565036356449127, 0.024379869922995567, 0.005715598817914724, 0.028918376192450523, -0.032506659626960754]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.13.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.002970, Std: 0.095002
     First 10: [0.00011130119673907757, 0.0001265045430045575, -1.2267950296518393e-05, 0.00021823168208356947, -1.0596942956908606e-05, 8.946791786001995e-05, 3.8624861190328375e-05, 6.086607027100399e-05, 6.948089867364615e-05, -7.838963210815564e-05]
     Last 10:  [-0.0565229095518589, -0.007918676361441612, 0.003706622403115034, -0.020507216453552246, 0.0603732168674469, -0.028576085343956947, 0.013171181082725525, -0.03303638845682144, -0.03214028850197792, -0.01975499466061592]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000009
     First 10: [-0.01312255859375, -0.00616455078125, -0.00885009765625, -0.014892578125, 0.0201416015625, 0.04638671875, 0.00885009765625, -0.0020599365234375, 0.025634765625, 0.00506591796875]
     Last 10:  [0.01190185546875, -0.0179443359375, 0.007293701171875, 0.0004138946533203125, -0.00897216796875, 0.00701904296875, 0.00836181640625, 0.001190185546875, 0.005401611328125, -0.01104736328125]

================================================================================
223_model.layers.13.mlp: Gemma3MLP (model.layers.13.mlp)
================================================================================

  → INPUT[0]: model.layers.13.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003589, Std: 0.349925
     First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
     Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.002970, Std: 0.095002
     First 10: [0.00011130119673907757, 0.0001265045430045575, -1.2267950296518393e-05, 0.00021823168208356947, -1.0596942956908606e-05, 8.946791786001995e-05, 3.8624861190328375e-05, 6.086607027100399e-05, 6.948089867364615e-05, -7.838963210815564e-05]
     Last 10:  [-0.0565229095518589, -0.007918676361441612, 0.003706622403115034, -0.020507216453552246, 0.0603732168674469, -0.028576085343956947, 0.013171181082725525, -0.03303638845682144, -0.03214028850197792, -0.01975499466061592]
     Zeros: 0, Total: 2560

================================================================================
224_model.layers.13.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.13.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.13.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.002970, Std: 0.095002
     First 10: [0.00011130119673907757, 0.0001265045430045575, -1.2267950296518393e-05, 0.00021823168208356947, -1.0596942956908606e-05, 8.946791786001995e-05, 3.8624861190328375e-05, 6.086607027100399e-05, 6.948089867364615e-05, -7.838963210815564e-05]
     Last 10:  [-0.0565229095518589, -0.007918676361441612, 0.003706622403115034, -0.020507216453552246, 0.0603732168674469, -0.028576085343956947, 0.013171181082725525, -0.03303638845682144, -0.03214028850197792, -0.01975499466061592]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.13.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.137734, Std: 118.920845
     First 10: [37.150020599365234, 27.90060806274414, -0.34425193071365356, 4.807733535766602, -0.07825306802988052, 19.4679012298584, 8.442662239074707, 0.6667066812515259, 2.6509201526641846, -7.834011077880859]
     Last 10:  [-5.837003231048584, -19.161617279052734, 5.452693939208984, -62.30247116088867, 68.54055786132812, -62.75143814086914, 28.501941680908203, -134.8792266845703, -71.6060562133789, -13.056365966796875]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 175.409515
     First 10: [338.0, 223.0, 27.5, 21.375, 6.5, 220.0, 221.0, 10.125, 37.75, 100.5]
     Last 10:  [8.6875, 226.0, 137.0, 284.0, 105.5, 205.0, 202.0, 382.0, 208.0, 61.0]

================================================================================
225_model.layers.14.input_layernorm: Gemma3RMSNorm (model.layers.14.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.14.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 63.497356, Std: 2119.710205
     First 10: [-32.024906158447266, -173.526123046875, -6.6310133934021, -19.62217903137207, -3.185760259628296, -56.87572479248047, -34.67244338989258, 4.278646945953369, 10.566010475158691, -4.804939270019531]
     Last 10:  [-6.384716987609863, 250.414794921875, -23.307445526123047, 48.6101188659668, 8.503742218017578, 121.25346374511719, -295.2083435058594, -307.86590576171875, 67.0378646850586, -60.993675231933594]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.14.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.011588, Std: 2.501117
     First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
     Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 16.293940
     First 10: [0.419921875, 1.8046875, 40.25, 40.25, 100.5, 4.0625, 5.59375, 80.5, 37.25, 8.8125]
     Last 10:  [87.0, 4.8125, 7.6875, 5.21875, 11.1875, 6.09375, 5.25, -0.158203125, 5.96875, 29.375]

================================================================================
226_model.layers.14.self_attn.q_proj: Linear (model.layers.14.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.011588, Std: 2.501117
     First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
     Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.14.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.086158, Std: 3.191247
     First 10: [-0.10394278168678284, 0.486113041639328, -0.0031670331954956055, -0.06406673043966293, 0.029022572562098503, -0.20472556352615356, 0.009121552109718323, 0.6428589820861816, -0.14011245965957642, 0.04786229878664017]
     Last 10:  [-2.0579731464385986, -0.22386649250984192, 1.4910355806350708, 2.1106791496276855, -2.5043864250183105, -0.44952327013015747, 3.063737392425537, 0.2651447057723999, -0.9167452454566956, 0.42851200699806213]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: 0.000052
     First 10: [0.017822265625, -0.0086669921875, 0.00092315673828125, 0.000446319580078125, 0.01068115234375, 0.002838134765625, -0.004974365234375, -0.0189208984375, -0.004180908203125, -0.0225830078125]
     Last 10:  [0.01177978515625, 0.0013885498046875, 0.01055908203125, 0.0203857421875, 0.00372314453125, -0.002593994140625, 0.0101318359375, 0.0211181640625, 0.0157470703125, -0.01226806640625]

================================================================================
227_model.layers.14.self_attn.k_proj: Linear (model.layers.14.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.011588, Std: 2.501117
     First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
     Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.14.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.046558, Std: 3.316509
     First 10: [0.001125101000070572, -1.4922305345535278, 2.9507172107696533, 0.0005945321172475815, 0.0058700889348983765, 0.0026159570552408695, -0.0005042143166065216, -1.2961325645446777, 2.3352885246276855, 2.1909499168395996]
     Last 10:  [-3.321159839630127, -2.7551631927490234, 0.9813976287841797, 2.119474172592163, -0.32746803760528564, -1.9363809823989868, 8.463736534118652, -2.101818799972534, 2.4371798038482666, 4.035786151885986]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000051
     First 10: [0.002227783203125, -0.01397705078125, -0.0022125244140625, -0.01226806640625, -0.004913330078125, 0.0191650390625, -0.022705078125, 0.00433349609375, -0.010498046875, 0.023193359375]
     Last 10:  [0.08837890625, -0.0303955078125, 0.0810546875, 0.1337890625, 0.01495361328125, 0.134765625, 0.0174560546875, 0.01025390625, -0.00909423828125, 0.007568359375]

================================================================================
228_model.layers.14.self_attn.v_proj: Linear (model.layers.14.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.011588, Std: 2.501117
     First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
     Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.14.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.056551, Std: 1.627543
     First 10: [-0.0014735064469277859, -0.11611250042915344, -0.013283902779221535, -0.02925250679254532, -0.38320019841194153, -0.009399885311722755, 0.02620873972773552, 0.09357590973377228, 0.3029884994029999, -0.3202112913131714]
     Last 10:  [-0.49650049209594727, -1.5914411544799805, 2.703547716140747, 0.1917542815208435, -0.9063766002655029, -0.3374021053314209, -3.201317071914673, -0.6980677843093872, -0.7329135537147522, 11.179521560668945]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000016
     First 10: [-0.0252685546875, -0.05322265625, -0.0294189453125, -0.01214599609375, 0.017822265625, 0.00133514404296875, -0.04248046875, -0.00023365020751953125, 0.011962890625, 0.03076171875]
     Last 10:  [-0.0037078857421875, 0.005523681640625, -0.022705078125, 0.058349609375, 0.0732421875, -0.005767822265625, 0.0302734375, -0.00732421875, -0.00183868408203125, 0.07763671875]

================================================================================
229_model.layers.14.self_attn.q_norm: Gemma3RMSNorm (model.layers.14.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.086158, Std: 3.191247
     First 10: [-0.10394278168678284, 0.486113041639328, -0.0031670331954956055, -0.06406673043966293, 0.029022572562098503, -0.20472556352615356, 0.009121552109718323, 0.6428589820861816, -0.14011245965957642, 0.04786229878664017]
     Last 10:  [-2.0579731464385986, -0.22386649250984192, 1.4910355806350708, 2.1106791496276855, -2.5043864250183105, -0.44952327013015747, 3.063737392425537, 0.2651447057723999, -0.9167452454566956, 0.42851200699806213]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.14.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.019768, Std: 1.260950
     First 10: [-0.7986220717430115, 0.0, -0.0, -0.3993980586528778, 0.2628081440925598, -2.1628267765045166, 0.09135883301496506, 0.0, -0.04805910959839821, 0.2963259816169739]
     Last 10:  [-0.537922739982605, -0.05352909862995148, 0.4146948754787445, 1.6062668561935425, -0.7488049864768982, -0.24494467675685883, 2.2118141651153564, 0.23347973823547363, -0.4236548840999603, 0.12459105998277664]
     Zeros: 160, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.993812
     First 10: [0.75, -1.0, -1.0, 0.419921875, 1.0625, 1.40625, 1.28125, -1.0, -0.921875, 0.41015625]
     Last 10:  [-0.11181640625, -0.1875, -0.054931640625, 1.5859375, 0.0159912109375, 0.8515625, 1.453125, 1.9921875, 0.5703125, -0.01202392578125]

================================================================================
230_model.layers.14.self_attn.k_norm: Gemma3RMSNorm (model.layers.14.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.046558, Std: 3.316509
     First 10: [0.001125101000070572, -1.4922305345535278, 2.9507172107696533, 0.0005945321172475815, 0.0058700889348983765, 0.0026159570552408695, -0.0005042143166065216, -1.2961325645446777, 2.3352885246276855, 2.1909499168395996]
     Last 10:  [-3.321159839630127, -2.7551631927490234, 0.9813976287841797, 2.119474172592163, -0.32746803760528564, -1.9363809823989868, 8.463736534118652, -2.101818799972534, 2.4371798038482666, 4.035786151885986]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.14.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.079954, Std: 2.678622
     First 10: [0.0025186347775161266, -0.0, 0.0, 0.0014210810186341405, 0.009686366654932499, 0.0034437980502843857, -0.0005070093902759254, -0.0, -0.01416732557117939, -0.01329167652875185]
     Last 10:  [-3.9326984882354736, -4.792883396148682, 1.3691725730895996, 1.2411812543869019, -0.3666151762008667, -1.3382400274276733, 3.562436103820801, -0.900505781173706, 2.2772750854492188, 4.344465255737305]
     Zeros: 36, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.554319
     First 10: [1.8828125, -1.0, -1.0, 2.078125, 1.125, 0.6953125, 0.294921875, -1.0, -1.0078125, -1.0078125]
     Last 10:  [3.296875, 5.3125, 4.0625, 1.125, 3.0625, 1.5078125, 0.52734375, 0.5546875, 2.390625, 2.90625]

================================================================================
231_model.layers.14.self_attn.o_proj: Linear (model.layers.14.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.14.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.013154, Std: 0.539498
     First 10: [-0.0014735064469277859, -0.11611250042915344, -0.013283902779221535, -0.02925250679254532, -0.38320019841194153, -0.009399885311722755, 0.02620873972773552, 0.09357590973377228, 0.3029884994029999, -0.3202112913131714]
     Last 10:  [0.33426612615585327, -0.10056263953447342, 0.5438634157180786, -0.014387309551239014, -0.04384348541498184, -0.13254477083683014, -0.08891195058822632, 0.22227303683757782, -0.02293824777007103, 3.8227529525756836]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.14.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.060172, Std: 2.121538
     First 10: [0.1662517786026001, -0.23888039588928223, -0.3164578378200531, -0.7439513206481934, 0.32570189237594604, 0.09402858465909958, 0.1297404170036316, 0.5364084243774414, -0.49359366297721863, 0.20015856623649597]
     Last 10:  [0.22487866878509521, 0.5776320099830627, -0.3982499837875366, 0.4278284013271332, -0.5090165734291077, 0.4245525002479553, -0.9604005813598633, -0.5169118046760559, 0.4945446252822876, 0.6503120064735413]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000019
     First 10: [-0.03515625, -0.047607421875, 0.05859375, 0.037841796875, -0.000759124755859375, -0.08349609375, -0.025146484375, -0.05224609375, 0.026611328125, 0.002044677734375]
     Last 10:  [-0.031494140625, -0.015869140625, 0.02001953125, 0.0244140625, 0.043212890625, -0.0238037109375, -0.0179443359375, -0.0281982421875, -3.504753112792969e-05, 0.015625]

================================================================================
232_model.layers.14.self_attn: Gemma3Attention (model.layers.14.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.14.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.060172, Std: 2.121538
     First 10: [0.1662517786026001, -0.23888039588928223, -0.3164578378200531, -0.7439513206481934, 0.32570189237594604, 0.09402858465909958, 0.1297404170036316, 0.5364084243774414, -0.49359366297721863, 0.20015856623649597]
     Last 10:  [0.22487866878509521, 0.5776320099830627, -0.3982499837875366, 0.4278284013271332, -0.5090165734291077, 0.4245525002479553, -0.9604005813598633, -0.5169118046760559, 0.4945446252822876, 0.6503120064735413]
     Zeros: 0, Total: 2560

================================================================================
233_model.layers.14.post_attention_layernorm: Gemma3RMSNorm (model.layers.14.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.14.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.060172, Std: 2.121538
     First 10: [0.1662517786026001, -0.23888039588928223, -0.3164578378200531, -0.7439513206481934, 0.32570189237594604, 0.09402858465909958, 0.1297404170036316, 0.5364084243774414, -0.49359366297721863, 0.20015856623649597]
     Last 10:  [0.22487866878509521, 0.5776320099830627, -0.3982499837875366, 0.4278284013271332, -0.5090165734291077, 0.4245525002479553, -0.9604005813598633, -0.5169118046760559, 0.4945446252822876, 0.6503120064735413]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.14.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 5.571620, Std: 103.344131
     First 10: [16.74628448486328, -19.964147567749023, -3.8279409408569336, -8.099093437194824, 0.8730148077011108, 9.84359073638916, 13.296823501586914, 2.403697967529297, -8.84736442565918, 9.508557319641113]
     Last 10:  [1.140194058418274, 66.31123352050781, -28.243852615356445, 54.57113265991211, -28.178316116333008, 44.6223030090332, -110.74247741699219, -68.30768585205078, 54.75434494018555, 23.225934982299805]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 161.183136
     First 10: [228.0, 189.0, 26.5, 23.75, 5.09375, 237.0, 232.0, 9.1875, 39.75, 107.0]
     Last 10:  [8.9375, 224.0, 138.0, 249.0, 107.5, 205.0, 225.0, 258.0, 216.0, 69.0]

================================================================================
234_model.layers.14.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.14.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.14.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 69.068985, Std: 2183.442139
     First 10: [-15.278621673583984, -193.49026489257812, -10.458953857421875, -27.721271514892578, -2.3127455711364746, -47.032135009765625, -21.375619888305664, 6.682344913482666, 1.7186460494995117, 4.703618049621582]
     Last 10:  [-5.244523048400879, 316.72601318359375, -51.551300048828125, 103.1812515258789, -19.67457389831543, 165.87576293945312, -405.9508056640625, -376.173583984375, 121.79220581054688, -37.767738342285156]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.14.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005377, Std: 0.326920
     First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
     Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 1.896982
     First 10: [-0.67578125, -0.41015625, 5.875, 7.5625, 23.375, -0.1318359375, 0.06884765625, 15.5625, 4.28125, 0.734375]
     Last 10:  [16.875, -0.0286865234375, 0.07763671875, -0.00193023681640625, 0.9765625, 0.10546875, 0.0322265625, -0.80078125, 0.05078125, 3.34375]

================================================================================
235_model.layers.14.mlp.gate_proj: Linear (model.layers.14.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.14.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005377, Std: 0.326920
     First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
     Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.14.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.078075, Std: 0.367016
     First 10: [0.003358383197337389, -0.018439356237649918, -0.023224003612995148, -0.002271893434226513, 0.004260982386767864, -0.0195404551923275, -0.012691360898315907, -0.009017166681587696, -0.012722431682050228, -0.02466193586587906]
     Last 10:  [1.6589471101760864, 0.30642417073249817, -0.2687648832798004, 0.04313495010137558, -0.5212712287902832, -0.13198071718215942, -0.07699524611234665, -0.17911800742149353, 0.6784461140632629, -0.05614028871059418]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000198
     First 10: [-0.058349609375, 0.015869140625, -0.002593994140625, -0.0185546875, -0.000446319580078125, 0.01397705078125, -0.037841796875, 0.0240478515625, -0.045166015625, -0.00274658203125]
     Last 10:  [-0.04736328125, -0.031005859375, -0.0181884765625, -0.087890625, 0.012939453125, 0.043212890625, 0.01446533203125, -0.040771484375, -0.0255126953125, -0.004425048828125]

================================================================================
236_model.layers.14.mlp.act_fn: PytorchGELUTanh (model.layers.14.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.14.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.078075, Std: 0.367016
     First 10: [0.003358383197337389, -0.018439356237649918, -0.023224003612995148, -0.002271893434226513, 0.004260982386767864, -0.0195404551923275, -0.012691360898315907, -0.009017166681587696, -0.012722431682050228, -0.02466193586587906]
     Last 10:  [1.6589471101760864, 0.30642417073249817, -0.2687648832798004, 0.04313495010137558, -0.5212712287902832, -0.13198071718215942, -0.07699524611234665, -0.17911800742149353, 0.6784461140632629, -0.05614028871059418]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.14.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.008428, Std: 0.187865
     First 10: [0.0016836911672726274, -0.00908404216170311, -0.011396849527955055, -0.0011338875629007816, 0.0021377343218773603, -0.009617909789085388, -0.006281424313783646, -0.004476145841181278, -0.006296644918620586, -0.012088351882994175]
     Last 10:  [1.5782272815704346, 0.1900903284549713, -0.10590986162424088, 0.022309524938464165, -0.15696901082992554, -0.0590614415705204, -0.036134932190179825, -0.07682807743549347, 0.5096364617347717, -0.0268134493380785]
     Zeros: 0, Total: 8192

================================================================================
237_model.layers.14.mlp.up_proj: Linear (model.layers.14.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.14.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005377, Std: 0.326920
     First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
     Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.14.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.000725, Std: 0.310555
     First 10: [5.1083858124911785e-05, 0.008441733196377754, 0.005577435716986656, 0.007136796601116657, 0.004195770248770714, -0.0062172384932637215, -0.01156560704112053, 0.0045717693865299225, -0.014713268727064133, -0.009971127845346928]
     Last 10:  [0.3196619749069214, -0.23418539762496948, -0.2523602247238159, 0.06940756738185883, -0.24214008450508118, 0.028921887278556824, 0.15471875667572021, 0.1980859786272049, 0.003678303211927414, 0.26619359850883484]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000020
     First 10: [0.0771484375, 0.032958984375, -0.0185546875, 0.0174560546875, -0.047119140625, -0.011962890625, 0.123046875, -0.05029296875, 0.03564453125, -0.1171875]
     Last 10:  [-0.0595703125, -0.02587890625, 0.035888671875, 0.08203125, -0.052001953125, -0.0162353515625, 0.051025390625, 0.056884765625, 0.04052734375, -0.035888671875]

================================================================================
238_model.layers.14.mlp.down_proj: Linear (model.layers.14.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.14.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.001576, Std: 0.096131
     First 10: [8.60094431232028e-08, -7.668505713809282e-05, -6.356519588734955e-05, -8.092324605968315e-06, 8.969442205852829e-06, 5.979683919576928e-05, 7.26484868209809e-05, -2.0463907276280224e-05, 9.264422988053411e-05, 0.00012053450336679816]
     Last 10:  [0.5044992566108704, -0.04451638087630272, 0.02672743611037731, 0.0015484498580917716, 0.03800848871469498, -0.0017081683035939932, -0.005590751767158508, -0.015218565240502357, 0.0018745973939076066, -0.0071375686675310135]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.14.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003619, Std: 0.071617
     First 10: [-8.315804734593257e-06, -4.014122532680631e-05, 2.1148025552975014e-06, -3.731943797902204e-05, -4.971388625563122e-05, 0.00021793674386572093, -2.8450585887185298e-05, -3.317261507618241e-05, -7.129867299227044e-05, -5.5519652960356325e-05]
     Last 10:  [0.018253840506076813, 0.015990853309631348, -0.01716352254152298, 0.021585891023278236, 0.018917512148618698, -0.08822397142648697, -0.06908410042524338, -0.034579940140247345, 0.06358908116817474, -0.05523169785737991]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000008
     First 10: [0.00933837890625, -0.00799560546875, -0.0218505859375, 0.00072479248046875, -0.000614166259765625, -0.020263671875, 0.005767822265625, 0.006378173828125, -0.005462646484375, -0.0128173828125]
     Last 10:  [0.01806640625, -0.005950927734375, -0.00250244140625, -0.002777099609375, -0.008544921875, 0.00994873046875, 7.963180541992188e-05, -0.00106048583984375, -0.00946044921875, 0.0093994140625]

================================================================================
239_model.layers.14.mlp: Gemma3MLP (model.layers.14.mlp)
================================================================================

  → INPUT[0]: model.layers.14.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005377, Std: 0.326920
     First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
     Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.14.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003619, Std: 0.071617
     First 10: [-8.315804734593257e-06, -4.014122532680631e-05, 2.1148025552975014e-06, -3.731943797902204e-05, -4.971388625563122e-05, 0.00021793674386572093, -2.8450585887185298e-05, -3.317261507618241e-05, -7.129867299227044e-05, -5.5519652960356325e-05]
     Last 10:  [0.018253840506076813, 0.015990853309631348, -0.01716352254152298, 0.021585891023278236, 0.018917512148618698, -0.08822397142648697, -0.06908410042524338, -0.034579940140247345, 0.06358908116817474, -0.05523169785737991]
     Zeros: 0, Total: 2560

================================================================================
240_model.layers.14.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.14.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.14.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.003619, Std: 0.071617
     First 10: [-8.315804734593257e-06, -4.014122532680631e-05, 2.1148025552975014e-06, -3.731943797902204e-05, -4.971388625563122e-05, 0.00021793674386572093, -2.8450585887185298e-05, -3.317261507618241e-05, -7.129867299227044e-05, -5.5519652960356325e-05]
     Last 10:  [0.018253840506076813, 0.015990853309631348, -0.01716352254152298, 0.021585891023278236, 0.018917512148618698, -0.08822397142648697, -0.06908410042524338, -0.034579940140247345, 0.06358908116817474, -0.05523169785737991]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.14.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -5.626893, Std: 148.392441
     First 10: [-2.6951563358306885, -9.367049217224121, 0.07012256979942322, -1.0327481031417847, -0.378019243478775, 58.02806091308594, -8.48318099975586, -0.37836167216300964, -3.4661900997161865, -6.450138092041016]
     Last 10:  [2.633345603942871, 64.98895263671875, -43.74867248535156, 119.51782989501953, 38.03981018066406, -326.0716857910156, -269.0275573730469, -180.69102478027344, 254.8325958251953, -63.351863861083984]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 207.324829
     First 10: [324.0, 233.0, 32.25, 26.75, 6.625, 266.0, 298.0, 10.4375, 47.75, 115.5]
     Last 10:  [9.1875, 286.0, 179.0, 390.0, 141.0, 260.0, 274.0, 368.0, 282.0, 80.0]

================================================================================
241_model.layers.15.input_layernorm: Gemma3RMSNorm (model.layers.15.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.15.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 63.442078, Std: 2157.183838
     First 10: [-17.973777770996094, -202.85731506347656, -10.38883113861084, -28.754018783569336, -2.690764904022217, 10.995925903320312, -29.858800888061523, 6.303983211517334, -1.7475440502166748, -1.7465200424194336]
     Last 10:  [-2.611177444458008, 381.7149658203125, -95.29997253417969, 222.69908142089844, 18.365236282348633, -160.1959228515625, -674.9783935546875, -556.8646240234375, 376.62481689453125, -101.11959838867188]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.15.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014763, Std: 3.070717
     First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
     Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 17.186687
     First 10: [2.734375, 4.96875, 29.75, 80.0, 197.0, 3.546875, 3.84375, 139.0, 26.5, 10.5]
     Last 10:  [137.0, 3.84375, 3.28125, 3.515625, 9.5, 4.34375, 4.59375, 1.953125, 4.21875, 25.875]

================================================================================
242_model.layers.15.self_attn.q_proj: Linear (model.layers.15.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014763, Std: 3.070717
     First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
     Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.15.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.039881, Std: 3.617062
     First 10: [0.5756588578224182, -0.00809498131275177, -0.030068835243582726, -0.27358072996139526, -0.4021490514278412, -0.2041524350643158, 0.44307151436805725, -0.39407554268836975, -0.16112014651298523, -0.5332531332969666]
     Last 10:  [-0.45996516942977905, -4.055240631103516, 0.29469096660614014, -0.6887553930282593, 1.0316474437713623, 4.096559524536133, -7.690218925476074, 5.1006317138671875, -1.2750024795532227, -3.287994623184204]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: 0.000040
     First 10: [0.018798828125, 0.0140380859375, -0.010498046875, 0.004302978515625, -0.0004863739013671875, -0.045654296875, -0.056640625, 0.048828125, 0.0252685546875, -0.004486083984375]
     Last 10:  [0.009521484375, -0.03369140625, -0.08056640625, -0.06982421875, 0.10693359375, 0.0238037109375, 0.12158203125, -0.0186767578125, -0.08447265625, -0.03759765625]

================================================================================
243_model.layers.15.self_attn.k_proj: Linear (model.layers.15.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014763, Std: 3.070717
     First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
     Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.15.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.137049, Std: 3.588787
     First 10: [-0.014390094205737114, -0.019971415400505066, 0.01907084509730339, 0.05025727301836014, 0.007350176572799683, 0.007772386074066162, 0.038482993841171265, -0.008845962584018707, -0.011193700134754181, -0.009361632168293]
     Last 10:  [4.851580619812012, -7.573591232299805, 2.444117784500122, -6.05485725402832, 3.5553221702575684, 10.141180992126465, -7.630295753479004, 11.47496223449707, 3.451714038848877, -1.9936109781265259]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000095
     First 10: [-0.00909423828125, 0.00946044921875, -0.04443359375, 0.01171875, 0.00150299072265625, 0.01177978515625, -0.010009765625, -0.005645751953125, -0.013916015625, -0.00017642974853515625]
     Last 10:  [0.1005859375, 0.0380859375, -0.07763671875, 0.04638671875, 0.060546875, -0.0274658203125, 0.053955078125, 0.0137939453125, 0.0556640625, -0.003936767578125]

================================================================================
244_model.layers.15.self_attn.v_proj: Linear (model.layers.15.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.014763, Std: 3.070717
     First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
     Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.15.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.068283, Std: 1.615148
     First 10: [0.31384724378585815, -0.3225434720516205, -0.22224679589271545, -0.11975084990262985, -0.12216020375490189, -0.10215240716934204, 0.017649676650762558, 0.03538923338055611, -0.7035523653030396, 0.32225507497787476]
     Last 10:  [-1.3302853107452393, 0.008603900671005249, 0.7378329038619995, -0.056910671293735504, 1.5762574672698975, 0.07345783710479736, 1.1585659980773926, -1.1401840448379517, -0.14527589082717896, 0.21385276317596436]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000015
     First 10: [-0.0250244140625, -0.01171875, -0.017578125, 0.0208740234375, -0.0517578125, 0.064453125, 0.0233154296875, -0.00115966796875, -0.0185546875, 0.0101318359375]
     Last 10:  [0.032958984375, 0.0235595703125, -0.023681640625, 0.0172119140625, 0.048095703125, 0.0172119140625, -0.0218505859375, -0.0225830078125, -0.0089111328125, 0.00732421875]

================================================================================
245_model.layers.15.self_attn.q_norm: Gemma3RMSNorm (model.layers.15.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.039881, Std: 3.617062
     First 10: [0.5756588578224182, -0.00809498131275177, -0.030068835243582726, -0.27358072996139526, -0.4021490514278412, -0.2041524350643158, 0.44307151436805725, -0.39407554268836975, -0.16112014651298523, -0.5332531332969666]
     Last 10:  [-0.45996516942977905, -4.055240631103516, 0.29469096660614014, -0.6887553930282593, 1.0316474437713623, 4.096559524536133, -7.690218925476074, 5.1006317138671875, -1.2750024795532227, -3.287994623184204]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.15.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.044521, Std: 2.353814
     First 10: [5.136882305145264, -0.024755684658885002, -0.1313643902540207, -1.0807807445526123, -2.467142105102539, -0.5239905714988708, 2.306349039077759, -2.674025297164917, -0.6275192499160767, -3.182230234146118]
     Last 10:  [-0.13460147380828857, -1.2661503553390503, 0.2504635453224182, -0.21333841979503632, 0.3078373074531555, 0.6432620286941528, -1.651972770690918, 1.7537012100219727, -0.4956802427768707, -1.299379825592041]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.964427
     First 10: [2.75, 0.28515625, 0.8359375, 0.66015625, 1.578125, 0.07861328125, 1.1875, 1.8515625, 0.63671875, 1.5078125]
     Last 10:  [-0.0208740234375, 0.044677734375, 1.84375, 0.036376953125, -0.0016021728515625, -0.474609375, -0.28125, 0.150390625, 0.30078125, 0.322265625]

================================================================================
246_model.layers.15.self_attn.k_norm: Gemma3RMSNorm (model.layers.15.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.137049, Std: 3.588787
     First 10: [-0.014390094205737114, -0.019971415400505066, 0.01907084509730339, 0.05025727301836014, 0.007350176572799683, 0.007772386074066162, 0.038482993841171265, -0.008845962584018707, -0.011193700134754181, -0.009361632168293]
     Last 10:  [4.851580619812012, -7.573591232299805, 2.444117784500122, -6.05485725402832, 3.5553221702575684, 10.141180992126465, -7.630295753479004, 11.47496223449707, 3.451714038848877, -1.9936109781265259]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.15.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: 0.104860, Std: 3.478912
     First 10: [-0.008882024325430393, -0.00844376441091299, 0.019058024510741234, 0.0564730167388916, 0.0012131271651014686, 0.016166940331459045, 0.01716214418411255, -0.0035100062377750874, -0.01310958992689848, -0.007990136742591858]
     Last 10:  [4.246663570404053, -5.206329822540283, 0.4700140058994293, -2.445855140686035, 5.595364570617676, 17.484474182128906, -5.093516826629639, 4.882603168487549, 1.6365584135055542, -0.5548264384269714]
     Zeros: 20, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.951797
     First 10: [0.06640625, -0.26953125, 0.7265625, 0.94140625, -0.71484375, 2.59375, -0.2294921875, -0.314453125, 1.0234375, 0.474609375]
     Last 10:  [2.09375, 1.4296875, -0.3203125, 0.427734375, 4.5625, 5.09375, 1.359375, 0.50390625, 0.67578125, -0.016357421875]

================================================================================
247_model.layers.15.self_attn.o_proj: Linear (model.layers.15.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.15.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.033516, Std: 0.768839
     First 10: [0.31384724378585815, -0.3225434720516205, -0.22224679589271545, -0.11975084990262985, -0.12216020375490189, -0.10215240716934204, 0.017649676650762558, 0.03538923338055611, -0.7035523653030396, 0.32225507497787476]
     Last 10:  [-0.8239046335220337, 0.11934424936771393, 0.5120292901992798, -0.2881278991699219, 0.8856801390647888, 0.03579059988260269, 0.8332778215408325, -0.7797176837921143, 0.019624115899205208, 0.12107152491807938]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.15.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.072226, Std: 1.377489
     First 10: [0.026933975517749786, 0.035171665251255035, -0.45747190713882446, -0.4167557954788208, -0.04208611696958542, 0.10210814327001572, 0.5136877298355103, -0.14254850149154663, -0.22901488840579987, -0.19083257019519806]
     Last 10:  [0.84844970703125, -0.1682589054107666, 0.6788538694381714, 0.11778327822685242, -0.8764616250991821, 0.6759559512138367, 1.3663198947906494, 0.09033279120922089, -0.4403201937675476, -1.29127037525177]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000017
     First 10: [-0.04150390625, 0.04931640625, -0.042236328125, 0.0269775390625, -0.0224609375, 0.01361083984375, -0.01275634765625, -0.048828125, 0.030029296875, -0.06982421875]
     Last 10:  [-0.005523681640625, 0.00946044921875, -0.0030364990234375, 0.07373046875, 0.0306396484375, -0.0166015625, -0.0003185272216796875, 0.006439208984375, -0.003082275390625, -0.014404296875]

================================================================================
248_model.layers.15.self_attn: Gemma3Attention (model.layers.15.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.15.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.072226, Std: 1.377489
     First 10: [0.026933975517749786, 0.035171665251255035, -0.45747190713882446, -0.4167557954788208, -0.04208611696958542, 0.10210814327001572, 0.5136877298355103, -0.14254850149154663, -0.22901488840579987, -0.19083257019519806]
     Last 10:  [0.84844970703125, -0.1682589054107666, 0.6788538694381714, 0.11778327822685242, -0.8764616250991821, 0.6759559512138367, 1.3663198947906494, 0.09033279120922089, -0.4403201937675476, -1.29127037525177]
     Zeros: 0, Total: 2560

================================================================================
249_model.layers.15.post_attention_layernorm: Gemma3RMSNorm (model.layers.15.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.15.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.072226, Std: 1.377489
     First 10: [0.026933975517749786, 0.035171665251255035, -0.45747190713882446, -0.4167557954788208, -0.04208611696958542, 0.10210814327001572, 0.5136877298355103, -0.14254850149154663, -0.22901488840579987, -0.19083257019519806]
     Last 10:  [0.84844970703125, -0.1682589054107666, 0.6788538694381714, 0.11778327822685242, -0.8764616250991821, 0.6759559512138367, 1.3663198947906494, 0.09033279120922089, -0.4403201937675476, -1.29127037525177]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.15.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 1.984967, Std: 76.433548
     First 10: [12.88828182220459, 11.31078052520752, -9.416020393371582, -15.889395713806152, -0.4592853784561157, 19.40865707397461, 93.0999984741211, -2.5598978996276855, -6.896629333496094, -22.038164138793945]
     Last 10:  [8.254154205322266, -20.814590454101562, 42.26712417602539, 17.75473976135254, -58.51987075805664, 83.61962127685547, 171.26028442382812, 41.516536712646484, -50.14137268066406, -53.42215347290039]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 170.959076
     First 10: [432.0, 290.0, 17.625, 33.5, 8.875, 171.0, 163.0, 15.25, 26.25, 103.5]
     Last 10:  [10.875, 150.0, 75.0, 183.0, 80.5, 150.0, 152.0, 560.0, 138.0, 49.5]

================================================================================
250_model.layers.15.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.15.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.15.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 65.427055, Std: 2142.266113
     First 10: [-5.085495948791504, -191.54653930664062, -19.804851531982422, -44.64341354370117, -3.150050163269043, 30.404582977294922, 63.24119567871094, 3.7440853118896484, -8.644173622131348, -23.784683227539062]
     Last 10:  [5.642976760864258, 360.900390625, -53.0328483581543, 240.45382690429688, -40.154632568359375, -76.57630157470703, -503.7181091308594, -515.3480834960938, 326.48345947265625, -154.541748046875]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.15.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007043, Std: 0.304590
     First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
     Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 1.125533
     First 10: [-0.73046875, -0.53515625, 3.953125, 6.21875, 15.625, -0.380859375, -0.26953125, 11.8125, 2.8125, 0.33984375]
     Last 10:  [12.125, -0.33984375, -0.2001953125, -0.384765625, 0.37109375, -0.25390625, -0.271484375, -0.8359375, -0.294921875, 2.234375]

================================================================================
251_model.layers.15.mlp.gate_proj: Linear (model.layers.15.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.15.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007043, Std: 0.304590
     First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
     Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.15.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.074398, Std: 0.335151
     First 10: [0.007968437857925892, 0.006577871739864349, -0.01365414634346962, -0.009177042171359062, -0.013787896372377872, -0.012795290909707546, 0.006965255830436945, 0.0033434145152568817, 0.005731020122766495, -0.021441299468278885]
     Last 10:  [-0.09840548038482666, 0.21514005959033966, 0.42105987668037415, -0.17200151085853577, -0.13599467277526855, 0.23943284153938293, -0.3367224335670471, 0.13549494743347168, -0.1427851766347885, -0.24270331859588623]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000213
     First 10: [0.0111083984375, -0.0286865234375, -0.02490234375, 0.0164794921875, 0.0147705078125, 0.00933837890625, -0.0166015625, 0.037353515625, 0.026611328125, -0.0380859375]
     Last 10:  [-0.0279541015625, -0.0030670166015625, 0.0284423828125, 0.004974365234375, 0.046875, -0.017333984375, 0.10009765625, 0.03271484375, -0.00994873046875, -0.056396484375]

================================================================================
252_model.layers.15.mlp.act_fn: PytorchGELUTanh (model.layers.15.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.15.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.074398, Std: 0.335151
     First 10: [0.007968437857925892, 0.006577871739864349, -0.01365414634346962, -0.009177042171359062, -0.013787896372377872, -0.012795290909707546, 0.006965255830436945, 0.0033434145152568817, 0.005731020122766495, -0.021441299468278885]
     Last 10:  [-0.09840548038482666, 0.21514005959033966, 0.42105987668037415, -0.17200151085853577, -0.13599467277526855, 0.23943284153938293, -0.3367224335670471, 0.13549494743347168, -0.1427851766347885, -0.24270331859588623]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.15.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.002563, Std: 0.190383
     First 10: [0.004009549971669912, 0.0033061972353607416, -0.006752698216587305, -0.00455492315813899, -0.006818109191954136, -0.006332332734018564, 0.0035019824281334877, 0.0016761667793616652, 0.002878613071516156, -0.010537258349359035]
     Last 10:  [-0.04534578323364258, 0.1258930265903473, 0.27921435236930847, -0.07425645738840103, -0.06064186617732048, 0.1423693299293518, -0.12397266924381256, 0.07504914700984955, -0.06328679621219635, -0.09808177500963211]
     Zeros: 0, Total: 8192

================================================================================
253_model.layers.15.mlp.up_proj: Linear (model.layers.15.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.15.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007043, Std: 0.304590
     First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
     Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.15.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.010442, Std: 0.322344
     First 10: [-0.0006205979734659195, -0.004707143642008305, -0.0023159529082477093, 0.002553769387304783, -0.014446679502725601, -0.002844909904524684, 0.001927623525261879, 0.0010072584263980389, 0.013269669376313686, -0.014595495536923409]
     Last 10:  [0.3198736608028412, 0.1653127372264862, -0.23942740261554718, 0.30876171588897705, -0.09417060017585754, 0.2475600242614746, 0.06429553031921387, 0.15138278901576996, -0.012139350175857544, 0.19579988718032837]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000008
     First 10: [0.022216796875, -0.0245361328125, 0.11669921875, 0.0194091796875, 0.0198974609375, 0.04931640625, 0.0002918243408203125, 0.00958251953125, 0.0081787109375, -0.014892578125]
     Last 10:  [0.0081787109375, 0.0103759765625, 0.017822265625, 0.01153564453125, 0.0361328125, 0.04345703125, 0.016357421875, 0.02783203125, 0.037109375, -0.0294189453125]

================================================================================
254_model.layers.15.mlp.down_proj: Linear (model.layers.15.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.15.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.000076, Std: 0.097279
     First 10: [-2.488318614268792e-06, -1.556274582981132e-05, 1.563893056300003e-05, -1.1632223504420836e-05, 9.849904017755762e-05, 1.801491634978447e-05, 6.75050387144438e-06, 1.6883330999917234e-06, 3.8198242691578344e-05, 0.000153796499944292]
     Last 10:  [-0.01450492162257433, 0.020811721682548523, -0.06685156375169754, -0.022927550598978996, 0.005710680969059467, 0.035244956612586975, -0.007970888167619705, 0.01136114913970232, 0.0007682605646550655, -0.01920440047979355]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.15.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000643, Std: 0.083960
     First 10: [-0.00012059349683113396, 0.00012339527893345803, 2.3760161639074795e-05, 1.2208345651743002e-05, 4.0931423427537084e-05, -7.923947850940749e-05, -4.043376975459978e-05, 0.0001011798667605035, 4.606084985425696e-05, 8.468073792755604e-05]
     Last 10:  [-0.004282730631530285, 0.06307205557823181, -0.03377075493335724, -0.027518657967448235, -0.021923158317804337, 0.022629866376519203, 0.01870272308588028, 0.00733309518545866, -0.01623145118355751, 0.01149715855717659]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: 0.000008
     First 10: [0.019287109375, 0.0302734375, 0.0194091796875, 0.000133514404296875, 0.009521484375, -0.0142822265625, -0.02294921875, -1.5974044799804688e-05, 0.01214599609375, -0.004302978515625]
     Last 10:  [-0.00726318359375, 0.00531005859375, 0.004180908203125, 0.0142822265625, 0.0233154296875, -0.001953125, 0.019775390625, 0.007080078125, 0.00518798828125, 0.0014190673828125]

================================================================================
255_model.layers.15.mlp: Gemma3MLP (model.layers.15.mlp)
================================================================================

  → INPUT[0]: model.layers.15.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.007043, Std: 0.304590
     First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
     Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.15.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000643, Std: 0.083960
     First 10: [-0.00012059349683113396, 0.00012339527893345803, 2.3760161639074795e-05, 1.2208345651743002e-05, 4.0931423427537084e-05, -7.923947850940749e-05, -4.043376975459978e-05, 0.0001011798667605035, 4.606084985425696e-05, 8.468073792755604e-05]
     Last 10:  [-0.004282730631530285, 0.06307205557823181, -0.03377075493335724, -0.027518657967448235, -0.021923158317804337, 0.022629866376519203, 0.01870272308588028, 0.00733309518545866, -0.01623145118355751, 0.01149715855717659]
     Zeros: 0, Total: 2560

================================================================================
256_model.layers.15.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.15.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.15.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.000643, Std: 0.083960
     First 10: [-0.00012059349683113396, 0.00012339527893345803, 2.3760161639074795e-05, 1.2208345651743002e-05, 4.0931423427537084e-05, -7.923947850940749e-05, -4.043376975459978e-05, 0.0001011798667605035, 4.606084985425696e-05, 8.468073792755604e-05]
     Last 10:  [-0.004282730631530285, 0.06307205557823181, -0.03377075493335724, -0.027518657967448235, -0.021923158317804337, 0.022629866376519203, 0.01870272308588028, 0.00733309518545866, -0.01623145118355751, 0.01149715855717659]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.15.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 2.358799, Std: 135.110718
     First 10: [-41.17707061767578, 35.25476837158203, 0.9224682450294495, 0.37523314356803894, 0.3539884388446808, -23.901308059692383, -14.772287368774414, 1.3975409269332886, 2.7053370475769043, 11.548954010009766]
     Last 10:  [-0.6145722270011902, 257.35882568359375, -84.28010559082031, -183.02432250976562, -45.13790512084961, 90.07971954345703, 73.51394653320312, 35.77819061279297, -70.28152465820312, 12.40969181060791]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 244.076950
     First 10: [342.0, 286.0, 38.0, 29.875, 7.6875, 302.0, 366.0, 12.875, 58.0, 136.0]
     Last 10:  [10.5, 326.0, 199.0, 532.0, 164.0, 318.0, 314.0, 390.0, 346.0, 85.5]

================================================================================
257_model.layers.16.input_layernorm: Gemma3RMSNorm (model.layers.16.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.16.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 67.785851, Std: 2119.610840
     First 10: [-46.26256561279297, -156.29177856445312, -18.882383346557617, -44.26818084716797, -2.7960617542266846, 6.503274917602539, 48.468910217285156, 5.141626358032227, -5.938836574554443, -12.235729217529297]
     Last 10:  [5.028404712677002, 618.2592163085938, -137.31295776367188, 57.42950439453125, -85.29254150390625, 13.50341796875, -430.20416259765625, -479.56988525390625, 256.2019348144531, -142.13204956054688]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.16.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023102, Std: 3.732058
     First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
     Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 20.347698
     First 10: [1.8046875, 3.421875, 52.75, 63.75, 130.0, 5.53125, 4.71875, 113.5, 39.75, 12.3125]
     Last 10:  [110.0, 5.46875, 7.78125, 3.671875, 12.375, 5.5, 5.8125, 0.73828125, 5.625, 46.0]

================================================================================
258_model.layers.16.self_attn.q_proj: Linear (model.layers.16.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023102, Std: 3.732058
     First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
     Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.16.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.186328, Std: 4.967947
     First 10: [0.5893191695213318, -0.37577053904533386, -0.5282257199287415, 0.13427528738975525, 0.27341386675834656, -0.3656720519065857, 0.1008819043636322, 0.31134817004203796, 0.4053191542625427, -0.5178776979446411]
     Last 10:  [-0.9423724412918091, -6.639535903930664, -16.042810440063477, -8.33527946472168, -2.719550609588623, 9.927096366882324, 6.2545294761657715, 5.284853935241699, -0.6582779884338379, -0.1685328483581543]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000129
     First 10: [-0.032470703125, -0.006744384765625, 0.035400390625, -0.0216064453125, -0.000331878662109375, 0.00823974609375, 0.0291748046875, 0.006317138671875, 0.018798828125, -0.0011444091796875]
     Last 10:  [-0.01275634765625, -0.0216064453125, 0.001708984375, -0.038818359375, -0.029296875, -0.055419921875, 0.00946044921875, -0.1591796875, 0.0263671875, -0.02783203125]

================================================================================
259_model.layers.16.self_attn.k_proj: Linear (model.layers.16.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023102, Std: 3.732058
     First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
     Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.16.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.099209, Std: 6.360083
     First 10: [0.04842047393321991, 0.003189757466316223, 0.01668868027627468, -0.033108457922935486, 0.008404365740716457, -0.01615632325410843, 0.030261045321822166, 0.02573673613369465, -0.001690443605184555, -0.0009182244539260864]
     Last 10:  [0.2804817855358124, -6.581336498260498, -7.865691184997559, -4.267409801483154, -0.8390086889266968, 16.434478759765625, 3.6271352767944336, 19.715147018432617, -8.06876277923584, -8.782459259033203]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000072
     First 10: [0.00152587890625, -0.01544189453125, 0.0050048828125, -0.01544189453125, 0.011474609375, 0.01214599609375, 0.03857421875, -6.031990051269531e-05, 0.00439453125, -0.021484375]
     Last 10:  [0.01171875, 0.10595703125, -0.0498046875, -0.005035400390625, 0.039794921875, 0.03955078125, 0.04541015625, 0.03466796875, 0.03369140625, -0.01129150390625]

================================================================================
260_model.layers.16.self_attn.v_proj: Linear (model.layers.16.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.023102, Std: 3.732058
     First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
     Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.16.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.063901, Std: 2.754364
     First 10: [0.6461917161941528, 0.08295643329620361, 0.13937117159366608, 0.045830078423023224, 0.6798956394195557, 2.1528165340423584, 0.24252240359783173, 0.08699430525302887, 0.18814261257648468, -0.22373616695404053]
     Last 10:  [-7.92251443862915, 4.208802223205566, 2.3891663551330566, 2.264019012451172, 1.9514652490615845, -2.2277495861053467, -0.9624172449111938, -1.7478519678115845, -2.6274936199188232, 1.2641088962554932]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: 0.000065
     First 10: [-0.005859375, 0.0517578125, -0.0247802734375, 0.0341796875, 0.00070953369140625, -0.00885009765625, 0.0341796875, -0.000698089599609375, 0.01220703125, -0.10498046875]
     Last 10:  [-0.09033203125, 0.0260009765625, -0.07958984375, 0.0233154296875, 0.04931640625, -0.0693359375, -0.0213623046875, 0.025634765625, -0.00135040283203125, -0.006805419921875]

================================================================================
261_model.layers.16.self_attn.q_norm: Gemma3RMSNorm (model.layers.16.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.186328, Std: 4.967947
     First 10: [0.5893191695213318, -0.37577053904533386, -0.5282257199287415, 0.13427528738975525, 0.27341386675834656, -0.3656720519065857, 0.1008819043636322, 0.31134817004203796, 0.4053191542625427, -0.5178776979446411]
     Last 10:  [-0.9423724412918091, -6.639535903930664, -16.042810440063477, -8.33527946472168, -2.719550609588623, 9.927096366882324, 6.2545294761657715, 5.284853935241699, -0.6582779884338379, -0.1685328483581543]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.16.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: 0.054711, Std: 2.067075
     First 10: [1.9357506036758423, -0.7670307755470276, -1.2294248342514038, 0.32008153200149536, 0.40414005517959595, -0.8819738030433655, 0.22438427805900574, 0.6925090551376343, 1.194421410560608, -1.0862643718719482]
     Last 10:  [-0.3352161645889282, -1.5399467945098877, -3.6919846534729004, -2.0684781074523926, -1.0262129306793213, 0.6203492283821106, 1.7550610303878784, 0.642247200012207, -0.1855081021785736, -0.01605072058737278]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.989109
     First 10: [1.734375, 0.69921875, 0.9375, 0.984375, 0.23046875, 1.0078125, 0.8515625, 0.8515625, 1.453125, 0.74609375]
     Last 10:  [1.3125, 0.5078125, 0.49609375, 0.61328125, 1.453125, -0.59375, 0.82421875, -0.2099609375, 0.83203125, -0.380859375]

================================================================================
262_model.layers.16.self_attn.k_norm: Gemma3RMSNorm (model.layers.16.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.099209, Std: 6.360083
     First 10: [0.04842047393321991, 0.003189757466316223, 0.01668868027627468, -0.033108457922935486, 0.008404365740716457, -0.01615632325410843, 0.030261045321822166, 0.02573673613369465, -0.001690443605184555, -0.0009182244539260864]
     Last 10:  [0.2804817855358124, -6.581336498260498, -7.865691184997559, -4.267409801483154, -0.8390086889266968, 16.434478759765625, 3.6271352767944336, 19.715147018432617, -8.06876277923584, -8.782459259033203]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.16.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.096356, Std: 2.323688
     First 10: [0.031165719032287598, 0.0021304215770214796, 0.010005900636315346, -0.024083418771624565, 0.010744797065854073, -0.009793558157980442, 0.023012736812233925, 0.017529809847474098, -0.0006614002049900591, -0.0005464845453388989]
     Last 10:  [0.0813940167427063, -1.6824040412902832, -4.743253231048584, -1.4443470239639282, -0.17123238742351532, 15.433734893798828, 1.443773627281189, 9.30430793762207, -2.8848090171813965, -6.02875280380249]
     Zeros: 4, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.232803
     First 10: [1.28125, 1.3671875, 1.125, 1.578125, 3.53125, 1.1484375, 1.6953125, 1.4140625, 0.38671875, 1.109375]
     Last 10:  [0.90234375, 0.67578125, 2.953125, 1.21875, 0.337890625, 5.15625, 1.609375, 2.09375, 1.34375, 3.5]

================================================================================
263_model.layers.16.self_attn.o_proj: Linear (model.layers.16.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.16.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.005848, Std: 0.867904
     First 10: [0.6461917161941528, 0.08295643329620361, 0.13937117159366608, 0.045830078423023224, 0.6798956394195557, 2.1528165340423584, 0.24252240359783173, 0.08699430525302887, 0.18814261257648468, -0.22373616695404053]
     Last 10:  [1.8183882236480713, 0.34517034888267517, -0.05921671539545059, 0.4055950939655304, 0.2430204600095749, -1.4908965826034546, 0.07559152692556381, -0.24137867987155914, 0.08871878683567047, 0.084270179271698]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.16.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.048478, Std: 1.649589
     First 10: [0.47678616642951965, -0.8856248259544373, 0.85146564245224, -0.986497163772583, 0.1115405410528183, 0.3004021644592285, -0.11529159545898438, -0.01679813861846924, -0.3944810628890991, 0.7859684228897095]
     Last 10:  [-0.7075170874595642, 0.4451914429664612, -1.0282460451126099, 0.5290817022323608, 1.171733021736145, -0.4137289226055145, -1.1703801155090332, 0.13357987999916077, 0.5770243406295776, -0.40150222182273865]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: 0.000010
     First 10: [-0.0283203125, -0.006317138671875, 0.0118408203125, 0.0186767578125, 0.010498046875, 0.003448486328125, -0.030517578125, -0.0164794921875, 0.006805419921875, 0.0390625]
     Last 10:  [0.0074462890625, -0.020751953125, -0.0234375, 0.006500244140625, -0.01348876953125, 0.00147247314453125, 0.00311279296875, -0.0022735595703125, -0.032958984375, -0.016845703125]

================================================================================
264_model.layers.16.self_attn: Gemma3Attention (model.layers.16.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.16.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.048478, Std: 1.649589
     First 10: [0.47678616642951965, -0.8856248259544373, 0.85146564245224, -0.986497163772583, 0.1115405410528183, 0.3004021644592285, -0.11529159545898438, -0.01679813861846924, -0.3944810628890991, 0.7859684228897095]
     Last 10:  [-0.7075170874595642, 0.4451914429664612, -1.0282460451126099, 0.5290817022323608, 1.171733021736145, -0.4137289226055145, -1.1703801155090332, 0.13357987999916077, 0.5770243406295776, -0.40150222182273865]
     Zeros: 0, Total: 2560

================================================================================
265_model.layers.16.post_attention_layernorm: Gemma3RMSNorm (model.layers.16.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.16.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.048478, Std: 1.649589
     First 10: [0.47678616642951965, -0.8856248259544373, 0.85146564245224, -0.986497163772583, 0.1115405410528183, 0.3004021644592285, -0.11529159545898438, -0.01679813861846924, -0.3944810628890991, 0.7859684228897095]
     Last 10:  [-0.7075170874595642, 0.4451914429664612, -1.0282460451126099, 0.5290817022323608, 1.171733021736145, -0.4137289226055145, -1.1703801155090332, 0.13357987999916077, 0.5770243406295776, -0.40150222182273865]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.16.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 3.245879, Std: 100.410545
     First 10: [69.68220520019531, -98.7623519897461, 14.080764770507812, -16.484619140625, 0.4901111423969269, 42.86334228515625, -16.21098518371582, -0.11708002537488937, -10.383063316345215, 53.0793342590332]
     Last 10:  [-3.5843472480773926, 53.66650390625, -66.86080932617188, 82.00186157226562, 67.49333190917969, -45.2057991027832, -134.830810546875, 16.657920837402344, 66.13204193115234, -13.470908164978027]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 140.424698
     First 10: [210.0, 160.0, 22.875, 23.125, 5.34375, 205.0, 202.0, 9.0625, 37.0, 96.5]
     Last 10:  [7.53125, 202.0, 108.5, 260.0, 96.0, 183.0, 193.0, 209.0, 192.0, 55.5]

================================================================================
266_model.layers.16.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.16.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.16.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 71.031731, Std: 2173.854004
     First 10: [23.419639587402344, -255.05413818359375, -4.801618576049805, -60.75279998779297, -2.30595064163208, 49.366615295410156, 32.25792694091797, 5.024546146392822, -16.3218994140625, 40.843605041503906]
     Last 10:  [1.4440574645996094, 671.9257202148438, -204.17376708984375, 139.43136596679688, -17.799209594726562, -31.702381134033203, -565.0349731445312, -462.9119567871094, 322.333984375, -155.6029510498047]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.16.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005618, Std: 0.274082
     First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
     Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 1.005519
     First 10: [-0.71484375, -0.54296875, 3.390625, 6.46875, 16.375, -0.439453125, -0.41015625, 11.5625, 2.4375, 0.201171875]
     Last 10:  [12.0625, -0.41796875, -0.240234375, -0.5234375, 0.201171875, -0.353515625, -0.36328125, -0.79296875, -0.412109375, 2.078125]

================================================================================
267_model.layers.16.mlp.gate_proj: Linear (model.layers.16.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.16.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005618, Std: 0.274082
     First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
     Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.16.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.083386, Std: 0.292270
     First 10: [-0.015908610075712204, -0.034116242080926895, 0.004871401935815811, 0.018630526959896088, -0.03868526220321655, 0.0034015411511063576, -0.010958747938275337, -0.016562752425670624, -0.01825706474483013, -0.012787245213985443]
     Last 10:  [0.02294713258743286, 0.1307971477508545, 0.26205554604530334, -0.38444581627845764, -0.009860396385192871, -0.36737382411956787, -0.01593894511461258, 0.08815891295671463, -0.02820269763469696, 0.05401284992694855]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000237
     First 10: [0.0269775390625, 0.0322265625, 0.01116943359375, -0.046875, 0.022216796875, -0.030029296875, 3.9577484130859375e-05, -0.0184326171875, -0.010498046875, 0.05224609375]
     Last 10:  [-0.011962890625, 0.014892578125, 0.043212890625, 0.064453125, 0.007293701171875, 0.024658203125, 0.0634765625, 0.005523681640625, 0.0439453125, -0.026611328125]

================================================================================
268_model.layers.16.mlp.act_fn: PytorchGELUTanh (model.layers.16.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.16.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.083386, Std: 0.292270
     First 10: [-0.015908610075712204, -0.034116242080926895, 0.004871401935815811, 0.018630526959896088, -0.03868526220321655, 0.0034015411511063576, -0.010958747938275337, -0.016562752425670624, -0.01825706474483013, -0.012787245213985443]
     Last 10:  [0.02294713258743286, 0.1307971477508545, 0.26205554604530334, -0.38444581627845764, -0.009860396385192871, -0.36737382411956787, -0.01593894511461258, 0.08815891295671463, -0.02820269763469696, 0.05401284992694855]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.16.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.009553, Std: 0.152747
     First 10: [-0.007853343151509762, -0.016593875363469124, 0.0024451680947095156, 0.00945372600108385, -0.018745744600892067, 0.0017053865594789386, -0.005431464407593012, -0.008171942085027695, -0.008995563723146915, -0.006328391842544079]
     Last 10:  [0.01168361958116293, 0.07220413535833359, 0.1581125408411026, -0.13468708097934723, -0.00489141047000885, -0.1310366988182068, -0.007868126034736633, 0.04717600345611572, -0.013784075155854225, 0.028169725090265274]
     Zeros: 0, Total: 8192

================================================================================
269_model.layers.16.mlp.up_proj: Linear (model.layers.16.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.16.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005618, Std: 0.274082
     First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
     Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.16.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.002067, Std: 0.277382
     First 10: [-0.0004719181451946497, 0.024181926622986794, -0.006201986223459244, -0.0009645405225455761, -0.0027911104261875153, 0.002578145358711481, -0.00860830582678318, 0.0001635453663766384, 0.008943207561969757, 0.026319943368434906]
     Last 10:  [0.14669443666934967, -0.05562060326337814, -0.0481213703751564, 0.03438417613506317, 0.42668527364730835, -0.5867384672164917, 0.2711637020111084, -0.20394685864448547, -0.291745126247406, -0.02758188545703888]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000014
     First 10: [-0.031494140625, 0.0322265625, -0.02197265625, 0.0089111328125, 0.006256103515625, 0.0595703125, 0.047119140625, 0.017333984375, -0.0067138671875, 0.0081787109375]
     Last 10:  [-0.0361328125, -0.041748046875, 0.08056640625, 0.0771484375, -0.02880859375, -0.01287841796875, -0.036865234375, -0.0115966796875, -0.087890625, -2.7298927307128906e-05]

================================================================================
270_model.layers.16.mlp.down_proj: Linear (model.layers.16.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.16.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.002369, Std: 0.092993
     First 10: [3.706135203174199e-06, -0.00040127188549377024, -1.5164899195951875e-05, -9.1185020210105e-06, 5.232144394540228e-05, 4.396734311740147e-06, 4.675570744439028e-05, -1.336483251179743e-06, -8.044919377425686e-05, -0.00016656290972605348]
     Last 10:  [0.0017139220144599676, -0.004016037564724684, -0.007608592044562101, -0.004631104413419962, -0.002087092725560069, 0.07688426971435547, -0.0021335501223802567, -0.009621397592127323, 0.004021436907351017, -0.0007769741350784898]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.16.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002130, Std: 0.082821
     First 10: [0.00042184742051176727, -0.0013882522471249104, 0.001752090873196721, -0.0008490878390148282, 0.0007450255798175931, 0.001499584992416203, -0.00012240168871358037, 0.003291474189609289, -0.0021785059943795204, -0.00023373335716314614]
     Last 10:  [0.05586051568388939, -0.03975488245487213, 0.020572563633322716, -0.034600820392370224, -0.054469577968120575, -0.012684420682489872, 0.011202637106180191, 0.03233649581670761, 0.020729579031467438, -0.06031930446624756]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000003
     First 10: [-0.0147705078125, -0.00909423828125, -0.0238037109375, 0.004791259765625, 0.01165771484375, -0.0021514892578125, 0.007568359375, 0.0172119140625, 0.0198974609375, -0.00109100341796875]
     Last 10:  [0.01611328125, -0.0205078125, -0.0164794921875, -0.0179443359375, -0.007232666015625, 0.0152587890625, -0.01177978515625, 0.0069580078125, -0.01275634765625, 0.008056640625]

================================================================================
271_model.layers.16.mlp: Gemma3MLP (model.layers.16.mlp)
================================================================================

  → INPUT[0]: model.layers.16.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.005618, Std: 0.274082
     First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
     Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.16.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002130, Std: 0.082821
     First 10: [0.00042184742051176727, -0.0013882522471249104, 0.001752090873196721, -0.0008490878390148282, 0.0007450255798175931, 0.001499584992416203, -0.00012240168871358037, 0.003291474189609289, -0.0021785059943795204, -0.00023373335716314614]
     Last 10:  [0.05586051568388939, -0.03975488245487213, 0.020572563633322716, -0.034600820392370224, -0.054469577968120575, -0.012684420682489872, 0.011202637106180191, 0.03233649581670761, 0.020729579031467438, -0.06031930446624756]
     Zeros: 0, Total: 2560

================================================================================
272_model.layers.16.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.16.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.16.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002130, Std: 0.082821
     First 10: [0.00042184742051176727, -0.0013882522471249104, 0.001752090873196721, -0.0008490878390148282, 0.0007450255798175931, 0.001499584992416203, -0.00012240168871358037, 0.003291474189609289, -0.0021785059943795204, -0.00023373335716314614]
     Last 10:  [0.05586051568388939, -0.03975488245487213, 0.020572563633322716, -0.034600820392370224, -0.054469577968120575, -0.012684420682489872, 0.011202637106180191, 0.03233649581670761, 0.020729579031467438, -0.06031930446624756]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.16.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -5.391714, Std: 466.573944
     First 10: [71.27413940429688, -175.76939392089844, 33.201168060302734, -11.595407485961914, 3.213974952697754, 227.96554565429688, -22.131912231445312, 22.126253128051758, -62.2681884765625, -14.153388023376465]
     Last 10:  [10.495523452758789, -207.1880340576172, 62.65562057495117, -278.4393005371094, -140.86526489257812, -62.110225677490234, 53.67805480957031, 190.6000213623047, 110.21207427978516, -75.22523498535156]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 283.390228
     First 10: [398.0, 298.0, 43.75, 31.25, 9.1875, 358.0, 426.0, 14.875, 66.5, 142.0]
     Last 10:  [13.3125, 396.0, 231.0, 612.0, 196.0, 372.0, 364.0, 448.0, 404.0, 94.0]

================================================================================
273_model.layers.17.input_layernorm: Gemma3RMSNorm (model.layers.17.input_layernorm)
================================================================================

  → INPUT[0]: model.layers.17.input_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 65.640030, Std: 1912.738525
     First 10: [94.69377899169922, -430.82354736328125, 28.39954948425293, -72.34820556640625, 0.9080243110656738, 277.3321533203125, 10.126014709472656, 27.150798797607422, -78.590087890625, 26.690216064453125]
     Last 10:  [11.939580917358398, 464.7376708984375, -141.5181427001953, -139.0079345703125, -158.6644744873047, -93.81260681152344, -511.35693359375, -272.31195068359375, 432.5460510253906, -230.82818603515625]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.17.input_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044712, Std: 2.101685
     First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
     Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 10.482544
     First 10: [0.89453125, 1.265625, 31.5, 31.0, 83.0, 1.4921875, 1.1015625, 61.75, 21.875, 4.0]
     Last 10:  [59.5, 1.4765625, 5.59375, 1.0078125, 4.59375, 1.8515625, 1.8203125, 0.6171875, 1.734375, 18.375]

================================================================================
274_model.layers.17.self_attn.q_proj: Linear (model.layers.17.self_attn.q_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.q_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044712, Std: 2.101685
     First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
     Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.17.self_attn.q_proj_output
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: -0.001358, Std: 1.868693
     First 10: [-0.22195491194725037, -3.241860866546631, 0.01906605064868927, -3.830760955810547, -1.901699423789978, 0.18472124636173248, 1.5615875720977783, -0.07261425256729126, -0.4270014762878418, 3.1288466453552246]
     Last 10:  [1.402298927307129, -0.39026933908462524, 0.6423313617706299, 1.2350013256072998, -0.25222915410995483, -2.0360782146453857, -1.9637935161590576, 1.0525981187820435, -1.6070647239685059, 2.18375563621521]
     Zeros: 0, Total: 4096

  → PARAMETER: weight
     Shape: [1024, 640]
     Mean: -0.000057
     First 10: [-0.00162506103515625, 0.00439453125, -0.005401611328125, 0.00653076171875, 0.0026397705078125, -0.006561279296875, 0.010009765625, -0.0025482177734375, 0.01153564453125, 0.0022430419921875]
     Last 10:  [-0.0093994140625, 0.015380859375, 0.0001544952392578125, 0.016845703125, 0.0247802734375, 0.0186767578125, 0.006317138671875, 0.0011749267578125, -0.01336669921875, 0.039306640625]

================================================================================
275_model.layers.17.self_attn.k_proj: Linear (model.layers.17.self_attn.k_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.k_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044712, Std: 2.101685
     First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
     Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.17.self_attn.k_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: -0.091316, Std: 1.733748
     First 10: [-0.025150056928396225, 0.3404127359390259, -0.005099445581436157, 3.1741743087768555, -0.018208907917141914, 0.016297534108161926, -0.5040911436080933, 0.0033912211656570435, 0.07137191295623779, -0.0493408739566803]
     Last 10:  [3.5663981437683105, -0.7459906339645386, 0.1854495406150818, 2.554460287094116, -2.814157724380493, 0.4416767358779907, -0.9082603454589844, 3.697408437728882, -2.477283477783203, 4.522344589233398]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000032
     First 10: [-0.006500244140625, -0.0024871826171875, -0.01806640625, -0.04443359375, 0.0087890625, 0.01507568359375, -0.0067138671875, -0.003265380859375, -0.0081787109375, 0.0006103515625]
     Last 10:  [-0.00958251953125, 0.00970458984375, 0.050048828125, 0.044921875, 0.0322265625, -0.0150146484375, -0.08056640625, -0.0306396484375, 0.0478515625, -0.06689453125]

================================================================================
276_model.layers.17.self_attn.v_proj: Linear (model.layers.17.self_attn.v_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.v_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.044712, Std: 2.101685
     First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
     Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.17.self_attn.v_proj_output
     Shape: [1, 4, 256]
     Dtype: torch.float32
     Mean: 0.053376, Std: 0.894670
     First 10: [0.023865699768066406, -0.06637456268072128, -0.04135437309741974, -0.01569271832704544, 0.0027111470699310303, 0.025548547506332397, -0.15465165674686432, 0.10315865278244019, -0.03206530213356018, -0.08218654990196228]
     Last 10:  [-0.9262914061546326, 0.13054382801055908, 0.9405217170715332, -0.6541187167167664, 0.2109556794166565, 0.5401588082313538, -0.06547743082046509, -0.03769075870513916, 0.5222419500350952, -0.4424525499343872]
     Zeros: 0, Total: 1024

  → PARAMETER: weight
     Shape: [256, 640]
     Mean: -0.000193
     First 10: [-0.019287109375, 0.016357421875, 0.00174713134765625, -0.01043701171875, 0.0205078125, -0.04443359375, 0.00177764892578125, -0.0478515625, 0.002105712890625, -0.0115966796875]
     Last 10:  [-0.001739501953125, -0.01226806640625, 0.035888671875, 0.01495361328125, 0.05078125, -0.0020904541015625, -0.0230712890625, 0.0247802734375, 0.0255126953125, 0.0033416748046875]

================================================================================
277_model.layers.17.self_attn.q_norm: Gemma3RMSNorm (model.layers.17.self_attn.q_norm)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.q_norm_input_0
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.001358, Std: 1.868693
     First 10: [-0.22195491194725037, -3.241860866546631, 0.01906605064868927, -3.830760955810547, -1.901699423789978, 0.18472124636173248, 1.5615875720977783, -0.07261425256729126, -0.4270014762878418, 3.1288466453552246]
     Last 10:  [1.402298927307129, -0.39026933908462524, 0.6423313617706299, 1.2350013256072998, -0.25222915410995483, -2.0360782146453857, -1.9637935161590576, 1.0525981187820435, -1.6070647239685059, 2.18375563621521]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.17.self_attn.q_norm_output
     Shape: [1, 4, 4, 256]
     Dtype: torch.float32
     Mean: -0.029777, Std: 1.425665
     First 10: [-0.2635241448879242, -0.0, 0.03937313333153725, -0.0, -0.0214015394449234, 0.40953049063682556, 0.0, -0.17814822494983673, -0.003604074940085411, -0.03521173447370529]
     Last 10:  [1.8586121797561646, -0.6090865135192871, 1.4457809925079346, 2.169586420059204, -0.4945346713066101, -6.195661544799805, -6.560952663421631, 1.9729766845703125, -3.9449288845062256, 2.8001627922058105]
     Zeros: 112, Total: 4096

  → PARAMETER: weight
     Shape: [256]
     Mean: 0.809194
     First 10: [0.6484375, -1.0, 1.8671875, -1.0, -0.984375, 2.078125, -1.0, 2.40625, -0.98828125, -1.015625]
     Last 10:  [1.640625, 2.109375, 3.484375, 2.5, 2.90625, 5.0625, 5.65625, 2.734375, 3.890625, 1.5546875]

================================================================================
278_model.layers.17.self_attn.k_norm: Gemma3RMSNorm (model.layers.17.self_attn.k_norm)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.k_norm_input_0
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.091316, Std: 1.733748
     First 10: [-0.025150056928396225, 0.3404127359390259, -0.005099445581436157, 3.1741743087768555, -0.018208907917141914, 0.016297534108161926, -0.5040911436080933, 0.0033912211656570435, 0.07137191295623779, -0.0493408739566803]
     Last 10:  [3.5663981437683105, -0.7459906339645386, 0.1854495406150818, 2.554460287094116, -2.814157724380493, 0.4416767358779907, -0.9082603454589844, 3.697408437728882, -2.477283477783203, 4.522344589233398]
     Zeros: 0, Total: 1024

  → OUTPUT[0]: model.layers.17.self_attn.k_norm_output
     Shape: [1, 1, 4, 256]
     Dtype: torch.float32
     Mean: -0.168407, Std: 2.112510
     First 10: [-0.08488571643829346, 0.0, -0.006594241596758366, 0.0, 0.03997278958559036, 0.02185743674635887, -0.0, 0.004873833619058132, -0.0009792371420189738, 0.002369383815675974]
     Last 10:  [3.3527562618255615, -0.49549075961112976, 0.12252038717269897, 2.7101473808288574, -1.9027397632598877, 0.7079503536224365, -2.9301326274871826, 4.092082500457764, -1.7781769037246704, 4.257190227508545]
     Zeros: 28, Total: 1024

  → PARAMETER: weight
     Shape: [256]
     Mean: 1.574963
     First 10: [2.84375, -1.0, 0.47265625, -1.0, -3.5, 0.52734375, -1.0, 0.63671875, -1.015625, -1.0546875]
     Last 10:  [0.443359375, 0.019775390625, 0.01434326171875, 0.62890625, 0.0380859375, 1.4609375, 3.953125, 0.69921875, 0.10205078125, 0.4453125]

================================================================================
279_model.layers.17.self_attn.o_proj: Linear (model.layers.17.self_attn.o_proj)
================================================================================

  → INPUT[0]: model.layers.17.self_attn.o_proj_input_0
     Shape: [1, 4, 1024]
     Dtype: torch.float32
     Mean: 0.002630, Std: 0.357873
     First 10: [0.023865699768066406, -0.06637456268072128, -0.04135437309741974, -0.01569271832704544, 0.0027111470699310303, 0.025548547506332397, -0.15465165674686432, 0.10315865278244019, -0.03206530213356018, -0.08218654990196228]
     Last 10:  [-0.09523449093103409, 0.022913454100489616, 0.03377542272210121, -0.06238217651844025, -0.018154432997107506, 0.020654987543821335, -0.022718222811818123, 0.05276947095990181, 0.03426535055041313, -0.08765309303998947]
     Zeros: 0, Total: 4096

  → OUTPUT[0]: model.layers.17.self_attn.o_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.010114, Std: 0.594780
     First 10: [-0.11932455003261566, -0.21940511465072632, -0.23479405045509338, -0.03199487924575806, 0.13006635010242462, 0.09975278377532959, -0.16305924952030182, 0.049375712871551514, 0.07185353338718414, -0.005557570606470108]
     Last 10:  [-0.12229359149932861, -0.03926490992307663, 0.07144839316606522, 0.01862509176135063, -0.003162730485200882, -0.3158438801765442, -0.10722315311431885, 0.07629942893981934, -0.11908232420682907, 0.20067761838436127]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 1024]
     Mean: -0.000040
     First 10: [-0.0400390625, -0.078125, -0.01275634765625, -0.00445556640625, 0.03955078125, -0.0166015625, -0.0693359375, 0.04443359375, -0.0294189453125, 0.0810546875]
     Last 10:  [0.0203857421875, 0.03759765625, -0.01556396484375, -0.0240478515625, -0.0091552734375, -0.000244140625, 0.0120849609375, 0.04296875, 0.00665283203125, -0.01080322265625]

================================================================================
280_model.layers.17.self_attn: Gemma3Attention (model.layers.17.self_attn)
================================================================================

  → OUTPUT[0]: model.layers.17.self_attn_output_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.010114, Std: 0.594780
     First 10: [-0.11932455003261566, -0.21940511465072632, -0.23479405045509338, -0.03199487924575806, 0.13006635010242462, 0.09975278377532959, -0.16305924952030182, 0.049375712871551514, 0.07185353338718414, -0.005557570606470108]
     Last 10:  [-0.12229359149932861, -0.03926490992307663, 0.07144839316606522, 0.01862509176135063, -0.003162730485200882, -0.3158438801765442, -0.10722315311431885, 0.07629942893981934, -0.11908232420682907, 0.20067761838436127]
     Zeros: 0, Total: 2560

================================================================================
281_model.layers.17.post_attention_layernorm: Gemma3RMSNorm (model.layers.17.post_attention_layernorm)
================================================================================

  → INPUT[0]: model.layers.17.post_attention_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.010114, Std: 0.594780
     First 10: [-0.11932455003261566, -0.21940511465072632, -0.23479405045509338, -0.03199487924575806, 0.13006635010242462, 0.09975278377532959, -0.16305924952030182, 0.049375712871551514, 0.07185353338718414, -0.005557570606470108]
     Last 10:  [-0.12229359149932861, -0.03926490992307663, 0.07144839316606522, 0.01862509176135063, -0.003162730485200882, -0.3158438801765442, -0.10722315311431885, 0.07629942893981934, -0.11908232420682907, 0.20067761838436127]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.17.post_attention_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -3.988911, Std: 56.292683
     First 10: [-37.41681671142578, -48.95947265625, -9.716760635375977, -0.869111180305481, 1.3160321712493896, 23.132402420043945, -41.1424446105957, 0.7156308889389038, 3.7726731300354004, -0.5917065739631653]
     Last 10:  [-2.146132230758667, -14.262785911560059, 25.505809783935547, 9.75938892364502, -0.5975321531295776, -112.75066375732422, -37.60523986816406, 34.72388458251953, -44.747657775878906, 23.56524658203125]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 134.691193
     First 10: [214.0, 152.0, 27.375, 17.625, 5.9375, 158.0, 172.0, 8.9375, 35.0, 72.0]
     Last 10:  [7.40625, 173.0, 170.0, 250.0, 89.5, 170.0, 167.0, 217.0, 179.0, 55.25]

================================================================================
282_model.layers.17.pre_feedforward_layernorm: Gemma3RMSNorm (model.layers.17.pre_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.17.pre_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 61.651123, Std: 1925.038818
     First 10: [57.27696228027344, -479.78302001953125, 18.682788848876953, -73.21731567382812, 2.2240564823150635, 300.4645690917969, -31.016429901123047, 27.866430282592773, -74.81741333007812, 26.098508834838867]
     Last 10:  [9.793448448181152, 450.4748840332031, -116.0123291015625, -129.24855041503906, -159.2620086669922, -206.56326293945312, -548.962158203125, -237.58807373046875, 387.79840087890625, -207.262939453125]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.17.pre_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.014325, Std: 0.293608
     First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
     Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
     Zeros: 4, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 1.167118
     First 10: [-0.63671875, -0.47265625, 3.09375, 14.25, 16.75, -0.400390625, -0.423828125, 14.1875, 2.625, 0.361328125]
     Last 10:  [12.25, -0.44921875, -0.33984375, -0.52734375, 0.181640625, -0.353515625, -0.376953125, -0.77734375, -0.4375, 3.109375]

================================================================================
283_model.layers.17.mlp.gate_proj: Linear (model.layers.17.mlp.gate_proj)
================================================================================

  → INPUT[0]: model.layers.17.mlp.gate_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.014325, Std: 0.293608
     First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
     Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.17.mlp.gate_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.073597, Std: 0.339770
     First 10: [0.05365362763404846, 0.055759791284799576, 0.04292953759431839, -0.1291077882051468, -0.13759341835975647, -0.004945250228047371, -0.13585807383060455, 0.03457877039909363, 0.06509476900100708, -0.018617281690239906]
     Last 10:  [-0.12517747282981873, 0.022637367248535156, -0.37455517053604126, -0.13395927846431732, -0.04859810695052147, -0.4463132619857788, -0.29756954312324524, -0.1041971817612648, -0.5414638519287109, -0.21968626976013184]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: -0.000132
     First 10: [-0.0791015625, 0.0155029296875, -0.01385498046875, -0.0118408203125, -0.0537109375, 0.056884765625, 0.01239013671875, 0.111328125, -0.0137939453125, -0.0179443359375]
     Last 10:  [-0.00848388671875, -0.0223388671875, -0.0263671875, -0.03369140625, 0.040283203125, 0.0059814453125, 0.0186767578125, 0.06591796875, 0.028564453125, 0.0084228515625]

================================================================================
284_model.layers.17.mlp.act_fn: PytorchGELUTanh (model.layers.17.mlp.act_fn)
================================================================================

  → INPUT[0]: model.layers.17.mlp.act_fn_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.073597, Std: 0.339770
     First 10: [0.05365362763404846, 0.055759791284799576, 0.04292953759431839, -0.1291077882051468, -0.13759341835975647, -0.004945250228047371, -0.13585807383060455, 0.03457877039909363, 0.06509476900100708, -0.018617281690239906]
     Last 10:  [-0.12517747282981873, 0.022637367248535156, -0.37455517053604126, -0.13395927846431732, -0.04859810695052147, -0.4463132619857788, -0.29756954312324524, -0.1041971817612648, -0.5414638519287109, -0.21968626976013184]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.17.mlp.act_fn_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.003373, Std: 0.202564
     First 10: [0.027974698692560196, 0.029119623824954033, 0.02219977043569088, -0.05792251601815224, -0.06126783415675163, -0.0024628688115626574, -0.0605882927775383, 0.017766300588846207, 0.03423663228750229, -0.009170373901724815]
     Last 10:  [-0.056353915482759476, 0.011523104272782803, -0.13259677588939667, -0.05984204262495041, -0.02335721254348755, -0.14626172184944153, -0.11397627741098404, -0.04777511581778526, -0.15926417708396912, -0.09074385464191437]
     Zeros: 0, Total: 8192

================================================================================
285_model.layers.17.mlp.up_proj: Linear (model.layers.17.mlp.up_proj)
================================================================================

  → INPUT[0]: model.layers.17.mlp.up_proj_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.014325, Std: 0.293608
     First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
     Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.17.mlp.up_proj_output
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: 0.011847, Std: 0.300520
     First 10: [0.04402327165007591, 0.08149217069149017, 0.007446791976690292, -0.07746966928243637, -0.014085261151194572, -0.06364326924085617, 0.08462493121623993, -0.08976950496435165, -0.05302724614739418, -0.020938824862241745]
     Last 10:  [0.5140440464019775, 0.6376049518585205, -0.11391889303922653, 0.1855631321668625, -0.24643316864967346, -0.25170210003852844, 0.5061348676681519, 0.2164064645767212, -0.032487645745277405, 0.019206563010811806]
     Zeros: 0, Total: 8192

  → PARAMETER: weight
     Shape: [2048, 640]
     Mean: 0.000012
     First 10: [-0.0654296875, -0.0224609375, 0.0888671875, 0.01263427734375, 0.0234375, -0.11865234375, -0.08349609375, -0.00994873046875, 0.042724609375, -0.00225830078125]
     Last 10:  [-0.0079345703125, 0.035400390625, 0.0274658203125, 0.057861328125, 0.04736328125, -0.0234375, -3.504753112792969e-05, 0.037353515625, 0.0634765625, -0.035888671875]

================================================================================
286_model.layers.17.mlp.down_proj: Linear (model.layers.17.mlp.down_proj)
================================================================================

  → INPUT[0]: model.layers.17.mlp.down_proj_input_0
     Shape: [1, 4, 2048]
     Dtype: torch.float32
     Mean: -0.000164, Std: 0.098172
     First 10: [0.001231537782587111, 0.0023730213288217783, 0.0001653170766076073, 0.004487238358706236, 0.0008629734511487186, 0.00015674502355977893, -0.00512728001922369, -0.0015948719810694456, -0.001815474359318614, 0.00019201685790903866]
     Last 10:  [-0.02896839566528797, 0.007347188424319029, 0.015105278231203556, -0.011104476638138294, 0.00575599167495966, 0.036814384162425995, -0.057687368243932724, -0.010338843800127506, 0.005174118094146252, -0.0017428775317966938]
     Zeros: 0, Total: 8192

  → OUTPUT[0]: model.layers.17.mlp.down_proj_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002273, Std: 0.064479
     First 10: [-0.0004225907614454627, -0.0025450338143855333, -0.001179103972390294, -0.002013721037656069, 0.000198440597159788, -8.550414349883795e-05, 0.00027018238324671984, 0.0020912857726216316, -0.002258156193420291, 0.00017601378203835338]
     Last 10:  [0.03127926588058472, -0.012165391817688942, 0.03675460070371628, 0.08866649866104126, -0.006740258075296879, -0.00034081749618053436, -0.0004915045574307442, -0.003268558531999588, 0.01683509722352028, -0.0456930473446846]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640, 2048]
     Mean: -0.000000
     First 10: [0.0159912109375, 0.0042724609375, -0.003509521484375, -0.00836181640625, -0.012939453125, 0.01348876953125, 0.004974365234375, 0.0302734375, -0.003753662109375, 0.01324462890625]
     Last 10:  [0.01275634765625, 0.0023956298828125, -0.0008544921875, 0.0198974609375, 0.0185546875, 0.0177001953125, 0.0196533203125, 0.00010347366333007812, 0.026611328125, -0.00628662109375]

================================================================================
287_model.layers.17.mlp: Gemma3MLP (model.layers.17.mlp)
================================================================================

  → INPUT[0]: model.layers.17.mlp_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -0.014325, Std: 0.293608
     First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
     Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
     Zeros: 4, Total: 2560

  → OUTPUT[0]: model.layers.17.mlp_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002273, Std: 0.064479
     First 10: [-0.0004225907614454627, -0.0025450338143855333, -0.001179103972390294, -0.002013721037656069, 0.000198440597159788, -8.550414349883795e-05, 0.00027018238324671984, 0.0020912857726216316, -0.002258156193420291, 0.00017601378203835338]
     Last 10:  [0.03127926588058472, -0.012165391817688942, 0.03675460070371628, 0.08866649866104126, -0.006740258075296879, -0.00034081749618053436, -0.0004915045574307442, -0.003268558531999588, 0.01683509722352028, -0.0456930473446846]
     Zeros: 0, Total: 2560

================================================================================
288_model.layers.17.post_feedforward_layernorm: Gemma3RMSNorm (model.layers.17.post_feedforward_layernorm)
================================================================================

  → INPUT[0]: model.layers.17.post_feedforward_layernorm_input_0
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: 0.002273, Std: 0.064479
     First 10: [-0.0004225907614454627, -0.0025450338143855333, -0.001179103972390294, -0.002013721037656069, 0.000198440597159788, -8.550414349883795e-05, 0.00027018238324671984, 0.0020912857726216316, -0.002258156193420291, 0.00017601378203835338]
     Last 10:  [0.03127926588058472, -0.012165391817688942, 0.03675460070371628, 0.08866649866104126, -0.006740258075296879, -0.00034081749618053436, -0.0004915045574307442, -0.003268558531999588, 0.01683509722352028, -0.0456930473446846]
     Zeros: 0, Total: 2560

  → OUTPUT[0]: model.layers.17.post_feedforward_layernorm_output
     Shape: [1, 4, 640]
     Dtype: torch.float32
     Mean: -3.856964, Std: 413.946991
     First 10: [-72.83763122558594, -394.4461669921875, -36.11780548095703, -50.635658264160156, 1.1397275924682617, -15.206567764282227, 55.21524429321289, 20.07830810546875, -97.56190490722656, 14.967665672302246]
     Last 10:  [8.63332748413086, -79.8128890991211, 207.27023315429688, 914.1156005859375, -24.198131561279297, -2.2901246547698975, -3.39635968208313, -22.482315063476562, 120.6115951538086, -137.18568420410156]
     Zeros: 0, Total: 2560

  → PARAMETER: weight
     Shape: [640]
     Mean: 323.013672
     First 10: [376.0, 338.0, 66.0, 54.0, 11.5625, 388.0, 446.0, 20.0, 93.5, 185.0]
     Last 10:  [16.375, 412.0, 354.0, 648.0, 225.0, 422.0, 434.0, 432.0, 450.0, 188.0]

Forward pass completed. Tracked 289 operations.

================================================================================
OPERATION SUMMARY (289 operations)
================================================================================

000_model.embed_tokens: model.embed_tokens (Gemma3TextScaledWordEmbedding)
  IN[0]:  [1, 4] | min=2 max=236888 | torch.int64
    First 10: [2, 10979, 147224, 236888]
    Last 10:  [2, 10979, 147224, 236888]
  OUT[0]: [1, 4, 640] | μ=-0.011280 σ=1.031319
    First 10: [-0.36131492257118225, 0.27021417021751404, 0.3381537199020386, 0.3149925172328949, 0.12352646887302399, -0.07913414388895035, 0.10345342010259628, 0.09496097266674042, 0.04844553768634796, -0.007479141931980848]
    Last 10:  [0.5991033911705017, -0.8770379424095154, -0.14514359831809998, -0.41999000310897827, -0.09882117807865143, -0.2856549620628357, -0.17448113858699799, -1.0437986850738525, 1.7664285898208618, 0.3072721064090729]
  WEIGHT: [262144, 640] | μ=-0.000165

001_model.layers.0.input_layernorm: model.layers.0.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.011280 σ=1.031319
    First 10: [-0.36131492257118225, 0.27021417021751404, 0.3381537199020386, 0.3149925172328949, 0.12352646887302399, -0.07913414388895035, 0.10345342010259628, 0.09496097266674042, 0.04844553768634796, -0.007479141931980848]
    Last 10:  [0.5991033911705017, -0.8770379424095154, -0.14514359831809998, -0.41999000310897827, -0.09882117807865143, -0.2856549620628357, -0.17448113858699799, -1.0437986850738525, 1.7664285898208618, 0.3072721064090729]
  OUT[0]: [1, 4, 640] | μ=-0.425515 σ=16.641912
    First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
    Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
  WEIGHT: [640] | μ=12.480810

002_model.layers.0.self_attn.q_proj: model.layers.0.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.425515 σ=16.641912
    First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
    Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
  OUT[0]: [1, 4, 1024] | μ=1.483598 σ=22.160713
    First 10: [6.2979254722595215, 7.329462051391602, 9.404305458068848, -2.4617486000061035, 10.235593795776367, -4.497312068939209, 7.944532871246338, 1.0305190086364746, 2.898792266845703, 5.074529647827148]
    Last 10:  [5.745026588439941, -3.353013038635254, 1.8782310485839844, -3.236016273498535, 3.272514820098877, -6.085381984710693, 30.446889877319336, 1.4016046524047852, 14.366205215454102, 3.6530263423919678]
  WEIGHT: [1024, 640] | μ=-0.000076

003_model.layers.0.self_attn.k_proj: model.layers.0.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.425515 σ=16.641912
    First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
    Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
  OUT[0]: [1, 4, 256] | μ=0.790188 σ=21.040928
    First 10: [0.05973079800605774, -0.3500518798828125, 0.07768797874450684, -0.2811100482940674, -0.29856228828430176, -0.3946802616119385, 0.1314244270324707, -0.004389762878417969, 0.18865251541137695, -0.2531353235244751]
    Last 10:  [6.589937686920166, 33.81639862060547, -25.562625885009766, 42.109092712402344, -34.03094482421875, -0.6761453151702881, 41.62080764770508, -18.552766799926758, 16.03182601928711, -44.43439865112305]
  WEIGHT: [256, 640] | μ=0.000193

004_model.layers.0.self_attn.v_proj: model.layers.0.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.425515 σ=16.641912
    First 10: [-5.603112697601318, 4.419522762298584, 3.543755531311035, 12.669859886169434, 2.0802154541015625, -0.7957479357719421, 0.9525591731071472, 1.5876609086990356, 0.4988916218280792, -0.08653437346220016]
    Last 10:  [9.186232566833496, -7.866748332977295, -2.0847854614257812, -4.098075866699219, -1.018155574798584, -3.046980142593384, -2.0091724395751953, -15.245756149291992, 17.87836265563965, 3.128582239151001]
  OUT[0]: [1, 4, 256] | μ=-0.047050 σ=10.486319
    First 10: [-0.8908199071884155, 0.20305782556533813, -0.7873555421829224, -0.2379053831100464, -0.3717365264892578, -3.8010311126708984, -0.3445277214050293, 0.35037922859191895, 0.3936748504638672, -0.18098047375679016]
    Last 10:  [-6.451592445373535, -13.575428009033203, 10.863025665283203, -16.696815490722656, -5.791408538818359, 1.2365117073059082, 16.345947265625, -0.33417809009552, 0.9071155786514282, -2.2307324409484863]
  WEIGHT: [256, 640] | μ=0.000008

005_model.layers.0.self_attn.q_norm: model.layers.0.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=1.483598 σ=22.160713
    First 10: [6.2979254722595215, 7.329462051391602, 9.404305458068848, -2.4617486000061035, 10.235593795776367, -4.497312068939209, 7.944532871246338, 1.0305190086364746, 2.898792266845703, 5.074529647827148]
    Last 10:  [5.745026588439941, -3.353013038635254, 1.8782310485839844, -3.236016273498535, 3.272514820098877, -6.085381984710693, 30.446889877319336, 1.4016046524047852, 14.366205215454102, 3.6530263423919678]
  OUT[0]: [1, 4, 4, 256] | μ=0.084367 σ=1.356608
    First 10: [1.1887798309326172, 1.2399744987487793, 2.0697598457336426, -0.5861437916755676, 1.595339059829712, -1.2152314186096191, 0.9924666881561279, 0.14730101823806763, 0.41718754172325134, 0.4930866062641144]
    Last 10:  [0.4484182894229889, -0.25760844349861145, 0.11411970853805542, -0.32984206080436707, 0.30250996351242065, -0.5327273607254028, 3.3829896450042725, 0.09996145218610764, 1.4335432052612305, 0.27059468626976013]
  WEIGHT: [256] | μ=0.965250

006_model.layers.0.self_attn.k_norm: model.layers.0.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=0.790188 σ=21.040928
    First 10: [0.05973079800605774, -0.3500518798828125, 0.07768797874450684, -0.2811100482940674, -0.29856228828430176, -0.3946802616119385, 0.1314244270324707, -0.004389762878417969, 0.18865251541137695, -0.2531353235244751]
    Last 10:  [6.589937686920166, 33.81639862060547, -25.562625885009766, 42.109092712402344, -34.03094482421875, -0.6761453151702881, 41.62080764770508, -18.552766799926758, 16.03182601928711, -44.43439865112305]
  OUT[0]: [1, 1, 4, 256] | μ=0.011500 σ=1.486134
    First 10: [0.0053739422000944614, -0.04209119826555252, 0.008173738606274128, -0.02857976034283638, -0.02694612927734852, -0.0404619462788105, 0.011740312911570072, -0.0003451482334639877, 0.024021603167057037, -0.019759371876716614]
    Last 10:  [0.578190803527832, 2.612980842590332, -3.020167350769043, 2.802427291870117, -2.2902610301971436, -0.04904336482286453, 2.9255452156066895, -1.6139174699783325, 0.9750345945358276, -3.5995635986328125]
  WEIGHT: [256] | μ=0.973218

007_model.layers.0.self_attn.o_proj: model.layers.0.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.274047 σ=4.680633
    First 10: [-0.8908199071884155, 0.20305782556533813, -0.7873555421829224, -0.2379053831100464, -0.3717365264892578, -3.8010311126708984, -0.3445277214050293, 0.35037922859191895, 0.3936748504638672, -0.18098047375679016]
    Last 10:  [-7.077987194061279, -1.1683422327041626, 6.293652534484863, -0.39658042788505554, -2.639127492904663, -1.5583627223968506, 5.5387773513793945, 1.0841830968856812, -2.716465711593628, 2.167811870574951]
  OUT[0]: [1, 4, 640] | μ=0.280524 σ=7.449708
    First 10: [-2.8339426517486572, 9.01751708984375, -1.5068289041519165, 3.324098825454712, -0.17821688950061798, -4.301394939422607, 1.325487732887268, -1.3300023078918457, 5.322543621063232, 6.099685192108154]
    Last 10:  [2.1495187282562256, -1.1287586688995361, -4.557039260864258, 1.957016944885254, 6.339928150177002, 8.688642501831055, 4.975260257720947, -1.811781883239746, -4.587636947631836, -1.24509859085083]
  WEIGHT: [640, 1024] | μ=0.000007

008_model.layers.0.self_attn: model.layers.0.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.280524 σ=7.449708
    First 10: [-2.8339426517486572, 9.01751708984375, -1.5068289041519165, 3.324098825454712, -0.17821688950061798, -4.301394939422607, 1.325487732887268, -1.3300023078918457, 5.322543621063232, 6.099685192108154]
    Last 10:  [2.1495187282562256, -1.1287586688995361, -4.557039260864258, 1.957016944885254, 6.339928150177002, 8.688642501831055, 4.975260257720947, -1.811781883239746, -4.587636947631836, -1.24509859085083]

009_model.layers.0.post_attention_layernorm: model.layers.0.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.280524 σ=7.449708
    First 10: [-2.8339426517486572, 9.01751708984375, -1.5068289041519165, 3.324098825454712, -0.17821688950061798, -4.301394939422607, 1.325487732887268, -1.3300023078918457, 5.322543621063232, 6.099685192108154]
    Last 10:  [2.1495187282562256, -1.1287586688995361, -4.557039260864258, 1.957016944885254, 6.339928150177002, 8.688642501831055, 4.975260257720947, -1.811781883239746, -4.587636947631836, -1.24509859085083]
  OUT[0]: [1, 4, 640] | μ=0.216064 σ=5.333917
    First 10: [-0.0490274615585804, 0.2558463513851166, -0.09280306845903397, 0.2116265445947647, -0.005303049925714731, -0.21729041635990143, 0.07062765955924988, -0.042336855083703995, 0.40147092938423157, 0.2701442837715149]
    Last 10:  [0.04692726209759712, -0.03696377947926521, -1.6453635692596436, 0.0714816302061081, 0.23689450323581696, 0.2553468942642212, 0.17128142714500427, -0.015973711386322975, -0.16371504962444305, -1.0831139087677002]
  WEIGHT: [640] | μ=-0.361111

010_model.layers.0.pre_feedforward_layernorm: model.layers.0.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.204784 σ=5.431576
    First 10: [-0.41034239530563354, 0.5260605216026306, 0.2453506588935852, 0.5266190767288208, 0.11822342127561569, -0.2964245676994324, 0.17408108711242676, 0.05262411758303642, 0.4499164819717407, 0.26266515254974365]
    Last 10:  [0.6460306644439697, -0.9140017032623291, -1.790507197380066, -0.348508358001709, 0.13807332515716553, -0.030308067798614502, -0.0031997114419937134, -1.0597723722457886, 1.6027135848999023, -0.7758418321609497]
  OUT[0]: [1, 4, 640] | μ=0.000537 σ=1.386575
    First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
    Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
  WEIGHT: [640] | μ=4.837134

011_model.layers.0.mlp.gate_proj: model.layers.0.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.000537 σ=1.386575
    First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
    Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
  OUT[0]: [1, 4, 2048] | μ=-0.667014 σ=1.205215
    First 10: [-0.72669917345047, -1.898198127746582, 0.5486508011817932, -1.5706493854522705, -0.07239216566085815, -2.1983630657196045, -2.428624153137207, -0.8708949089050293, -1.4670095443725586, 0.9035918712615967]
    Last 10:  [1.127816915512085, -0.9413745999336243, -3.129889965057373, 1.5869991779327393, -0.5720880627632141, -1.2598730325698853, 0.22689089179039001, -0.9452670812606812, -0.8135976791381836, -0.32619887590408325]
  WEIGHT: [2048, 640] | μ=-0.000155

012_model.layers.0.mlp.act_fn: model.layers.0.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.667014 σ=1.205215
    First 10: [-0.72669917345047, -1.898198127746582, 0.5486508011817932, -1.5706493854522705, -0.07239216566085815, -2.1983630657196045, -2.428624153137207, -0.8708949089050293, -1.4670095443725586, 0.9035918712615967]
    Last 10:  [1.127816915512085, -0.9413745999336243, -3.129889965057373, 1.5869991779327393, -0.5720880627632141, -1.2598730325698853, 0.22689089179039001, -0.9452670812606812, -0.8135976791381836, -0.32619887590408325]
  OUT[0]: [1, 4, 2048] | μ=0.091257 σ=0.491099
    First 10: [-0.1698956936597824, -0.0547233410179615, 0.3886277973651886, -0.09150096774101257, -0.034107208251953125, -0.030427876859903336, -0.01799318566918373, -0.1672370731830597, -0.10465618968009949, 0.7380202412605286]
    Last 10:  [0.9813459515571594, -0.16323119401931763, -0.0023764432407915592, 1.4975306987762451, -0.16228988766670227, -0.131072536110878, 0.13380709290504456, -0.1629662662744522, -0.1692659705877304, -0.12139410525560379]

013_model.layers.0.mlp.up_proj: model.layers.0.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.000537 σ=1.386575
    First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
    Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
  OUT[0]: [1, 4, 2048] | μ=-0.010635 σ=1.031289
    First 10: [0.011507511138916016, 1.0420186519622803, 0.839897632598877, -2.1196374893188477, -1.6762338876724243, 1.1423001289367676, -0.15204039216041565, -1.172397494316101, 0.0031760111451148987, -0.05712355673313141]
    Last 10:  [-1.1421189308166504, -0.577163815498352, 0.19567438960075378, -0.7690945863723755, -0.37523365020751953, -0.21140074729919434, -1.7868062257766724, -0.3990073800086975, 1.549891710281372, -0.8119871020317078]
  WEIGHT: [2048, 640] | μ=-0.000002

014_model.layers.0.mlp.down_proj: model.layers.0.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.021364 σ=1.561842
    First 10: [-0.0019550765864551067, -0.05702274292707443, 0.3264075815677643, 0.1939488798379898, 0.05717165768146515, -0.034757766872644424, 0.002735690912231803, 0.19606833159923553, -0.0003323892306070775, -0.042158342897892]
    Last 10:  [-1.1208138465881348, 0.0942111387848854, -0.00046500907046720386, -1.151742696762085, 0.060896627604961395, 0.027708832174539566, -0.23908734321594238, 0.06502474099397659, -0.2623439133167267, 0.09857045114040375]
  OUT[0]: [1, 4, 640] | μ=0.039222 σ=1.005863
    First 10: [0.6804015040397644, -2.12420916557312, 0.6089076995849609, 0.8251789808273315, -0.2686074376106262, 1.042865514755249, -0.7763306498527527, -1.0072749853134155, -0.09280750155448914, 0.5881297588348389]
    Last 10:  [-0.7066440582275391, -0.32972604036331177, 0.15044572949409485, 0.05314284563064575, 0.4384809136390686, -0.34976643323898315, 0.6943364143371582, 0.4238384962081909, -0.35441577434539795, -2.0779619216918945]
  WEIGHT: [640, 2048] | μ=0.000005

015_model.layers.0.mlp: model.layers.0.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.000537 σ=1.386575
    First 10: [-0.4463714063167572, 0.5908092856407166, 0.2524663805961609, 1.4739466905593872, 0.13764072954654694, -0.31547942757606506, 0.17810621857643127, 0.06126723438501358, 0.4470919668674469, 0.2702830135822296]
    Last 10:  [0.7281925082206726, -0.9968307018280029, -1.1836605072021484, -0.3864612281322479, 0.148062065243721, -0.03490128740668297, -0.003782107727602124, -1.117069125175476, 1.7284249067306519, -0.2682627737522125]
  OUT[0]: [1, 4, 640] | μ=0.039222 σ=1.005863
    First 10: [0.6804015040397644, -2.12420916557312, 0.6089076995849609, 0.8251789808273315, -0.2686074376106262, 1.042865514755249, -0.7763306498527527, -1.0072749853134155, -0.09280750155448914, 0.5881297588348389]
    Last 10:  [-0.7066440582275391, -0.32972604036331177, 0.15044572949409485, 0.05314284563064575, 0.4384809136390686, -0.34976643323898315, 0.6943364143371582, 0.4238384962081909, -0.35441577434539795, -2.0779619216918945]

016_model.layers.0.post_feedforward_layernorm: model.layers.0.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.039222 σ=1.005863
    First 10: [0.6804015040397644, -2.12420916557312, 0.6089076995849609, 0.8251789808273315, -0.2686074376106262, 1.042865514755249, -0.7763306498527527, -1.0072749853134155, -0.09280750155448914, 0.5881297588348389]
    Last 10:  [-0.7066440582275391, -0.32972604036331177, 0.15044572949409485, 0.05314284563064575, 0.4384809136390686, -0.34976643323898315, 0.6943364143371582, 0.4238384962081909, -0.35441577434539795, -2.0779619216918945]
  OUT[0]: [1, 4, 640] | μ=0.953321 σ=56.678665
    First 10: [0.16077499091625214, -0.6173266172409058, 0.4341249465942383, 0.4364752173423767, -0.11782149225473404, 0.5955226421356201, -0.5246955752372742, -0.4500366747379303, -0.06944462656974792, 0.31428369879722595]
    Last 10:  [-0.548168420791626, -0.31990712881088257, 1.8832556009292603, 0.05056198313832283, 0.4598255157470703, -0.3420562744140625, 0.6352964639663696, 0.11427898705005646, -0.38371118903160095, -67.89466857910156]
  WEIGHT: [640] | μ=0.805890

017_model.layers.1.input_layernorm: model.layers.1.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=1.158105 σ=59.186935
    First 10: [-0.2495674043893814, -0.09126609563827515, 0.6794756054878235, 0.9630942940711975, 0.00040192902088165283, 0.29909807443618774, -0.3506144881248474, -0.39741256833076477, 0.3804718554019928, 0.576948881149292]
    Last 10:  [0.09786224365234375, -1.2339088916778564, 0.09274840354919434, -0.29794636368751526, 0.5978988409042358, -0.372364342212677, 0.6320967674255371, -0.9454934000968933, 1.219002366065979, -68.6705093383789]
  OUT[0]: [1, 4, 640] | μ=-0.131824 σ=7.272413
    First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
    Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
  WEIGHT: [640] | μ=24.677223

018_model.layers.1.self_attn.q_proj: model.layers.1.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.131824 σ=7.272413
    First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
    Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
  OUT[0]: [1, 4, 1024] | μ=-0.172085 σ=17.123793
    First 10: [0.001455068588256836, 0.3045678734779358, 0.5205649733543396, 0.19726812839508057, 0.5817791819572449, -0.5869358777999878, 0.756453275680542, -0.10041530430316925, -0.03724992275238037, -0.0005526244640350342]
    Last 10:  [-0.07848024368286133, -3.776020050048828, -0.15801990032196045, -0.27927279472351074, -1.1166198253631592, -1.2101819515228271, 0.6111103296279907, -3.3597755432128906, 3.5492029190063477, 4.276642799377441]
  WEIGHT: [1024, 640] | μ=-0.000090

019_model.layers.1.self_attn.k_proj: model.layers.1.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.131824 σ=7.272413
    First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
    Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
  OUT[0]: [1, 4, 256] | μ=-0.449574 σ=10.299052
    First 10: [-0.002749115228652954, -0.014828629791736603, -0.0026351213455200195, -0.012497276067733765, 0.006182482466101646, 0.009873345494270325, -0.005913525819778442, -0.012482799589633942, -0.01061733067035675, -0.00227385014295578]
    Last 10:  [-7.018460273742676, -15.688098907470703, -7.955350399017334, 4.832880020141602, -8.894634246826172, -13.616569519042969, 1.6221272945404053, -9.751404762268066, 6.787450790405273, -8.939099311828613]
  WEIGHT: [256, 640] | μ=0.000090

020_model.layers.1.self_attn.v_proj: model.layers.1.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.131824 σ=7.272413
    First 10: [-0.06789010018110275, -0.025889862328767776, 0.13665103912353516, 0.39757484197616577, 0.00010635914804879576, 0.06648419052362442, -0.06939959526062012, -0.10600531101226807, 0.07812854647636414, 0.13374173641204834]
    Last 10:  [0.470498651266098, -4.5873589515686035, 0.07988513261079788, -1.1192874908447266, 2.2111976146698, -1.4785789251327515, 2.4853134155273438, -5.079416275024414, 4.15230131149292, -24.31025505065918]
  OUT[0]: [1, 4, 256] | μ=-0.408808 σ=6.869521
    First 10: [-0.1930898129940033, 0.703927755355835, -0.025813505053520203, -0.04328576475381851, 0.4046694040298462, 0.4346885681152344, -0.43850207328796387, -0.13784170150756836, 0.3647642433643341, 0.2999514639377594]
    Last 10:  [11.794339179992676, -1.5482444763183594, 4.548500061035156, 1.098741054534912, 0.5812458395957947, -4.630569934844971, 0.211034893989563, 3.003812789916992, -0.725727915763855, -1.6081442832946777]
  WEIGHT: [256, 640] | μ=-0.000044

021_model.layers.1.self_attn.q_norm: model.layers.1.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.172085 σ=17.123793
    First 10: [0.001455068588256836, 0.3045678734779358, 0.5205649733543396, 0.19726812839508057, 0.5817791819572449, -0.5869358777999878, 0.756453275680542, -0.10041530430316925, -0.03724992275238037, -0.0005526244640350342]
    Last 10:  [-0.07848024368286133, -3.776020050048828, -0.15801990032196045, -0.27927279472351074, -1.1166198253631592, -1.2101819515228271, 0.6111103296279907, -3.3597755432128906, 3.5492029190063477, 4.276642799377441]
  OUT[0]: [1, 4, 4, 256] | μ=0.152854 σ=3.089273
    First 10: [0.003626024816185236, 1.0341506004333496, 1.7830668687820435, 0.6071447134017944, 1.7270421981811523, -1.9229950904846191, 1.4982993602752686, -0.2891155779361725, -0.14793092012405396, -0.0022275615483522415]
    Last 10:  [-0.03866415098309517, -1.2068872451782227, -0.07613452523946762, -0.11029685288667679, -0.47434157133102417, -0.4795953929424286, 0.2687237858772278, -1.372518539428711, 1.4499026536941528, 1.8457444906234741]
  WEIGHT: [256] | μ=1.696144

022_model.layers.1.self_attn.k_norm: model.layers.1.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.449574 σ=10.299052
    First 10: [-0.002749115228652954, -0.014828629791736603, -0.0026351213455200195, -0.012497276067733765, 0.006182482466101646, 0.009873345494270325, -0.005913525819778442, -0.012482799589633942, -0.01061733067035675, -0.00227385014295578]
    Last 10:  [-7.018460273742676, -15.688098907470703, -7.955350399017334, 4.832880020141602, -8.894634246826172, -13.616569519042969, 1.6221272945404053, -9.751404762268066, 6.787450790405273, -8.939099311828613]
  OUT[0]: [1, 1, 4, 256] | μ=-0.149437 σ=2.266396
    First 10: [-0.0024325994309037924, -0.01985745318233967, -0.003616052446886897, -0.015493613667786121, 0.0073283580131828785, 0.014342959970235825, -0.005029808729887009, -0.016716081649065018, -0.013414135202765465, -0.002415540162473917]
    Last 10:  [-2.336909294128418, -5.823294162750244, -2.7769036293029785, 1.9543578624725342, -3.292668104171753, -4.739317893981934, 0.6037521362304688, -3.550976514816284, 2.799384593963623, -3.2371909618377686]
  WEIGHT: [256] | μ=1.660587

023_model.layers.1.self_attn.o_proj: model.layers.1.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.059337 σ=0.904794
    First 10: [-0.1930898129940033, 0.703927755355835, -0.025813505053520203, -0.04328576475381851, 0.4046694040298462, 0.4346885681152344, -0.43850207328796387, -0.13784170150756836, 0.3647642433643341, 0.2999514639377594]
    Last 10:  [2.724358558654785, -0.04271163418889046, 0.23944006860256195, 0.05596053600311279, 0.2332592010498047, -0.7823140621185303, 0.36907970905303955, -0.10994230955839157, -0.051993563771247864, -0.4925673007965088]
  OUT[0]: [1, 4, 640] | μ=0.133937 σ=3.610773
    First 10: [-0.032756257802248, 0.42673060297966003, 0.20813901722431183, -0.0850721225142479, 0.024427227675914764, 0.42045068740844727, -0.31599098443984985, -0.04048251360654831, 0.46051836013793945, 0.4792925715446472]
    Last 10:  [0.8076938390731812, 0.09550453722476959, -0.4400442838668823, 0.29874366521835327, 0.26817309856414795, 0.6036581993103027, 0.013121120631694794, 0.6340376734733582, -0.5236000418663025, 8.386391639709473]
  WEIGHT: [640, 1024] | μ=-0.000010

024_model.layers.1.self_attn: model.layers.1.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.133937 σ=3.610773
    First 10: [-0.032756257802248, 0.42673060297966003, 0.20813901722431183, -0.0850721225142479, 0.024427227675914764, 0.42045068740844727, -0.31599098443984985, -0.04048251360654831, 0.46051836013793945, 0.4792925715446472]
    Last 10:  [0.8076938390731812, 0.09550453722476959, -0.4400442838668823, 0.29874366521835327, 0.26817309856414795, 0.6036581993103027, 0.013121120631694794, 0.6340376734733582, -0.5236000418663025, 8.386391639709473]

025_model.layers.1.post_attention_layernorm: model.layers.1.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.133937 σ=3.610773
    First 10: [-0.032756257802248, 0.42673060297966003, 0.20813901722431183, -0.0850721225142479, 0.024427227675914764, 0.42045068740844727, -0.31599098443984985, -0.04048251360654831, 0.46051836013793945, 0.4792925715446472]
    Last 10:  [0.8076938390731812, 0.09550453722476959, -0.4400442838668823, 0.29874366521835327, 0.26817309856414795, 0.6036581993103027, 0.013121120631694794, 0.6340376734733582, -0.5236000418663025, 8.386391639709473]
  OUT[0]: [1, 4, 640] | μ=1.137094 σ=36.479416
    First 10: [-0.005863717291504145, 0.10485374927520752, 0.1501697152853012, -0.031876277178525925, 0.009240099228918552, 0.20948316156864166, -0.18926937878131866, -0.014359270222485065, 0.3322582542896271, 0.19834774732589722]
    Last 10:  [0.20410719513893127, 0.03427312150597572, -3.931753158569336, 0.10012345761060715, 0.10703735798597336, 0.20852768421173096, 0.004086349159479141, 0.05787626653909683, -0.19024401903152466, 123.92533874511719]
  WEIGHT: [640] | μ=2.747667

026_model.layers.1.pre_feedforward_layernorm: model.layers.1.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=2.295199 σ=82.305878
    First 10: [-0.2554311156272888, 0.013587653636932373, 0.8296453356742859, 0.9312180280685425, 0.009642028249800205, 0.5085812211036682, -0.5398838520050049, -0.41177183389663696, 0.7127301096916199, 0.7752966284751892]
    Last 10:  [0.301969438791275, -1.1996357440948486, -3.8390047550201416, -0.19782289862632751, 0.7049362063407898, -0.16383665800094604, 0.6361831426620483, -0.8876171112060547, 1.0287582874298096, 55.25482940673828]
  OUT[0]: [1, 4, 640] | μ=0.019897 σ=0.753058
    First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
    Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
  WEIGHT: [640] | μ=28.944311

027_model.layers.1.mlp.gate_proj: model.layers.1.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.019897 σ=0.753058
    First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
    Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
  OUT[0]: [1, 4, 2048] | μ=-0.280730 σ=0.806038
    First 10: [-0.1903638392686844, -0.1722165197134018, 0.004699863493442535, -0.7316675186157227, -0.015919163823127747, 0.3490030765533447, -0.31952163577079773, -0.38248202204704285, -0.10825420916080475, -0.2816339135169983]
    Last 10:  [-1.3022987842559814, 0.10207065939903259, -0.5640626549720764, -0.7606185078620911, 0.5842303037643433, -0.8458268046379089, -0.18360048532485962, 0.922817587852478, -0.8576288819313049, -0.40376460552215576]
  WEIGHT: [2048, 640] | μ=-0.000077

028_model.layers.1.mlp.act_fn: model.layers.1.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.280730 σ=0.806038
    First 10: [-0.1903638392686844, -0.1722165197134018, 0.004699863493442535, -0.7316675186157227, -0.015919163823127747, 0.3490030765533447, -0.31952163577079773, -0.38248202204704285, -0.10825420916080475, -0.2816339135169983]
    Last 10:  [-1.3022987842559814, 0.10207065939903259, -0.5640626549720764, -0.7606185078620911, 0.5842303037643433, -0.8458268046379089, -0.18360048532485962, 0.922817587852478, -0.8576288819313049, -0.40376460552215576]
  OUT[0]: [1, 4, 2048] | μ=0.067388 σ=0.410792
    First 10: [-0.08081215620040894, -0.07433472573757172, 0.0023587439209222794, -0.16994653642177582, -0.007858486846089363, 0.22212080657482147, -0.11971691995859146, -0.13427741825580597, -0.04946107044816017, -0.10958912968635559]
    Last 10:  [-0.12578149139881134, 0.05518443509936333, -0.16154912114143372, -0.1700265109539032, 0.42088884115219116, -0.16827009618282318, -0.07842777669429779, 0.7583834528923035, -0.1678110510110855, -0.138576939702034]

029_model.layers.1.mlp.up_proj: model.layers.1.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.019897 σ=0.753058
    First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
    Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
  OUT[0]: [1, 4, 2048] | μ=-0.001219 σ=0.665121
    First 10: [0.007821436040103436, -0.19044017791748047, 0.09799472987651825, 0.13676457107067108, -0.15552160143852234, -0.11899124085903168, 0.07684407383203506, -0.013862053863704205, -0.3710987865924835, 0.2844812870025635]
    Last 10:  [0.9240207672119141, 0.512592077255249, -0.366669237613678, 0.09850356727838516, -0.019777856767177582, 0.7324585318565369, 0.23628492653369904, 0.27160388231277466, 0.3232074975967407, 0.18295952677726746]
  WEIGHT: [2048, 640] | μ=0.000028

030_model.layers.1.mlp.down_proj: model.layers.1.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.000663 σ=0.359935
    First 10: [-0.0006320670945569873, 0.01415631826967001, 0.00023114446958061308, -0.023242665454745293, 0.0012221644865348935, -0.02643042989075184, -0.009199535474181175, 0.0018613608554005623, 0.01835494302213192, -0.03117605671286583]
    Last 10:  [-0.11622471362352371, 0.02828710339963436, 0.059235092252492905, -0.0167482178658247, -0.008324279449880123, -0.12325086444616318, -0.018531301990151405, 0.20597988367080688, -0.054237790405750275, -0.025353971868753433]
  OUT[0]: [1, 4, 640] | μ=-0.001554 σ=0.288509
    First 10: [0.048990920186042786, -0.021225783973932266, -0.009275750257074833, -0.007051009684801102, -0.041074201464653015, 0.04439251124858856, -0.09249815344810486, -0.018146635964512825, -0.022141508758068085, 0.010069301351904869]
    Last 10:  [-0.0689162164926529, 0.14432072639465332, -0.04976645112037659, -0.18366581201553345, -0.07328064739704132, -0.25845474004745483, -0.042715590447187424, -0.21431973576545715, 0.14199259877204895, -0.3241061568260193]
  WEIGHT: [640, 2048] | μ=-0.000008

031_model.layers.1.mlp: model.layers.1.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.019897 σ=0.753058
    First 10: [-0.06035635247826576, 0.0033472785726189613, 0.16684137284755707, 0.34644508361816406, 0.002229859586805105, 0.11591223627328873, -0.11173708736896515, -0.09729842096567154, 0.14273257553577423, 0.17799939215183258]
    Last 10:  [0.2425084114074707, -0.9563305974006653, -0.3372100591659546, -0.16003760695457458, 0.5265810489654541, -0.1306079924106598, 0.5372087955474854, -0.6787663698196411, 0.7806230783462524, 3.568725824356079]
  OUT[0]: [1, 4, 640] | μ=-0.001554 σ=0.288509
    First 10: [0.048990920186042786, -0.021225783973932266, -0.009275750257074833, -0.007051009684801102, -0.041074201464653015, 0.04439251124858856, -0.09249815344810486, -0.018146635964512825, -0.022141508758068085, 0.010069301351904869]
    Last 10:  [-0.0689162164926529, 0.14432072639465332, -0.04976645112037659, -0.18366581201553345, -0.07328064739704132, -0.25845474004745483, -0.042715590447187424, -0.21431973576545715, 0.14199259877204895, -0.3241061568260193]

032_model.layers.1.post_feedforward_layernorm: model.layers.1.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.001554 σ=0.288509
    First 10: [0.048990920186042786, -0.021225783973932266, -0.009275750257074833, -0.007051009684801102, -0.041074201464653015, 0.04439251124858856, -0.09249815344810486, -0.018146635964512825, -0.022141508758068085, 0.010069301351904869]
    Last 10:  [-0.0689162164926529, 0.14432072639465332, -0.04976645112037659, -0.18366581201553345, -0.07328064739704132, -0.25845474004745483, -0.042715590447187424, -0.21431973576545715, 0.14199259877204895, -0.3241061568260193]
  OUT[0]: [1, 4, 640] | μ=0.556356 σ=27.924011
    First 10: [0.24489133059978485, -0.15261176228523254, -0.2940796911716461, -0.080872543156147, -0.48376286029815674, 0.8408840298652649, -2.2738535404205322, -0.25659677386283875, -0.7489781379699707, 0.162377268075943]
    Last 10:  [-0.17841671407222748, 0.5911997556686401, -4.173530101776123, -0.620212197303772, -0.3332844376564026, -0.9440721869468689, -0.161939337849617, -0.12814737856388092, 0.6250165104866028, -22.430341720581055]
  WEIGHT: [640] | μ=1.878360

033_model.layers.2.input_layernorm: model.layers.2.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=2.851555 σ=107.769287
    First 10: [-0.010539785027503967, -0.13902410864830017, 0.5355656147003174, 0.8503454923629761, -0.4741208255290985, 1.349465250968933, -2.813737392425537, -0.6683685779571533, -0.03624802827835083, 0.9376739263534546]
    Last 10:  [0.12355272471904755, -0.6084359884262085, -8.012535095214844, -0.8180351257324219, 0.3716517686843872, -1.107908844947815, 0.47424381971359253, -1.0157644748687744, 1.6537747383117676, 32.824485778808594]
  OUT[0]: [1, 4, 640] | μ=0.044388 σ=1.581807
    First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
    Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
  WEIGHT: [640] | μ=28.830969

034_model.layers.2.self_attn.q_proj: model.layers.2.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.044388 σ=1.581807
    First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
    Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
  OUT[0]: [1, 4, 1024] | μ=0.034577 σ=2.643258
    First 10: [0.03415389358997345, -0.2231147140264511, 0.1425134390592575, -0.15751111507415771, 0.5739494562149048, 0.4084988236427307, 0.02576206624507904, -0.061044082045555115, -0.29220426082611084, 0.05253136157989502]
    Last 10:  [0.24127504229545593, 0.07862727344036102, 0.17503884434700012, 0.6170549392700195, -0.6452509164810181, 0.049342215061187744, -0.5003542900085449, -0.003355562686920166, 0.20023030042648315, 0.6399357318878174]
  WEIGHT: [1024, 640] | μ=-0.000122

035_model.layers.2.self_attn.k_proj: model.layers.2.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.044388 σ=1.581807
    First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
    Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
  OUT[0]: [1, 4, 256] | μ=-0.100959 σ=1.934242
    First 10: [-0.04070580005645752, 0.02559863030910492, -0.16828960180282593, -0.02843756228685379, 0.006929993629455566, 0.03646424412727356, -0.006288886070251465, -0.15539485216140747, 0.030380934476852417, 0.056104954332113266]
    Last 10:  [5.311874866485596, -1.214493989944458, -9.939475059509277, 1.7636876106262207, 1.1449414491653442, -2.0439324378967285, 1.882590889930725, -2.367800235748291, -2.0842020511627197, -0.03061804175376892]
  WEIGHT: [256, 640] | μ=-0.000123

036_model.layers.2.self_attn.v_proj: model.layers.2.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.044388 σ=1.581807
    First 10: [-0.0025890960823744535, -0.03724030405282974, 0.06016132980585098, 0.258222371339798, -0.0901307761669159, 0.20656025409698486, -0.347333699464798, -0.14025820791721344, -0.0036243717186152935, 0.18172506988048553]
    Last 10:  [0.11133069545030594, -0.37839236855506897, -0.2782216966152191, -0.619537353515625, 0.21264320611953735, -0.7502657175064087, 0.32377520203590393, -0.9602053165435791, 0.9416478872299194, 6.577803611755371]
  OUT[0]: [1, 4, 256] | μ=0.022111 σ=1.073037
    First 10: [-0.10747533291578293, -0.2006467878818512, -0.16723695397377014, -0.16673851013183594, 0.07862196862697601, -0.0007174760103225708, 0.0013808012008666992, -0.144599050283432, 0.04181528091430664, 0.06180395185947418]
    Last 10:  [0.6358954906463623, -0.13744869828224182, -0.10551083087921143, 0.3188331723213196, 0.36714404821395874, -0.3428195118904114, -0.4894849359989166, 0.7987401485443115, 0.253814160823822, -0.11407024413347244]
  WEIGHT: [256, 640] | μ=-0.000002

037_model.layers.2.self_attn.q_norm: model.layers.2.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.034577 σ=2.643258
    First 10: [0.03415389358997345, -0.2231147140264511, 0.1425134390592575, -0.15751111507415771, 0.5739494562149048, 0.4084988236427307, 0.02576206624507904, -0.061044082045555115, -0.29220426082611084, 0.05253136157989502]
    Last 10:  [0.24127504229545593, 0.07862727344036102, 0.17503884434700012, 0.6170549392700195, -0.6452509164810181, 0.049342215061187744, -0.5003542900085449, -0.003355562686920166, 0.20023030042648315, 0.6399357318878174]
  OUT[0]: [1, 4, 4, 256] | μ=0.023420 σ=1.366390
    First 10: [0.058887600898742676, -0.64952152967453, 0.23641671240329742, -0.4916297197341919, 1.5433884859085083, 1.3044458627700806, 0.05713716894388199, -0.21618181467056274, -0.9225626587867737, 0.12738655507564545]
    Last 10:  [0.18464112281799316, 0.04977922514081001, 0.11395949125289917, 0.32571765780448914, -0.46431252360343933, 0.032285403460264206, -0.40086886286735535, -0.0019245706498622894, 0.13395436108112335, 0.4980786442756653]
  WEIGHT: [256] | μ=0.776192

038_model.layers.2.self_attn.k_norm: model.layers.2.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.100959 σ=1.934242
    First 10: [-0.04070580005645752, 0.02559863030910492, -0.16828960180282593, -0.02843756228685379, 0.006929993629455566, 0.03646424412727356, -0.006288886070251465, -0.15539485216140747, 0.030380934476852417, 0.056104954332113266]
    Last 10:  [5.311874866485596, -1.214493989944458, -9.939475059509277, 1.7636876106262207, 1.1449414491653442, -2.0439324378967285, 1.882590889930725, -2.367800235748291, -2.0842020511627197, -0.03061804175376892]
  OUT[0]: [1, 1, 4, 256] | μ=0.071700 σ=3.341894
    First 10: [-0.03898235410451889, 0.02625017613172531, -0.22932708263397217, -0.036701835691928864, 0.009443454444408417, 0.06270648539066315, -0.013599465601146221, -0.10867804288864136, 0.043694090098142624, 0.08704568445682526]
    Last 10:  [3.7353341579437256, -1.5010356903076172, -5.9545392990112305, 2.3028018474578857, 1.1200815439224243, -2.209407091140747, 1.6338379383087158, -3.3300955295562744, -2.4386627674102783, -0.027402766048908234]
  WEIGHT: [256] | μ=1.072401

039_model.layers.2.self_attn.o_proj: model.layers.2.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.028122 σ=0.576621
    First 10: [-0.10747533291578293, -0.2006467878818512, -0.16723695397377014, -0.16673851013183594, 0.07862196862697601, -0.0007174760103225708, 0.0013808012008666992, -0.144599050283432, 0.04181528091430664, 0.06180395185947418]
    Last 10:  [0.4177733361721039, 0.15704254806041718, -0.1114543229341507, 0.14303936064243317, -0.2807626724243164, 0.03595169633626938, -0.13535909354686737, 0.6102434396743774, 0.19527554512023926, 0.4993634819984436]
  OUT[0]: [1, 4, 640] | μ=0.021032 σ=0.689930
    First 10: [0.14985978603363037, 0.5789029002189636, -0.15678927302360535, -0.3829214572906494, -0.1666674017906189, -0.13670817017555237, 0.8231567144393921, 0.04849042743444443, -0.5143294930458069, 0.5368988513946533]
    Last 10:  [0.8777404427528381, -0.5002163648605347, -0.5048267841339111, 0.4002346694469452, 0.5999507904052734, 0.02103293500840664, 0.8238273859024048, -0.17568057775497437, -0.36470288038253784, 0.966009795665741]
  WEIGHT: [640, 1024] | μ=0.000049

040_model.layers.2.self_attn: model.layers.2.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.021032 σ=0.689930
    First 10: [0.14985978603363037, 0.5789029002189636, -0.15678927302360535, -0.3829214572906494, -0.1666674017906189, -0.13670817017555237, 0.8231567144393921, 0.04849042743444443, -0.5143294930458069, 0.5368988513946533]
    Last 10:  [0.8777404427528381, -0.5002163648605347, -0.5048267841339111, 0.4002346694469452, 0.5999507904052734, 0.02103293500840664, 0.8238273859024048, -0.17568057775497437, -0.36470288038253784, 0.966009795665741]

041_model.layers.2.post_attention_layernorm: model.layers.2.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.021032 σ=0.689930
    First 10: [0.14985978603363037, 0.5789029002189636, -0.15678927302360535, -0.3829214572906494, -0.1666674017906189, -0.13670817017555237, 0.8231567144393921, 0.04849042743444443, -0.5143294930458069, 0.5368988513946533]
    Last 10:  [0.8777404427528381, -0.5002163648605347, -0.5048267841339111, 0.4002346694469452, 0.5999507904052734, 0.02103293500840664, 0.8238273859024048, -0.17568057775497437, -0.36470288038253784, 0.966009795665741]
  OUT[0]: [1, 4, 640] | μ=0.432045 σ=11.733462
    First 10: [0.048595644533634186, 0.23778271675109863, -0.286325603723526, -0.28697413206100464, -0.1128959208726883, -0.14678458869457245, 1.2990515232086182, 0.035990919917821884, -1.0025554895401, 0.5087658762931824]
    Last 10:  [0.4946292042732239, -0.5539035201072693, -9.604140281677246, 0.3755282461643219, 0.7395666837692261, 0.02109762281179428, 0.7242265939712524, -0.029700148850679398, -0.4387839138507843, 13.587505340576172]
  WEIGHT: [640] | μ=0.800125

042_model.layers.2.pre_feedforward_layernorm: model.layers.2.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=3.283600 σ=115.910568
    First 10: [0.03805585950613022, 0.09875860810279846, 0.24924001097679138, 0.5633713603019714, -0.587016761302948, 1.2026807069778442, -1.514685869216919, -0.6323776841163635, -1.0388035774230957, 1.4464397430419922]
    Last 10:  [0.6181819438934326, -1.162339448928833, -17.616676330566406, -0.4425068795681, 1.1112184524536133, -1.0868111848831177, 1.1984703540802002, -1.0454646348953247, 1.2149908542633057, 46.411991119384766]
  OUT[0]: [1, 4, 640] | μ=0.006670 σ=0.717162
    First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
    Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
  WEIGHT: [640] | μ=22.039240

043_model.layers.2.mlp.gate_proj: model.layers.2.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.006670 σ=0.717162
    First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
    Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
  OUT[0]: [1, 4, 2048] | μ=-0.410459 σ=0.725702
    First 10: [-0.14806775748729706, -0.14914625883102417, 0.032802388072013855, 0.03333088755607605, 0.03008955344557762, -0.3349064886569977, -0.47867676615715027, 0.07680312544107437, -0.08538328111171722, -0.13642045855522156]
    Last 10:  [0.0741187334060669, 0.4117686152458191, -0.47728630900382996, 0.12611374258995056, -0.3743247985839844, 0.3294275999069214, 0.813797652721405, -0.0584227591753006, -2.4035611152648926, -0.04887950420379639]
  WEIGHT: [2048, 640] | μ=-0.000000

044_model.layers.2.mlp.act_fn: model.layers.2.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.410459 σ=0.725702
    First 10: [-0.14806775748729706, -0.14914625883102417, 0.032802388072013855, 0.03333088755607605, 0.03008955344557762, -0.3349064886569977, -0.47867676615715027, 0.07680312544107437, -0.08538328111171722, -0.13642045855522156]
    Last 10:  [0.0741187334060669, 0.4117686152458191, -0.47728630900382996, 0.12611374258995056, -0.3743247985839844, 0.3294275999069214, 0.813797652721405, -0.0584227591753006, -2.4035611152648926, -0.04887950420379639]
  OUT[0]: [1, 4, 2048] | μ=-0.005121 σ=0.290989
    First 10: [-0.06531945616006851, -0.06573176383972168, 0.016830377280712128, 0.01710856519639492, 0.01540591660887003, -0.12353335320949554, -0.15131688117980957, 0.04075248911976814, -0.039786774665117264, -0.06080877408385277]
    Last 10:  [0.03924897685647011, 0.27165406942367554, -0.15111340582370758, 0.06938505917787552, -0.1325472891330719, 0.2072339504957199, 0.6445366740226746, -0.027850478887557983, -0.019113343209028244, -0.023486977443099022]

045_model.layers.2.mlp.up_proj: model.layers.2.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.006670 σ=0.717162
    First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
    Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
  OUT[0]: [1, 4, 2048] | μ=-0.003614 σ=0.595459
    First 10: [-0.12416496872901917, 0.04907379299402237, -0.14548656344413757, -0.29387781023979187, -0.06467214226722717, 0.13093741238117218, -0.2734249532222748, -0.005912665277719498, 0.16412962973117828, -0.0791151225566864]
    Last 10:  [0.05273684859275818, 0.35549306869506836, 0.8563618659973145, 0.2688782215118408, -0.7359681129455566, 0.7246559858322144, 0.45818108320236206, -0.5504655838012695, -0.10353678464889526, 0.3993242383003235]
  WEIGHT: [2048, 640] | μ=0.000001

046_model.layers.2.mlp.down_proj: model.layers.2.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.001639 σ=0.280641
    First 10: [0.008110388182103634, -0.003225706983357668, -0.002448593731969595, -0.0050278278067708015, -0.0009963336633518338, -0.01617513783276081, 0.04137381166219711, -0.0002409558219369501, -0.006530188489705324, 0.004810893442481756]
    Last 10:  [0.0020698674488812685, 0.09657113999128342, -0.12940776348114014, 0.018656130880117416, 0.09755057841539383, 0.15017332136631012, 0.2953145205974579, 0.015330730006098747, 0.001978934044018388, -0.009378919377923012]
  OUT[0]: [1, 4, 640] | μ=0.002925 σ=0.364637
    First 10: [0.0013268934562802315, -0.01318470761179924, -0.02286485582590103, 0.011747154407203197, 0.0012242470402270555, 0.038976557552814484, -0.027684427797794342, -0.0072929104790091515, -0.0064275446347892284, 0.0015067718923091888]
    Last 10:  [0.07402534782886505, 0.09985014796257019, 0.1316312998533249, -0.09039339423179626, 0.021122772246599197, -0.009412260726094246, -0.12378717958927155, -0.03229038417339325, -0.05691048875451088, -0.33706724643707275]
  WEIGHT: [640, 2048] | μ=0.000007

047_model.layers.2.mlp: model.layers.2.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.006670 σ=0.717162
    First 10: [0.006383512634783983, 0.016797518357634544, 0.021342378109693527, 0.12489894032478333, -0.07953067123889923, 0.15377262234687805, -0.16168366372585297, -0.08604719489812851, -0.0883432999253273, 0.207844540476799]
    Last 10:  [0.30403169989585876, -0.5027519464492798, -0.4726126194000244, -0.22540448606014252, 0.43916377425193787, -0.5178076028823853, 0.5894267559051514, -0.5876298546791077, 0.5121872425079346, 2.2609612941741943]
  OUT[0]: [1, 4, 640] | μ=0.002925 σ=0.364637
    First 10: [0.0013268934562802315, -0.01318470761179924, -0.02286485582590103, 0.011747154407203197, 0.0012242470402270555, 0.038976557552814484, -0.027684427797794342, -0.0072929104790091515, -0.0064275446347892284, 0.0015067718923091888]
    Last 10:  [0.07402534782886505, 0.09985014796257019, 0.1316312998533249, -0.09039339423179626, 0.021122772246599197, -0.009412260726094246, -0.12378717958927155, -0.03229038417339325, -0.05691048875451088, -0.33706724643707275]

048_model.layers.2.post_feedforward_layernorm: model.layers.2.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.002925 σ=0.364637
    First 10: [0.0013268934562802315, -0.01318470761179924, -0.02286485582590103, 0.011747154407203197, 0.0012242470402270555, 0.038976557552814484, -0.027684427797794342, -0.0072929104790091515, -0.0064275446347892284, 0.0015067718923091888]
    Last 10:  [0.07402534782886505, 0.09985014796257019, 0.1316312998533249, -0.09039339423179626, 0.021122772246599197, -0.009412260726094246, -0.12378717958927155, -0.03229038417339325, -0.05691048875451088, -0.33706724643707275]
  OUT[0]: [1, 4, 640] | μ=0.128989 σ=19.335550
    First 10: [0.008209633640944958, -0.11348165571689606, -1.1819368600845337, 0.15876971185207367, 0.01840187981724739, 1.0073938369750977, -0.9669403433799744, -0.13372859358787537, -0.3630426228046417, 0.031125325709581375]
    Last 10:  [0.41185635328292847, 0.9461872577667236, 27.30350685119629, -0.6319677829742432, 0.24449625611305237, -0.08425233513116837, -1.0507475137710571, -0.04186127707362175, -0.5726640224456787, -39.61899948120117]
  WEIGHT: [640] | μ=4.605221

049_model.layers.3.input_layernorm: model.layers.3.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=3.412589 σ=130.642181
    First 10: [0.04626549407839775, -0.014723047614097595, -0.9326968193054199, 0.7221410870552063, -0.5686149001121521, 2.2100744247436523, -2.481626272201538, -0.7661062479019165, -1.401846170425415, 1.477565050125122]
    Last 10:  [1.0300383567810059, -0.21615219116210938, 9.686830520629883, -1.0744746923446655, 1.3557146787643433, -1.1710635423660278, 0.14772284030914307, -1.087325930595398, 0.642326831817627, 6.792991638183594]
  OUT[0]: [1, 4, 640] | μ=-0.008138 σ=1.227236
    First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
    Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
  WEIGHT: [640] | μ=24.891054

050_model.layers.3.self_attn.q_proj: model.layers.3.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.008138 σ=1.227236
    First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
    Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
  OUT[0]: [1, 4, 1024] | μ=0.062938 σ=1.508438
    First 10: [-0.25074583292007446, -0.09411583840847015, 0.6649511456489563, 0.17810159921646118, 0.37636786699295044, -0.050610389560461044, -0.24055519700050354, 0.29217803478240967, -0.5770397186279297, -0.42095810174942017]
    Last 10:  [-1.995829701423645, 1.4491097927093506, 1.3992702960968018, 0.02862408757209778, 0.9588150978088379, -1.9767050743103027, -1.431427001953125, -0.3884492516517639, 0.8695794343948364, 0.9551999568939209]
  WEIGHT: [1024, 640] | μ=0.000061

051_model.layers.3.self_attn.k_proj: model.layers.3.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.008138 σ=1.227236
    First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
    Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
  OUT[0]: [1, 4, 256] | μ=-0.056727 σ=1.464699
    First 10: [0.0008884966373443604, 0.009633071720600128, -0.010251611471176147, -0.004567145369946957, 0.0030151456594467163, 0.006089635193347931, 0.016491279006004333, 0.01666705310344696, -0.024582069367170334, 0.0029260367155075073]
    Last 10:  [-5.540679454803467, 2.2137465476989746, 2.601306915283203, 2.405923843383789, 1.5562833547592163, -4.712998867034912, -2.8254618644714355, -1.679091453552246, 3.1283516883850098, -0.06130021810531616]
  WEIGHT: [256, 640] | μ=0.000066

052_model.layers.3.self_attn.v_proj: model.layers.3.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.008138 σ=1.227236
    First 10: [0.009539620019495487, -0.002784040756523609, -0.056756533682346344, 0.15543751418590546, -0.09493936598300934, 0.23007383942604065, -0.20966970920562744, -0.1263725906610489, -0.08142773807048798, 0.1865135282278061]
    Last 10:  [0.7188964486122131, -0.10378275066614151, 0.19678105413913727, -0.6488581895828247, 0.5368489027023315, -0.5535764694213867, 0.07604572176933289, -1.0979528427124023, 0.2607136368751526, 0.5085694193840027]
  OUT[0]: [1, 4, 256] | μ=0.028558 σ=0.968695
    First 10: [-0.284274160861969, -0.08881960064172745, 0.11178702116012573, -0.00050383061170578, 0.2276776283979416, 0.02515965700149536, 0.03421882912516594, 0.10633990168571472, 0.0369911715388298, -0.03175998479127884]
    Last 10:  [-0.13296908140182495, 0.3866719603538513, -0.4184430241584778, -0.8877232074737549, -0.4196290671825409, 1.2389272451400757, -0.5988315343856812, 0.26545244455337524, 4.780564785003662, -0.6869760155677795]
  WEIGHT: [256, 640] | μ=0.000034

053_model.layers.3.self_attn.q_norm: model.layers.3.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.062938 σ=1.508438
    First 10: [-0.25074583292007446, -0.09411583840847015, 0.6649511456489563, 0.17810159921646118, 0.37636786699295044, -0.050610389560461044, -0.24055519700050354, 0.29217803478240967, -0.5770397186279297, -0.42095810174942017]
    Last 10:  [-1.995829701423645, 1.4491097927093506, 1.3992702960968018, 0.02862408757209778, 0.9588150978088379, -1.9767050743103027, -1.431427001953125, -0.3884492516517639, 0.8695794343948364, 0.9551999568939209]
  OUT[0]: [1, 4, 4, 256] | μ=-0.011989 σ=1.193427
    First 10: [-1.2376598119735718, -0.421333909034729, 2.577834129333496, 0.7378442883491516, 1.6417055130004883, -0.20386064052581787, -0.7050732374191284, 1.25922691822052, -2.2882091999053955, -1.4342619180679321]
    Last 10:  [-3.796841621398926, 2.1623401641845703, 1.0315072536468506, 0.03525478020310402, 0.8253771662712097, -3.130582094192505, -1.6760005950927734, -0.6096593141555786, 1.8238403797149658, 1.3742276430130005]
  WEIGHT: [256] | μ=0.527080

054_model.layers.3.self_attn.k_norm: model.layers.3.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.056727 σ=1.464699
    First 10: [0.0008884966373443604, 0.009633071720600128, -0.010251611471176147, -0.004567145369946957, 0.0030151456594467163, 0.006089635193347931, 0.016491279006004333, 0.01666705310344696, -0.024582069367170334, 0.0029260367155075073]
    Last 10:  [-5.540679454803467, 2.2137465476989746, 2.601306915283203, 2.405923843383789, 1.5562833547592163, -4.712998867034912, -2.8254618644714355, -1.679091453552246, 3.1283516883850098, -0.06130021810531616]
  OUT[0]: [1, 1, 4, 256] | μ=-0.032657 σ=1.912227
    First 10: [0.0016501785721629858, 0.02505628950893879, -0.022007884457707405, -0.007282273378223181, 0.005465649999678135, 0.0070645748637616634, 0.06816186010837555, 0.029099399223923683, -0.02096652053296566, 0.005551732145249844]
    Last 10:  [-4.393723487854004, 2.531566858291626, 7.053295612335205, 3.15594220161438, 3.0681941509246826, -4.88968563079834, -3.4577198028564453, -1.7811373472213745, 2.1929256916046143, -0.07184547930955887]
  WEIGHT: [256] | μ=0.747193

055_model.layers.3.self_attn.o_proj: model.layers.3.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.014414 σ=0.528458
    First 10: [-0.284274160861969, -0.08881960064172745, 0.11178702116012573, -0.00050383061170578, 0.2276776283979416, 0.02515965700149536, 0.03421882912516594, 0.10633990168571472, 0.0369911715388298, -0.03175998479127884]
    Last 10:  [0.0789569690823555, 0.07366140186786652, -0.05541395768523216, 0.06708015501499176, 0.07130322605371475, 0.047814298421144485, -0.0368620865046978, 0.07006406784057617, 3.0681588649749756, -0.04629486799240112]
  OUT[0]: [1, 4, 640] | μ=0.023480 σ=0.587806
    First 10: [-0.09550150483846664, 0.12872424721717834, -0.040142640471458435, -0.013998221606016159, -0.08677597343921661, -0.4873543977737427, 0.0505685955286026, 0.00827064923942089, 0.15530873835086823, -0.01480042189359665]
    Last 10:  [-0.33203160762786865, -0.2759011387825012, -0.2136671394109726, -0.9174982309341431, 0.01963462308049202, -0.07284867018461227, 0.6251364350318909, 0.9127824306488037, 0.7735033631324768, 0.6552510261535645]
  WEIGHT: [640, 1024] | μ=0.000012

056_model.layers.3.self_attn: model.layers.3.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.023480 σ=0.587806
    First 10: [-0.09550150483846664, 0.12872424721717834, -0.040142640471458435, -0.013998221606016159, -0.08677597343921661, -0.4873543977737427, 0.0505685955286026, 0.00827064923942089, 0.15530873835086823, -0.01480042189359665]
    Last 10:  [-0.33203160762786865, -0.2759011387825012, -0.2136671394109726, -0.9174982309341431, 0.01963462308049202, -0.07284867018461227, 0.6251364350318909, 0.9127824306488037, 0.7735033631324768, 0.6552510261535645]

057_model.layers.3.post_attention_layernorm: model.layers.3.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.023480 σ=0.587806
    First 10: [-0.09550150483846664, 0.12872424721717834, -0.040142640471458435, -0.013998221606016159, -0.08677597343921661, -0.4873543977737427, 0.0505685955286026, 0.00827064923942089, 0.15530873835086823, -0.01480042189359665]
    Last 10:  [-0.33203160762786865, -0.2759011387825012, -0.2136671394109726, -0.9174982309341431, 0.01963462308049202, -0.07284867018461227, 0.6251364350318909, 0.9127824306488037, 0.7735033631324768, 0.6552510261535645]
  OUT[0]: [1, 4, 640] | μ=0.836139 σ=25.852365
    First 10: [-0.04815233126282692, 0.10227203369140625, -0.28949499130249023, -0.0247697401791811, -0.13258446753025055, -1.7833786010742188, 0.2163376361131668, 0.017414916306734085, 1.2766484022140503, -0.038951702415943146]
    Last 10:  [-0.4969940483570099, -0.9015001058578491, -12.054423332214355, -2.113629102706909, 0.07246356457471848, -0.21534118056297302, 1.678914189338684, 0.225296288728714, 2.6001338958740234, 17.128129959106445]
  WEIGHT: [640] | μ=4.098013

058_model.layers.3.pre_feedforward_layernorm: model.layers.3.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=4.248727 σ=148.026031
    First 10: [-0.0018868371844291687, 0.08754898607730865, -1.2221918106079102, 0.6973713636398315, -0.7011993527412415, 0.4266958236694336, -2.2652885913848877, -0.7486913204193115, -0.12519776821136475, 1.4386132955551147]
    Last 10:  [0.5330443382263184, -1.1176522970199585, -2.3675928115844727, -3.188103675842285, 1.4281781911849976, -1.3864047527313232, 1.8266370296478271, -0.8620296716690063, 3.2424607276916504, 23.92112159729004]
  OUT[0]: [1, 4, 640] | μ=-0.018251 σ=0.816628
    First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
    Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
  WEIGHT: [640] | μ=24.323166

059_model.layers.3.mlp.gate_proj: model.layers.3.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.018251 σ=0.816628
    First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
    Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
  OUT[0]: [1, 4, 2048] | μ=-0.165914 σ=0.967657
    First 10: [-0.19981473684310913, 0.15687692165374756, 0.10429394245147705, 0.25707221031188965, -0.17747117578983307, 0.219674214720726, -0.06518803536891937, 0.02652152068912983, -0.29083094000816345, -0.30567118525505066]
    Last 10:  [-0.625910222530365, -1.4436973333358765, -0.4778515100479126, -0.7620107531547546, 0.28050899505615234, 0.5467347502708435, -0.6734350323677063, -0.4590957462787628, -0.20380201935768127, -0.2578234076499939]
  WEIGHT: [2048, 640] | μ=0.000237

060_model.layers.3.mlp.act_fn: model.layers.3.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.165914 σ=0.967657
    First 10: [-0.19981473684310913, 0.15687692165374756, 0.10429394245147705, 0.25707221031188965, -0.17747117578983307, 0.219674214720726, -0.06518803536891937, 0.02652152068912983, -0.29083094000816345, -0.30567118525505066]
    Last 10:  [-0.625910222530365, -1.4436973333358765, -0.4778515100479126, -0.7620107531547546, 0.28050899505615234, 0.5467347502708435, -0.6734350323677063, -0.4590957462787628, -0.20380201935768127, -0.2578234076499939]
  OUT[0]: [1, 4, 2048] | μ=0.160577 σ=0.624851
    First 10: [-0.08408509939908981, 0.08821625262498856, 0.05647846311330795, 0.1546117514371872, -0.07623646408319473, 0.12893430888652802, -0.030899925157427788, 0.013541338965296745, -0.11214381456375122, -0.11613558232784271]
    Last 10:  [-0.16633424162864685, -0.10765676945447922, -0.15119627118110657, -0.17002122104167938, 0.17123660445213318, 0.3869110941886902, -0.16863325238227844, -0.14833848178386688, -0.08544538170099258, -0.10268513113260269]

061_model.layers.3.mlp.up_proj: model.layers.3.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.018251 σ=0.816628
    First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
    Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
  OUT[0]: [1, 4, 2048] | μ=-0.012218 σ=0.597607
    First 10: [0.09197308868169785, -0.13978458940982819, 0.09174202382564545, -0.07309075444936752, -0.01398362498730421, -0.1726754605770111, 0.1710388958454132, 0.16714079678058624, 0.0477653332054615, -0.03941812738776207]
    Last 10:  [1.0646008253097534, -0.8506368398666382, 0.22772520780563354, -0.07077012956142426, 0.6200867891311646, -1.0112954378128052, 0.17853108048439026, 0.3297814428806305, 0.28989270329475403, -0.2636798024177551]
  WEIGHT: [2048, 640] | μ=-0.000018

062_model.layers.3.mlp.down_proj: model.layers.3.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.006141 σ=0.426529
    First 10: [-0.007733566220849752, -0.012331272475421429, 0.00518144853413105, -0.011300689540803432, 0.0010660621337592602, -0.022263791412115097, -0.005285088904201984, 0.002263310132548213, -0.005356586538255215, 0.004577847197651863]
    Last 10:  [-0.17707957327365875, 0.09157681465148926, -0.034431200474500656, 0.012032424099743366, 0.1061815544962883, -0.3912814259529114, -0.030106276273727417, -0.048919279128313065, -0.024769993498921394, 0.02707599475979805]
  OUT[0]: [1, 4, 640] | μ=0.001021 σ=0.283839
    First 10: [0.0129196522757411, 0.011055509559810162, -0.003166582202538848, 0.015654226765036583, -0.009001096710562706, 0.027541810646653175, -0.030250385403633118, 0.00432765856385231, -0.009179489687085152, -0.014656886458396912]
    Last 10:  [-0.20065413415431976, 0.05632609874010086, -0.09334658831357956, 0.25770553946495056, 0.03187824413180351, 0.11675828695297241, -0.15372350811958313, -0.04842228442430496, 0.14027810096740723, -0.271599143743515]
  WEIGHT: [640, 2048] | μ=0.000011

063_model.layers.3.mlp: model.layers.3.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.018251 σ=0.816628
    First 10: [-0.0004205613804515451, 0.017146185040473938, -0.05727597698569298, 0.13007420301437378, -0.08435838669538498, 0.036212362349033356, -0.155277281999588, -0.08727902919054031, -0.005283384583890438, 0.1549612581729889]
    Last 10:  [0.18954122066497803, -0.28114256262779236, -0.029503807425498962, -1.0791797637939453, 0.30159640312194824, -0.3681213855743408, 0.5247215032577515, -0.6772945523262024, 0.7753545045852661, 1.1468149423599243]
  OUT[0]: [1, 4, 640] | μ=0.001021 σ=0.283839
    First 10: [0.0129196522757411, 0.011055509559810162, -0.003166582202538848, 0.015654226765036583, -0.009001096710562706, 0.027541810646653175, -0.030250385403633118, 0.00432765856385231, -0.009179489687085152, -0.014656886458396912]
    Last 10:  [-0.20065413415431976, 0.05632609874010086, -0.09334658831357956, 0.25770553946495056, 0.03187824413180351, 0.11675828695297241, -0.15372350811958313, -0.04842228442430496, 0.14027810096740723, -0.271599143743515]

064_model.layers.3.post_feedforward_layernorm: model.layers.3.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.001021 σ=0.283839
    First 10: [0.0129196522757411, 0.011055509559810162, -0.003166582202538848, 0.015654226765036583, -0.009001096710562706, 0.027541810646653175, -0.030250385403633118, 0.00432765856385231, -0.009179489687085152, -0.014656886458396912]
    Last 10:  [-0.20065413415431976, 0.05632609874010086, -0.09334658831357956, 0.25770553946495056, 0.03187824413180351, 0.11675828695297241, -0.15372350811958313, -0.04842228442430496, 0.14027810096740723, -0.271599143743515]
  OUT[0]: [1, 4, 640] | μ=4.277775 σ=213.252136
    First 10: [0.5105754137039185, 0.5711506009101868, -0.98434978723526, 1.4723032712936401, -0.5842500925064087, 3.125443458557129, -3.8602442741394043, 0.4089336097240448, -2.934561014175415, -1.3267271518707275]
    Last 10:  [-3.0965893268585205, 1.326377511024475, -50.243263244628906, 3.2726540565490723, 0.9919631481170654, 2.611973762512207, -3.128636360168457, -0.34309518337249756, 3.704409122467041, -81.49906921386719]
  WEIGHT: [640] | μ=12.406023

065_model.layers.4.input_layernorm: model.layers.4.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=8.526503 σ=282.475952
    First 10: [0.5086885690689087, 0.6586995720863342, -2.2065415382385254, 2.1696746349334717, -1.285449504852295, 3.5521392822265625, -6.125533103942871, -0.3397577106952667, -3.0597586631774902, 0.11188614368438721]
    Last 10:  [-2.563544988632202, 0.2087252140045166, -52.61085510253906, 0.08455038070678711, 2.4201412200927734, 1.2255690097808838, -1.3019993305206299, -1.205124855041504, 6.946869850158691, -57.57794952392578]
  OUT[0]: [1, 4, 640] | μ=-0.007862 σ=1.382002
    First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
    Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
  WEIGHT: [640] | μ=34.187679

066_model.layers.4.self_attn.q_proj: model.layers.4.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.007862 σ=1.382002
    First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
    Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
  OUT[0]: [1, 4, 1024] | μ=-0.013430 σ=1.545442
    First 10: [-0.47012460231781006, 0.27169206738471985, 0.35053449869155884, 0.1959158182144165, -0.04061535745859146, 0.5272778868675232, -0.1352287232875824, 0.11941427737474442, -0.3794046938419342, -0.06725836545228958]
    Last 10:  [0.5097470283508301, -1.4366296529769897, -0.5288398265838623, 1.1322662830352783, 0.511859118938446, -1.3537980318069458, -0.7387264966964722, 0.4904572367668152, 3.2224183082580566, -2.416114568710327]
  WEIGHT: [1024, 640] | μ=0.000030

067_model.layers.4.self_attn.k_proj: model.layers.4.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.007862 σ=1.382002
    First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
    Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
  OUT[0]: [1, 4, 256] | μ=-0.125669 σ=1.655496
    First 10: [0.012314993888139725, -0.013491928577423096, -0.015415927395224571, -0.008481740951538086, -0.005269467830657959, -0.02654258906841278, 0.0031440891325473785, 0.017254464328289032, -0.04134698957204819, 0.0066382139921188354]
    Last 10:  [0.8974237442016602, -2.876885175704956, -2.4859397411346436, 4.968405246734619, 2.090125560760498, -1.0859206914901733, -2.987549066543579, 1.7265398502349854, 0.718114972114563, -6.725058078765869]
  WEIGHT: [256, 640] | μ=0.000031

068_model.layers.4.self_attn.v_proj: model.layers.4.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.007862 σ=1.382002
    First 10: [0.09622326493263245, 0.07772333174943924, -0.027353113517165184, 0.15539966523647308, -0.06432982534170151, 0.1851043999195099, -0.2418651133775711, -0.0180949829518795, -0.047412388026714325, 0.006934911943972111]
    Last 10:  [-0.7771941423416138, 0.059082452207803726, -0.2771943509578705, 0.054405327886343, 0.5989536046981812, 0.3886190950870514, -0.4108406603336334, -1.3719629049301147, 1.8696956634521484, -2.783163547515869]
  OUT[0]: [1, 4, 256] | μ=-0.041414 σ=0.990852
    First 10: [0.17033590376377106, 0.05051655322313309, -0.3656069338321686, -0.04745890572667122, 0.050696223974227905, -0.10772868245840073, 0.08719030022621155, -0.09973283857107162, -0.0852748230099678, -0.2342635691165924]
    Last 10:  [-0.4386290907859802, 0.8114195466041565, -0.7877724766731262, -0.5426243543624878, 1.1548233032226562, 0.08396440744400024, 0.23847681283950806, 0.8766622543334961, 0.12181052565574646, -1.042970895767212]
  WEIGHT: [256, 640] | μ=-0.000018

069_model.layers.4.self_attn.q_norm: model.layers.4.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.013430 σ=1.545442
    First 10: [-0.47012460231781006, 0.27169206738471985, 0.35053449869155884, 0.1959158182144165, -0.04061535745859146, 0.5272778868675232, -0.1352287232875824, 0.11941427737474442, -0.3794046938419342, -0.06725836545228958]
    Last 10:  [0.5097470283508301, -1.4366296529769897, -0.5288398265838623, 1.1322662830352783, 0.511859118938446, -1.3537980318069458, -0.7387264966964722, 0.4904572367668152, 3.2224183082580566, -2.416114568710327]
  OUT[0]: [1, 4, 4, 256] | μ=-0.072409 σ=1.290268
    First 10: [-1.2681182622909546, 1.14251708984375, 1.100699543952942, 0.642288327217102, -0.10042653232812881, 0.607205867767334, -0.3053755760192871, 0.3468867242336273, -0.9459978342056274, -0.12019293755292892]
    Last 10:  [0.48473209142684937, -0.8378839492797852, -0.2729683816432953, 0.5095483064651489, 0.3404342830181122, -0.7808080315589905, -0.40487968921661377, 0.2808314561843872, 1.701303482055664, -1.151113748550415]
  WEIGHT: [256] | μ=0.551699

070_model.layers.4.self_attn.k_norm: model.layers.4.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.125669 σ=1.655496
    First 10: [0.012314993888139725, -0.013491928577423096, -0.015415927395224571, -0.008481740951538086, -0.005269467830657959, -0.02654258906841278, 0.0031440891325473785, 0.017254464328289032, -0.04134698957204819, 0.0066382139921188354]
    Last 10:  [0.8974237442016602, -2.876885175704956, -2.4859397411346436, 4.968405246734619, 2.090125560760498, -1.0859206914901733, -2.987549066543579, 1.7265398502349854, 0.718114972114563, -6.725058078765869]
  OUT[0]: [1, 1, 4, 256] | μ=-0.012677 σ=2.899725
    First 10: [0.01264932006597519, -0.031221136450767517, -0.03175630792975426, -0.016086656600236893, -0.010137646459043026, -0.056242506951093674, 0.006918964441865683, 0.029515309259295464, -0.073166623711586, 0.016325069591403008]
    Last 10:  [0.7501626014709473, -1.7800770998001099, -1.8761016130447388, 3.371260404586792, 1.2316701412200928, -0.9506036043167114, -1.6067391633987427, 1.5620344877243042, 0.40281760692596436, -4.028378009796143]
  WEIGHT: [256] | μ=1.060109

071_model.layers.4.self_attn.o_proj: model.layers.4.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.022439 σ=0.505021
    First 10: [0.17033590376377106, 0.05051655322313309, -0.3656069338321686, -0.04745890572667122, 0.050696223974227905, -0.10772868245840073, 0.08719030022621155, -0.09973283857107162, -0.0852748230099678, -0.2342635691165924]
    Last 10:  [0.0033289827406406403, 0.02252907305955887, -0.12790189683437347, 0.016578562557697296, 0.11266755312681198, 0.029179105535149574, 0.18660880625247955, 0.022074971348047256, 0.023214150220155716, 0.08293838053941727]
  OUT[0]: [1, 4, 640] | μ=0.007702 σ=0.440621
    First 10: [0.014555143192410469, -0.09736758470535278, 0.16419917345046997, -0.18441641330718994, -0.0365825891494751, -0.21398252248764038, 0.3922446370124817, -0.03422272205352783, -0.1021280288696289, -0.03742174804210663]
    Last 10:  [0.37619465589523315, 0.21999165415763855, 2.294915199279785, 0.2593448758125305, 0.20895904302597046, -0.20583750307559967, 0.13486140966415405, 0.18887373805046082, 0.29951274394989014, 0.2394901067018509]
  WEIGHT: [640, 1024] | μ=0.000009

072_model.layers.4.self_attn: model.layers.4.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.007702 σ=0.440621
    First 10: [0.014555143192410469, -0.09736758470535278, 0.16419917345046997, -0.18441641330718994, -0.0365825891494751, -0.21398252248764038, 0.3922446370124817, -0.03422272205352783, -0.1021280288696289, -0.03742174804210663]
    Last 10:  [0.37619465589523315, 0.21999165415763855, 2.294915199279785, 0.2593448758125305, 0.20895904302597046, -0.20583750307559967, 0.13486140966415405, 0.18887373805046082, 0.29951274394989014, 0.2394901067018509]

073_model.layers.4.post_attention_layernorm: model.layers.4.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.007702 σ=0.440621
    First 10: [0.014555143192410469, -0.09736758470535278, 0.16419917345046997, -0.18441641330718994, -0.0365825891494751, -0.21398252248764038, 0.3922446370124817, -0.03422272205352783, -0.1021280288696289, -0.03742174804210663]
    Last 10:  [0.37619465589523315, 0.21999165415763855, 2.294915199279785, 0.2593448758125305, 0.20895904302597046, -0.20583750307559967, 0.13486140966415405, 0.18887373805046082, 0.29951274394989014, 0.2394901067018509]
  OUT[0]: [1, 4, 640] | μ=1.473291 σ=27.055550
    First 10: [0.127812460064888, -0.41483813524246216, 0.5580591559410095, -0.5135622620582581, -0.03599090874195099, -1.116982340812683, 2.0475053787231445, -0.06017327308654785, -0.5563559532165527, -0.15152572095394135]
    Last 10:  [0.6357937455177307, 1.333924412727356, 0.8210874795913696, 2.2757928371429443, 1.5109899044036865, -1.2481005191802979, 0.7542465329170227, 0.8642652034759521, 2.0191450119018555, 6.566260814666748]
  WEIGHT: [640] | μ=3.181015

074_model.layers.4.pre_feedforward_layernorm: model.layers.4.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=9.999792 σ=293.349976
    First 10: [0.6365010142326355, 0.24386143684387207, -1.648482322692871, 1.6561124324798584, -1.3214404582977295, 2.43515682220459, -4.078027725219727, -0.3999309837818146, -3.616114616394043, -0.03963957726955414]
    Last 10:  [-1.9277513027191162, 1.5426496267318726, -51.78976821899414, 2.3603432178497314, 3.93113112449646, -0.022531509399414062, -0.5477527976036072, -0.34085965156555176, 8.966014862060547, -51.011688232421875]
  OUT[0]: [1, 4, 640] | μ=-0.017291 σ=0.834793
    First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
    Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
  WEIGHT: [640] | μ=12.549014

075_model.layers.4.mlp.gate_proj: model.layers.4.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.017291 σ=0.834793
    First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
    Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
  OUT[0]: [1, 4, 2048] | μ=-0.295693 σ=0.736810
    First 10: [-0.19264259934425354, 0.19654880464076996, 0.052477043122053146, 0.3934393525123596, -0.04491916298866272, 0.09266506880521774, -0.36228710412979126, 0.013112090528011322, 0.17016193270683289, 0.13246354460716248]
    Last 10:  [-0.35891932249069214, -0.613654375076294, -0.14002388715744019, -3.3150644302368164, -0.7791419625282288, -1.9079103469848633, -1.279184103012085, 0.570376992225647, 0.014779150485992432, -1.9315078258514404]
  WEIGHT: [2048, 640] | μ=-0.000163

076_model.layers.4.mlp.act_fn: model.layers.4.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.295693 σ=0.736810
    First 10: [-0.19264259934425354, 0.19654880464076996, 0.052477043122053146, 0.3934393525123596, -0.04491916298866272, 0.09266506880521774, -0.36228710412979126, 0.013112090528011322, 0.17016193270683289, 0.13246354460716248]
    Last 10:  [-0.35891932249069214, -0.613654375076294, -0.14002388715744019, -3.3150644302368164, -0.7791419625282288, -1.9079103469848633, -1.279184103012085, 0.570376992225647, 0.014779150485992432, -1.9315078258514404]
  OUT[0]: [1, 4, 2048] | μ=0.022001 σ=0.293080
    First 10: [-0.08160758763551712, 0.11358698457479477, 0.027336636558175087, 0.2569097876548767, -0.021654896438121796, 0.04975325986742973, -0.12990999221801758, 0.006624632515013218, 0.09657660126686096, 0.07321132719516754]
    Last 10:  [-0.12915411591529846, -0.1655515879392624, -0.062215618789196014, -0.00124137953389436, -0.16989000141620636, -0.053786467760801315, -0.12867867946624756, 0.40824198722839355, 0.007476710248738527, -0.05155172571539879]

077_model.layers.4.mlp.up_proj: model.layers.4.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.017291 σ=0.834793
    First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
    Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
  OUT[0]: [1, 4, 2048] | μ=0.009083 σ=0.565768
    First 10: [0.1912337988615036, -0.054631222039461136, -0.11619031429290771, 0.02423485368490219, -0.15307192504405975, -0.07235513627529144, 0.9513946771621704, -0.02000294253230095, 0.27787795662879944, 0.3723047375679016]
    Last 10:  [0.15171466767787933, -0.20338484644889832, -0.1197502464056015, -0.19090571999549866, 1.0182992219924927, 0.23071669042110443, -0.10596521198749542, -0.05943194776773453, 0.9068560600280762, -0.03951413929462433]
  WEIGHT: [2048, 640] | μ=-0.000034

078_model.layers.4.mlp.down_proj: model.layers.4.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.000240 σ=0.224250
    First 10: [-0.015606128610670567, -0.006205395795404911, -0.003176252357661724, 0.006226171273738146, 0.003314756788313389, -0.0035999040119349957, -0.12359567731618881, -0.00013251214113552123, 0.026836508885025978, 0.027256924659013748]
    Last 10:  [-0.019594574347138405, 0.033670682460069656, 0.007450335659086704, 0.00023698645236436278, -0.17299886047840118, -0.012409435585141182, 0.013635464012622833, -0.02426261641085148, 0.006780299823731184, 0.002037022029981017]
  OUT[0]: [1, 4, 640] | μ=0.001320 σ=0.120257
    First 10: [0.02510225772857666, 0.029887359589338303, -0.012600895017385483, 0.010254492983222008, -0.039633609354496, -0.014592068269848824, 0.007261150050908327, 0.024423446506261826, -0.03147883713245392, 0.013053882867097855]
    Last 10:  [-0.03207657113671303, 0.011519163846969604, 0.10274564474821091, -0.07696116715669632, -0.0006829332560300827, -0.1984306275844574, 0.012257646769285202, 0.03350318223237991, 0.1248776763677597, -0.07188175618648529]
  WEIGHT: [640, 2048] | μ=-0.000012

079_model.layers.4.mlp: model.layers.4.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.017291 σ=0.834793
    First 10: [0.015924246981739998, 0.012093107216060162, -0.01159942988306284, 0.06843894720077515, -0.047229066491127014, 0.07017112523317337, -0.1038476899266243, -0.01268572174012661, -0.0337241068482399, -0.0014521607663482428]
    Last 10:  [-0.30505600571632385, 0.22284813225269318, -0.1552167683839798, 0.4074668288230896, 0.42650169134140015, -0.003389913821592927, -0.08733551949262619, -0.11850269138813019, 1.2145979404449463, -1.2918766736984253]
  OUT[0]: [1, 4, 640] | μ=0.001320 σ=0.120257
    First 10: [0.02510225772857666, 0.029887359589338303, -0.012600895017385483, 0.010254492983222008, -0.039633609354496, -0.014592068269848824, 0.007261150050908327, 0.024423446506261826, -0.03147883713245392, 0.013053882867097855]
    Last 10:  [-0.03207657113671303, 0.011519163846969604, 0.10274564474821091, -0.07696116715669632, -0.0006829332560300827, -0.1984306275844574, 0.012257646769285202, 0.03350318223237991, 0.1248776763677597, -0.07188175618648529]

080_model.layers.4.post_feedforward_layernorm: model.layers.4.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.001320 σ=0.120257
    First 10: [0.02510225772857666, 0.029887359589338303, -0.012600895017385483, 0.010254492983222008, -0.039633609354496, -0.014592068269848824, 0.007261150050908327, 0.024423446506261826, -0.03147883713245392, 0.013053882867097855]
    Last 10:  [-0.03207657113671303, 0.011519163846969604, 0.10274564474821091, -0.07696116715669632, -0.0006829332560300827, -0.1984306275844574, 0.012257646769285202, 0.03350318223237991, 0.1248776763677597, -0.07188175618648529]
  OUT[0]: [1, 4, 640] | μ=5.266237 σ=271.957733
    First 10: [0.6422917246818542, 1.3990375995635986, -0.6423391103744507, 0.3681481182575226, -0.6318506598472595, -1.0245895385742188, 0.5242478251457214, 0.6709443926811218, -2.2602529525756836, 0.7508748173713684]
    Last 10:  [-0.4026384949684143, 0.5052456855773926, 18.121112823486328, -2.3629329204559326, -0.031530898064374924, -8.428584098815918, 0.4980212152004242, 0.6303354501724243, 5.70792293548584, -11.881185531616211]
  WEIGHT: [640] | μ=6.994090

081_model.layers.5.input_layernorm: model.layers.5.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=15.266031 σ=527.773193
    First 10: [1.2787927389144897, 1.6428990364074707, -2.2908215522766113, 2.0242605209350586, -1.9532911777496338, 1.410567283630371, -3.5537798404693604, 0.27101340889930725, -5.876367568969727, 0.7112352252006531]
    Last 10:  [-2.3303897380828857, 2.0478954315185547, -33.66865539550781, -0.002589702606201172, 3.8996002674102783, -8.451115608215332, -0.04973158240318298, 0.28947579860687256, 14.673937797546387, -62.89287567138672]
  OUT[0]: [1, 4, 640] | μ=0.002493 σ=1.256721
    First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
    Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
  WEIGHT: [640] | μ=13.077600

082_model.layers.5.self_attn.q_proj: model.layers.5.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.002493 σ=1.256721
    First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
    Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
  OUT[0]: [1, 4, 1024] | μ=-0.001296 σ=1.526073
    First 10: [0.15951862931251526, 0.0761331170797348, 0.3921384811401367, -0.16702556610107422, -0.303239643573761, -0.29362058639526367, -0.12124329805374146, -0.13774695992469788, 0.8641344308853149, 0.17867092788219452]
    Last 10:  [-1.8424277305603027, -0.708545982837677, 0.208127960562706, -0.2384515106678009, 0.16396889090538025, 0.26922449469566345, 0.044506654143333435, -0.5637449622154236, 0.2543957829475403, -0.4659898579120636]
  WEIGHT: [1024, 640] | μ=0.000024

083_model.layers.5.self_attn.k_proj: model.layers.5.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.002493 σ=1.256721
    First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
    Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
  OUT[0]: [1, 4, 256] | μ=-0.026624 σ=1.072770
    First 10: [0.0052673304453492165, -0.6102632880210876, 0.0014539677649736404, 0.07531185448169708, -0.031955406069755554, -0.00020262273028492928, -0.07609817385673523, -0.0008615069091320038, -0.023195087909698486, 1.6033649444580078e-05]
    Last 10:  [-2.0749192237854004, -3.9119951725006104, 3.5476362705230713, 1.3466933965682983, 0.11295855045318604, 0.95540452003479, -1.5809605121612549, -2.5169272422790527, 4.610164642333984, -5.188906192779541]
  WEIGHT: [256, 640] | μ=0.000002

084_model.layers.5.self_attn.v_proj: model.layers.5.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.002493 σ=1.256721
    First 10: [0.0345287062227726, 0.03824813291430473, -0.01587599329650402, 0.03765266761183739, -0.032113321125507355, 0.01658889837563038, -0.03624992445111275, 0.0047158109955489635, -0.03243877366185188, 0.011181039735674858]
    Last 10:  [-0.34746482968330383, 0.22123071551322937, -0.093533955514431, -0.0006090635433793068, 0.3532501757144928, -0.9319798350334167, -0.005624251905828714, 0.08860283344984055, 1.387050747871399, -1.2075650691986084]
  OUT[0]: [1, 4, 256] | μ=-0.000814 σ=0.839529
    First 10: [-0.03275470435619354, -0.02581084705889225, -0.0416322723031044, 0.054272882640361786, -0.024547144770622253, 0.005798856727778912, -0.06249213218688965, -0.0034544747322797775, -0.04761842265725136, 0.09810389578342438]
    Last 10:  [-0.1880067139863968, 0.9147224426269531, -0.10676532983779907, -0.6548353433609009, -0.020590394735336304, 1.4306422472000122, -0.7914350032806396, -0.4583362340927124, -0.256325364112854, -0.2087811827659607]
  WEIGHT: [256, 640] | μ=-0.000052

085_model.layers.5.self_attn.q_norm: model.layers.5.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.001296 σ=1.526073
    First 10: [0.15951862931251526, 0.0761331170797348, 0.3921384811401367, -0.16702556610107422, -0.303239643573761, -0.29362058639526367, -0.12124329805374146, -0.13774695992469788, 0.8641344308853149, 0.17867092788219452]
    Last 10:  [-1.8424277305603027, -0.708545982837677, 0.208127960562706, -0.2384515106678009, 0.16396889090538025, 0.26922449469566345, 0.044506654143333435, -0.5637449622154236, 0.2543957829475403, -0.4659898579120636]
  OUT[0]: [1, 4, 4, 256] | μ=-0.058599 σ=1.411086
    First 10: [0.03264807537198067, -0.006232751067727804, -0.11771100759506226, -0.0, 0.03310022130608559, -3.0287485122680664, -0.0, -0.8589208722114563, 0.24760279059410095, 1.2481844425201416]
    Last 10:  [-0.7923656702041626, -0.38339516520500183, 0.1996859312057495, -0.3475509583950043, 0.12855012714862823, 0.14062532782554626, 0.028502393513917923, -0.9027866125106812, 0.11219190061092377, -0.35171329975128174]
  WEIGHT: [256] | μ=1.001856

086_model.layers.5.self_attn.k_norm: model.layers.5.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.026624 σ=1.072770
    First 10: [0.0052673304453492165, -0.6102632880210876, 0.0014539677649736404, 0.07531185448169708, -0.031955406069755554, -0.00020262273028492928, -0.07609817385673523, -0.0008615069091320038, -0.023195087909698486, 1.6033649444580078e-05]
    Last 10:  [-2.0749192237854004, -3.9119951725006104, 3.5476362705230713, 1.3466933965682983, 0.11295855045318604, 0.95540452003479, -1.5809605121612549, -2.5169272422790527, 4.610164642333984, -5.188906192779541]
  OUT[0]: [1, 1, 4, 256] | μ=0.178844 σ=6.838080
    First 10: [0.00024941281299106777, -0.036120641976594925, 0.005296891089528799, 0.0, -0.0, -0.0013384142657741904, -0.0, -0.005588657688349485, 0.008786465041339397, 9.015590330818668e-05]
    Last 10:  [-12.835454940795898, -27.23606300354004, 14.769530296325684, 3.801056385040283, 0.5738880634307861, 6.651710510253906, -10.188867568969727, -8.702466011047363, 35.783653259277344, -27.460765838623047]
  WEIGHT: [256] | μ=1.930750

087_model.layers.5.self_attn.o_proj: model.layers.5.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.023490 σ=0.338652
    First 10: [-0.03275470435619354, -0.02581084705889225, -0.0416322723031044, 0.054272882640361786, -0.024547144770622253, 0.005798856727778912, -0.06249213218688965, -0.0034544747322797775, -0.04761842265725136, 0.09810389578342438]
    Last 10:  [-0.31355229020118713, 0.142403244972229, 0.0572931170463562, -0.2752956449985504, 0.250468909740448, 0.36510512232780457, -0.46638888120651245, -0.31967693567276, 0.34967198967933655, -0.04819459095597267]
  OUT[0]: [1, 4, 640] | μ=0.010112 σ=0.687828
    First 10: [-0.30982542037963867, -0.06090114265680313, 0.09604249149560928, 0.004388913512229919, -0.023113245144486427, -0.1547355204820633, 0.1662079095840454, 0.05067374184727669, -0.014496896415948868, -0.07123209536075592]
    Last 10:  [-0.004512708634138107, -0.04602029174566269, -2.4307641983032227, -2.854137897491455, -0.08219689875841141, -0.021359354257583618, -0.16004371643066406, -0.05735214054584503, 0.22727367281913757, 0.7559831738471985]
  WEIGHT: [640, 1024] | μ=-0.000068

088_model.layers.5.self_attn: model.layers.5.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.010112 σ=0.687828
    First 10: [-0.30982542037963867, -0.06090114265680313, 0.09604249149560928, 0.004388913512229919, -0.023113245144486427, -0.1547355204820633, 0.1662079095840454, 0.05067374184727669, -0.014496896415948868, -0.07123209536075592]
    Last 10:  [-0.004512708634138107, -0.04602029174566269, -2.4307641983032227, -2.854137897491455, -0.08219689875841141, -0.021359354257583618, -0.16004371643066406, -0.05735214054584503, 0.22727367281913757, 0.7559831738471985]

089_model.layers.5.post_attention_layernorm: model.layers.5.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.010112 σ=0.687828
    First 10: [-0.30982542037963867, -0.06090114265680313, 0.09604249149560928, 0.004388913512229919, -0.023113245144486427, -0.1547355204820633, 0.1662079095840454, 0.05067374184727669, -0.014496896415948868, -0.07123209536075592]
    Last 10:  [-0.004512708634138107, -0.04602029174566269, -2.4307641983032227, -2.854137897491455, -0.08219689875841141, -0.021359354257583618, -0.16004371643066406, -0.05735214054584503, 0.22727367281913757, 0.7559831738471985]
  OUT[0]: [1, 4, 640] | μ=7.332312 σ=127.745911
    First 10: [-6.5338053703308105, -0.430414617061615, 0.611114501953125, 0.03002096898853779, -0.037653516978025436, -1.1392951011657715, 1.1104530096054077, 0.1992187649011612, -0.13177596032619476, -0.4516282379627228]
    Last 10:  [-0.011925594881176949, -0.3543650209903717, -2.069706678390503, -174.77890014648438, -0.8126993775367737, -0.16933710873126984, -1.239659309387207, -0.2678478956222534, 1.977867841720581, 37.75178146362305]
  WEIGHT: [640] | μ=9.760829

090_model.layers.5.pre_feedforward_layernorm: model.layers.5.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=22.598341 σ=589.394226
    First 10: [-5.255012512207031, 1.212484359741211, -1.6797070503234863, 2.05428147315979, -1.990944743156433, 0.2712721824645996, -2.443326950073242, 0.47023218870162964, -6.008143424987793, 0.2596069872379303]
    Last 10:  [-2.342315435409546, 1.6935304403305054, -35.73836135864258, -174.781494140625, 3.0869009494781494, -8.620452880859375, -1.2893909215927124, 0.02162790298461914, 16.651805877685547, -25.141094207763672]
  OUT[0]: [1, 4, 640] | μ=0.002586 σ=0.504907
    First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
    Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
  WEIGHT: [640] | μ=11.425471

091_model.layers.5.mlp.gate_proj: model.layers.5.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.002586 σ=0.504907
    First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
    Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
  OUT[0]: [1, 4, 2048] | μ=-0.281477 σ=0.603702
    First 10: [-0.036246344447135925, -0.008793199434876442, -0.009725552052259445, -0.0047374144196510315, 0.0344904363155365, -0.05973176658153534, -0.03852725028991699, -0.029716668650507927, -0.06765959411859512, 0.027868853881955147]
    Last 10:  [-0.3453226685523987, -0.10118064284324646, -0.2203465700149536, -1.264524221420288, 0.051933228969573975, -0.14620453119277954, -0.20661771297454834, -1.612801432609558, -0.978604793548584, -0.16097387671470642]
  WEIGHT: [2048, 640] | μ=0.000070

092_model.layers.5.mlp.act_fn: model.layers.5.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.281477 σ=0.603702
    First 10: [-0.036246344447135925, -0.008793199434876442, -0.009725552052259445, -0.0047374144196510315, 0.0344904363155365, -0.05973176658153534, -0.03852725028991699, -0.029716668650507927, -0.06765959411859512, 0.027868853881955147]
    Last 10:  [-0.3453226685523987, -0.10118064284324646, -0.2203465700149536, -1.264524221420288, 0.051933228969573975, -0.14620453119277954, -0.20661771297454834, -1.612801432609558, -0.978604793548584, -0.16097387671470642]
  OUT[0]: [1, 4, 2048] | μ=-0.006933 σ=0.228916
    First 10: [-0.017599157989025116, -0.004365753848105669, -0.0048250420950353146, -0.002359753707423806, 0.017719700932502747, -0.02844335325062275, -0.018671603873372078, -0.014506088569760323, -0.032004911452531815, 0.014244234189391136]
    Last 10:  [-0.12602148950099945, -0.04651313275098801, -0.09095995128154755, -0.13049839437007904, 0.027042098343372345, -0.0646049976348877, -0.0863986536860466, -0.08629217743873596, -0.16052642464637756, -0.07019399851560593]

093_model.layers.5.mlp.up_proj: model.layers.5.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.002586 σ=0.504907
    First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
    Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
  OUT[0]: [1, 4, 2048] | μ=0.015976 σ=0.463782
    First 10: [0.03354082629084587, -0.047890804708004, -0.04949067160487175, -0.01926650106906891, -0.025972086936235428, 0.021167386323213577, 0.0427279993891716, 0.03771296143531799, -0.011903483420610428, -0.007411304861307144]
    Last 10:  [-0.44587942957878113, 0.10131098330020905, 0.4576833248138428, -0.10138928890228271, 0.000503448536619544, 0.09858541190624237, -0.20070575177669525, 0.8169538378715515, -0.27038806676864624, 0.04573577642440796]
  WEIGHT: [2048, 640] | μ=0.000013

094_model.layers.5.mlp.down_proj: model.layers.5.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.001273 σ=0.120330
    First 10: [-0.0005902902921661735, 0.00020907945872750133, 0.00023879457148723304, 4.546419586404227e-05, -0.0004602176195476204, -0.0006020714645273983, -0.0007978002540767193, -0.0005470675532706082, 0.0003809699264820665, -0.00010556836059549823]
    Last 10:  [0.05619039013981819, -0.004712291061878204, -0.04163085296750069, 0.01323113963007927, 1.3614304407383315e-05, -0.006369110196828842, 0.017340706661343575, -0.07049672305583954, 0.04340443015098572, -0.00321037694811821]
  OUT[0]: [1, 4, 640] | μ=-0.000349 σ=0.132691
    First 10: [-0.002620988292619586, 0.00015947571955621243, 0.00011993785301456228, -0.0002842181420419365, 0.0017182312440127134, -0.00039728463161736727, -0.0024894720409065485, -0.0003424918104428798, 0.0012876114342361689, 0.00023883699032012373]
    Last 10:  [-0.09005661308765411, 0.026190225034952164, 0.07193337380886078, 0.12247564643621445, -0.0817975327372551, 0.03725900128483772, 0.04256312549114227, 0.05825946107506752, -0.10471723973751068, -0.09494146704673767]
  WEIGHT: [640, 2048] | μ=-0.000007

095_model.layers.5.mlp: model.layers.5.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.002586 σ=0.504907
    First 10: [-0.045367926359176636, 0.021862920373678207, -0.007663693279027939, 0.044899143278598785, -0.05591655895113945, 0.0035425929818302393, -0.030439289286732674, 0.009095649234950542, -0.035455379635095596, 0.0042271800339221954]
    Last 10:  [-0.3025868833065033, 0.1293771117925644, -0.0798269733786583, -3.7679033279418945, 0.19230245053768158, -0.6302944421768188, -0.10357601940631866, 0.0033612537663429976, 1.1465399265289307, -0.41421782970428467]
  OUT[0]: [1, 4, 640] | μ=-0.000349 σ=0.132691
    First 10: [-0.002620988292619586, 0.00015947571955621243, 0.00011993785301456228, -0.0002842181420419365, 0.0017182312440127134, -0.00039728463161736727, -0.0024894720409065485, -0.0003424918104428798, 0.0012876114342361689, 0.00023883699032012373]
    Last 10:  [-0.09005661308765411, 0.026190225034952164, 0.07193337380886078, 0.12247564643621445, -0.0817975327372551, 0.03725900128483772, 0.04256312549114227, 0.05825946107506752, -0.10471723973751068, -0.09494146704673767]

096_model.layers.5.post_feedforward_layernorm: model.layers.5.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.000349 σ=0.132691
    First 10: [-0.002620988292619586, 0.00015947571955621243, 0.00011993785301456228, -0.0002842181420419365, 0.0017182312440127134, -0.00039728463161736727, -0.0024894720409065485, -0.0003424918104428798, 0.0012876114342361689, 0.00023883699032012373]
    Last 10:  [-0.09005661308765411, 0.026190225034952164, 0.07193337380886078, 0.12247564643621445, -0.0817975327372551, 0.03725900128483772, 0.04256312549114227, 0.05825946107506752, -0.10471723973751068, -0.09494146704673767]
  OUT[0]: [1, 4, 640] | μ=4.475021 σ=291.647949
    First 10: [-9.993051528930664, 0.3081437349319458, 0.153636172413826, -0.3027816414833069, 0.7355173230171204, -0.8567469120025635, -5.2611918449401855, -0.2518577575683594, 2.4324264526367188, 0.4408858120441437]
    Last 10:  [-1.7385258674621582, 2.0523505210876465, 15.470702171325684, 19.755617141723633, -5.988829135894775, 2.6533362865448, 2.945849895477295, 2.0577621459960938, -7.547106742858887, -22.048229217529297]
  WEIGHT: [640] | μ=10.625275

097_model.layers.6.input_layernorm: model.layers.6.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=27.073360 σ=860.292725
    First 10: [-15.248064041137695, 1.5206280946731567, -1.5260708332061768, 1.751499891281128, -1.255427360534668, -0.5854747295379639, -7.704518795013428, 0.21837443113327026, -3.575716972351074, 0.700492799282074]
    Last 10:  [-4.080841064453125, 3.7458810806274414, -20.267658233642578, -155.02587890625, -2.901928186416626, -5.967116355895996, 1.6564589738845825, 2.079390048980713, 9.10469913482666, -47.18932342529297]
  OUT[0]: [1, 4, 640] | μ=-0.005187 σ=1.405668
    First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
    Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
  WEIGHT: [640] | μ=20.808052

098_model.layers.6.self_attn.q_proj: model.layers.6.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.005187 σ=1.405668
    First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
    Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
  OUT[0]: [1, 4, 1024] | μ=0.072990 σ=2.141624
    First 10: [-0.11726157367229462, -0.17229698598384857, 2.3283441066741943, -0.17828825116157532, 0.18344822525978088, -0.8387015461921692, -0.1320740282535553, -0.12437295913696289, -0.4533459544181824, 0.09086351096630096]
    Last 10:  [-2.0913147926330566, -1.2477000951766968, 0.7917398810386658, 0.7085075378417969, -1.003490686416626, 3.7832610607147217, 0.5595666170120239, 0.24933119118213654, -0.47622621059417725, 1.0557661056518555]
  WEIGHT: [1024, 640] | μ=0.000035

099_model.layers.6.self_attn.k_proj: model.layers.6.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.005187 σ=1.405668
    First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
    Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
  OUT[0]: [1, 4, 256] | μ=-0.023396 σ=1.857035
    First 10: [-0.004414652474224567, 0.00013446807861328125, -0.630158543586731, -1.7858089208602905, 2.456576108932495, -0.03904158994555473, -2.072155475616455, 0.0025050006806850433, -0.0065973661839962006, -0.002747509628534317]
    Last 10:  [-5.209957599639893, -4.453174591064453, 3.201387405395508, -0.8471319675445557, -3.2109005451202393, 2.397477149963379, 3.8828206062316895, 1.761521816253662, -2.1639976501464844, 1.8167604207992554]
  WEIGHT: [256, 640] | μ=-0.000060

100_model.layers.6.self_attn.v_proj: model.layers.6.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.005187 σ=1.405668
    First 10: [-0.14038068056106567, 0.033911384642124176, -0.006856929510831833, 0.04471345990896225, -0.04328509420156479, -0.008546929806470871, -0.10738608241081238, 0.005414613056927919, -0.024656981229782104, 0.012487021274864674]
    Last 10:  [-1.445360779762268, 0.7370688915252686, -0.11154001951217651, -22.87807846069336, -0.448239803314209, -1.103688359260559, 0.33082690834999084, 0.8919624090194702, 1.5317425727844238, -1.6945745944976807]
  OUT[0]: [1, 4, 256] | μ=-0.003487 σ=0.971380
    First 10: [0.16002824902534485, -0.021760500967502594, 0.030557751655578613, 0.03504304215312004, -0.017011435702443123, -0.045198749750852585, -0.03244747221469879, 0.021938040852546692, 0.10085819661617279, -0.09421645849943161]
    Last 10:  [-0.05353561043739319, -0.6082468032836914, 0.18518412113189697, -0.25615179538726807, -0.4734961688518524, -0.1703626662492752, -0.1812119483947754, -0.5853073596954346, -0.48112261295318604, 0.3949637711048126]
  WEIGHT: [256, 640] | μ=0.000199

101_model.layers.6.self_attn.q_norm: model.layers.6.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.072990 σ=2.141624
    First 10: [-0.11726157367229462, -0.17229698598384857, 2.3283441066741943, -0.17828825116157532, 0.18344822525978088, -0.8387015461921692, -0.1320740282535553, -0.12437295913696289, -0.4533459544181824, 0.09086351096630096]
    Last 10:  [-2.0913147926330566, -1.2477000951766968, 0.7917398810386658, 0.7085075378417969, -1.003490686416626, 3.7832610607147217, 0.5595666170120239, 0.24933119118213654, -0.47622621059417725, 1.0557661056518555]
  OUT[0]: [1, 4, 4, 256] | μ=0.058286 σ=1.042379
    First 10: [-0.6014613509178162, -0.7614458203315735, 0.8263858556747437, -0.6042107343673706, 0.6043699979782104, 0.05761462077498436, -0.42509981989860535, -0.7974216341972351, -3.1142609119415283, 0.25663191080093384]
    Last 10:  [-1.4753828048706055, -0.6252681612968445, 0.6235255599021912, 0.46850427985191345, -0.5549523830413818, 3.6850197315216064, 0.4126531779766083, 0.12734036147594452, -0.3740314245223999, 0.7459479570388794]
  WEIGHT: [256] | μ=0.460508

102_model.layers.6.self_attn.k_norm: model.layers.6.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.023396 σ=1.857035
    First 10: [-0.004414652474224567, 0.00013446807861328125, -0.630158543586731, -1.7858089208602905, 2.456576108932495, -0.03904158994555473, -2.072155475616455, 0.0025050006806850433, -0.0065973661839962006, -0.002747509628534317]
    Last 10:  [-5.209957599639893, -4.453174591064453, 3.201387405395508, -0.8471319675445557, -3.2109005451202393, 2.397477149963379, 3.8828206062316895, 1.761521816253662, -2.1639976501464844, 1.8167604207992554]
  OUT[0]: [1, 1, 4, 256] | μ=-0.026852 σ=1.826840
    First 10: [-0.010893047787249088, 3.8865557144163176e-05, -0.013491547666490078, 0.019116876646876335, 0.0, -0.010030455887317657, -0.01109109167009592, 0.004987727850675583, -0.011900150217115879, -0.008382354862987995]
    Last 10:  [-2.5537240505218506, -2.9274468421936035, 1.7809685468673706, -0.45804303884506226, -3.0606534481048584, 3.1836488246917725, 1.9319273233413696, 1.2911664247512817, -1.4545860290527344, 1.0017272233963013]
  WEIGHT: [256] | μ=0.674679

103_model.layers.6.self_attn.o_proj: model.layers.6.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.015269 σ=0.259612
    First 10: [0.16002824902534485, -0.021760500967502594, 0.030557751655578613, 0.03504304215312004, -0.017011435702443123, -0.045198749750852585, -0.03244747221469879, 0.021938040852546692, 0.10085819661617279, -0.09421645849943161]
    Last 10:  [-0.18812942504882812, 0.0840262919664383, 0.02867027372121811, 0.1367187201976776, 0.2910793125629425, 0.06432031840085983, 0.42376574873924255, -0.2333696335554123, -0.044937193393707275, 0.16788272559642792]
  OUT[0]: [1, 4, 640] | μ=-0.009015 σ=0.331181
    First 10: [0.2307932823896408, -0.033523980528116226, -0.07949678599834442, 0.02954702079296112, -0.018485646694898605, 0.0454990416765213, -0.025795429944992065, -0.06685611605644226, -0.10976025462150574, -0.0029549933969974518]
    Last 10:  [0.35093921422958374, 0.28944385051727295, 0.22064557671546936, 0.7390665411949158, -0.25340667366981506, 0.3190060555934906, 0.3734707832336426, -0.15005674958229065, 0.03949996456503868, 0.39223021268844604]
  WEIGHT: [640, 1024] | μ=-0.000019

104_model.layers.6.self_attn: model.layers.6.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=-0.009015 σ=0.331181
    First 10: [0.2307932823896408, -0.033523980528116226, -0.07949678599834442, 0.02954702079296112, -0.018485646694898605, 0.0454990416765213, -0.025795429944992065, -0.06685611605644226, -0.10976025462150574, -0.0029549933969974518]
    Last 10:  [0.35093921422958374, 0.28944385051727295, 0.22064557671546936, 0.7390665411949158, -0.25340667366981506, 0.3190060555934906, 0.3734707832336426, -0.15005674958229065, 0.03949996456503868, 0.39223021268844604]

105_model.layers.6.post_attention_layernorm: model.layers.6.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.009015 σ=0.331181
    First 10: [0.2307932823896408, -0.033523980528116226, -0.07949678599834442, 0.02954702079296112, -0.018485646694898605, 0.0454990416765213, -0.025795429944992065, -0.06685611605644226, -0.10976025462150574, -0.0029549933969974518]
    Last 10:  [0.35093921422958374, 0.28944385051727295, 0.22064557671546936, 0.7390665411949158, -0.25340667366981506, 0.3190060555934906, 0.3734707832336426, -0.15005674958229065, 0.03949996456503868, 0.39223021268844604]
  OUT[0]: [1, 4, 640] | μ=0.680589 σ=14.775277
    First 10: [10.979560852050781, -0.9313870668411255, -0.5635048151016235, 0.3204869329929352, -0.05084148049354553, 1.2640868425369263, -0.6234021186828613, -0.3371388614177704, -1.34196138381958, -0.06916452944278717]
    Last 10:  [1.2219395637512207, 7.662448883056641, 0.16867250204086304, 94.6069564819336, -4.946410179138184, 6.627760410308838, 8.69796371459961, -1.8856581449508667, 0.7875711917877197, 16.101016998291016]
  WEIGHT: [640] | μ=8.017050

106_model.layers.6.pre_feedforward_layernorm: model.layers.6.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=27.753948 σ=857.399353
    First 10: [-4.268503189086914, 0.5892410278320312, -2.08957576751709, 2.0719869136810303, -1.3062688112258911, 0.6786121129989624, -8.327920913696289, -0.11876443028450012, -4.917678356170654, 0.631328284740448]
    Last 10:  [-2.8589015007019043, 11.408329963684082, -20.09898567199707, -60.418922424316406, -7.8483381271362305, 0.6606440544128418, 10.354422569274902, 0.1937319040298462, 9.8922700881958, -31.088306427001953]
  OUT[0]: [1, 4, 640] | μ=0.006409 σ=0.610577
    First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
    Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
  WEIGHT: [640] | μ=7.150157

107_model.layers.6.mlp.gate_proj: model.layers.6.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.006409 σ=0.610577
    First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
    Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
  OUT[0]: [1, 4, 2048] | μ=-0.419021 σ=0.716203
    First 10: [0.010462353006005287, -0.007480908185243607, -0.09548194706439972, -0.02929212711751461, 0.06714053452014923, -0.005614057183265686, -0.1212737187743187, 0.11738111823797226, -0.008310857228934765, -0.02469593472778797]
    Last 10:  [-0.278298556804657, -0.6951957941055298, -0.11705362051725388, -0.5295006036758423, 0.41540366411209106, -0.2696792483329773, -1.5020802021026611, -0.7568891644477844, -0.7355124354362488, -0.7378833889961243]
  WEIGHT: [2048, 640] | μ=-0.000132

108_model.layers.6.mlp.act_fn: model.layers.6.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.419021 σ=0.716203
    First 10: [0.010462353006005287, -0.007480908185243607, -0.09548194706439972, -0.02929212711751461, 0.06714053452014923, -0.005614057183265686, -0.1212737187743187, 0.11738111823797226, -0.008310857228934765, -0.02469593472778797]
    Last 10:  [-0.278298556804657, -0.6951957941055298, -0.11705362051725388, -0.5295006036758423, 0.41540366411209106, -0.2696792483329773, -1.5020802021026611, -0.7568891644477844, -0.7355124354362488, -0.7378833889961243]
  OUT[0]: [1, 4, 2048] | μ=-0.024126 σ=0.240961
    First 10: [0.005274844355881214, -0.00371812772937119, -0.044109441339969635, -0.014303809031844139, 0.03536728397011757, -0.002794455038383603, -0.054783910512924194, 0.06417465955018997, -0.004127874039113522, -0.012104681693017483]
    Last 10:  [-0.10864728689193726, -0.1693108081817627, -0.053073201328516006, -0.15793368220329285, 0.27460506558418274, -0.10617532581090927, -0.10016284883022308, -0.17003653943538666, -0.1699783056974411, -0.16999460756778717]

109_model.layers.6.mlp.up_proj: model.layers.6.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.006409 σ=0.610577
    First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
    Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
  OUT[0]: [1, 4, 2048] | μ=0.015625 σ=0.514268
    First 10: [0.09926675260066986, -0.08424293249845505, 0.020935427397489548, 0.16355112195014954, 0.10422397404909134, 0.009894141927361488, 0.03576844930648804, 0.02773936092853546, 0.010855473577976227, 0.11359960585832596]
    Last 10:  [-0.09728927165269852, 0.7029109001159668, 0.7152701020240784, 0.16923286020755768, -0.6261065602302551, 0.518202543258667, -0.7293983697891235, -0.01047445833683014, -0.1029612272977829, -0.03194883465766907]
  WEIGHT: [2048, 640] | μ=-0.000054

110_model.layers.6.mlp.down_proj: model.layers.6.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.003705 σ=0.141725
    First 10: [0.0005236166762188077, 0.0003132259880658239, -0.000923449988476932, -0.0023394040763378143, 0.003686118870973587, -2.7648735340335406e-05, -0.001959535526111722, 0.001780164078809321, -4.4810029066866264e-05, -0.001375087071210146]
    Last 10:  [0.010570215061306953, -0.11901041120290756, -0.037961672991514206, -0.026727568358182907, -0.17193202674388885, -0.055020324885845184, 0.07305862009525299, 0.0017810406861826777, 0.01750117540359497, 0.005431129597127438]
  OUT[0]: [1, 4, 640] | μ=-0.004191 σ=0.106109
    First 10: [-0.0004089747089892626, -0.002305450616404414, 4.928826820105314e-05, -0.0062036216259002686, -0.0016609018202871084, 0.002725818660110235, -0.005698984023183584, 0.002841054229065776, -0.0001353949774056673, -0.002037203637883067]
    Last 10:  [0.050114359706640244, -0.010155703872442245, -0.07401532679796219, 0.22497253119945526, 0.010645193979144096, 0.10378660261631012, -0.04157324135303497, 0.06269757449626923, -0.1329967975616455, 0.11691781133413315]
  WEIGHT: [640, 2048] | μ=-0.000006

111_model.layers.6.mlp: model.layers.6.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.006409 σ=0.610577
    First 10: [-0.012973707169294357, 0.0032996744848787785, -0.005658222362399101, 0.02259504608809948, -0.018768591806292534, 0.0035376313608139753, -0.044487617909908295, -0.0012688753195106983, -0.016758548095822334, 0.004023795481771231]
    Last 10:  [-0.40822726488113403, 0.648589015007019, -0.05979098379611969, -5.645035266876221, -0.46003106236457825, 0.040907375514507294, 0.634305477142334, 0.02390655316412449, 0.6147136688232422, -0.6319624185562134]
  OUT[0]: [1, 4, 640] | μ=-0.004191 σ=0.106109
    First 10: [-0.0004089747089892626, -0.002305450616404414, 4.928826820105314e-05, -0.0062036216259002686, -0.0016609018202871084, 0.002725818660110235, -0.005698984023183584, 0.002841054229065776, -0.0001353949774056673, -0.002037203637883067]
    Last 10:  [0.050114359706640244, -0.010155703872442245, -0.07401532679796219, 0.22497253119945526, 0.010645193979144096, 0.10378660261631012, -0.04157324135303497, 0.06269757449626923, -0.1329967975616455, 0.11691781133413315]

112_model.layers.6.post_feedforward_layernorm: model.layers.6.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.004191 σ=0.106109
    First 10: [-0.0004089747089892626, -0.002305450616404414, 4.928826820105314e-05, -0.0062036216259002686, -0.0016609018202871084, 0.002725818660110235, -0.005698984023183584, 0.002841054229065776, -0.0001353949774056673, -0.002037203637883067]
    Last 10:  [0.050114359706640244, -0.010155703872442245, -0.07401532679796219, 0.22497253119945526, 0.010645193979144096, 0.10378660261631012, -0.04157324135303497, 0.06269757449626923, -0.1329967975616455, 0.11691781133413315]
  OUT[0]: [1, 4, 640] | μ=5.007032 σ=263.156586
    First 10: [-1.8638916015625, -6.594838619232178, 0.04450780525803566, -6.617048263549805, -0.5397810339927673, 7.499963760375977, -14.506180763244629, 1.7218126058578491, -0.19118985533714294, -4.765715599060059]
    Last 10:  [1.139736294746399, -1.5634770393371582, -16.484041213989258, 42.711524963378906, 1.2177900075912476, 12.820303916931152, -5.565414905548096, 3.4717869758605957, -16.50943946838379, 25.469749450683594]
  WEIGHT: [640] | μ=13.812119

113_model.layers.7.input_layernorm: model.layers.7.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=32.760986 σ=1110.876953
    First 10: [-6.132394790649414, -6.0055975914001465, -2.0450680255889893, -4.545061111450195, -1.8460497856140137, 8.17857551574707, -22.834102630615234, 1.6030482053756714, -5.10886812210083, -4.134387493133545]
    Last 10:  [-1.7191652059555054, 9.844852447509766, -36.58302688598633, -17.7073974609375, -6.630548000335693, 13.480947494506836, 4.789007663726807, 3.6655187606811523, -6.617169380187988, -5.618556976318359]
  OUT[0]: [1, 4, 640] | μ=0.067677 σ=2.138058
    First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
    Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
  WEIGHT: [640] | μ=20.039852

114_model.layers.7.self_attn.q_proj: model.layers.7.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.067677 σ=2.138058
    First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
    Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
  OUT[0]: [1, 4, 1024] | μ=-0.050716 σ=2.782881
    First 10: [0.24431437253952026, 0.5895841121673584, -0.12216698378324509, -0.11651404201984406, -0.32702210545539856, -0.03953804820775986, -3.5625674724578857, -0.03614358976483345, -0.07490988820791245, -0.015936069190502167]
    Last 10:  [-1.6367186307907104, -0.6669793128967285, 0.028153032064437866, -1.376692771911621, 0.3988007605075836, -0.31629204750061035, -0.018605593591928482, -0.8642804026603699, 0.7570029497146606, 0.875950813293457]
  WEIGHT: [1024, 640] | μ=-0.000049

115_model.layers.7.self_attn.k_proj: model.layers.7.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.067677 σ=2.138058
    First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
    Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
  OUT[0]: [1, 4, 256] | μ=-0.195011 σ=2.468267
    First 10: [0.03433772921562195, -0.008914414793252945, 2.665456771850586, 0.0015624389052391052, -0.002694077789783478, 1.466333270072937, 0.2729759216308594, 0.004630564711987972, 0.0040669627487659454, 2.3641457557678223]
    Last 10:  [1.7067885398864746, 5.15956974029541, -3.6862330436706543, 5.445260047912598, 1.0766410827636719, -4.450538635253906, -1.3090513944625854, -5.713581085205078, 2.5216875076293945, -0.4549422264099121]
  WEIGHT: [256, 640] | μ=-0.000023

116_model.layers.7.self_attn.v_proj: model.layers.7.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.067677 σ=2.138058
    First 10: [-0.035298313945531845, -0.05449216440320015, -0.010611741803586483, -0.10670822858810425, -0.059044621884822845, 0.06817934662103653, -0.20330189168453217, 0.04454515874385834, -0.03853311762213707, -0.04665756598114967]
    Last 10:  [-0.6197692155838013, 1.0281202793121338, -0.17356383800506592, -4.2810797691345215, -0.9200957417488098, 1.6585582494735718, 0.5515098571777344, 0.9910809397697449, -0.752577543258667, -0.28735101222991943]
  OUT[0]: [1, 4, 256] | μ=-0.006311 σ=1.410400
    First 10: [0.0021059364080429077, -0.060208164155483246, 0.12020843476057053, 0.010753985494375229, 0.3733794093132019, 0.259493887424469, 0.3000595271587372, 0.003366784192621708, 0.020796898752450943, 0.06453762948513031]
    Last 10:  [-0.7692805528640747, -0.31057482957839966, -0.04113595932722092, -3.161863327026367, -1.126147985458374, 0.13373272120952606, -0.4966580271720886, -0.09629499912261963, -0.441213995218277, 1.758063554763794]
  WEIGHT: [256, 640] | μ=-0.000136

117_model.layers.7.self_attn.q_norm: model.layers.7.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.050716 σ=2.782881
    First 10: [0.24431437253952026, 0.5895841121673584, -0.12216698378324509, -0.11651404201984406, -0.32702210545539856, -0.03953804820775986, -3.5625674724578857, -0.03614358976483345, -0.07490988820791245, -0.015936069190502167]
    Last 10:  [-1.6367186307907104, -0.6669793128967285, 0.028153032064437866, -1.376692771911621, 0.3988007605075836, -0.31629204750061035, -0.018605593591928482, -0.8642804026603699, 0.7570029497146606, 0.875950813293457]
  OUT[0]: [1, 4, 4, 256] | μ=0.035273 σ=1.107397
    First 10: [0.6284729838371277, 1.3573774099349976, -0.29251089692115784, -0.2600197196006775, -1.2929643392562866, -0.09369709342718124, 0.26246291399002075, -0.09785735607147217, -0.2138531655073166, -0.026318253949284554]
    Last 10:  [-1.3133049011230469, -0.3256201446056366, 0.05516459420323372, -0.8160979747772217, 0.3986717760562897, -0.23766811192035675, -0.015197214670479298, -0.5823863744735718, 0.8070967793464661, 1.1455516815185547]
  WEIGHT: [256] | μ=0.792706

118_model.layers.7.self_attn.k_norm: model.layers.7.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.195011 σ=2.468267
    First 10: [0.03433772921562195, -0.008914414793252945, 2.665456771850586, 0.0015624389052391052, -0.002694077789783478, 1.466333270072937, 0.2729759216308594, 0.004630564711987972, 0.0040669627487659454, 2.3641457557678223]
    Last 10:  [1.7067885398864746, 5.15956974029541, -3.6862330436706543, 5.445260047912598, 1.0766410827636719, -4.450538635253906, -1.3090513944625854, -5.713581085205078, 2.5216875076293945, -0.4549422264099121]
  OUT[0]: [1, 1, 4, 256] | μ=-0.193346 σ=2.207407
    First 10: [0.008690139278769493, -0.020341401919722557, 0.011058518663048744, 0.0030272286385297775, -0.0030639669857919216, 0.0, 0.01698794774711132, 0.009086997248232365, 0.007795383222401142, 0.009808431379497051]
    Last 10:  [1.591261625289917, 6.954370021820068, -1.075203537940979, 4.032341003417969, 0.5836186408996582, -3.189030170440674, -0.7601634860038757, -4.931138515472412, 1.450904369354248, -0.24176473915576935]
  WEIGHT: [256] | μ=1.029377

119_model.layers.7.self_attn.o_proj: model.layers.7.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.000988 σ=0.737207
    First 10: [0.0021059364080429077, -0.060208164155483246, 0.12020843476057053, 0.010753985494375229, 0.3733794093132019, 0.259493887424469, 0.3000595271587372, 0.003366784192621708, 0.020796898752450943, 0.06453762948513031]
    Last 10:  [0.16163520514965057, -0.325320303440094, 0.019032606855034828, -0.5384095907211304, -0.11045888811349869, 0.1203482523560524, -0.11890638619661331, -1.359337568283081, -0.6725009679794312, 0.5041097402572632]
  OUT[0]: [1, 4, 640] | μ=0.063095 σ=1.067218
    First 10: [0.6702954769134521, 0.01799793541431427, -0.1798892468214035, -0.003955386579036713, 0.021287810057401657, -0.15691615641117096, -0.003825247287750244, -0.04575778916478157, 0.4134564995765686, 0.09486135840415955]
    Last 10:  [0.3193379342556, -0.03784262388944626, 0.17737647891044617, 1.494147539138794, 0.32569509744644165, 0.4048355221748352, -0.5796380639076233, -0.5703885555267334, 0.19646018743515015, 0.34248271584510803]
  WEIGHT: [640, 1024] | μ=0.000078

120_model.layers.7.self_attn: model.layers.7.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.063095 σ=1.067218
    First 10: [0.6702954769134521, 0.01799793541431427, -0.1798892468214035, -0.003955386579036713, 0.021287810057401657, -0.15691615641117096, -0.003825247287750244, -0.04575778916478157, 0.4134564995765686, 0.09486135840415955]
    Last 10:  [0.3193379342556, -0.03784262388944626, 0.17737647891044617, 1.494147539138794, 0.32569509744644165, 0.4048355221748352, -0.5796380639076233, -0.5703885555267334, 0.19646018743515015, 0.34248271584510803]

121_model.layers.7.post_attention_layernorm: model.layers.7.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.063095 σ=1.067218
    First 10: [0.6702954769134521, 0.01799793541431427, -0.1798892468214035, -0.003955386579036713, 0.021287810057401657, -0.15691615641117096, -0.003825247287750244, -0.04575778916478157, 0.4134564995765686, 0.09486135840415955]
    Last 10:  [0.3193379342556, -0.03784262388944626, 0.17737647891044617, 1.494147539138794, 0.32569509744644165, 0.4048355221748352, -0.5796380639076233, -0.5703885555267334, 0.19646018743515015, 0.34248271584510803]
  OUT[0]: [1, 4, 640] | μ=1.753366 σ=36.837067
    First 10: [11.900168418884277, 0.19371411204338074, -0.611291229724884, -0.02161533758044243, 0.031519241631031036, -1.6366747617721558, -0.034380435943603516, -0.1256629079580307, 2.4085516929626465, 0.8631177544593811]
    Last 10:  [1.062107801437378, -0.7601159811019897, 3.192662477493286, 23.775585174560547, 4.474607467651367, 5.984311580657959, -8.870665550231934, -4.0669732093811035, 2.6820130348205566, 6.134688377380371]
  WEIGHT: [640] | μ=10.429689

122_model.layers.7.pre_feedforward_layernorm: model.layers.7.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=34.514355 σ=1121.432617
    First 10: [5.767773628234863, -5.811883449554443, -2.6563591957092285, -4.566676616668701, -1.8145304918289185, 6.541900634765625, -22.86848258972168, 1.4773852825164795, -2.7003164291381836, -3.2712697982788086]
    Last 10:  [-0.6570574045181274, 9.084736824035645, -33.39036560058594, 6.068187713623047, -2.155940532684326, 19.465259552001953, -4.081657886505127, -0.40145444869995117, -3.9351563453674316, 0.5161314010620117]
  OUT[0]: [1, 4, 640] | μ=0.009573 σ=0.531535
    First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
    Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
  WEIGHT: [640] | μ=6.327952

123_model.layers.7.mlp.gate_proj: model.layers.7.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.009573 σ=0.531535
    First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
    Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
  OUT[0]: [1, 4, 2048] | μ=-0.209894 σ=0.635020
    First 10: [-0.041100479662418365, -0.020136309787631035, -0.03717191517353058, -0.005653719417750835, 0.054934289306402206, -0.028081879019737244, 0.03883669897913933, -0.0863206535577774, -0.06271971017122269, 0.0029556734953075647]
    Last 10:  [0.8696548938751221, 0.3737149238586426, 0.4447750747203827, -0.9978352785110474, 0.31425875425338745, -0.4645819067955017, -0.19631743431091309, -0.4586385488510132, -0.8861578702926636, -0.2358107566833496]
  WEIGHT: [2048, 640] | μ=-0.000139

124_model.layers.7.mlp.act_fn: model.layers.7.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.209894 σ=0.635020
    First 10: [-0.041100479662418365, -0.020136309787631035, -0.03717191517353058, -0.005653719417750835, 0.054934289306402206, -0.028081879019737244, 0.03883669897913933, -0.0863206535577774, -0.06271971017122269, 0.0029556734953075647]
    Last 10:  [0.8696548938751221, 0.3737149238586426, 0.4447750747203827, -0.9978352785110474, 0.31425875425338745, -0.4645819067955017, -0.19631743431091309, -0.4586385488510132, -0.8861578702926636, -0.2358107566833496]
  OUT[0]: [1, 4, 2048] | μ=0.022117 σ=0.293096
    First 10: [-0.019876517355442047, -0.0099064065143466, -0.01803484559059143, -0.002814107807353139, 0.028670454397797585, -0.013726378791034222, 0.02001991868019104, -0.0401914119720459, -0.029791545122861862, 0.0014813219895586371]
    Last 10:  [0.7023615837097168, 0.2412988543510437, 0.29877039790153503, -0.15898704528808594, 0.19588634371757507, -0.14919757843017578, -0.08288193494081497, -0.1482660174369812, -0.16650310158729553, -0.09592638164758682]

125_model.layers.7.mlp.up_proj: model.layers.7.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.009573 σ=0.531535
    First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
    Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
  OUT[0]: [1, 4, 2048] | μ=-0.000177 σ=0.486977
    First 10: [-0.05950107425451279, 0.040578581392765045, 0.027255475521087646, 0.04677848145365715, -0.001685265451669693, -0.030796855688095093, 0.006998740136623383, -0.05120092257857323, -0.04394146427512169, 0.0010621780529618263]
    Last 10:  [0.20231947302818298, -0.519631564617157, 1.0311037302017212, -1.3209960460662842, 0.2371266931295395, -0.16631898283958435, -1.0834150314331055, 0.12040401995182037, 1.0218755006790161, -0.2611122131347656]
  WEIGHT: [2048, 640] | μ=0.000006

126_model.layers.7.mlp.down_proj: model.layers.7.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.000408 σ=0.178466
    First 10: [0.0011826740810647607, -0.00040198792703449726, -0.0004915482713840902, -0.00013163969560991973, -4.8317328037228435e-05, 0.00042272929567843676, 0.00014011420716997236, 0.0020578373223543167, 0.0013090841239318252, 1.5734276530565694e-06]
    Last 10:  [0.14210142195224762, -0.12538650631904602, 0.308063268661499, 0.2100212574005127, 0.04644988104701042, 0.024814389646053314, 0.0897955372929573, -0.017851823940873146, -0.1701454371213913, 0.025047549977898598]
  OUT[0]: [1, 4, 640] | μ=0.001097 σ=0.130703
    First 10: [0.005046057049185038, -0.004904364235699177, 0.000976785086095333, -0.003465132787823677, -0.0002820085792336613, -0.0024692232254892588, 0.002825802657753229, 0.0004924096865579486, 0.001484330277889967, -0.00023496626818086952]
    Last 10:  [-0.010282468050718307, -0.0758746862411499, -0.13669851422309875, -0.05427517741918564, -0.09429673850536346, 0.1394980102777481, 0.03237081319093704, 0.034539345651865005, -0.12881425023078918, 0.02473367564380169]
  WEIGHT: [640, 2048] | μ=0.000014

127_model.layers.7.mlp: model.layers.7.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.009573 σ=0.531535
    First 10: [0.009837018325924873, -0.017121154814958572, -0.006365105509757996, -0.03578861802816391, -0.022916175425052643, 0.019640540704131126, -0.07864969223737717, 0.013743824325501919, -0.008030951954424381, -0.0122189000248909]
    Last 10:  [-0.07159125059843063, 0.24613183736801147, -0.07273808121681213, 0.3181454539299011, -0.08019627630710602, 0.6813053488731384, -0.13748273253440857, -0.02974887192249298, -0.14119255542755127, 0.008163337595760822]
  OUT[0]: [1, 4, 640] | μ=0.001097 σ=0.130703
    First 10: [0.005046057049185038, -0.004904364235699177, 0.000976785086095333, -0.003465132787823677, -0.0002820085792336613, -0.0024692232254892588, 0.002825802657753229, 0.0004924096865579486, 0.001484330277889967, -0.00023496626818086952]
    Last 10:  [-0.010282468050718307, -0.0758746862411499, -0.13669851422309875, -0.05427517741918564, -0.09429673850536346, 0.1394980102777481, 0.03237081319093704, 0.034539345651865005, -0.12881425023078918, 0.02473367564380169]

128_model.layers.7.post_feedforward_layernorm: model.layers.7.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.001097 σ=0.130703
    First 10: [0.005046057049185038, -0.004904364235699177, 0.000976785086095333, -0.003465132787823677, -0.0002820085792336613, -0.0024692232254892588, 0.002825802657753229, 0.0004924096865579486, 0.001484330277889967, -0.00023496626818086952]
    Last 10:  [-0.010282468050718307, -0.0758746862411499, -0.13669851422309875, -0.05427517741918564, -0.09429673850536346, 0.1394980102777481, 0.03237081319093704, 0.034539345651865005, -0.12881425023078918, 0.02473367564380169]
  OUT[0]: [1, 4, 640] | μ=5.195705 σ=269.921295
    First 10: [28.188772201538086, -18.884523391723633, 0.6966924071311951, -2.8171727657318115, -0.06584612280130386, -8.818188667297363, 8.174775123596191, 0.21060405671596527, 1.710206389427185, -0.555507481098175]
    Last 10:  [-0.20901812613010406, -17.84382438659668, -31.29306411743164, -9.708910942077637, -13.329330444335938, 26.17534828186035, 6.478981971740723, 2.5707762241363525, -23.687240600585938, 5.6929802894592285]
  WEIGHT: [640] | μ=16.627895

129_model.layers.8.input_layernorm: model.layers.8.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=39.710060 σ=1385.020020
    First 10: [33.956546783447266, -24.696407318115234, -1.9596667289733887, -7.383849143981934, -1.8803765773773193, -2.2762880325317383, -14.693707466125488, 1.687989354133606, -0.9901100397109985, -3.826777219772339]
    Last 10:  [-0.8660755157470703, -8.759087562561035, -64.68342590332031, -3.64072322845459, -15.485271453857422, 45.64060974121094, 2.3973240852355957, 2.1693217754364014, -27.62239646911621, 6.20911169052124]
  OUT[0]: [1, 4, 640] | μ=0.050684 σ=2.506804
    First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
    Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
  WEIGHT: [640] | μ=19.921200

130_model.layers.8.self_attn.q_proj: model.layers.8.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.050684 σ=2.506804
    First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
    Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
  OUT[0]: [1, 4, 1024] | μ=0.033616 σ=2.241807
    First 10: [-0.20592297613620758, 0.1215912327170372, -3.636873483657837, 0.4720737934112549, -0.3297586739063263, -0.04457107186317444, 0.30267030000686646, -0.42607206106185913, -3.6061618328094482, -0.15356773138046265]
    Last 10:  [1.1324197053909302, 1.2875440120697021, 4.043564796447754, 1.1188546419143677, -0.38098055124282837, 3.2240681648254395, -1.9775700569152832, -0.6778863668441772, 1.2239301204681396, -2.886838674545288]
  WEIGHT: [1024, 640] | μ=0.000092

131_model.layers.8.self_attn.k_proj: model.layers.8.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.050684 σ=2.506804
    First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
    Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
  OUT[0]: [1, 4, 256] | μ=0.038035 σ=2.222081
    First 10: [0.000681266188621521, -0.0008283713832497597, 0.001509547233581543, 0.0006539206951856613, -0.00836212933063507, -0.007144542410969734, -0.0020429715514183044, -0.004627332091331482, 0.0010004937648773193, 0.000714537687599659]
    Last 10:  [5.0788421630859375, 1.1583746671676636, 7.681475639343262, 7.742763042449951, -2.562598466873169, 7.324058532714844, 3.7039167881011963, -3.3107681274414062, 4.2640228271484375, -1.4356439113616943]
  WEIGHT: [256, 640] | μ=0.000161

132_model.layers.8.self_attn.v_proj: model.layers.8.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.050684 σ=2.506804
    First 10: [0.12385936081409454, -0.14582060277462006, -0.01005194429308176, -0.1346660554409027, -0.04389652609825134, -0.013647978194057941, -0.0984836295247078, 0.039251409471035004, -0.00681672478094697, -0.03123212791979313]
    Last 10:  [-0.21243183314800262, -0.575964093208313, -0.28838178515434265, -0.45030006766319275, -1.5758665800094604, 3.739530563354492, 0.17765650153160095, 0.43246737122535706, -2.1911439895629883, 0.37588366866111755]
  OUT[0]: [1, 4, 256] | μ=0.047027 σ=1.579868
    First 10: [-0.0016240105032920837, -0.05041125416755676, 0.563729465007782, -0.1687164157629013, 0.23277026414871216, -0.46235352754592896, 0.44956421852111816, -0.5913743376731873, 0.4182305335998535, -0.7496524453163147]
    Last 10:  [0.7890281081199646, 0.3912888765335083, 0.08059287071228027, -0.6172995567321777, -1.8521642684936523, -2.1216602325439453, 0.15696388483047485, 3.0107362270355225, -2.226724624633789, -0.9957597255706787]
  WEIGHT: [256, 640] | μ=-0.000041

133_model.layers.8.self_attn.q_norm: model.layers.8.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.033616 σ=2.241807
    First 10: [-0.20592297613620758, 0.1215912327170372, -3.636873483657837, 0.4720737934112549, -0.3297586739063263, -0.04457107186317444, 0.30267030000686646, -0.42607206106185913, -3.6061618328094482, -0.15356773138046265]
    Last 10:  [1.1324197053909302, 1.2875440120697021, 4.043564796447754, 1.1188546419143677, -0.38098055124282837, 3.2240681648254395, -1.9775700569152832, -0.6778863668441772, 1.2239301204681396, -2.886838674545288]
  OUT[0]: [1, 4, 4, 256] | μ=0.131853 σ=2.182832
    First 10: [-1.1646696329116821, 0.467898428440094, -0.5858433842658997, 1.8799692392349243, -1.2896103858947754, -0.25128957629203796, 0.6155375242233276, -1.3688578605651855, -0.6131682991981506, -0.6885231137275696]
    Last 10:  [0.3508286476135254, 0.4712815284729004, 0.8499079346656799, 0.18231385946273804, -0.09207602590322495, 0.6600008606910706, -1.1779319047927856, -0.2148854285478592, 0.4847625494003296, -1.6562870740890503]
  WEIGHT: [256] | μ=0.649311

134_model.layers.8.self_attn.k_norm: model.layers.8.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=0.038035 σ=2.222081
    First 10: [0.000681266188621521, -0.0008283713832497597, 0.001509547233581543, 0.0006539206951856613, -0.00836212933063507, -0.007144542410969734, -0.0020429715514183044, -0.004627332091331482, 0.0010004937648773193, 0.000714537687599659]
    Last 10:  [5.0788421630859375, 1.1583746671676636, 7.681475639343262, 7.742763042449951, -2.562598466873169, 7.324058532714844, 3.7039167881011963, -3.3107681274414062, 4.2640228271484375, -1.4356439113616943]
  OUT[0]: [1, 1, 4, 256] | μ=-0.011741 σ=2.187317
    First 10: [0.001158080529421568, -0.0010838708840310574, 0.001208157860673964, 0.0007276220712810755, -0.007600624114274979, -0.0062353103421628475, -0.00393295893445611, -0.004937946796417236, 0.0006314041092991829, 0.0008985260501503944]
    Last 10:  [3.083397388458252, 0.8997129201889038, 8.332574844360352, 12.890395164489746, -2.0659613609313965, 8.49692153930664, 1.8177518844604492, -2.0506763458251953, 2.6131701469421387, -0.8292441368103027]
  WEIGHT: [256] | μ=0.914341

135_model.layers.8.self_attn.o_proj: model.layers.8.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.032283 σ=0.876659
    First 10: [-0.0016240105032920837, -0.05041125416755676, 0.563729465007782, -0.1687164157629013, 0.23277026414871216, -0.46235352754592896, 0.44956421852111816, -0.5913743376731873, 0.4182305335998535, -0.7496524453163147]
    Last 10:  [-0.14225426316261292, 0.12044709175825119, 0.1741883009672165, -0.5136483907699585, -0.278732568025589, -0.5724631547927856, 0.12035499513149261, 0.8774973750114441, -0.678219199180603, -0.4439106583595276]
  OUT[0]: [1, 4, 640] | μ=-0.004267 σ=0.962400
    First 10: [0.5100055932998657, -0.024546053260564804, 0.09209299087524414, 0.4076889455318451, -0.22412195801734924, 0.041946008801460266, -0.2892238199710846, 0.038145676255226135, -0.41832679510116577, -0.18366047739982605]
    Last 10:  [-0.21824318170547485, 0.17788654565811157, -3.678676128387451, 0.6267490386962891, 0.3044055998325348, 0.21921947598457336, 0.7549707889556885, 0.39805445075035095, 0.35519930720329285, 0.12530586123466492]
  WEIGHT: [640, 1024] | μ=0.000009

136_model.layers.8.self_attn: model.layers.8.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=-0.004267 σ=0.962400
    First 10: [0.5100055932998657, -0.024546053260564804, 0.09209299087524414, 0.4076889455318451, -0.22412195801734924, 0.041946008801460266, -0.2892238199710846, 0.038145676255226135, -0.41832679510116577, -0.18366047739982605]
    Last 10:  [-0.21824318170547485, 0.17788654565811157, -3.678676128387451, 0.6267490386962891, 0.3044055998325348, 0.21921947598457336, 0.7549707889556885, 0.39805445075035095, 0.35519930720329285, 0.12530586123466492]

137_model.layers.8.post_attention_layernorm: model.layers.8.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.004267 σ=0.962400
    First 10: [0.5100055932998657, -0.024546053260564804, 0.09209299087524414, 0.4076889455318451, -0.22412195801734924, 0.041946008801460266, -0.2892238199710846, 0.038145676255226135, -0.41832679510116577, -0.18366047739982605]
    Last 10:  [-0.21824318170547485, 0.17788654565811157, -3.678676128387451, 0.6267490386962891, 0.3044055998325348, 0.21921947598457336, 0.7549707889556885, 0.39805445075035095, 0.35519930720329285, 0.12530586123466492]
  OUT[0]: [1, 4, 640] | μ=0.164990 σ=26.675339
    First 10: [34.708866119384766, -1.252876877784729, 0.3928047716617584, 2.7938294410705566, -0.29426538944244385, 1.9348328113555908, -9.732335090637207, 0.097802072763443, -2.9853525161743164, -4.617749214172363]
    Last 10:  [-0.26583927869796753, 5.822103023529053, -3.0774121284484863, 15.263654708862305, 4.530411720275879, 5.5365376472473145, 19.359149932861328, 5.513837814331055, 8.284259796142578, 2.187830686569214]
  WEIGHT: [640] | μ=18.394093

138_model.layers.8.pre_feedforward_layernorm: model.layers.8.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=39.875053 σ=1377.840210
    First 10: [68.66541290283203, -25.949283599853516, -1.5668619871139526, -4.590019702911377, -2.1746420860290527, -0.34145522117614746, -24.426042556762695, 1.7857913970947266, -3.9754624366760254, -8.444526672363281]
    Last 10:  [-1.1319148540496826, -2.9369845390319824, -67.7608413696289, 11.622931480407715, -10.954859733581543, 51.177146911621094, 21.756473541259766, 7.683159828186035, -19.338136672973633, 8.396942138671875]
  OUT[0]: [1, 4, 640] | μ=0.009105 σ=0.584839
    First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
    Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
  WEIGHT: [640] | μ=5.790224

139_model.layers.8.mlp.gate_proj: model.layers.8.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.009105 σ=0.584839
    First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
    Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
  OUT[0]: [1, 4, 2048] | μ=-0.225966 σ=0.643881
    First 10: [-0.1339147835969925, -0.06319180130958557, -0.08838286250829697, 0.0020267926156520844, -0.006209836341440678, -0.020326368510723114, -0.007588140666484833, 0.02135574445128441, 0.014291232451796532, 0.1522691547870636]
    Last 10:  [-0.1036207377910614, -0.08026671409606934, -2.0917277336120605, 0.24701295793056488, -2.812556743621826, 0.7814949750900269, -1.3370630741119385, -0.335783988237381, -0.07243295013904572, -1.0749191045761108]
  WEIGHT: [2048, 640] | μ=-0.000145

140_model.layers.8.mlp.act_fn: model.layers.8.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.225966 σ=0.643881
    First 10: [-0.1339147835969925, -0.06319180130958557, -0.08838286250829697, 0.0020267926156520844, -0.006209836341440678, -0.020326368510723114, -0.007588140666484833, 0.02135574445128441, 0.014291232451796532, 0.1522691547870636]
    Last 10:  [-0.1036207377910614, -0.08026671409606934, -2.0917277336120605, 0.24701295793056488, -2.812556743621826, 0.7814949750900269, -1.3370630741119385, -0.335783988237381, -0.07243295013904572, -1.0749191045761108]
  OUT[0]: [1, 4, 2048] | μ=0.019959 σ=0.253419
    First 10: [-0.05982452258467674, -0.030003907158970833, -0.04107915237545967, 0.0010150352027267218, -0.0030895343516021967, -0.009998368099331856, -0.0037710994947701693, 0.010859803296625614, 0.007227092981338501, 0.08534861356019974]
    Last 10:  [-0.0475345142185688, -0.037565842270851135, -0.037957943975925446, 0.1476016491651535, -0.006448244210332632, 0.6116324663162231, -0.12137235701084137, -0.12374593317508698, -0.034125249832868576, -0.1519627571105957]

141_model.layers.8.mlp.up_proj: model.layers.8.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.009105 σ=0.584839
    First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
    Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
  OUT[0]: [1, 4, 2048] | μ=0.011525 σ=0.511516
    First 10: [0.0069982074201107025, -0.0018233507871627808, -0.02581140398979187, 0.04949656501412392, 0.05429309979081154, 0.03644245117902756, -0.06297539174556732, 0.0073771290481090546, -0.07512594759464264, -0.0323769748210907]
    Last 10:  [-0.3077259063720703, 0.8118508458137512, -0.4542738199234009, -0.467755526304245, 0.3847951292991638, -0.054115667939186096, -0.3261362314224243, 0.1422538459300995, -0.22436045110225677, 1.3267168998718262]
  WEIGHT: [2048, 640] | μ=0.000069

142_model.layers.8.mlp.down_proj: model.layers.8.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.001944 σ=0.132516
    First 10: [-0.0004186644218862057, 5.470764881465584e-05, 0.0010603106347844005, 5.024075653636828e-05, -0.000167740392498672, -0.00036436502705328166, 0.0002374864707235247, 8.011417230591178e-05, -0.000542942201718688, -0.00276333000510931]
    Last 10:  [0.014627601020038128, -0.03049786016345024, 0.01724329963326454, -0.06904149055480957, -0.0024812528863549232, -0.03309889882802963, 0.03958392143249512, -0.017603334039449692, 0.007656356319785118, -0.20161156356334686]
  OUT[0]: [1, 4, 640] | μ=-0.003363 σ=0.078347
    First 10: [0.002752384403720498, -0.0028649973683059216, 0.0008750008419156075, 0.0019868325907737017, 0.0019726967439055443, 0.0018472479423508048, -0.0012814633082598448, -0.0007704828749410808, 0.0017381240613758564, 0.0009488607756793499]
    Last 10:  [-0.03431360796093941, -0.05467401444911957, 0.08141867816448212, 0.06532250344753265, -0.0410686619579792, -0.06537225842475891, 0.03702259063720703, 0.08513130247592926, 0.03497286140918732, 0.017679432407021523]
  WEIGHT: [640, 2048] | μ=-0.000003

143_model.layers.8.mlp: model.layers.8.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.009105 σ=0.584839
    First 10: [0.08390109986066818, -0.046741753816604614, -0.003002114361152053, -0.029911823570728302, -0.01956067606806755, -0.0006346419686451554, -0.05829031765460968, 0.01368633471429348, -0.009258991107344627, -0.02267102524638176]
    Last 10:  [-0.08885595202445984, -0.05534025654196739, -0.11829652637243271, 0.43661177158355713, -0.3139127492904663, 1.213856816291809, 0.5107969045639038, 0.3866707682609558, -0.5005854964256287, 0.13799996674060822]
  OUT[0]: [1, 4, 640] | μ=-0.003363 σ=0.078347
    First 10: [0.002752384403720498, -0.0028649973683059216, 0.0008750008419156075, 0.0019868325907737017, 0.0019726967439055443, 0.0018472479423508048, -0.0012814633082598448, -0.0007704828749410808, 0.0017381240613758564, 0.0009488607756793499]
    Last 10:  [-0.03431360796093941, -0.05467401444911957, 0.08141867816448212, 0.06532250344753265, -0.0410686619579792, -0.06537225842475891, 0.03702259063720703, 0.08513130247592926, 0.03497286140918732, 0.017679432407021523]

144_model.layers.8.post_feedforward_layernorm: model.layers.8.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.003363 σ=0.078347
    First 10: [0.002752384403720498, -0.0028649973683059216, 0.0008750008419156075, 0.0019868325907737017, 0.0019726967439055443, 0.0018472479423508048, -0.0012814633082598448, -0.0007704828749410808, 0.0017381240613758564, 0.0009488607756793499]
    Last 10:  [-0.03431360796093941, -0.05467401444911957, 0.08141867816448212, 0.06532250344753265, -0.0410686619579792, -0.06537225842475891, 0.03702259063720703, 0.08513130247592926, 0.03497286140918732, 0.017679432407021523]
  OUT[0]: [1, 4, 640] | μ=4.812262 σ=297.411346
    First 10: [32.48869705200195, -25.36347198486328, 0.9016271829605103, 2.7562763690948486, 0.5583570599555969, 15.760944366455078, -7.93302583694458, -0.46029332280158997, 2.7457590103149414, 4.687039375305176]
    Last 10:  [-0.6746705770492554, -20.607059478759766, 15.527055740356445, 19.42179298400879, -7.924557209014893, -19.043928146362305, 11.285587310791016, 10.993830680847168, 9.76799488067627, 4.539685249328613]
  WEIGHT: [640] | μ=20.080070

145_model.layers.9.input_layernorm: model.layers.9.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=44.687309 σ=1667.299927
    First 10: [101.15411376953125, -51.3127555847168, -0.6652348041534424, -1.8337433338165283, -1.6162850856781006, 15.419488906860352, -32.35906982421875, 1.325498104095459, -1.229703426361084, -3.7574872970581055]
    Last 10:  [-1.806585431098938, -23.544044494628906, -52.233787536621094, 31.044723510742188, -18.879417419433594, 32.133216857910156, 33.04206085205078, 18.676990509033203, -9.570141792297363, 12.936627388000488]
  OUT[0]: [1, 4, 640] | μ=0.056097 σ=2.281155
    First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
    Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
  WEIGHT: [640] | μ=16.710205

146_model.layers.9.self_attn.q_proj: model.layers.9.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.056097 σ=2.281155
    First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
    Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
  OUT[0]: [1, 4, 1024] | μ=0.166783 σ=2.789583
    First 10: [0.02467852458357811, -0.026729445904493332, -0.05607378110289574, 0.1246224045753479, 0.18721359968185425, -0.06746292114257812, 0.010742666199803352, 0.016207100823521614, -0.08036583662033081, 1.1960859298706055]
    Last 10:  [-1.2501798868179321, -1.1280786991119385, -1.1663203239440918, -6.917331218719482, 1.7192314863204956, 2.2820048332214355, -0.8183374404907227, -3.1023542881011963, 6.532040596008301, -5.053793907165527]
  WEIGHT: [1024, 640] | μ=-0.000029

147_model.layers.9.self_attn.k_proj: model.layers.9.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.056097 σ=2.281155
    First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
    Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
  OUT[0]: [1, 4, 256] | μ=0.125679 σ=2.435753
    First 10: [-0.005412565544247627, 0.0009385999292135239, 0.00040969252586364746, -1.9190289974212646, 0.012705106288194656, 0.0020060623064637184, -0.0035470612347126007, -0.0073425909504294395, 0.0016313884407281876, -0.1791614592075348]
    Last 10:  [-1.3604893684387207, -5.823982238769531, -0.28018632531166077, -5.998579502105713, -1.6269752979278564, -0.4232148826122284, -1.4390008449554443, -3.0131161212921143, 11.10321044921875, -4.0316667556762695]
  WEIGHT: [256, 640] | μ=-0.000128

148_model.layers.9.self_attn.v_proj: model.layers.9.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.056097 σ=2.281155
    First 10: [0.2437140792608261, -0.17744481563568115, -0.0026147200260311365, -0.025780919939279556, -0.02651090733706951, 0.05303081125020981, -0.1485898792743683, 0.02504759281873703, -0.006738841999322176, -0.02087525464594364]
    Last 10:  [-0.4189130663871765, -1.1716562509536743, -0.20340104401111603, 2.7611396312713623, -1.599189281463623, 1.9988645315170288, 2.0116677284240723, 3.5002691745758057, -0.6257151365280151, 0.7704862356185913]
  OUT[0]: [1, 4, 256] | μ=-0.064794 σ=1.284427
    First 10: [0.0455450564622879, 0.08494949340820312, -0.1192307248711586, -0.0823470801115036, -0.23964686691761017, 0.07151760160923004, 0.06058730185031891, 0.2196996510028839, -0.1093779131770134, -0.13625472784042358]
    Last 10:  [1.2232604026794434, -0.4954414367675781, 1.7907648086547852, 1.1888755559921265, 0.13683170080184937, -0.012479547411203384, 0.003463447093963623, -1.7538161277770996, -0.3638496398925781, -0.520622193813324]
  WEIGHT: [256, 640] | μ=-0.000083

149_model.layers.9.self_attn.q_norm: model.layers.9.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.166783 σ=2.789583
    First 10: [0.02467852458357811, -0.026729445904493332, -0.05607378110289574, 0.1246224045753479, 0.18721359968185425, -0.06746292114257812, 0.010742666199803352, 0.016207100823521614, -0.08036583662033081, 1.1960859298706055]
    Last 10:  [-1.2501798868179321, -1.1280786991119385, -1.1663203239440918, -6.917331218719482, 1.7192314863204956, 2.2820048332214355, -0.8183374404907227, -3.1023542881011963, 6.532040596008301, -5.053793907165527]
  OUT[0]: [1, 4, 4, 256] | μ=0.048057 σ=1.091970
    First 10: [0.19254617393016815, -0.10917513817548752, -0.39707162976264954, 0.5999670028686523, 0.6902206540107727, -0.693884015083313, 0.06273304671049118, 0.057837534695863724, -0.5253132581710815, 0.11497420072555542]
    Last 10:  [-0.7786942720413208, -0.28496021032333374, -0.4475806951522827, -3.7436676025390625, 1.3278546333312988, 1.7814663648605347, -0.28243133425712585, -5.359051704406738, 1.8692885637283325, -1.5957776308059692]
  WEIGHT: [256] | μ=0.828014

150_model.layers.9.self_attn.k_norm: model.layers.9.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=0.125679 σ=2.435753
    First 10: [-0.005412565544247627, 0.0009385999292135239, 0.00040969252586364746, -1.9190289974212646, 0.012705106288194656, 0.0020060623064637184, -0.0035470612347126007, -0.0073425909504294395, 0.0016313884407281876, -0.1791614592075348]
    Last 10:  [-1.3604893684387207, -5.823982238769531, -0.28018632531166077, -5.998579502105713, -1.6269752979278564, -0.4232148826122284, -1.4390008449554443, -3.0131161212921143, 11.10321044921875, -4.0316667556762695]
  OUT[0]: [1, 1, 4, 256] | μ=0.029138 σ=2.373605
    First 10: [-0.008878723718225956, 0.0031822719611227512, 0.0008405378321185708, -0.008768684230744839, 0.03901220113039017, 0.003721545683220029, -0.007763491477817297, -0.021807987242937088, 0.003488639835268259, -0.014735673554241657]
    Last 10:  [-0.9166615605354309, -6.881414413452148, -0.42879942059516907, -6.851455211639404, -1.7484478950500488, -0.42147573828697205, -1.2833014726638794, -0.6569403409957886, 13.61894416809082, -5.5803070068359375]
  WEIGHT: [256] | μ=1.036833

151_model.layers.9.self_attn.o_proj: model.layers.9.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.043312 σ=0.331764
    First 10: [0.0455450564622879, 0.08494949340820312, -0.1192307248711586, -0.0823470801115036, -0.23964686691761017, 0.07151760160923004, 0.06058730185031891, 0.2196996510028839, -0.1093779131770134, -0.13625472784042358]
    Last 10:  [0.15846066176891327, -0.06593266874551773, 0.1664915531873703, 0.2848215401172638, 0.0798199400305748, -0.30302372574806213, -0.020718175917863846, -0.29387107491493225, -0.06467234343290329, 0.024669818580150604]
  OUT[0]: [1, 4, 640] | μ=-0.000331 σ=0.693421
    First 10: [0.12421678006649017, -0.3773933947086334, 0.06552499532699585, -0.016443565487861633, 0.014863226562738419, -0.08929777145385742, -0.11459504812955856, -0.021725639700889587, 0.004523247480392456, -0.13275565207004547]
    Last 10:  [-0.062127433717250824, 0.1397394835948944, 0.12208817154169083, -0.050935082137584686, -0.035726457834243774, 0.3017229437828064, -0.1014072373509407, 0.015057563781738281, -0.0320780947804451, -0.07806655764579773]
  WEIGHT: [640, 1024] | μ=-0.000006

152_model.layers.9.self_attn: model.layers.9.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=-0.000331 σ=0.693421
    First 10: [0.12421678006649017, -0.3773933947086334, 0.06552499532699585, -0.016443565487861633, 0.014863226562738419, -0.08929777145385742, -0.11459504812955856, -0.021725639700889587, 0.004523247480392456, -0.13275565207004547]
    Last 10:  [-0.062127433717250824, 0.1397394835948944, 0.12208817154169083, -0.050935082137584686, -0.035726457834243774, 0.3017229437828064, -0.1014072373509407, 0.015057563781738281, -0.0320780947804451, -0.07806655764579773]

153_model.layers.9.post_attention_layernorm: model.layers.9.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.000331 σ=0.693421
    First 10: [0.12421678006649017, -0.3773933947086334, 0.06552499532699585, -0.016443565487861633, 0.014863226562738419, -0.08929777145385742, -0.11459504812955856, -0.021725639700889587, 0.004523247480392456, -0.13275565207004547]
    Last 10:  [-0.062127433717250824, 0.1397394835948944, 0.12208817154169083, -0.050935082137584686, -0.035726457834243774, 0.3017229437828064, -0.1014072373509407, 0.015057563781738281, -0.0320780947804451, -0.07806655764579773]
  OUT[0]: [1, 4, 640] | μ=0.377312 σ=18.516899
    First 10: [10.098784446716309, -25.69425392150879, 0.31244587898254395, -0.20332732796669006, 0.02286282368004322, -5.650547504425049, -4.497640132904053, -0.09625396877527237, 0.03713615983724594, -4.731902599334717]
    Last 10:  [-0.14627674221992493, 7.923409461975098, 2.0506889820098877, -1.9286869764328003, -1.019805669784546, 13.651311874389648, -4.469973087310791, 0.36841335892677307, -1.3330070972442627, -1.6826668977737427]
  WEIGHT: [640] | μ=21.472797

154_model.layers.9.pre_feedforward_layernorm: model.layers.9.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=45.064625 σ=1674.910156
    First 10: [111.25289916992188, -77.00701141357422, -0.35278892517089844, -2.0370707511901855, -1.593422293663025, 9.768941879272461, -36.85670852661133, 1.2292441129684448, -1.192567229270935, -8.489389419555664]
    Last 10:  [-1.9528621435165405, -15.620635032653809, -50.18309783935547, 29.116037368774414, -19.89922332763672, 45.78453063964844, 28.57208824157715, 19.0454044342041, -10.903148651123047, 11.253960609436035]
  OUT[0]: [1, 4, 640] | μ=0.003974 σ=0.403407
    First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
    Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
  WEIGHT: [640] | μ=3.108285

155_model.layers.9.mlp.gate_proj: model.layers.9.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.003974 σ=0.403407
    First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
    Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
  OUT[0]: [1, 4, 2048] | μ=-0.082205 σ=0.430109
    First 10: [-0.03826673701405525, -0.011213406920433044, 0.009805092588067055, 0.04224740341305733, 0.021607521921396255, -0.028274821117520332, 0.013320150785148144, -0.007507678121328354, 0.018904097378253937, -0.017052382230758667]
    Last 10:  [-0.11123155057430267, 0.08569207787513733, -0.04644787684082985, -0.8346119523048401, -0.15960519015789032, -0.9508532285690308, -0.010694049298763275, -0.7839912176132202, -0.3340357542037964, 0.3532089591026306]
  WEIGHT: [2048, 640] | μ=-0.000209

156_model.layers.9.mlp.act_fn: model.layers.9.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.082205 σ=0.430109
    First 10: [-0.03826673701405525, -0.011213406920433044, 0.009805092588067055, 0.04224740341305733, 0.021607521921396255, -0.028274821117520332, 0.013320150785148144, -0.007507678121328354, 0.018904097378253937, -0.017052382230758667]
    Last 10:  [-0.11123155057430267, 0.08569207787513733, -0.04644787684082985, -0.8346119523048401, -0.15960519015789032, -0.9508532285690308, -0.010694049298763275, -0.7839912176132202, -0.3340357542037964, 0.3532089591026306]
  OUT[0]: [1, 4, 2048] | μ=0.022524 σ=0.206507
    First 10: [-0.018549323081970215, -0.005556541029363871, 0.004940900020301342, 0.021835537627339363, 0.010990006849169731, -0.013818512670695782, 0.0067308563739061356, -0.0037313527427613735, 0.009594608098268509, -0.008410191163420677]
    Last 10:  [-0.05069008842110634, 0.045771922916173935, -0.0223635695874691, -0.16866013407707214, -0.06968320161104202, -0.16257856786251068, -0.005301401484757662, -0.16983085870742798, -0.12332186847925186, 0.22535467147827148]

157_model.layers.9.mlp.up_proj: model.layers.9.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.003974 σ=0.403407
    First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
    Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
  OUT[0]: [1, 4, 2048] | μ=0.006251 σ=0.338116
    First 10: [0.05998493358492851, 0.04723457992076874, -0.03580149635672569, 0.011865541338920593, 0.008993403054773808, 0.0197139885276556, 0.04562540352344513, 0.013179447501897812, -0.039749689400196075, -0.019456282258033752]
    Last 10:  [1.8716135025024414, -0.06909598410129547, 0.029802411794662476, -0.013266190886497498, 0.09007932245731354, -0.08685919642448425, 0.38390839099884033, -0.33110395073890686, 0.02308119647204876, 0.03661017119884491]
  WEIGHT: [2048, 640] | μ=0.000016

158_model.layers.9.mlp.down_proj: model.layers.9.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.000056 σ=0.077191
    First 10: [-0.0011126799508929253, -0.0002624608750920743, -0.00017689161177258939, 0.00025909047690220177, 9.883756138151512e-05, -0.00027241799398325384, 0.00030709803104400635, -4.917716796626337e-05, -0.00038138270610943437, 0.00016363104805350304]
    Last 10:  [-0.09487225115299225, -0.0031626559793949127, -0.0006664883112534881, 0.002237477572634816, -0.006277015432715416, 0.014121443964540958, -0.0020352525170892477, 0.05623167008161545, -0.0028464163187891245, 0.008250272832810879]
  OUT[0]: [1, 4, 640] | μ=0.000846 σ=0.065763
    First 10: [0.0005802260129712522, -0.0010055621387436986, -0.0003045388148166239, -0.00030722582596354187, 0.0008253133855760098, -0.0002181680902140215, 0.0007850056863389909, 0.0006270183948799968, 0.0002043709100689739, -0.0001967987627722323]
    Last 10:  [0.00511874258518219, -0.028077974915504456, 0.11696401238441467, 0.010940564796328545, -0.020559977740049362, -0.08478829264640808, -0.14219513535499573, -0.020652109757065773, 0.025808138772845268, -0.022718943655490875]
  WEIGHT: [640, 2048] | μ=0.000007

159_model.layers.9.mlp: model.layers.9.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.003974 σ=0.403407
    First 10: [0.0547192245721817, -0.04838646203279495, -0.00047987166908569634, -0.00793868862092495, -0.009779593907296658, 0.006666960194706917, -0.03712296485900879, 0.0062947627156972885, -0.0018522907048463821, -0.010468635708093643]
    Last 10:  [-0.12632247805595398, -0.14784443378448486, -0.07706424593925476, 0.49274858832359314, -0.3517341911792755, 0.5538669228553772, 0.3304210603237152, 0.4751192331314087, -0.1387322098016739, 0.16224174201488495]
  OUT[0]: [1, 4, 640] | μ=0.000846 σ=0.065763
    First 10: [0.0005802260129712522, -0.0010055621387436986, -0.0003045388148166239, -0.00030722582596354187, 0.0008253133855760098, -0.0002181680902140215, 0.0007850056863389909, 0.0006270183948799968, 0.0002043709100689739, -0.0001967987627722323]
    Last 10:  [0.00511874258518219, -0.028077974915504456, 0.11696401238441467, 0.010940564796328545, -0.020559977740049362, -0.08478829264640808, -0.14219513535499573, -0.020652109757065773, 0.025808138772845268, -0.022718943655490875]

160_model.layers.9.post_feedforward_layernorm: model.layers.9.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.000846 σ=0.065763
    First 10: [0.0005802260129712522, -0.0010055621387436986, -0.0003045388148166239, -0.00030722582596354187, 0.0008253133855760098, -0.0002181680902140215, 0.0007850056863389909, 0.0006270183948799968, 0.0002043709100689739, -0.0001967987627722323]
    Last 10:  [0.00511874258518219, -0.028077974915504456, 0.11696401238441467, 0.010940564796328545, -0.020559977740049362, -0.08478829264640808, -0.14219513535499573, -0.020652109757065773, 0.025808138772845268, -0.022718943655490875]
  OUT[0]: [1, 4, 640] | μ=1.331436 σ=103.208344
    First 10: [31.50017738342285, -44.134002685546875, -1.0937179327011108, -1.5636688470840454, 0.7751011252403259, -8.152524948120117, 20.478567123413086, 1.2779006958007812, 1.1302326917648315, -4.0412278175354]
    Last 10:  [0.13835592567920685, -18.577091217041016, 28.818344116210938, 5.315811634063721, -5.9867353439331055, -45.28749465942383, -81.82987976074219, -4.803733825683594, 14.051558494567871, -8.57260799407959]
  WEIGHT: [640] | μ=30.176754

161_model.layers.10.input_layernorm: model.layers.10.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=46.396053 σ=1768.300903
    First 10: [142.75308227539062, -121.1410140991211, -1.4465068578720093, -3.6007394790649414, -0.818321168422699, 1.6164169311523438, -16.378141403198242, 2.5071449279785156, -0.062334537506103516, -12.530616760253906]
    Last 10:  [-1.81450617313385, -34.19772720336914, -21.36475372314453, 34.43185043334961, -25.88595962524414, 0.4970359802246094, -53.257789611816406, 14.241670608520508, 3.148409843444824, 2.6813526153564453]
  OUT[0]: [1, 4, 640] | μ=0.094682 σ=2.859625
    First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
    Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
  WEIGHT: [640] | μ=16.907379

162_model.layers.10.self_attn.q_proj: model.layers.10.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.094682 σ=2.859625
    First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
    Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
  OUT[0]: [1, 4, 1024] | μ=-0.190860 σ=3.272703
    First 10: [-0.0828784927725792, 0.00469246506690979, -0.036376941949129105, -0.014535492286086082, 0.18007519841194153, 0.23827996850013733, 0.04098324477672577, -0.21951580047607422, 1.7257674932479858, 0.006941231898963451]
    Last 10:  [2.5783162117004395, -0.4095580577850342, -1.7054486274719238, -0.5070802569389343, 2.8596291542053223, 0.2969181537628174, -2.062356472015381, -1.1114716529846191, 0.864229142665863, 1.78420090675354]
  WEIGHT: [1024, 640] | μ=-0.000038

163_model.layers.10.self_attn.k_proj: model.layers.10.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.094682 σ=2.859625
    First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
    Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
  OUT[0]: [1, 4, 256] | μ=-0.140907 σ=2.868601
    First 10: [0.008754164911806583, 0.006492519751191139, 0.006138555705547333, 2.6632561683654785, -0.0065263547003269196, -0.003983864560723305, 0.009535718709230423, 0.005177866201847792, -0.0042121317237615585, -0.012321025133132935]
    Last 10:  [2.639613151550293, 3.187025785446167, -4.759749412536621, -6.3840155601501465, 1.4393818378448486, 13.828171730041504, 2.076052665710449, -5.20221471786499, 7.615484714508057, -3.292590856552124]
  WEIGHT: [256, 640] | μ=-0.000004

164_model.layers.10.self_attn.v_proj: model.layers.10.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.094682 σ=2.859625
    First 10: [0.2503182590007782, -0.25770917534828186, -0.008858286775648594, -0.07692078500986099, -0.02144383266568184, 0.003553784918040037, -0.0536479651927948, 0.06926947832107544, -0.0004327769565861672, -0.05755233392119408]
    Last 10:  [-0.6622883081436157, -1.3805903196334839, -0.111690953373909, 2.06602144241333, -2.204599380493164, 0.025838088244199753, -2.444589614868164, 1.921743631362915, 0.1453859508037567, 0.1912882775068283]
  OUT[0]: [1, 4, 256] | μ=-0.091402 σ=1.538940
    First 10: [0.11426025629043579, -0.021701347082853317, -0.14884179830551147, -0.08573133498430252, 0.07151083648204803, 0.09273259341716766, -0.10236135125160217, -0.12006890028715134, -0.08674372732639313, 0.012592658400535583]
    Last 10:  [3.297092914581299, -1.8792952299118042, -1.078461766242981, 2.0830676555633545, -1.9314913749694824, 0.3150622844696045, 0.9158605933189392, 1.11196768283844, -1.8287694454193115, -0.6908064484596252]
  WEIGHT: [256, 640] | μ=-0.000024

165_model.layers.10.self_attn.q_norm: model.layers.10.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.190860 σ=3.272703
    First 10: [-0.0828784927725792, 0.00469246506690979, -0.036376941949129105, -0.014535492286086082, 0.18007519841194153, 0.23827996850013733, 0.04098324477672577, -0.21951580047607422, 1.7257674932479858, 0.006941231898963451]
    Last 10:  [2.5783162117004395, -0.4095580577850342, -1.7054486274719238, -0.5070802569389343, 2.8596291542053223, 0.2969181537628174, -2.062356472015381, -1.1114716529846191, 0.864229142665863, 1.78420090675354]
  OUT[0]: [1, 4, 4, 256] | μ=-0.055874 σ=1.145066
    First 10: [-0.8290181159973145, 0.030477747321128845, -0.2700224816799164, -0.06940840929746628, 1.0880906581878662, 0.6856436729431152, 0.24578315019607544, -0.4862251877784729, 1.1716649532318115, 0.032359637320041656]
    Last 10:  [0.3373975157737732, -0.042468246072530746, -0.5925214290618896, -0.3073346018791199, 0.6559616923332214, 0.04470551386475563, -1.054265022277832, -0.5358565449714661, 0.09854276478290558, 0.9175349473953247]
  WEIGHT: [256] | μ=0.608988

166_model.layers.10.self_attn.k_norm: model.layers.10.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.140907 σ=2.868601
    First 10: [0.008754164911806583, 0.006492519751191139, 0.006138555705547333, 2.6632561683654785, -0.0065263547003269196, -0.003983864560723305, 0.009535718709230423, 0.005177866201847792, -0.0042121317237615585, -0.012321025133132935]
    Last 10:  [2.639613151550293, 3.187025785446167, -4.759749412536621, -6.3840155601501465, 1.4393818378448486, 13.828171730041504, 2.076052665710449, -5.20221471786499, 7.615484714508057, -3.292590856552124]
  OUT[0]: [1, 1, 4, 256] | μ=-0.220218 σ=3.583435
    First 10: [0.019340284168720245, 0.00747619429603219, 0.0065431478433310986, 0.0, -0.012795715592801571, -0.011302190832793713, 0.012515787035226822, 0.014520718716084957, -0.0030389532912522554, -0.0023102210834622383]
    Last 10:  [3.158682346343994, 5.162819862365723, -2.397439479827881, -1.6778192520141602, 2.2262704372406006, 15.252899169921875, 0.8661249876022339, -1.7574570178985596, 13.204607009887695, -1.8762216567993164]
  WEIGHT: [256] | μ=1.687784

167_model.layers.10.self_attn.o_proj: model.layers.10.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.038541 σ=0.492254
    First 10: [0.11426025629043579, -0.021701347082853317, -0.14884179830551147, -0.08573133498430252, 0.07151083648204803, 0.09273259341716766, -0.10236135125160217, -0.12006890028715134, -0.08674372732639313, 0.012592658400535583]
    Last 10:  [0.8527554869651794, -0.3474169969558716, -0.3893723785877228, 0.4288720488548279, -0.3392183482646942, -0.09674204140901566, 0.11644220352172852, 0.22969727218151093, -0.41802793741226196, -0.27048882842063904]
  OUT[0]: [1, 4, 640] | μ=-0.000173 σ=1.198253
    First 10: [0.08139391243457794, 0.3662101626396179, -0.17216959595680237, 0.20862720906734467, 0.1282903254032135, 0.1841568499803543, 0.13195739686489105, -0.46017882227897644, -0.24284854531288147, 0.39935487508773804]
    Last 10:  [0.09876037389039993, 1.2092902660369873, -4.387842655181885, 0.7453374862670898, -0.4389907717704773, -0.1877751350402832, 0.35236942768096924, 0.18171849846839905, -0.8472557067871094, -0.6854297518730164]
  WEIGHT: [640, 1024] | μ=-0.000029

168_model.layers.10.self_attn: model.layers.10.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=-0.000173 σ=1.198253
    First 10: [0.08139391243457794, 0.3662101626396179, -0.17216959595680237, 0.20862720906734467, 0.1282903254032135, 0.1841568499803543, 0.13195739686489105, -0.46017882227897644, -0.24284854531288147, 0.39935487508773804]
    Last 10:  [0.09876037389039993, 1.2092902660369873, -4.387842655181885, 0.7453374862670898, -0.4389907717704773, -0.1877751350402832, 0.35236942768096924, 0.18171849846839905, -0.8472557067871094, -0.6854297518730164]

169_model.layers.10.post_attention_layernorm: model.layers.10.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.000173 σ=1.198253
    First 10: [0.08139391243457794, 0.3662101626396179, -0.17216959595680237, 0.20862720906734467, 0.1282903254032135, 0.1841568499803543, 0.13195739686489105, -0.46017882227897644, -0.24284854531288147, 0.39935487508773804]
    Last 10:  [0.09876037389039993, 1.2092902660369873, -4.387842655181885, 0.7453374862670898, -0.4389907717704773, -0.1877751350402832, 0.35236942768096924, 0.18171849846839905, -0.8472557067871094, -0.6854297518730164]
  OUT[0]: [1, 4, 640] | μ=3.082355 σ=75.407829
    First 10: [7.217288494110107, 27.952926635742188, -0.6049535274505615, 2.1097676753997803, 0.12769657373428345, 10.858203887939453, 5.126643180847168, -1.2817156314849854, -1.220980167388916, 14.146242141723633]
    Last 10:  [0.14265073835849762, 55.39801025390625, -2.1548707485198975, 25.722963333129883, -9.874798774719238, -7.521966934204102, 16.069780349731445, 13.140156745910645, -32.199188232421875, -10.067641258239746]
  WEIGHT: [640] | μ=42.107410

170_model.layers.10.pre_feedforward_layernorm: model.layers.10.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=49.478416 σ=1801.616699
    First 10: [149.97036743164062, -93.1880874633789, -2.0514602661132812, -1.4909718036651611, -0.6906245946884155, 12.474620819091797, -11.251498222351074, 1.2254292964935303, -1.2833147048950195, 1.6156253814697266]
    Last 10:  [-1.6718554496765137, 21.20028305053711, -23.519624710083008, 60.154815673828125, -35.76075744628906, -7.024930953979492, -37.188011169433594, 27.38182830810547, -29.050777435302734, -7.386288642883301]
  OUT[0]: [1, 4, 640] | μ=0.016517 σ=0.574370
    First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
    Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
  WEIGHT: [640] | μ=3.507970

171_model.layers.10.mlp.gate_proj: model.layers.10.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.016517 σ=0.574370
    First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
    Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
  OUT[0]: [1, 4, 2048] | μ=-0.124841 σ=0.583820
    First 10: [0.017862141132354736, -0.041865307837724686, 0.06635323166847229, -0.019236870110034943, -0.03254745528101921, 0.020630240440368652, -0.009831612929701805, -0.033729176968336105, -0.04689794406294823, 0.022470058873295784]
    Last 10:  [-0.5746508836746216, 0.030100315809249878, 0.4244081974029541, -0.12016615271568298, -0.08698640018701553, -0.1862708330154419, -0.632300615310669, -0.30126065015792847, -0.5871347188949585, -0.792340874671936]
  WEIGHT: [2048, 640] | μ=-0.000101

172_model.layers.10.mlp.act_fn: model.layers.10.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.124841 σ=0.583820
    First 10: [0.017862141132354736, -0.041865307837724686, 0.06635323166847229, -0.019236870110034943, -0.03254745528101921, 0.020630240440368652, -0.009831612929701805, -0.033729176968336105, -0.04689794406294823, 0.022470058873295784]
    Last 10:  [-0.5746508836746216, 0.030100315809249878, 0.4244081974029541, -0.12016615271568298, -0.08698640018701553, -0.1862708330154419, -0.632300615310669, -0.30126065015792847, -0.5871347188949585, -0.792340874671936]
  OUT[0]: [1, 4, 2048] | μ=0.042985 σ=0.289494
    First 10: [0.009058347903192043, -0.020233631134033203, 0.034931764006614685, -0.009470812976360321, -0.015851188451051712, 0.010484900325536728, -0.004877245053648949, -0.016410816460847855, -0.022571854293346405, 0.01143644005060196]
    Last 10:  [-0.16251875460147858, 0.015411555767059326, 0.28195294737815857, -0.054336290806531906, -0.04047837108373642, -0.07937340438365936, -0.16671083867549896, -0.11496598273515701, -0.1635807901620865, -0.16970670223236084]

173_model.layers.10.mlp.up_proj: model.layers.10.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.016517 σ=0.574370
    First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
    Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
  OUT[0]: [1, 4, 2048] | μ=0.011889 σ=0.442743
    First 10: [-0.01862650364637375, 0.029584746807813644, -0.048310358077287674, 0.03148055076599121, -0.00691143237054348, -0.04060946777462959, -0.08718965202569962, -0.024562746286392212, 0.06944655627012253, 0.05524211749434471]
    Last 10:  [-0.5216392278671265, 0.18255525827407837, 0.34192290902137756, -0.7979309558868408, 0.5336673259735107, -1.235034465789795, -0.3385167717933655, 0.4499995708465576, 0.5553113222122192, -0.2656688690185547]
  WEIGHT: [2048, 640] | μ=0.000046

174_model.layers.10.mlp.down_proj: model.layers.10.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.002149 σ=0.134331
    First 10: [-0.00016872535343281925, -0.0005986068281345069, -0.0016875660512596369, -0.0002981464203912765, 0.00010955441393889487, -0.0004257862165104598, 0.00042524529271759093, 0.0004030947166029364, -0.0015675375470891595, 0.0006317731458693743]
    Last 10:  [0.08477615565061569, 0.002813460538163781, 0.09640616923570633, 0.04335660859942436, -0.021601984277367592, 0.09802889078855515, 0.05643441528081894, -0.05173464119434357, -0.09083826839923859, 0.045085787773132324]
  OUT[0]: [1, 4, 640] | μ=-0.002034 σ=0.094973
    First 10: [-0.0017821466317400336, -0.0003282146353740245, 0.0012456298572942615, 0.001877032220363617, 0.0007391266408376396, -0.0003862714802380651, 6.981969636399299e-05, 0.0002841940149664879, 0.001292796223424375, -0.0012813075445592403]
    Last 10:  [0.004799460992217064, 0.17821981012821198, 0.07964351773262024, -0.15814101696014404, -0.16745144128799438, 0.0569375678896904, 0.051184915006160736, 0.06220697611570358, 0.05926840752363205, -0.004520704969763756]
  WEIGHT: [640, 2048] | μ=-0.000008

175_model.layers.10.mlp: model.layers.10.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.016517 σ=0.574370
    First 10: [0.06054177135229111, -0.04395727068185806, -0.00403276551514864, -0.005495542194694281, -0.006497236434370279, 0.007006468251347542, -0.009528608992695808, 0.008345297537744045, -0.0027029390912503004, 0.0016659821849316359]
    Last 10:  [-0.14508575201034546, 0.15946272015571594, -0.05204951390624046, 0.6925531029701233, -0.525156557559967, -0.06470155715942383, -0.28828081488609314, 0.12574684619903564, -0.2690521776676178, -0.15344491600990295]
  OUT[0]: [1, 4, 640] | μ=-0.002034 σ=0.094973
    First 10: [-0.0017821466317400336, -0.0003282146353740245, 0.0012456298572942615, 0.001877032220363617, 0.0007391266408376396, -0.0003862714802380651, 6.981969636399299e-05, 0.0002841940149664879, 0.001292796223424375, -0.0012813075445592403]
    Last 10:  [0.004799460992217064, 0.17821981012821198, 0.07964351773262024, -0.15814101696014404, -0.16745144128799438, 0.0569375678896904, 0.051184915006160736, 0.06220697611570358, 0.05926840752363205, -0.004520704969763756]

176_model.layers.10.post_feedforward_layernorm: model.layers.10.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.002034 σ=0.094973
    First 10: [-0.0017821466317400336, -0.0003282146353740245, 0.0012456298572942615, 0.001877032220363617, 0.0007391266408376396, -0.0003862714802380651, 6.981969636399299e-05, 0.0002841940149664879, 0.001292796223424375, -0.0012813075445592403]
    Last 10:  [0.004799460992217064, 0.17821981012821198, 0.07964351773262024, -0.15814101696014404, -0.16745144128799438, 0.0569375678896904, 0.051184915006160736, 0.06220697611570358, 0.05926840752363205, -0.004520704969763756]
  OUT[0]: [1, 4, 640] | μ=-0.354804 σ=105.579086
    First 10: [-117.80230712890625, -19.375795364379883, 3.495473623275757, 10.778473854064941, 0.6409636735916138, -15.89794921875, 2.046351909637451, 0.5464367866516113, 5.777657508850098, -30.36277961730957]
    Last 10:  [0.13558687269687653, 135.1779022216797, 20.809425354003906, -91.35601043701172, -58.705299377441406, 36.90945053100586, 38.59751892089844, 20.025495529174805, 38.6817626953125, -1.6048089265823364]
  WEIGHT: [640] | μ=55.949501

177_model.layers.11.input_layernorm: model.layers.11.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=49.123611 σ=1837.341187
    First 10: [32.168060302734375, -112.56388092041016, 1.4440133571624756, 9.28750228881836, -0.04966092109680176, -3.423328399658203, -9.205146789550781, 1.7718660831451416, 4.494342803955078, -28.747154235839844]
    Last 10:  [-1.5362685918807983, 156.37818908691406, -2.7101993560791016, -31.201194763183594, -94.46605682373047, 29.884519577026367, 1.4095077514648438, 47.407325744628906, 9.630985260009766, -8.991097450256348]
  OUT[0]: [1, 4, 640] | μ=0.014602 σ=0.654240
    First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
    Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
  WEIGHT: [640] | μ=4.293710

178_model.layers.11.self_attn.q_proj: model.layers.11.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.014602 σ=0.654240
    First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
    Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
  OUT[0]: [1, 4, 1024] | μ=-0.007995 σ=0.924809
    First 10: [0.01365344412624836, -0.11098835617303848, 0.06696315109729767, 0.10959981381893158, 0.055122341960668564, 0.29333293437957764, -0.1122264489531517, 0.0718197450041771, 0.07852256298065186, 0.010455025359988213]
    Last 10:  [-0.44298988580703735, -0.45652490854263306, -0.24241551756858826, -0.5209836959838867, 0.06410010904073715, -0.321913480758667, 0.06172740459442139, -1.0597397089004517, 0.6172072887420654, 0.03667899966239929]
  WEIGHT: [1024, 640] | μ=-0.000012

179_model.layers.11.self_attn.k_proj: model.layers.11.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.014602 σ=0.654240
    First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
    Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
  OUT[0]: [1, 4, 256] | μ=-0.026486 σ=0.587689
    First 10: [-0.002469163853675127, -0.00035997433587908745, 0.24178196489810944, 0.001079668290913105, -0.27047163248062134, -0.00366036593914032, 0.0005518437828868628, 0.0009618494659662247, 0.0008495703223161399, 0.00015368242748081684]
    Last 10:  [-1.6532350778579712, -0.46099531650543213, 0.036373320966959, -2.6158387660980225, -0.32249176502227783, -1.019518494606018, -0.31767696142196655, -3.3103952407836914, -1.6283916234970093, 0.9027724266052246]
  WEIGHT: [256, 640] | μ=-0.000080

180_model.layers.11.self_attn.v_proj: model.layers.11.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.014602 σ=0.654240
    First 10: [0.010450228117406368, -0.03257065638899803, 0.004691852256655693, 0.026645401492714882, -0.0006797593669034541, -0.001214728457853198, -0.005349439103156328, 0.014209019020199776, 0.012350330129265785, -0.01819649524986744]
    Last 10:  [-0.14537839591503143, 0.6566701531410217, -0.010312169790267944, -0.24727986752986908, -0.8045462965965271, 0.15495069324970245, 0.0064051588997244835, 0.400019109249115, 0.06018960103392601, -0.13187944889068604]
  OUT[0]: [1, 4, 256] | μ=-0.011266 σ=0.384210
    First 10: [-0.0047807153314352036, 0.014551674015820026, -0.06743748486042023, 0.009456025436520576, 0.014122195541858673, 0.002366265282034874, -0.006114509887993336, -0.010338650085031986, 0.012651808559894562, 0.0396566279232502]
    Last 10:  [-0.4023992717266083, -0.1231396347284317, -0.02763575315475464, 0.33686989545822144, 0.49059170484542847, -0.10026510059833527, -0.2628980278968811, 0.2927035689353943, -0.12586474418640137, -0.0731324553489685]
  WEIGHT: [256, 640] | μ=-0.000228

181_model.layers.11.self_attn.q_norm: model.layers.11.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.007995 σ=0.924809
    First 10: [0.01365344412624836, -0.11098835617303848, 0.06696315109729767, 0.10959981381893158, 0.055122341960668564, 0.29333293437957764, -0.1122264489531517, 0.0718197450041771, 0.07852256298065186, 0.010455025359988213]
    Last 10:  [-0.44298988580703735, -0.45652490854263306, -0.24241551756858826, -0.5209836959838867, 0.06410010904073715, -0.321913480758667, 0.06172740459442139, -1.0597397089004517, 0.6172072887420654, 0.03667899966239929]
  OUT[0]: [1, 4, 4, 256] | μ=0.007903 σ=1.376288
    First 10: [0.13516652584075928, -1.280019760131836, 0.8400245308876038, 1.340036153793335, 0.5512772798538208, -0.016957318410277367, -3.451462984085083, 0.7089260220527649, 1.9791436195373535, 0.08982832729816437]
    Last 10:  [-0.12306046485900879, -0.1152913048863411, -0.0839586928486824, -0.11277410387992859, 0.020813029259443283, -0.12891283631324768, 0.014697923325002193, -0.6136323809623718, 0.3618415594100952, 0.0071457079611718655]
  WEIGHT: [256] | μ=0.544234

182_model.layers.11.self_attn.k_norm: model.layers.11.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.026486 σ=0.587689
    First 10: [-0.002469163853675127, -0.00035997433587908745, 0.24178196489810944, 0.001079668290913105, -0.27047163248062134, -0.00366036593914032, 0.0005518437828868628, 0.0009618494659662247, 0.0008495703223161399, 0.00015368242748081684]
    Last 10:  [-1.6532350778579712, -0.46099531650543213, 0.036373320966959, -2.6158387660980225, -0.32249176502227783, -1.019518494606018, -0.31767696142196655, -3.3103952407836914, -1.6283916234970093, 0.9027724266052246]
  OUT[0]: [1, 1, 4, 256] | μ=-0.514531 σ=8.139625
    First 10: [-0.04140526056289673, -0.005979708395898342, -0.009517434984445572, 0.009838691912591457, -0.0, -0.0015849412884563208, 0.004154450725764036, 0.01245658565312624, 0.005752064753323793, 0.003000557189807296]
    Last 10:  [-19.985376358032227, -6.603254318237305, 0.34512636065483093, -53.93631362915039, -3.6336863040924072, -9.022537231445312, -6.173439025878906, -27.786224365234375, -71.60902404785156, 16.555267333984375]
  WEIGHT: [256] | μ=2.486512

183_model.layers.11.self_attn.o_proj: model.layers.11.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.005071 σ=0.116178
    First 10: [-0.0047807153314352036, 0.014551674015820026, -0.06743748486042023, 0.009456025436520576, 0.014122195541858673, 0.002366265282034874, -0.006114509887993336, -0.010338650085031986, 0.012651808559894562, 0.0396566279232502]
    Last 10:  [-0.029682962223887444, -0.0044352649711072445, 0.0013895579613745213, 0.06387273967266083, 0.052462149411439896, 0.022754201665520668, -0.015520865097641945, -0.014873801730573177, 0.01218338217586279, -0.03349282965064049]
  OUT[0]: [1, 4, 640] | μ=-0.000749 σ=0.181747
    First 10: [-0.010984765365719795, 0.01847337745130062, -0.019123654812574387, -0.036313023418188095, -0.005859001073986292, 0.01295386254787445, -0.022312497720122337, -0.006964103318750858, 0.04751380532979965, -0.029459405690431595]
    Last 10:  [0.00819660909473896, -0.09836061298847198, 0.025486791506409645, 0.0856885090470314, -0.05444877967238426, -0.012221388518810272, -0.11825872957706451, -0.004064660053700209, -0.06401020288467407, -0.042394526302814484]
  WEIGHT: [640, 1024] | μ=0.000008

184_model.layers.11.self_attn: model.layers.11.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=-0.000749 σ=0.181747
    First 10: [-0.010984765365719795, 0.01847337745130062, -0.019123654812574387, -0.036313023418188095, -0.005859001073986292, 0.01295386254787445, -0.022312497720122337, -0.006964103318750858, 0.04751380532979965, -0.029459405690431595]
    Last 10:  [0.00819660909473896, -0.09836061298847198, 0.025486791506409645, 0.0856885090470314, -0.05444877967238426, -0.012221388518810272, -0.11825872957706451, -0.004064660053700209, -0.06401020288467407, -0.042394526302814484]

185_model.layers.11.post_attention_layernorm: model.layers.11.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.000749 σ=0.181747
    First 10: [-0.010984765365719795, 0.01847337745130062, -0.019123654812574387, -0.036313023418188095, -0.005859001073986292, 0.01295386254787445, -0.022312497720122337, -0.006964103318750858, 0.04751380532979965, -0.029459405690431595]
    Last 10:  [0.00819660909473896, -0.09836061298847198, 0.025486791506409645, 0.0856885090470314, -0.05444877967238426, -0.012221388518810272, -0.11825872957706451, -0.004064660053700209, -0.06401020288467407, -0.042394526302814484]
  OUT[0]: [1, 4, 640] | μ=2.086629 σ=52.864017
    First 10: [-31.27505874633789, 30.821630477905273, -1.974318504333496, -5.576315879821777, -0.22645039856433868, 7.741854667663574, -9.491947174072266, -0.39019545912742615, 4.806707382202148, -15.03877067565918]
    Last 10:  [0.3318438231945038, -38.16423034667969, 15.001043319702148, 32.261260986328125, -10.697426795959473, -4.420450210571289, -48.60673904418945, -12.108927726745605, -23.047117233276367, -8.677656173706055]
  WEIGHT: [640] | μ=110.148514

186_model.layers.11.pre_feedforward_layernorm: model.layers.11.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=51.210228 σ=1853.544312
    First 10: [0.8930015563964844, -81.74224853515625, -0.5303051471710205, 3.711186408996582, -0.27611130475997925, 4.318526268005371, -18.697093963623047, 1.381670594215393, 9.301050186157227, -43.785926818847656]
    Last 10:  [-1.2044247388839722, 118.21395874023438, 12.290843963623047, 1.0600662231445312, -105.16348266601562, 25.464069366455078, -47.19723129272461, 35.298397064208984, -13.416131973266602, -17.66875457763672]
  OUT[0]: [1, 4, 640] | μ=0.003438 σ=0.681187
    First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
    Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
  WEIGHT: [640] | μ=4.350527

187_model.layers.11.mlp.gate_proj: model.layers.11.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.003438 σ=0.681187
    First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
    Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
  OUT[0]: [1, 4, 2048] | μ=-0.235493 σ=0.750462
    First 10: [0.020761700347065926, 0.011689679697155952, -0.1729077398777008, 0.0051129050552845, 0.01828872039914131, -0.0018476257100701332, 0.07961639761924744, -0.0219118595123291, 0.04736284911632538, -0.022017337381839752]
    Last 10:  [0.6845484972000122, 0.0765204206109047, 0.09464208036661148, 0.6403622031211853, -1.0202829837799072, -0.6993950009346008, -0.06335951387882233, -3.185143232345581, -0.7923871278762817, 2.0591392517089844]
  WEIGHT: [2048, 640] | μ=-0.000025

188_model.layers.11.mlp.act_fn: model.layers.11.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.235493 σ=0.750462
    First 10: [0.020761700347065926, 0.011689679697155952, -0.1729077398777008, 0.0051129050552845, 0.01828872039914131, -0.0018476257100701332, 0.07961639761924744, -0.0219118595123291, 0.04736284911632538, -0.022017337381839752]
    Last 10:  [0.6845484972000122, 0.0765204206109047, 0.09464208036661148, 0.6403622031211853, -1.0202829837799072, -0.6993950009346008, -0.06335951387882233, -3.185143232345581, -0.7923871278762817, 2.0591392517089844]
  OUT[0]: [1, 4, 2048] | μ=0.046215 σ=0.343515
    First 10: [0.010552801191806793, 0.005899353418499231, -0.07458611577749252, 0.002566881477832794, 0.00927778985351324, -0.0009224509703926742, 0.04233432188630104, -0.010764401406049728, 0.024576010182499886, -0.010815291665494442]
    Last 10:  [0.5155406594276428, 0.04059387743473053, 0.050889063626527786, 0.47320663928985596, -0.15707643330097198, -0.16941547393798828, -0.03007930889725685, -0.0019682629499584436, -0.16970594227313995, 2.0186400413513184]

189_model.layers.11.mlp.up_proj: model.layers.11.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.003438 σ=0.681187
    First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
    Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
  OUT[0]: [1, 4, 2048] | μ=-0.003224 σ=0.536166
    First 10: [0.0007078398484736681, 0.007055371999740601, -0.014281325042247772, -0.006269518751651049, 0.07185100018978119, 0.027219807729125023, -0.08948952704668045, 0.05133172497153282, 0.0224746223539114, 0.08964072167873383]
    Last 10:  [0.7427328824996948, 0.4149007499217987, -0.11974874138832092, -0.3620299696922302, 0.12279263138771057, -0.5135499238967896, -0.740970253944397, -0.8525660037994385, -0.09037508815526962, -0.5341997146606445]
  WEIGHT: [2048, 640] | μ=-0.000010

190_model.layers.11.mlp.down_proj: model.layers.11.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.002809 σ=0.219256
    First 10: [7.469693173334235e-06, 4.162213372183032e-05, 0.0010651885531842709, -1.6093112208181992e-05, 0.0006666184635832906, -2.510893864382524e-05, -0.0037884784396737814, -0.000552555313333869, 0.0005523365689441562, -0.0009694905602373183]
    Last 10:  [0.38290899991989136, 0.0168424304574728, -0.006093901116400957, -0.17131498456001282, -0.019287828356027603, 0.08700330555438995, 0.022287873551249504, 0.0016780741279944777, 0.01533718965947628, -1.0783569812774658]
  OUT[0]: [1, 4, 640] | μ=0.004118 σ=0.211647
    First 10: [-0.00044058426283299923, -0.0012573034036904573, -0.000139257637783885, 0.0012291735038161278, 0.0008612052770331502, -0.0016133333556354046, -0.00015306833665817976, -0.0006767365266568959, 0.0015748918522149324, 0.0005461339605972171]
    Last 10:  [-0.018172457814216614, 0.12842777371406555, -0.035472191870212555, 0.066526859998703, 0.10862579941749573, 0.015471599996089935, -0.09423895925283432, -0.17461246252059937, 0.16095152497291565, 0.024173054844141006]
  WEIGHT: [640, 2048] | μ=-0.000002

191_model.layers.11.mlp: model.layers.11.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=0.003438 σ=0.681187
    First 10: [0.00010933098383247852, -0.019686352461576462, -0.001494431053288281, 0.015878824517130852, -0.00305545749142766, 0.00227721082046628, -0.01646547205746174, 0.01111108809709549, 0.027329662814736366, -0.037807397544384]
    Last 10:  [-0.08812801539897919, 0.6687048673629761, 0.03298535943031311, 0.009345213882625103, -1.2090779542922974, 0.1711685210466385, -0.2687157988548279, 0.015234837308526039, -0.09387874603271484, -0.3816174864768982]
  OUT[0]: [1, 4, 640] | μ=0.004118 σ=0.211647
    First 10: [-0.00044058426283299923, -0.0012573034036904573, -0.000139257637783885, 0.0012291735038161278, 0.0008612052770331502, -0.0016133333556354046, -0.00015306833665817976, -0.0006767365266568959, 0.0015748918522149324, 0.0005461339605972171]
    Last 10:  [-0.018172457814216614, 0.12842777371406555, -0.035472191870212555, 0.066526859998703, 0.10862579941749573, 0.015471599996089935, -0.09423895925283432, -0.17461246252059937, 0.16095152497291565, 0.024173054844141006]

192_model.layers.11.post_feedforward_layernorm: model.layers.11.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.004118 σ=0.211647
    First 10: [-0.00044058426283299923, -0.0012573034036904573, -0.000139257637783885, 0.0012291735038161278, 0.0008612052770331502, -0.0016133333556354046, -0.00015306833665817976, -0.0006767365266568959, 0.0015748918522149324, 0.0005461339605972171]
    Last 10:  [-0.018172457814216614, 0.12842777371406555, -0.035472191870212555, 0.066526859998703, 0.10862579941749573, 0.015471599996089935, -0.09423895925283432, -0.17461246252059937, 0.16095152497291565, 0.024173054844141006]
  OUT[0]: [1, 4, 640] | μ=2.406040 σ=174.350357
    First 10: [-38.334495544433594, -78.37940216064453, -0.5744926333427429, 8.605036735534668, 1.0496941804885864, -84.43917846679688, -6.557053089141846, -1.5933027267456055, 9.712760925292969, 14.564963340759277]
    Last 10:  [-0.549500584602356, 94.41680908203125, -13.518502235412598, 40.81730651855469, 38.021060943603516, 10.328889846801758, -73.86690521240234, -272.78753662109375, 112.237060546875, 6.860287666320801]
  WEIGHT: [640] | μ=112.834496

193_model.layers.12.input_layernorm: model.layers.12.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=53.616283 σ=1991.643799
    First 10: [-37.44149398803711, -160.12164306640625, -1.1047978401184082, 12.31622314453125, 0.7735828757286072, -80.12065124511719, -25.254146575927734, -0.2116321325302124, 19.013811111450195, -29.220962524414062]
    Last 10:  [-1.7539253234863281, 212.63076782226562, -1.2276582717895508, 41.87737274169922, -67.14242553710938, 35.79296112060547, -121.06413269042969, -237.4891357421875, 98.82093048095703, -10.808466911315918]
  OUT[0]: [1, 4, 640] | μ=0.023097 σ=2.113587
    First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
    Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
  WEIGHT: [640] | μ=14.549128

194_model.layers.12.self_attn.q_proj: model.layers.12.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.023097 σ=2.113587
    First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
    Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
  OUT[0]: [1, 4, 1024] | μ=0.045301 σ=2.664215
    First 10: [-0.06358160078525543, -0.2240535318851471, -0.039401229470968246, 0.2656867206096649, 0.009820550680160522, -0.015561763197183609, -0.04221489280462265, 0.03709699213504791, -0.03146548569202423, 0.7696008086204529]
    Last 10:  [-0.975795567035675, -0.050007522106170654, -0.6796281933784485, -0.043120525777339935, -1.231899619102478, -0.07505464553833008, -3.2128655910491943, -0.7609623670578003, -1.1725811958312988, 0.5478821396827698]
  WEIGHT: [1024, 640] | μ=-0.000030

195_model.layers.12.self_attn.k_proj: model.layers.12.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.023097 σ=2.113587
    First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
    Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
  OUT[0]: [1, 4, 256] | μ=0.089197 σ=2.426896
    First 10: [-0.8677104711532593, -0.009737581014633179, 1.256080150604248, -0.3138313591480255, 0.4551379084587097, 0.004774325527250767, 0.6414008140563965, -0.6928508877754211, 0.0025312090292572975, 0.8959704637527466]
    Last 10:  [0.7062675356864929, 1.3677343130111694, 3.0223946571350098, -3.2614693641662598, -6.981871604919434, -2.9776196479797363, 1.636380910873413, -3.199155807495117, -9.21869945526123, 0.8566823601722717]
  WEIGHT: [256, 640] | μ=0.000031

196_model.layers.12.self_attn.v_proj: model.layers.12.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.023097 σ=2.113587
    First 10: [-0.010148247703909874, -0.09141430258750916, -0.010515190660953522, 0.10620853304862976, 0.030834684148430824, -0.07772823423147202, -0.039119429886341095, -0.004974821582436562, 0.16396494209766388, -0.04829731211066246]
    Last 10:  [-0.4213528335094452, 2.339259147644043, -0.012869013473391533, 0.8518866896629333, -1.5516672134399414, 0.45073792338371277, -1.2900047302246094, -0.26599815487861633, 1.2239314317703247, -0.45469772815704346]
  OUT[0]: [1, 4, 256] | μ=-0.107021 σ=1.453971
    First 10: [-0.16694942116737366, -0.16491203010082245, 0.004787806421518326, -0.07429400831460953, 0.0851186066865921, -0.1673336625099182, 0.147842139005661, -0.1453549861907959, 0.005079973489046097, 0.020989656448364258]
    Last 10:  [-1.0213571786880493, 0.07794189453125, 0.8037082552909851, 0.10917162895202637, -0.7665489912033081, 0.25748157501220703, -0.5276670455932617, 2.0314958095550537, 1.360581874847412, 0.7988132834434509]
  WEIGHT: [256, 640] | μ=-0.000004

197_model.layers.12.self_attn.q_norm: model.layers.12.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.045301 σ=2.664215
    First 10: [-0.06358160078525543, -0.2240535318851471, -0.039401229470968246, 0.2656867206096649, 0.009820550680160522, -0.015561763197183609, -0.04221489280462265, 0.03709699213504791, -0.03146548569202423, 0.7696008086204529]
    Last 10:  [-0.975795567035675, -0.050007522106170654, -0.6796281933784485, -0.043120525777339935, -1.231899619102478, -0.07505464553833008, -3.2128655910491943, -0.7609623670578003, -1.1725811958312988, 0.5478821396827698]
  OUT[0]: [1, 4, 4, 256] | μ=0.033059 σ=0.882198
    First 10: [-0.0, -0.06432300806045532, -0.0, 0.0, -0.0008674950804561377, -0.16976843774318695, -0.0, 0.0, -0.5155959725379944, 0.0]
    Last 10:  [-0.37094971537590027, -0.020429814234375954, -0.6866517066955566, -0.02420341596007347, -0.8171816468238831, -0.08017074316740036, -2.5083255767822266, -0.41806554794311523, -0.3001636564731598, 0.3690294325351715]
  WEIGHT: [256] | μ=0.781188

198_model.layers.12.self_attn.k_norm: model.layers.12.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=0.089197 σ=2.426896
    First 10: [-0.8677104711532593, -0.009737581014633179, 1.256080150604248, -0.3138313591480255, 0.4551379084587097, 0.004774325527250767, 0.6414008140563965, -0.6928508877754211, 0.0025312090292572975, 0.8959704637527466]
    Last 10:  [0.7062675356864929, 1.3677343130111694, 3.0223946571350098, -3.2614693641662598, -6.981871604919434, -2.9776196479797363, 1.636380910873413, -3.199155807495117, -9.21869945526123, 0.8566823601722717]
  OUT[0]: [1, 1, 4, 256] | μ=0.069580 σ=3.169030
    First 10: [-0.0, -0.0028694800566881895, 0.0, -0.0, -0.042674701660871506, 0.01777813769876957, 0.0, -0.0, 0.007747179828584194, 0.0]
    Last 10:  [1.1483677625656128, 1.85691237449646, 2.148998975753784, -4.988009452819824, -7.1935319900512695, -2.0612375736236572, 1.7474573850631714, -6.317603588104248, -21.568763732910156, 0.8780562281608582]
  WEIGHT: [256] | μ=1.218889

199_model.layers.12.self_attn.o_proj: model.layers.12.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=-0.029761 σ=0.172917
    First 10: [-0.16694942116737366, -0.16491203010082245, 0.004787806421518326, -0.07429400831460953, 0.0851186066865921, -0.1673336625099182, 0.147842139005661, -0.1453549861907959, 0.005079973489046097, 0.020989656448364258]
    Last 10:  [-0.08664517849683762, 0.3716493844985962, 0.008566631004214287, -0.1938609629869461, -0.04727737605571747, -0.058128852397203445, -0.003436692524701357, -0.06921088695526123, 0.3843255639076233, -0.005756429396569729]
  OUT[0]: [1, 4, 640] | μ=0.006972 σ=0.737182
    First 10: [-0.04924732446670532, -0.20066849887371063, -0.12120641767978668, -0.1883285492658615, -0.07893367111682892, 0.27536875009536743, -0.1806802600622177, 0.06752364337444305, 0.006717264652252197, -0.11898058652877808]
    Last 10:  [0.05691812187433243, 0.19453245401382446, 0.01863277703523636, 0.6745097041130066, 0.27898094058036804, -0.20499172806739807, -0.11778879165649414, 0.0917646512389183, 0.06570817530155182, -1.2257717847824097]
  WEIGHT: [640, 1024] | μ=0.000039

200_model.layers.12.self_attn: model.layers.12.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.006972 σ=0.737182
    First 10: [-0.04924732446670532, -0.20066849887371063, -0.12120641767978668, -0.1883285492658615, -0.07893367111682892, 0.27536875009536743, -0.1806802600622177, 0.06752364337444305, 0.006717264652252197, -0.11898058652877808]
    Last 10:  [0.05691812187433243, 0.19453245401382446, 0.01863277703523636, 0.6745097041130066, 0.27898094058036804, -0.20499172806739807, -0.11778879165649414, 0.0917646512389183, 0.06570817530155182, -1.2257717847824097]

201_model.layers.12.post_attention_layernorm: model.layers.12.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.006972 σ=0.737182
    First 10: [-0.04924732446670532, -0.20066849887371063, -0.12120641767978668, -0.1883285492658615, -0.07893367111682892, 0.27536875009536743, -0.1806802600622177, 0.06752364337444305, 0.006717264652252197, -0.11898058652877808]
    Last 10:  [0.05691812187433243, 0.19453245401382446, 0.01863277703523636, 0.6745097041130066, 0.27898094058036804, -0.20499172806739807, -0.11778879165649414, 0.0917646512389183, 0.06570817530155182, -1.2257717847824097]
  OUT[0]: [1, 4, 640] | μ=1.488417 σ=85.229561
    First 10: [-24.665367126464844, -63.00262451171875, -2.0726065635681152, -6.827913761138916, -0.7560078501701355, 52.696861267089844, -24.041460037231445, 1.0032130479812622, 0.18202482163906097, -13.697047233581543]
    Last 10:  [0.602192223072052, 26.04939842224121, 1.2181133031845093, 114.18071746826172, 17.621557235717773, -25.378278732299805, -16.74003028869629, 54.83228302001953, 7.678225994110107, -45.09987258911133]
  WEIGHT: [640] | μ=130.185471

202_model.layers.12.pre_feedforward_layernorm: model.layers.12.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=55.104698 σ=2034.907104
    First 10: [-62.10686111450195, -223.124267578125, -3.1774044036865234, 5.488309383392334, 0.01757502555847168, -27.423789978027344, -49.29560852050781, 0.7915809154510498, 19.19583511352539, -42.91801071166992]
    Last 10:  [-1.151733160018921, 238.68016052246094, -0.009544968605041504, 156.05809020996094, -49.52086639404297, 10.414682388305664, -137.80416870117188, -182.6568603515625, 106.49915313720703, -55.90834045410156]
  OUT[0]: [1, 4, 640] | μ=-0.004842 σ=0.458555
    First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
    Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
  WEIGHT: [640] | μ=2.400574

203_model.layers.12.mlp.gate_proj: model.layers.12.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.004842 σ=0.458555
    First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
    Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
  OUT[0]: [1, 4, 2048] | μ=-0.130874 σ=0.495230
    First 10: [-0.028002562001347542, -0.03427787870168686, -0.019265055656433105, 0.02978655695915222, -0.014230594038963318, 0.008247947320342064, 0.01807716116309166, -0.004126193001866341, -0.03685946762561798, -0.04242347925901413]
    Last 10:  [-0.6921693682670593, -0.7171858549118042, -0.3842568099498749, 0.10186724364757538, 0.1851509064435959, -0.4304801821708679, -0.35439372062683105, -0.7095569968223572, 0.748548686504364, -0.25060468912124634]
  WEIGHT: [2048, 640] | μ=-0.000209

204_model.layers.12.mlp.act_fn: model.layers.12.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.130874 σ=0.495230
    First 10: [-0.028002562001347542, -0.03427787870168686, -0.019265055656433105, 0.02978655695915222, -0.014230594038963318, 0.008247947320342064, 0.01807716116309166, -0.004126193001866341, -0.03685946762561798, -0.04242347925901413]
    Last 10:  [-0.6921693682670593, -0.7171858549118042, -0.3842568099498749, 0.10186724364757538, 0.1851509064435959, -0.4304801821708679, -0.35439372062683105, -0.7095569968223572, 0.748548686504364, -0.25060468912124634]
  OUT[0]: [1, 4, 2048] | μ=0.017502 σ=0.261244
    First 10: [-0.013688494451344013, -0.016670284792780876, -0.009484472684562206, 0.015247182920575142, -0.007034510374069214, 0.004151112865656614, 0.009168940596282482, -0.0020563043653964996, -0.017887845635414124, -0.02049395814538002]
    Last 10:  [-0.16923022270202637, -0.16976729035377502, -0.13464777171611786, 0.05506623908877373, 0.10617342591285706, -0.14354197680950165, -0.1281258463859558, -0.16963443160057068, 0.5785112380981445, -0.10050871968269348]

205_model.layers.12.mlp.up_proj: model.layers.12.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.004842 σ=0.458555
    First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
    Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
  OUT[0]: [1, 4, 2048] | μ=-0.008261 σ=0.387421
    First 10: [0.014339018613100052, 0.003976594191044569, 0.00027756672352552414, -0.0006734868511557579, 0.026239831000566483, -0.011569378897547722, 0.022204114124178886, 0.024107232689857483, 0.004735920578241348, 0.0035862582735717297]
    Last 10:  [-0.24145108461380005, -0.0487213209271431, -0.6878368258476257, -0.36254680156707764, -0.25074443221092224, 0.5296995639801025, -0.2416045218706131, 0.13893000781536102, -0.29465484619140625, 0.44786354899406433]
  WEIGHT: [2048, 640] | μ=-0.000002

206_model.layers.12.mlp.down_proj: model.layers.12.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.001761 σ=0.143187
    First 10: [-0.00019627957954071462, -6.629095878452063e-05, -2.632574023664347e-06, -1.0268777259625494e-05, -0.0001845843653427437, -4.802579860552214e-05, 0.00020358820620458573, -4.9571808631299064e-05, -8.471541514154524e-05, -7.349662337219343e-05]
    Last 10:  [0.040860820561647415, 0.008271286264061928, 0.09261569380760193, -0.01996408961713314, -0.02662239596247673, -0.07603412121534348, 0.030955784022808075, -0.023567313328385353, -0.17046113312244415, -0.045014191418886185]
  OUT[0]: [1, 4, 640] | μ=0.004615 σ=0.154089
    First 10: [5.988735210848972e-05, 0.0005273829447105527, -0.0003281553217675537, -0.0016731981886550784, -0.0008336306782439351, -0.0005642225150950253, -8.936149242799729e-05, 2.3320109903579578e-05, -0.0006813560030423105, 0.0008459886885248125]
    Last 10:  [-0.0208803191781044, -0.03335457295179367, -0.02766421064734459, -0.025533266365528107, -0.02498169057071209, 0.13545185327529907, -0.09248585999011993, -0.00780028011649847, 0.03345503658056259, -0.024605419486761093]
  WEIGHT: [640, 2048] | μ=0.000005

207_model.layers.12.mlp: model.layers.12.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.004842 σ=0.458555
    First 10: [-0.004024290945380926, -0.02738184481859207, -0.006338727194815874, 0.01284549105912447, 9.88336541922763e-05, -0.00670313835144043, -0.01882624439895153, 0.003406996838748455, 0.033319246023893356, -0.018623774871230125]
    Last 10:  [-0.0411175973713398, 0.5647327303886414, -2.099384437315166e-05, 0.5550338625907898, -0.24027182161808014, 0.028813770040869713, -0.3263115882873535, -0.053338423371315, 0.2826845049858093, -0.6262500286102295]
  OUT[0]: [1, 4, 640] | μ=0.004615 σ=0.154089
    First 10: [5.988735210848972e-05, 0.0005273829447105527, -0.0003281553217675537, -0.0016731981886550784, -0.0008336306782439351, -0.0005642225150950253, -8.936149242799729e-05, 2.3320109903579578e-05, -0.0006813560030423105, 0.0008459886885248125]
    Last 10:  [-0.0208803191781044, -0.03335457295179367, -0.02766421064734459, -0.025533266365528107, -0.02498169057071209, 0.13545185327529907, -0.09248585999011993, -0.00780028011649847, 0.03345503658056259, -0.024605419486761093]

208_model.layers.12.post_feedforward_layernorm: model.layers.12.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.004615 σ=0.154089
    First 10: [5.988735210848972e-05, 0.0005273829447105527, -0.0003281553217675537, -0.0016731981886550784, -0.0008336306782439351, -0.0005642225150950253, -8.936149242799729e-05, 2.3320109903579578e-05, -0.0006813560030423105, 0.0008459886885248125]
    Last 10:  [-0.0208803191781044, -0.03335457295179367, -0.02766421064734459, -0.025533266365528107, -0.02498169057071209, 0.13545185327529907, -0.09248585999011993, -0.00780028011649847, 0.03345503658056259, -0.024605419486761093]
  OUT[0]: [1, 4, 640] | μ=1.285441 σ=109.526093
    First 10: [10.229170799255371, 61.50520324707031, -3.8312931060791016, -22.6649112701416, -2.9574930667877197, -58.81378173828125, -8.346537590026855, 0.12485215067863464, -10.987565040588379, 44.747188568115234]
    Last 10:  [-1.3894346952438354, -46.02555465698242, -22.284019470214844, -37.55805206298828, -16.186052322387695, 169.83038330078125, -117.90280151367188, -22.34657096862793, 42.64914321899414, -11.978239059448242]
  WEIGHT: [640] | μ=162.501419

209_model.layers.13.input_layernorm: model.layers.13.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=56.390137 σ=2027.257568
    First 10: [-51.877689361572266, -161.6190643310547, -7.008697509765625, -17.17660140991211, -2.939918041229248, -86.2375717163086, -57.642147064208984, 0.9164330959320068, 8.208270072937012, 1.8291778564453125]
    Last 10:  [-2.541167736053467, 192.65460205078125, -22.293563842773438, 118.50003814697266, -65.70691680908203, 180.2450714111328, -255.70697021484375, -205.00343322753906, 149.14830017089844, -67.88658142089844]
  OUT[0]: [1, 4, 640] | μ=0.127432 σ=3.706955
    First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
    Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
  WEIGHT: [640] | μ=11.329188

210_model.layers.13.self_attn.q_proj: model.layers.13.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.127432 σ=3.706955
    First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
    Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
  OUT[0]: [1, 4, 1024] | μ=0.011610 σ=2.330506
    First 10: [0.3612440824508667, 0.12880228459835052, 0.2316921502351761, -0.16731218993663788, -0.19560670852661133, 0.008259095251560211, -3.1987829208374023, 0.03870191425085068, -0.4358840584754944, -0.21641740202903748]
    Last 10:  [0.5578280091285706, 1.0190964937210083, 1.664643406867981, -1.571599006652832, 1.6427546739578247, -1.2382352352142334, -0.09782607853412628, -2.3036954402923584, 0.18529659509658813, 0.5044712424278259]
  WEIGHT: [1024, 640] | μ=-0.000054

211_model.layers.13.self_attn.k_proj: model.layers.13.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.127432 σ=3.706955
    First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
    Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
  OUT[0]: [1, 4, 256] | μ=-0.011416 σ=2.053910
    First 10: [-0.0020686164498329163, 0.006217023357748985, 4.091761112213135, 0.0024563726037740707, -3.9802873134613037, -0.0017994018271565437, -0.12375594675540924, 1.24365234375, -0.0009178835898637772, -4.232999801635742]
    Last 10:  [1.1919364929199219, 2.0350561141967773, 4.547500133514404, 0.6425000429153442, 2.003917932510376, -2.9013776779174805, -2.4789624214172363, -2.9661006927490234, 3.473496198654175, -2.4101815223693848]
  WEIGHT: [256, 640] | μ=0.000086

212_model.layers.13.self_attn.v_proj: model.layers.13.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.127432 σ=3.706955
    First 10: [-0.01621065102517605, -0.07639294117689133, -0.0678643137216568, -0.11305331438779831, -0.050607725977897644, -0.0668567568063736, -0.06269972026348114, 0.013629544526338577, 0.05973958224058151, 0.002517838729545474]
    Last 10:  [-0.24011510610580444, 1.4248416423797607, -0.2645454704761505, 0.9516206979751587, -0.8957574963569641, 1.512130856513977, -1.9052796363830566, -0.3408557176589966, 1.1277719736099243, -1.506233811378479]
  OUT[0]: [1, 4, 256] | μ=0.050492 σ=1.432910
    First 10: [-0.046511050313711166, -0.020304512232542038, -0.026450205594301224, -0.04027312994003296, 0.004717771429568529, -0.18583889305591583, 0.05088362842798233, 0.06272327899932861, 0.0631592720746994, 0.043709319084882736]
    Last 10:  [1.0932190418243408, -0.8873891830444336, 0.487265944480896, 1.277878999710083, 0.08657288551330566, 1.2546601295471191, -1.3693642616271973, -0.5918368101119995, 0.7305423021316528, 0.6351077556610107]
  WEIGHT: [256, 640] | μ=-0.000082

213_model.layers.13.self_attn.q_norm: model.layers.13.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.011610 σ=2.330506
    First 10: [0.3612440824508667, 0.12880228459835052, 0.2316921502351761, -0.16731218993663788, -0.19560670852661133, 0.008259095251560211, -3.1987829208374023, 0.03870191425085068, -0.4358840584754944, -0.21641740202903748]
    Last 10:  [0.5578280091285706, 1.0190964937210083, 1.664643406867981, -1.571599006652832, 1.6427546739578247, -1.2382352352142334, -0.09782607853412628, -2.3036954402923584, 0.18529659509658813, 0.5044712424278259]
  OUT[0]: [1, 4, 4, 256] | μ=-0.030150 σ=1.478100
    First 10: [1.4899060726165771, 0.4347286820411682, 0.5620079040527344, -0.5361603498458862, -0.4073679447174072, 0.029775021597743034, 0.0474567636847496, 0.12115135043859482, -1.3838789463043213, -0.41221046447753906]
    Last 10:  [0.2117370367050171, 0.4798690676689148, 0.9413792490959167, -0.6158853769302368, 1.1914803981781006, -0.9018956422805786, -0.041046179831027985, -1.6094108819961548, 0.0780324712395668, 0.8860025405883789]
  WEIGHT: [256] | μ=0.572764

214_model.layers.13.self_attn.k_norm: model.layers.13.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.011416 σ=2.053910
    First 10: [-0.0020686164498329163, 0.006217023357748985, 4.091761112213135, 0.0024563726037740707, -3.9802873134613037, -0.0017994018271565437, -0.12375594675540924, 1.24365234375, -0.0009178835898637772, -4.232999801635742]
    Last 10:  [1.1919364929199219, 2.0350561141967773, 4.547500133514404, 0.6425000429153442, 2.003917932510376, -2.9013776779174805, -2.4789624214172363, -2.9661006927490234, 3.473496198654175, -2.4101815223693848]
  OUT[0]: [1, 1, 4, 256] | μ=-0.097803 σ=1.928435
    First 10: [-0.00532769039273262, 0.017870904877781868, 0.0, 0.005283806007355452, -0.0, -0.005102972965687513, -0.004775004927068949, 0.005998142529278994, -0.0014144129818305373, -0.020415782928466797]
    Last 10:  [0.9455217123031616, 1.3490456342697144, 2.8842170238494873, 0.479971706867218, 1.1931551694869995, -1.5974127054214478, -1.8152073621749878, -2.599712610244751, 2.7361350059509277, -3.957526206970215]
  WEIGHT: [256] | μ=1.041344

215_model.layers.13.self_attn.o_proj: model.layers.13.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.013005 σ=0.407037
    First 10: [-0.046511050313711166, -0.020304512232542038, -0.026450205594301224, -0.04027312994003296, 0.004717771429568529, -0.18583889305591583, 0.05088362842798233, 0.06272327899932861, 0.0631592720746994, 0.043709319084882736]
    Last 10:  [0.07559388130903244, -0.05784866213798523, -0.13291990756988525, 0.28604987263679504, -0.030800852924585342, 0.23273949325084686, -0.21756193041801453, -0.09324836730957031, 0.5016294121742249, 0.04357055202126503]
  OUT[0]: [1, 4, 640] | μ=0.009107 σ=1.110920
    First 10: [-0.06809940934181213, -0.24627932906150818, 0.044035181403160095, -0.4424225687980652, -0.04004305973649025, 0.07336953282356262, 0.1429828256368637, 0.4113996624946594, -0.014029568061232567, 0.019683316349983215]
    Last 10:  [0.38772135972976685, 0.6424385905265808, -0.0994315892457962, -0.056463561952114105, 0.09680076688528061, 0.036679089069366455, -0.616761326789856, 0.10469985008239746, -0.10467465966939926, 0.5417362451553345]
  WEIGHT: [640, 1024] | μ=-0.000032

216_model.layers.13.self_attn: model.layers.13.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.009107 σ=1.110920
    First 10: [-0.06809940934181213, -0.24627932906150818, 0.044035181403160095, -0.4424225687980652, -0.04004305973649025, 0.07336953282356262, 0.1429828256368637, 0.4113996624946594, -0.014029568061232567, 0.019683316349983215]
    Last 10:  [0.38772135972976685, 0.6424385905265808, -0.0994315892457962, -0.056463561952114105, 0.09680076688528061, 0.036679089069366455, -0.616761326789856, 0.10469985008239746, -0.10467465966939926, 0.5417362451553345]

217_model.layers.13.post_attention_layernorm: model.layers.13.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.009107 σ=1.110920
    First 10: [-0.06809940934181213, -0.24627932906150818, 0.044035181403160095, -0.4424225687980652, -0.04004305973649025, 0.07336953282356262, 0.1429828256368637, 0.4113996624946594, -0.014029568061232567, 0.019683316349983215]
    Last 10:  [0.38772135972976685, 0.6424385905265808, -0.0994315892457962, -0.056463561952114105, 0.09680076688528061, 0.036679089069366455, -0.616761326789856, 0.10469985008239746, -0.10467465966939926, 0.5417362451553345]
  OUT[0]: [1, 4, 640] | μ=7.244958 σ=140.101364
    First 10: [-17.297237396240234, -39.80766296386719, 0.7219362258911133, -7.2533111572265625, -0.16758917272090912, 9.893940925598145, 14.527043342590332, 2.695507287979126, -0.2931794822216034, 1.1998939514160156]
    Last 10:  [1.9934537410736084, 76.92179870605469, -6.466576099395752, -7.587450981140137, 5.670100212097168, 3.7598328590393066, -68.00332641601562, 32.01676559448242, -10.504383087158203, 19.94927215576172]
  WEIGHT: [640] | μ=124.406898

218_model.layers.13.pre_feedforward_layernorm: model.layers.13.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=63.635101 σ=2113.373291
    First 10: [-69.1749267578125, -201.42672729492188, -6.286761283874512, -24.429912567138672, -3.1075072288513184, -76.3436279296875, -43.11510467529297, 3.611940383911133, 7.915090560913086, 3.029071807861328]
    Last 10:  [-0.5477139949798584, 269.576416015625, -28.76013946533203, 110.91259002685547, -60.03681564331055, 184.00489807128906, -323.7102966308594, -172.98666381835938, 138.6439208984375, -47.93730926513672]
  OUT[0]: [1, 4, 640] | μ=-0.003589 σ=0.349925
    First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
    Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
  WEIGHT: [640] | μ=2.165846

219_model.layers.13.mlp.gate_proj: model.layers.13.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.003589 σ=0.349925
    First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
    Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
  OUT[0]: [1, 4, 2048] | μ=-0.102763 σ=0.371830
    First 10: [-0.01274721696972847, -0.0031851953826844692, -0.001140909269452095, 0.0010472651338204741, -0.003474391996860504, 0.006230496801435947, 0.026194987818598747, -0.0018616141751408577, -0.021434715017676353, 0.004909653682261705]
    Last 10:  [-0.3608516454696655, -0.07765829563140869, -0.4888724982738495, -0.33890244364738464, -0.10866956412792206, 1.145201563835144, 0.11459872871637344, -0.2678413987159729, 0.7660181522369385, -0.30814293026924133]
  WEIGHT: [2048, 640] | μ=-0.000189

220_model.layers.13.mlp.act_fn: model.layers.13.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.102763 σ=0.371830
    First 10: [-0.01274721696972847, -0.0031851953826844692, -0.001140909269452095, 0.0010472651338204741, -0.003474391996860504, 0.006230496801435947, 0.026194987818598747, -0.0018616141751408577, -0.021434715017676353, 0.004909653682261705]
    Last 10:  [-0.3608516454696655, -0.07765829563140869, -0.4888724982738495, -0.33890244364738464, -0.10866956412792206, 1.145201563835144, 0.11459872871637344, -0.2678413987159729, 0.7660181522369385, -0.30814293026924133]
  OUT[0]: [1, 4, 2048] | μ=0.000218 σ=0.189299
    First 10: [-0.006308785639703274, -0.0015885502798482776, -0.0005699353059753776, 0.0005240700556896627, -0.0017323802458122373, 0.0031307346653193235, 0.013371208682656288, -0.0009294245392084122, -0.010534078814089298, 0.002464443212375045]
    Last 10:  [-0.129588782787323, -0.03642563149333, -0.15277190506458282, -0.12449698150157928, -0.04963294416666031, 1.0006345510482788, 0.0625271201133728, -0.10564112663269043, 0.5960166454315186, -0.11678487062454224]

221_model.layers.13.mlp.up_proj: model.layers.13.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.003589 σ=0.349925
    First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
    Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
  OUT[0]: [1, 4, 2048] | μ=0.007174 σ=0.318259
    First 10: [-0.0016608042642474174, 0.0066913217306137085, 0.01708505116403103, 0.008786545135080814, 0.008463648147881031, 0.014211398549377918, -0.009687371551990509, -0.0016897639725357294, -0.012761260382831097, -0.0016886701341718435]
    Last 10:  [-0.20554886758327484, 0.4232608675956726, 0.21830642223358154, -0.5607266426086426, 0.512967586517334, 0.35627755522727966, 0.38990873098373413, -0.05410391837358475, 0.04851941019296646, 0.2783465087413788]
  WEIGHT: [2048, 640] | μ=-0.000058

222_model.layers.13.mlp.down_proj: model.layers.13.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.002321 σ=0.104542
    First 10: [1.0477658179297578e-05, -1.0629501048242673e-05, -9.737374057294801e-06, 4.604765308613423e-06, -1.4662256944575347e-05, 4.449211701285094e-05, -0.00012953186524100602, 1.5705080613770406e-06, 0.00013442811905406415, -4.1616317503212485e-06]
    Last 10:  [0.026636827737092972, -0.015417544171214104, -0.03335108980536461, 0.06980877369642258, -0.02546009235084057, 0.3565036356449127, 0.024379869922995567, 0.005715598817914724, 0.028918376192450523, -0.032506659626960754]
  OUT[0]: [1, 4, 640] | μ=-0.002970 σ=0.095002
    First 10: [0.00011130119673907757, 0.0001265045430045575, -1.2267950296518393e-05, 0.00021823168208356947, -1.0596942956908606e-05, 8.946791786001995e-05, 3.8624861190328375e-05, 6.086607027100399e-05, 6.948089867364615e-05, -7.838963210815564e-05]
    Last 10:  [-0.0565229095518589, -0.007918676361441612, 0.003706622403115034, -0.020507216453552246, 0.0603732168674469, -0.028576085343956947, 0.013171181082725525, -0.03303638845682144, -0.03214028850197792, -0.01975499466061592]
  WEIGHT: [640, 2048] | μ=-0.000009

223_model.layers.13.mlp: model.layers.13.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.003589 σ=0.349925
    First 10: [-0.00531279481947422, -0.028812948614358902, -0.012119228951632977, -0.05553770810365677, -0.018806705251336098, -0.017416033893823624, -0.013886931352317333, 0.0162004753947258, 0.012583473697304726, 0.0013289544731378555]
    Last 10:  [-0.014699136838316917, 0.423907607793808, -0.04623018205165863, 0.21430709958076477, -0.19087201356887817, 0.3449467718601227, -0.5212318897247314, -0.042669910937547684, 0.24547086656093597, -0.31426894664764404]
  OUT[0]: [1, 4, 640] | μ=-0.002970 σ=0.095002
    First 10: [0.00011130119673907757, 0.0001265045430045575, -1.2267950296518393e-05, 0.00021823168208356947, -1.0596942956908606e-05, 8.946791786001995e-05, 3.8624861190328375e-05, 6.086607027100399e-05, 6.948089867364615e-05, -7.838963210815564e-05]
    Last 10:  [-0.0565229095518589, -0.007918676361441612, 0.003706622403115034, -0.020507216453552246, 0.0603732168674469, -0.028576085343956947, 0.013171181082725525, -0.03303638845682144, -0.03214028850197792, -0.01975499466061592]

224_model.layers.13.post_feedforward_layernorm: model.layers.13.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.002970 σ=0.095002
    First 10: [0.00011130119673907757, 0.0001265045430045575, -1.2267950296518393e-05, 0.00021823168208356947, -1.0596942956908606e-05, 8.946791786001995e-05, 3.8624861190328375e-05, 6.086607027100399e-05, 6.948089867364615e-05, -7.838963210815564e-05]
    Last 10:  [-0.0565229095518589, -0.007918676361441612, 0.003706622403115034, -0.020507216453552246, 0.0603732168674469, -0.028576085343956947, 0.013171181082725525, -0.03303638845682144, -0.03214028850197792, -0.01975499466061592]
  OUT[0]: [1, 4, 640] | μ=-0.137734 σ=118.920845
    First 10: [37.150020599365234, 27.90060806274414, -0.34425193071365356, 4.807733535766602, -0.07825306802988052, 19.4679012298584, 8.442662239074707, 0.6667066812515259, 2.6509201526641846, -7.834011077880859]
    Last 10:  [-5.837003231048584, -19.161617279052734, 5.452693939208984, -62.30247116088867, 68.54055786132812, -62.75143814086914, 28.501941680908203, -134.8792266845703, -71.6060562133789, -13.056365966796875]
  WEIGHT: [640] | μ=175.409515

225_model.layers.14.input_layernorm: model.layers.14.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=63.497356 σ=2119.710205
    First 10: [-32.024906158447266, -173.526123046875, -6.6310133934021, -19.62217903137207, -3.185760259628296, -56.87572479248047, -34.67244338989258, 4.278646945953369, 10.566010475158691, -4.804939270019531]
    Last 10:  [-6.384716987609863, 250.414794921875, -23.307445526123047, 48.6101188659668, 8.503742218017578, 121.25346374511719, -295.2083435058594, -307.86590576171875, 67.0378646850586, -60.993675231933594]
  OUT[0]: [1, 4, 640] | μ=0.011588 σ=2.501117
    First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
    Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
  WEIGHT: [640] | μ=16.293940

226_model.layers.14.self_attn.q_proj: model.layers.14.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.011588 σ=2.501117
    First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
    Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
  OUT[0]: [1, 4, 1024] | μ=-0.086158 σ=3.191247
    First 10: [-0.10394278168678284, 0.486113041639328, -0.0031670331954956055, -0.06406673043966293, 0.029022572562098503, -0.20472556352615356, 0.009121552109718323, 0.6428589820861816, -0.14011245965957642, 0.04786229878664017]
    Last 10:  [-2.0579731464385986, -0.22386649250984192, 1.4910355806350708, 2.1106791496276855, -2.5043864250183105, -0.44952327013015747, 3.063737392425537, 0.2651447057723999, -0.9167452454566956, 0.42851200699806213]
  WEIGHT: [1024, 640] | μ=0.000052

227_model.layers.14.self_attn.k_proj: model.layers.14.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.011588 σ=2.501117
    First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
    Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
  OUT[0]: [1, 4, 256] | μ=0.046558 σ=3.316509
    First 10: [0.001125101000070572, -1.4922305345535278, 2.9507172107696533, 0.0005945321172475815, 0.0058700889348983765, 0.0026159570552408695, -0.0005042143166065216, -1.2961325645446777, 2.3352885246276855, 2.1909499168395996]
    Last 10:  [-3.321159839630127, -2.7551631927490234, 0.9813976287841797, 2.119474172592163, -0.32746803760528564, -1.9363809823989868, 8.463736534118652, -2.101818799972534, 2.4371798038482666, 4.035786151885986]
  WEIGHT: [256, 640] | μ=0.000051

228_model.layers.14.self_attn.v_proj: model.layers.14.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.011588 σ=2.501117
    First 10: [-0.011235620826482773, -0.12025249004364014, -0.06758473813533783, -0.19999352097511292, -0.07989578694105148, -0.0711437463760376, -0.05648871138691902, 0.08616061508655548, 0.09985900670289993, -0.011649634689092636]
    Last 10:  [-0.7207742929458618, 1.8672305345535278, -0.25975534319877625, 0.3877972960472107, 0.132953479886055, 1.1034306287765503, -2.3669204711914062, -0.33246344327926636, 0.5993079543113708, -2.376709222793579]
  OUT[0]: [1, 4, 256] | μ=0.056551 σ=1.627543
    First 10: [-0.0014735064469277859, -0.11611250042915344, -0.013283902779221535, -0.02925250679254532, -0.38320019841194153, -0.009399885311722755, 0.02620873972773552, 0.09357590973377228, 0.3029884994029999, -0.3202112913131714]
    Last 10:  [-0.49650049209594727, -1.5914411544799805, 2.703547716140747, 0.1917542815208435, -0.9063766002655029, -0.3374021053314209, -3.201317071914673, -0.6980677843093872, -0.7329135537147522, 11.179521560668945]
  WEIGHT: [256, 640] | μ=0.000016

229_model.layers.14.self_attn.q_norm: model.layers.14.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.086158 σ=3.191247
    First 10: [-0.10394278168678284, 0.486113041639328, -0.0031670331954956055, -0.06406673043966293, 0.029022572562098503, -0.20472556352615356, 0.009121552109718323, 0.6428589820861816, -0.14011245965957642, 0.04786229878664017]
    Last 10:  [-2.0579731464385986, -0.22386649250984192, 1.4910355806350708, 2.1106791496276855, -2.5043864250183105, -0.44952327013015747, 3.063737392425537, 0.2651447057723999, -0.9167452454566956, 0.42851200699806213]
  OUT[0]: [1, 4, 4, 256] | μ=-0.019768 σ=1.260950
    First 10: [-0.7986220717430115, 0.0, -0.0, -0.3993980586528778, 0.2628081440925598, -2.1628267765045166, 0.09135883301496506, 0.0, -0.04805910959839821, 0.2963259816169739]
    Last 10:  [-0.537922739982605, -0.05352909862995148, 0.4146948754787445, 1.6062668561935425, -0.7488049864768982, -0.24494467675685883, 2.2118141651153564, 0.23347973823547363, -0.4236548840999603, 0.12459105998277664]
  WEIGHT: [256] | μ=0.993812

230_model.layers.14.self_attn.k_norm: model.layers.14.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=0.046558 σ=3.316509
    First 10: [0.001125101000070572, -1.4922305345535278, 2.9507172107696533, 0.0005945321172475815, 0.0058700889348983765, 0.0026159570552408695, -0.0005042143166065216, -1.2961325645446777, 2.3352885246276855, 2.1909499168395996]
    Last 10:  [-3.321159839630127, -2.7551631927490234, 0.9813976287841797, 2.119474172592163, -0.32746803760528564, -1.9363809823989868, 8.463736534118652, -2.101818799972534, 2.4371798038482666, 4.035786151885986]
  OUT[0]: [1, 1, 4, 256] | μ=0.079954 σ=2.678622
    First 10: [0.0025186347775161266, -0.0, 0.0, 0.0014210810186341405, 0.009686366654932499, 0.0034437980502843857, -0.0005070093902759254, -0.0, -0.01416732557117939, -0.01329167652875185]
    Last 10:  [-3.9326984882354736, -4.792883396148682, 1.3691725730895996, 1.2411812543869019, -0.3666151762008667, -1.3382400274276733, 3.562436103820801, -0.900505781173706, 2.2772750854492188, 4.344465255737305]
  WEIGHT: [256] | μ=1.554319

231_model.layers.14.self_attn.o_proj: model.layers.14.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.013154 σ=0.539498
    First 10: [-0.0014735064469277859, -0.11611250042915344, -0.013283902779221535, -0.02925250679254532, -0.38320019841194153, -0.009399885311722755, 0.02620873972773552, 0.09357590973377228, 0.3029884994029999, -0.3202112913131714]
    Last 10:  [0.33426612615585327, -0.10056263953447342, 0.5438634157180786, -0.014387309551239014, -0.04384348541498184, -0.13254477083683014, -0.08891195058822632, 0.22227303683757782, -0.02293824777007103, 3.8227529525756836]
  OUT[0]: [1, 4, 640] | μ=0.060172 σ=2.121538
    First 10: [0.1662517786026001, -0.23888039588928223, -0.3164578378200531, -0.7439513206481934, 0.32570189237594604, 0.09402858465909958, 0.1297404170036316, 0.5364084243774414, -0.49359366297721863, 0.20015856623649597]
    Last 10:  [0.22487866878509521, 0.5776320099830627, -0.3982499837875366, 0.4278284013271332, -0.5090165734291077, 0.4245525002479553, -0.9604005813598633, -0.5169118046760559, 0.4945446252822876, 0.6503120064735413]
  WEIGHT: [640, 1024] | μ=-0.000019

232_model.layers.14.self_attn: model.layers.14.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.060172 σ=2.121538
    First 10: [0.1662517786026001, -0.23888039588928223, -0.3164578378200531, -0.7439513206481934, 0.32570189237594604, 0.09402858465909958, 0.1297404170036316, 0.5364084243774414, -0.49359366297721863, 0.20015856623649597]
    Last 10:  [0.22487866878509521, 0.5776320099830627, -0.3982499837875366, 0.4278284013271332, -0.5090165734291077, 0.4245525002479553, -0.9604005813598633, -0.5169118046760559, 0.4945446252822876, 0.6503120064735413]

233_model.layers.14.post_attention_layernorm: model.layers.14.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.060172 σ=2.121538
    First 10: [0.1662517786026001, -0.23888039588928223, -0.3164578378200531, -0.7439513206481934, 0.32570189237594604, 0.09402858465909958, 0.1297404170036316, 0.5364084243774414, -0.49359366297721863, 0.20015856623649597]
    Last 10:  [0.22487866878509521, 0.5776320099830627, -0.3982499837875366, 0.4278284013271332, -0.5090165734291077, 0.4245525002479553, -0.9604005813598633, -0.5169118046760559, 0.4945446252822876, 0.6503120064735413]
  OUT[0]: [1, 4, 640] | μ=5.571620 σ=103.344131
    First 10: [16.74628448486328, -19.964147567749023, -3.8279409408569336, -8.099093437194824, 0.8730148077011108, 9.84359073638916, 13.296823501586914, 2.403697967529297, -8.84736442565918, 9.508557319641113]
    Last 10:  [1.140194058418274, 66.31123352050781, -28.243852615356445, 54.57113265991211, -28.178316116333008, 44.6223030090332, -110.74247741699219, -68.30768585205078, 54.75434494018555, 23.225934982299805]
  WEIGHT: [640] | μ=161.183136

234_model.layers.14.pre_feedforward_layernorm: model.layers.14.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=69.068985 σ=2183.442139
    First 10: [-15.278621673583984, -193.49026489257812, -10.458953857421875, -27.721271514892578, -2.3127455711364746, -47.032135009765625, -21.375619888305664, 6.682344913482666, 1.7186460494995117, 4.703618049621582]
    Last 10:  [-5.244523048400879, 316.72601318359375, -51.551300048828125, 103.1812515258789, -19.67457389831543, 165.87576293945312, -405.9508056640625, -376.173583984375, 121.79220581054688, -37.767738342285156]
  OUT[0]: [1, 4, 640] | μ=-0.005377 σ=0.326920
    First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
    Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
  WEIGHT: [640] | μ=1.896982

235_model.layers.14.mlp.gate_proj: model.layers.14.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.005377 σ=0.326920
    First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
    Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
  OUT[0]: [1, 4, 2048] | μ=-0.078075 σ=0.367016
    First 10: [0.003358383197337389, -0.018439356237649918, -0.023224003612995148, -0.002271893434226513, 0.004260982386767864, -0.0195404551923275, -0.012691360898315907, -0.009017166681587696, -0.012722431682050228, -0.02466193586587906]
    Last 10:  [1.6589471101760864, 0.30642417073249817, -0.2687648832798004, 0.04313495010137558, -0.5212712287902832, -0.13198071718215942, -0.07699524611234665, -0.17911800742149353, 0.6784461140632629, -0.05614028871059418]
  WEIGHT: [2048, 640] | μ=-0.000198

236_model.layers.14.mlp.act_fn: model.layers.14.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.078075 σ=0.367016
    First 10: [0.003358383197337389, -0.018439356237649918, -0.023224003612995148, -0.002271893434226513, 0.004260982386767864, -0.0195404551923275, -0.012691360898315907, -0.009017166681587696, -0.012722431682050228, -0.02466193586587906]
    Last 10:  [1.6589471101760864, 0.30642417073249817, -0.2687648832798004, 0.04313495010137558, -0.5212712287902832, -0.13198071718215942, -0.07699524611234665, -0.17911800742149353, 0.6784461140632629, -0.05614028871059418]
  OUT[0]: [1, 4, 2048] | μ=0.008428 σ=0.187865
    First 10: [0.0016836911672726274, -0.00908404216170311, -0.011396849527955055, -0.0011338875629007816, 0.0021377343218773603, -0.009617909789085388, -0.006281424313783646, -0.004476145841181278, -0.006296644918620586, -0.012088351882994175]
    Last 10:  [1.5782272815704346, 0.1900903284549713, -0.10590986162424088, 0.022309524938464165, -0.15696901082992554, -0.0590614415705204, -0.036134932190179825, -0.07682807743549347, 0.5096364617347717, -0.0268134493380785]

237_model.layers.14.mlp.up_proj: model.layers.14.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.005377 σ=0.326920
    First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
    Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
  OUT[0]: [1, 4, 2048] | μ=0.000725 σ=0.310555
    First 10: [5.1083858124911785e-05, 0.008441733196377754, 0.005577435716986656, 0.007136796601116657, 0.004195770248770714, -0.0062172384932637215, -0.01156560704112053, 0.0045717693865299225, -0.014713268727064133, -0.009971127845346928]
    Last 10:  [0.3196619749069214, -0.23418539762496948, -0.2523602247238159, 0.06940756738185883, -0.24214008450508118, 0.028921887278556824, 0.15471875667572021, 0.1980859786272049, 0.003678303211927414, 0.26619359850883484]
  WEIGHT: [2048, 640] | μ=0.000020

238_model.layers.14.mlp.down_proj: model.layers.14.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.001576 σ=0.096131
    First 10: [8.60094431232028e-08, -7.668505713809282e-05, -6.356519588734955e-05, -8.092324605968315e-06, 8.969442205852829e-06, 5.979683919576928e-05, 7.26484868209809e-05, -2.0463907276280224e-05, 9.264422988053411e-05, 0.00012053450336679816]
    Last 10:  [0.5044992566108704, -0.04451638087630272, 0.02672743611037731, 0.0015484498580917716, 0.03800848871469498, -0.0017081683035939932, -0.005590751767158508, -0.015218565240502357, 0.0018745973939076066, -0.0071375686675310135]
  OUT[0]: [1, 4, 640] | μ=-0.003619 σ=0.071617
    First 10: [-8.315804734593257e-06, -4.014122532680631e-05, 2.1148025552975014e-06, -3.731943797902204e-05, -4.971388625563122e-05, 0.00021793674386572093, -2.8450585887185298e-05, -3.317261507618241e-05, -7.129867299227044e-05, -5.5519652960356325e-05]
    Last 10:  [0.018253840506076813, 0.015990853309631348, -0.01716352254152298, 0.021585891023278236, 0.018917512148618698, -0.08822397142648697, -0.06908410042524338, -0.034579940140247345, 0.06358908116817474, -0.05523169785737991]
  WEIGHT: [640, 2048] | μ=0.000008

239_model.layers.14.mlp: model.layers.14.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.005377 σ=0.326920
    First 10: [-0.0011999221751466393, -0.0276456531137228, -0.01741773635149002, -0.05749690905213356, -0.013655363582074642, -0.009890705347061157, -0.005534333176910877, 0.02680930681526661, 0.002198639092966914, 0.0019760860595852137]
    Last 10:  [-0.10579658299684525, 0.347186416387558, -0.06269481033086777, 0.11622010916471481, -0.04388696327805519, 0.20694218575954437, -0.4728986322879791, -0.08457423746585846, 0.14442801475524902, -0.18514218926429749]
  OUT[0]: [1, 4, 640] | μ=-0.003619 σ=0.071617
    First 10: [-8.315804734593257e-06, -4.014122532680631e-05, 2.1148025552975014e-06, -3.731943797902204e-05, -4.971388625563122e-05, 0.00021793674386572093, -2.8450585887185298e-05, -3.317261507618241e-05, -7.129867299227044e-05, -5.5519652960356325e-05]
    Last 10:  [0.018253840506076813, 0.015990853309631348, -0.01716352254152298, 0.021585891023278236, 0.018917512148618698, -0.08822397142648697, -0.06908410042524338, -0.034579940140247345, 0.06358908116817474, -0.05523169785737991]

240_model.layers.14.post_feedforward_layernorm: model.layers.14.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.003619 σ=0.071617
    First 10: [-8.315804734593257e-06, -4.014122532680631e-05, 2.1148025552975014e-06, -3.731943797902204e-05, -4.971388625563122e-05, 0.00021793674386572093, -2.8450585887185298e-05, -3.317261507618241e-05, -7.129867299227044e-05, -5.5519652960356325e-05]
    Last 10:  [0.018253840506076813, 0.015990853309631348, -0.01716352254152298, 0.021585891023278236, 0.018917512148618698, -0.08822397142648697, -0.06908410042524338, -0.034579940140247345, 0.06358908116817474, -0.05523169785737991]
  OUT[0]: [1, 4, 640] | μ=-5.626893 σ=148.392441
    First 10: [-2.6951563358306885, -9.367049217224121, 0.07012256979942322, -1.0327481031417847, -0.378019243478775, 58.02806091308594, -8.48318099975586, -0.37836167216300964, -3.4661900997161865, -6.450138092041016]
    Last 10:  [2.633345603942871, 64.98895263671875, -43.74867248535156, 119.51782989501953, 38.03981018066406, -326.0716857910156, -269.0275573730469, -180.69102478027344, 254.8325958251953, -63.351863861083984]
  WEIGHT: [640] | μ=207.324829

241_model.layers.15.input_layernorm: model.layers.15.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=63.442078 σ=2157.183838
    First 10: [-17.973777770996094, -202.85731506347656, -10.38883113861084, -28.754018783569336, -2.690764904022217, 10.995925903320312, -29.858800888061523, 6.303983211517334, -1.7475440502166748, -1.7465200424194336]
    Last 10:  [-2.611177444458008, 381.7149658203125, -95.29997253417969, 222.69908142089844, 18.365236282348633, -160.1959228515625, -674.9783935546875, -556.8646240234375, 376.62481689453125, -101.11959838867188]
  OUT[0]: [1, 4, 640] | μ=0.014763 σ=3.070717
    First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
    Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
  WEIGHT: [640] | μ=17.186687

242_model.layers.15.self_attn.q_proj: model.layers.15.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.014763 σ=3.070717
    First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
    Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
  OUT[0]: [1, 4, 1024] | μ=-0.039881 σ=3.617062
    First 10: [0.5756588578224182, -0.00809498131275177, -0.030068835243582726, -0.27358072996139526, -0.4021490514278412, -0.2041524350643158, 0.44307151436805725, -0.39407554268836975, -0.16112014651298523, -0.5332531332969666]
    Last 10:  [-0.45996516942977905, -4.055240631103516, 0.29469096660614014, -0.6887553930282593, 1.0316474437713623, 4.096559524536133, -7.690218925476074, 5.1006317138671875, -1.2750024795532227, -3.287994623184204]
  WEIGHT: [1024, 640] | μ=0.000040

243_model.layers.15.self_attn.k_proj: model.layers.15.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.014763 σ=3.070717
    First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
    Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
  OUT[0]: [1, 4, 256] | μ=-0.137049 σ=3.588787
    First 10: [-0.014390094205737114, -0.019971415400505066, 0.01907084509730339, 0.05025727301836014, 0.007350176572799683, 0.007772386074066162, 0.038482993841171265, -0.008845962584018707, -0.011193700134754181, -0.009361632168293]
    Last 10:  [4.851580619812012, -7.573591232299805, 2.444117784500122, -6.05485725402832, 3.5553221702575684, 10.141180992126465, -7.630295753479004, 11.47496223449707, 3.451714038848877, -1.9936109781265259]
  WEIGHT: [256, 640] | μ=0.000095

244_model.layers.15.self_attn.v_proj: model.layers.15.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.014763 σ=3.070717
    First 10: [-0.016297025606036186, -0.293984979391098, -0.07756447792053223, -0.5655026435852051, -0.12935760617256165, 0.012139362283051014, -0.035116005688905716, 0.21428616344928741, -0.011668415740132332, -0.00487666018307209]
    Last 10:  [-0.4335930049419403, 2.224783182144165, -0.49094197154045105, 1.2100495100021362, 0.23203453421592712, -1.0300644636154175, -4.543177604675293, -1.978783369064331, 2.365060329437256, -3.270021677017212]
  OUT[0]: [1, 4, 256] | μ=0.068283 σ=1.615148
    First 10: [0.31384724378585815, -0.3225434720516205, -0.22224679589271545, -0.11975084990262985, -0.12216020375490189, -0.10215240716934204, 0.017649676650762558, 0.03538923338055611, -0.7035523653030396, 0.32225507497787476]
    Last 10:  [-1.3302853107452393, 0.008603900671005249, 0.7378329038619995, -0.056910671293735504, 1.5762574672698975, 0.07345783710479736, 1.1585659980773926, -1.1401840448379517, -0.14527589082717896, 0.21385276317596436]
  WEIGHT: [256, 640] | μ=0.000015

245_model.layers.15.self_attn.q_norm: model.layers.15.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.039881 σ=3.617062
    First 10: [0.5756588578224182, -0.00809498131275177, -0.030068835243582726, -0.27358072996139526, -0.4021490514278412, -0.2041524350643158, 0.44307151436805725, -0.39407554268836975, -0.16112014651298523, -0.5332531332969666]
    Last 10:  [-0.45996516942977905, -4.055240631103516, 0.29469096660614014, -0.6887553930282593, 1.0316474437713623, 4.096559524536133, -7.690218925476074, 5.1006317138671875, -1.2750024795532227, -3.287994623184204]
  OUT[0]: [1, 4, 4, 256] | μ=-0.044521 σ=2.353814
    First 10: [5.136882305145264, -0.024755684658885002, -0.1313643902540207, -1.0807807445526123, -2.467142105102539, -0.5239905714988708, 2.306349039077759, -2.674025297164917, -0.6275192499160767, -3.182230234146118]
    Last 10:  [-0.13460147380828857, -1.2661503553390503, 0.2504635453224182, -0.21333841979503632, 0.3078373074531555, 0.6432620286941528, -1.651972770690918, 1.7537012100219727, -0.4956802427768707, -1.299379825592041]
  WEIGHT: [256] | μ=0.964427

246_model.layers.15.self_attn.k_norm: model.layers.15.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.137049 σ=3.588787
    First 10: [-0.014390094205737114, -0.019971415400505066, 0.01907084509730339, 0.05025727301836014, 0.007350176572799683, 0.007772386074066162, 0.038482993841171265, -0.008845962584018707, -0.011193700134754181, -0.009361632168293]
    Last 10:  [4.851580619812012, -7.573591232299805, 2.444117784500122, -6.05485725402832, 3.5553221702575684, 10.141180992126465, -7.630295753479004, 11.47496223449707, 3.451714038848877, -1.9936109781265259]
  OUT[0]: [1, 1, 4, 256] | μ=0.104860 σ=3.478912
    First 10: [-0.008882024325430393, -0.00844376441091299, 0.019058024510741234, 0.0564730167388916, 0.0012131271651014686, 0.016166940331459045, 0.01716214418411255, -0.0035100062377750874, -0.01310958992689848, -0.007990136742591858]
    Last 10:  [4.246663570404053, -5.206329822540283, 0.4700140058994293, -2.445855140686035, 5.595364570617676, 17.484474182128906, -5.093516826629639, 4.882603168487549, 1.6365584135055542, -0.5548264384269714]
  WEIGHT: [256] | μ=0.951797

247_model.layers.15.self_attn.o_proj: model.layers.15.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.033516 σ=0.768839
    First 10: [0.31384724378585815, -0.3225434720516205, -0.22224679589271545, -0.11975084990262985, -0.12216020375490189, -0.10215240716934204, 0.017649676650762558, 0.03538923338055611, -0.7035523653030396, 0.32225507497787476]
    Last 10:  [-0.8239046335220337, 0.11934424936771393, 0.5120292901992798, -0.2881278991699219, 0.8856801390647888, 0.03579059988260269, 0.8332778215408325, -0.7797176837921143, 0.019624115899205208, 0.12107152491807938]
  OUT[0]: [1, 4, 640] | μ=0.072226 σ=1.377489
    First 10: [0.026933975517749786, 0.035171665251255035, -0.45747190713882446, -0.4167557954788208, -0.04208611696958542, 0.10210814327001572, 0.5136877298355103, -0.14254850149154663, -0.22901488840579987, -0.19083257019519806]
    Last 10:  [0.84844970703125, -0.1682589054107666, 0.6788538694381714, 0.11778327822685242, -0.8764616250991821, 0.6759559512138367, 1.3663198947906494, 0.09033279120922089, -0.4403201937675476, -1.29127037525177]
  WEIGHT: [640, 1024] | μ=0.000017

248_model.layers.15.self_attn: model.layers.15.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.072226 σ=1.377489
    First 10: [0.026933975517749786, 0.035171665251255035, -0.45747190713882446, -0.4167557954788208, -0.04208611696958542, 0.10210814327001572, 0.5136877298355103, -0.14254850149154663, -0.22901488840579987, -0.19083257019519806]
    Last 10:  [0.84844970703125, -0.1682589054107666, 0.6788538694381714, 0.11778327822685242, -0.8764616250991821, 0.6759559512138367, 1.3663198947906494, 0.09033279120922089, -0.4403201937675476, -1.29127037525177]

249_model.layers.15.post_attention_layernorm: model.layers.15.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.072226 σ=1.377489
    First 10: [0.026933975517749786, 0.035171665251255035, -0.45747190713882446, -0.4167557954788208, -0.04208611696958542, 0.10210814327001572, 0.5136877298355103, -0.14254850149154663, -0.22901488840579987, -0.19083257019519806]
    Last 10:  [0.84844970703125, -0.1682589054107666, 0.6788538694381714, 0.11778327822685242, -0.8764616250991821, 0.6759559512138367, 1.3663198947906494, 0.09033279120922089, -0.4403201937675476, -1.29127037525177]
  OUT[0]: [1, 4, 640] | μ=1.984967 σ=76.433548
    First 10: [12.88828182220459, 11.31078052520752, -9.416020393371582, -15.889395713806152, -0.4592853784561157, 19.40865707397461, 93.0999984741211, -2.5598978996276855, -6.896629333496094, -22.038164138793945]
    Last 10:  [8.254154205322266, -20.814590454101562, 42.26712417602539, 17.75473976135254, -58.51987075805664, 83.61962127685547, 171.26028442382812, 41.516536712646484, -50.14137268066406, -53.42215347290039]
  WEIGHT: [640] | μ=170.959076

250_model.layers.15.pre_feedforward_layernorm: model.layers.15.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=65.427055 σ=2142.266113
    First 10: [-5.085495948791504, -191.54653930664062, -19.804851531982422, -44.64341354370117, -3.150050163269043, 30.404582977294922, 63.24119567871094, 3.7440853118896484, -8.644173622131348, -23.784683227539062]
    Last 10:  [5.642976760864258, 360.900390625, -53.0328483581543, 240.45382690429688, -40.154632568359375, -76.57630157470703, -503.7181091308594, -515.3480834960938, 326.48345947265625, -154.541748046875]
  OUT[0]: [1, 4, 640] | μ=-0.007043 σ=0.304590
    First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
    Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
  WEIGHT: [640] | μ=1.125533

251_model.layers.15.mlp.gate_proj: model.layers.15.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.007043 σ=0.304590
    First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
    Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
  OUT[0]: [1, 4, 2048] | μ=-0.074398 σ=0.335151
    First 10: [0.007968437857925892, 0.006577871739864349, -0.01365414634346962, -0.009177042171359062, -0.013787896372377872, -0.012795290909707546, 0.006965255830436945, 0.0033434145152568817, 0.005731020122766495, -0.021441299468278885]
    Last 10:  [-0.09840548038482666, 0.21514005959033966, 0.42105987668037415, -0.17200151085853577, -0.13599467277526855, 0.23943284153938293, -0.3367224335670471, 0.13549494743347168, -0.1427851766347885, -0.24270331859588623]
  WEIGHT: [2048, 640] | μ=-0.000213

252_model.layers.15.mlp.act_fn: model.layers.15.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.074398 σ=0.335151
    First 10: [0.007968437857925892, 0.006577871739864349, -0.01365414634346962, -0.009177042171359062, -0.013787896372377872, -0.012795290909707546, 0.006965255830436945, 0.0033434145152568817, 0.005731020122766495, -0.021441299468278885]
    Last 10:  [-0.09840548038482666, 0.21514005959033966, 0.42105987668037415, -0.17200151085853577, -0.13599467277526855, 0.23943284153938293, -0.3367224335670471, 0.13549494743347168, -0.1427851766347885, -0.24270331859588623]
  OUT[0]: [1, 4, 2048] | μ=0.002563 σ=0.190383
    First 10: [0.004009549971669912, 0.0033061972353607416, -0.006752698216587305, -0.00455492315813899, -0.006818109191954136, -0.006332332734018564, 0.0035019824281334877, 0.0016761667793616652, 0.002878613071516156, -0.010537258349359035]
    Last 10:  [-0.04534578323364258, 0.1258930265903473, 0.27921435236930847, -0.07425645738840103, -0.06064186617732048, 0.1423693299293518, -0.12397266924381256, 0.07504914700984955, -0.06328679621219635, -0.09808177500963211]

253_model.layers.15.mlp.up_proj: model.layers.15.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.007043 σ=0.304590
    First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
    Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
  OUT[0]: [1, 4, 2048] | μ=0.010442 σ=0.322344
    First 10: [-0.0006205979734659195, -0.004707143642008305, -0.0023159529082477093, 0.002553769387304783, -0.014446679502725601, -0.002844909904524684, 0.001927623525261879, 0.0010072584263980389, 0.013269669376313686, -0.014595495536923409]
    Last 10:  [0.3198736608028412, 0.1653127372264862, -0.23942740261554718, 0.30876171588897705, -0.09417060017585754, 0.2475600242614746, 0.06429553031921387, 0.15138278901576996, -0.012139350175857544, 0.19579988718032837]
  WEIGHT: [2048, 640] | μ=0.000008

254_model.layers.15.mlp.down_proj: model.layers.15.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.000076 σ=0.097279
    First 10: [-2.488318614268792e-06, -1.556274582981132e-05, 1.563893056300003e-05, -1.1632223504420836e-05, 9.849904017755762e-05, 1.801491634978447e-05, 6.75050387144438e-06, 1.6883330999917234e-06, 3.8198242691578344e-05, 0.000153796499944292]
    Last 10:  [-0.01450492162257433, 0.020811721682548523, -0.06685156375169754, -0.022927550598978996, 0.005710680969059467, 0.035244956612586975, -0.007970888167619705, 0.01136114913970232, 0.0007682605646550655, -0.01920440047979355]
  OUT[0]: [1, 4, 640] | μ=0.000643 σ=0.083960
    First 10: [-0.00012059349683113396, 0.00012339527893345803, 2.3760161639074795e-05, 1.2208345651743002e-05, 4.0931423427537084e-05, -7.923947850940749e-05, -4.043376975459978e-05, 0.0001011798667605035, 4.606084985425696e-05, 8.468073792755604e-05]
    Last 10:  [-0.004282730631530285, 0.06307205557823181, -0.03377075493335724, -0.027518657967448235, -0.021923158317804337, 0.022629866376519203, 0.01870272308588028, 0.00733309518545866, -0.01623145118355751, 0.01149715855717659]
  WEIGHT: [640, 2048] | μ=0.000008

255_model.layers.15.mlp: model.layers.15.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.007043 σ=0.304590
    First 10: [-0.00033695058664307, -0.021887948736548424, -0.024114297702908516, -0.07922151684761047, -0.012873684987425804, 0.00462756073102355, 0.011356002651154995, 0.011792431585490704, -0.008101344108581543, -0.00783385057002306]
    Last 10:  [0.08356591314077377, 0.26881635189056396, -0.04785755276679993, 0.1669144183397293, -0.06211898848414421, -0.06446283310651779, -0.41404545307159424, -0.09539631009101868, 0.25972872972488403, -0.5639722943305969]
  OUT[0]: [1, 4, 640] | μ=0.000643 σ=0.083960
    First 10: [-0.00012059349683113396, 0.00012339527893345803, 2.3760161639074795e-05, 1.2208345651743002e-05, 4.0931423427537084e-05, -7.923947850940749e-05, -4.043376975459978e-05, 0.0001011798667605035, 4.606084985425696e-05, 8.468073792755604e-05]
    Last 10:  [-0.004282730631530285, 0.06307205557823181, -0.03377075493335724, -0.027518657967448235, -0.021923158317804337, 0.022629866376519203, 0.01870272308588028, 0.00733309518545866, -0.01623145118355751, 0.01149715855717659]

256_model.layers.15.post_feedforward_layernorm: model.layers.15.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.000643 σ=0.083960
    First 10: [-0.00012059349683113396, 0.00012339527893345803, 2.3760161639074795e-05, 1.2208345651743002e-05, 4.0931423427537084e-05, -7.923947850940749e-05, -4.043376975459978e-05, 0.0001011798667605035, 4.606084985425696e-05, 8.468073792755604e-05]
    Last 10:  [-0.004282730631530285, 0.06307205557823181, -0.03377075493335724, -0.027518657967448235, -0.021923158317804337, 0.022629866376519203, 0.01870272308588028, 0.00733309518545866, -0.01623145118355751, 0.01149715855717659]
  OUT[0]: [1, 4, 640] | μ=2.358799 σ=135.110718
    First 10: [-41.17707061767578, 35.25476837158203, 0.9224682450294495, 0.37523314356803894, 0.3539884388446808, -23.901308059692383, -14.772287368774414, 1.3975409269332886, 2.7053370475769043, 11.548954010009766]
    Last 10:  [-0.6145722270011902, 257.35882568359375, -84.28010559082031, -183.02432250976562, -45.13790512084961, 90.07971954345703, 73.51394653320312, 35.77819061279297, -70.28152465820312, 12.40969181060791]
  WEIGHT: [640] | μ=244.076950

257_model.layers.16.input_layernorm: model.layers.16.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=67.785851 σ=2119.610840
    First 10: [-46.26256561279297, -156.29177856445312, -18.882383346557617, -44.26818084716797, -2.7960617542266846, 6.503274917602539, 48.468910217285156, 5.141626358032227, -5.938836574554443, -12.235729217529297]
    Last 10:  [5.028404712677002, 618.2592163085938, -137.31295776367188, 57.42950439453125, -85.29254150390625, 13.50341796875, -430.20416259765625, -479.56988525390625, 256.2019348144531, -142.13204956054688]
  OUT[0]: [1, 4, 640] | μ=0.023102 σ=3.732058
    First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
    Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
  WEIGHT: [640] | μ=20.347698

258_model.layers.16.self_attn.q_proj: model.layers.16.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.023102 σ=3.732058
    First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
    Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
  OUT[0]: [1, 4, 1024] | μ=0.186328 σ=4.967947
    First 10: [0.5893191695213318, -0.37577053904533386, -0.5282257199287415, 0.13427528738975525, 0.27341386675834656, -0.3656720519065857, 0.1008819043636322, 0.31134817004203796, 0.4053191542625427, -0.5178776979446411]
    Last 10:  [-0.9423724412918091, -6.639535903930664, -16.042810440063477, -8.33527946472168, -2.719550609588623, 9.927096366882324, 6.2545294761657715, 5.284853935241699, -0.6582779884338379, -0.1685328483581543]
  WEIGHT: [1024, 640] | μ=-0.000129

259_model.layers.16.self_attn.k_proj: model.layers.16.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.023102 σ=3.732058
    First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
    Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
  OUT[0]: [1, 4, 256] | μ=-0.099209 σ=6.360083
    First 10: [0.04842047393321991, 0.003189757466316223, 0.01668868027627468, -0.033108457922935486, 0.008404365740716457, -0.01615632325410843, 0.030261045321822166, 0.02573673613369465, -0.001690443605184555, -0.0009182244539260864]
    Last 10:  [0.2804817855358124, -6.581336498260498, -7.865691184997559, -4.267409801483154, -0.8390086889266968, 16.434478759765625, 3.6271352767944336, 19.715147018432617, -8.06876277923584, -8.782459259033203]
  WEIGHT: [256, 640] | μ=0.000072

260_model.layers.16.self_attn.v_proj: model.layers.16.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.023102 σ=3.732058
    First 10: [-0.032418958842754364, -0.17267420887947083, -0.25358301401138306, -0.7161703705787659, -0.09151723980903625, 0.01061239279806614, 0.06925469636917114, 0.14709262549877167, -0.06046636775135994, -0.04069812223315239]
    Last 10:  [0.5918641090393066, 4.240916728973389, -1.2786058187484741, 0.2845083773136139, -1.209688663482666, 0.09307345747947693, -3.1077773571014404, -0.8839764595031738, 1.7998530864715576, -7.083675384521484]
  OUT[0]: [1, 4, 256] | μ=-0.063901 σ=2.754364
    First 10: [0.6461917161941528, 0.08295643329620361, 0.13937117159366608, 0.045830078423023224, 0.6798956394195557, 2.1528165340423584, 0.24252240359783173, 0.08699430525302887, 0.18814261257648468, -0.22373616695404053]
    Last 10:  [-7.92251443862915, 4.208802223205566, 2.3891663551330566, 2.264019012451172, 1.9514652490615845, -2.2277495861053467, -0.9624172449111938, -1.7478519678115845, -2.6274936199188232, 1.2641088962554932]
  WEIGHT: [256, 640] | μ=0.000065

261_model.layers.16.self_attn.q_norm: model.layers.16.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=0.186328 σ=4.967947
    First 10: [0.5893191695213318, -0.37577053904533386, -0.5282257199287415, 0.13427528738975525, 0.27341386675834656, -0.3656720519065857, 0.1008819043636322, 0.31134817004203796, 0.4053191542625427, -0.5178776979446411]
    Last 10:  [-0.9423724412918091, -6.639535903930664, -16.042810440063477, -8.33527946472168, -2.719550609588623, 9.927096366882324, 6.2545294761657715, 5.284853935241699, -0.6582779884338379, -0.1685328483581543]
  OUT[0]: [1, 4, 4, 256] | μ=0.054711 σ=2.067075
    First 10: [1.9357506036758423, -0.7670307755470276, -1.2294248342514038, 0.32008153200149536, 0.40414005517959595, -0.8819738030433655, 0.22438427805900574, 0.6925090551376343, 1.194421410560608, -1.0862643718719482]
    Last 10:  [-0.3352161645889282, -1.5399467945098877, -3.6919846534729004, -2.0684781074523926, -1.0262129306793213, 0.6203492283821106, 1.7550610303878784, 0.642247200012207, -0.1855081021785736, -0.01605072058737278]
  WEIGHT: [256] | μ=0.989109

262_model.layers.16.self_attn.k_norm: model.layers.16.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.099209 σ=6.360083
    First 10: [0.04842047393321991, 0.003189757466316223, 0.01668868027627468, -0.033108457922935486, 0.008404365740716457, -0.01615632325410843, 0.030261045321822166, 0.02573673613369465, -0.001690443605184555, -0.0009182244539260864]
    Last 10:  [0.2804817855358124, -6.581336498260498, -7.865691184997559, -4.267409801483154, -0.8390086889266968, 16.434478759765625, 3.6271352767944336, 19.715147018432617, -8.06876277923584, -8.782459259033203]
  OUT[0]: [1, 1, 4, 256] | μ=-0.096356 σ=2.323688
    First 10: [0.031165719032287598, 0.0021304215770214796, 0.010005900636315346, -0.024083418771624565, 0.010744797065854073, -0.009793558157980442, 0.023012736812233925, 0.017529809847474098, -0.0006614002049900591, -0.0005464845453388989]
    Last 10:  [0.0813940167427063, -1.6824040412902832, -4.743253231048584, -1.4443470239639282, -0.17123238742351532, 15.433734893798828, 1.443773627281189, 9.30430793762207, -2.8848090171813965, -6.02875280380249]
  WEIGHT: [256] | μ=1.232803

263_model.layers.16.self_attn.o_proj: model.layers.16.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.005848 σ=0.867904
    First 10: [0.6461917161941528, 0.08295643329620361, 0.13937117159366608, 0.045830078423023224, 0.6798956394195557, 2.1528165340423584, 0.24252240359783173, 0.08699430525302887, 0.18814261257648468, -0.22373616695404053]
    Last 10:  [1.8183882236480713, 0.34517034888267517, -0.05921671539545059, 0.4055950939655304, 0.2430204600095749, -1.4908965826034546, 0.07559152692556381, -0.24137867987155914, 0.08871878683567047, 0.084270179271698]
  OUT[0]: [1, 4, 640] | μ=0.048478 σ=1.649589
    First 10: [0.47678616642951965, -0.8856248259544373, 0.85146564245224, -0.986497163772583, 0.1115405410528183, 0.3004021644592285, -0.11529159545898438, -0.01679813861846924, -0.3944810628890991, 0.7859684228897095]
    Last 10:  [-0.7075170874595642, 0.4451914429664612, -1.0282460451126099, 0.5290817022323608, 1.171733021736145, -0.4137289226055145, -1.1703801155090332, 0.13357987999916077, 0.5770243406295776, -0.40150222182273865]
  WEIGHT: [640, 1024] | μ=0.000010

264_model.layers.16.self_attn: model.layers.16.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=0.048478 σ=1.649589
    First 10: [0.47678616642951965, -0.8856248259544373, 0.85146564245224, -0.986497163772583, 0.1115405410528183, 0.3004021644592285, -0.11529159545898438, -0.01679813861846924, -0.3944810628890991, 0.7859684228897095]
    Last 10:  [-0.7075170874595642, 0.4451914429664612, -1.0282460451126099, 0.5290817022323608, 1.171733021736145, -0.4137289226055145, -1.1703801155090332, 0.13357987999916077, 0.5770243406295776, -0.40150222182273865]

265_model.layers.16.post_attention_layernorm: model.layers.16.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.048478 σ=1.649589
    First 10: [0.47678616642951965, -0.8856248259544373, 0.85146564245224, -0.986497163772583, 0.1115405410528183, 0.3004021644592285, -0.11529159545898438, -0.01679813861846924, -0.3944810628890991, 0.7859684228897095]
    Last 10:  [-0.7075170874595642, 0.4451914429664612, -1.0282460451126099, 0.5290817022323608, 1.171733021736145, -0.4137289226055145, -1.1703801155090332, 0.13357987999916077, 0.5770243406295776, -0.40150222182273865]
  OUT[0]: [1, 4, 640] | μ=3.245879 σ=100.410545
    First 10: [69.68220520019531, -98.7623519897461, 14.080764770507812, -16.484619140625, 0.4901111423969269, 42.86334228515625, -16.21098518371582, -0.11708002537488937, -10.383063316345215, 53.0793342590332]
    Last 10:  [-3.5843472480773926, 53.66650390625, -66.86080932617188, 82.00186157226562, 67.49333190917969, -45.2057991027832, -134.830810546875, 16.657920837402344, 66.13204193115234, -13.470908164978027]
  WEIGHT: [640] | μ=140.424698

266_model.layers.16.pre_feedforward_layernorm: model.layers.16.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=71.031731 σ=2173.854004
    First 10: [23.419639587402344, -255.05413818359375, -4.801618576049805, -60.75279998779297, -2.30595064163208, 49.366615295410156, 32.25792694091797, 5.024546146392822, -16.3218994140625, 40.843605041503906]
    Last 10:  [1.4440574645996094, 671.9257202148438, -204.17376708984375, 139.43136596679688, -17.799209594726562, -31.702381134033203, -565.0349731445312, -462.9119567871094, 322.333984375, -155.6029510498047]
  OUT[0]: [1, 4, 640] | μ=-0.005618 σ=0.274082
    First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
    Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
  WEIGHT: [640] | μ=1.005519

267_model.layers.16.mlp.gate_proj: model.layers.16.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.005618 σ=0.274082
    First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
    Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
  OUT[0]: [1, 4, 2048] | μ=-0.083386 σ=0.292270
    First 10: [-0.015908610075712204, -0.034116242080926895, 0.004871401935815811, 0.018630526959896088, -0.03868526220321655, 0.0034015411511063576, -0.010958747938275337, -0.016562752425670624, -0.01825706474483013, -0.012787245213985443]
    Last 10:  [0.02294713258743286, 0.1307971477508545, 0.26205554604530334, -0.38444581627845764, -0.009860396385192871, -0.36737382411956787, -0.01593894511461258, 0.08815891295671463, -0.02820269763469696, 0.05401284992694855]
  WEIGHT: [2048, 640] | μ=-0.000237

268_model.layers.16.mlp.act_fn: model.layers.16.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.083386 σ=0.292270
    First 10: [-0.015908610075712204, -0.034116242080926895, 0.004871401935815811, 0.018630526959896088, -0.03868526220321655, 0.0034015411511063576, -0.010958747938275337, -0.016562752425670624, -0.01825706474483013, -0.012787245213985443]
    Last 10:  [0.02294713258743286, 0.1307971477508545, 0.26205554604530334, -0.38444581627845764, -0.009860396385192871, -0.36737382411956787, -0.01593894511461258, 0.08815891295671463, -0.02820269763469696, 0.05401284992694855]
  OUT[0]: [1, 4, 2048] | μ=-0.009553 σ=0.152747
    First 10: [-0.007853343151509762, -0.016593875363469124, 0.0024451680947095156, 0.00945372600108385, -0.018745744600892067, 0.0017053865594789386, -0.005431464407593012, -0.008171942085027695, -0.008995563723146915, -0.006328391842544079]
    Last 10:  [0.01168361958116293, 0.07220413535833359, 0.1581125408411026, -0.13468708097934723, -0.00489141047000885, -0.1310366988182068, -0.007868126034736633, 0.04717600345611572, -0.013784075155854225, 0.028169725090265274]

269_model.layers.16.mlp.up_proj: model.layers.16.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.005618 σ=0.274082
    First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
    Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
  OUT[0]: [1, 4, 2048] | μ=-0.002067 σ=0.277382
    First 10: [-0.0004719181451946497, 0.024181926622986794, -0.006201986223459244, -0.0009645405225455761, -0.0027911104261875153, 0.002578145358711481, -0.00860830582678318, 0.0001635453663766384, 0.008943207561969757, 0.026319943368434906]
    Last 10:  [0.14669443666934967, -0.05562060326337814, -0.0481213703751564, 0.03438417613506317, 0.42668527364730835, -0.5867384672164917, 0.2711637020111084, -0.20394685864448547, -0.291745126247406, -0.02758188545703888]
  WEIGHT: [2048, 640] | μ=0.000014

270_model.layers.16.mlp.down_proj: model.layers.16.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=0.002369 σ=0.092993
    First 10: [3.706135203174199e-06, -0.00040127188549377024, -1.5164899195951875e-05, -9.1185020210105e-06, 5.232144394540228e-05, 4.396734311740147e-06, 4.675570744439028e-05, -1.336483251179743e-06, -8.044919377425686e-05, -0.00016656290972605348]
    Last 10:  [0.0017139220144599676, -0.004016037564724684, -0.007608592044562101, -0.004631104413419962, -0.002087092725560069, 0.07688426971435547, -0.0021335501223802567, -0.009621397592127323, 0.004021436907351017, -0.0007769741350784898]
  OUT[0]: [1, 4, 640] | μ=0.002130 σ=0.082821
    First 10: [0.00042184742051176727, -0.0013882522471249104, 0.001752090873196721, -0.0008490878390148282, 0.0007450255798175931, 0.001499584992416203, -0.00012240168871358037, 0.003291474189609289, -0.0021785059943795204, -0.00023373335716314614]
    Last 10:  [0.05586051568388939, -0.03975488245487213, 0.020572563633322716, -0.034600820392370224, -0.054469577968120575, -0.012684420682489872, 0.011202637106180191, 0.03233649581670761, 0.020729579031467438, -0.06031930446624756]
  WEIGHT: [640, 2048] | μ=-0.000003

271_model.layers.16.mlp: model.layers.16.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.005618 σ=0.274082
    First 10: [0.001642976189032197, -0.028677841648459435, -0.005186592694371939, -0.111630380153656, -0.00985695980489254, 0.006807904224842787, 0.004681031685322523, 0.015528914518654346, -0.013803256675601006, 0.012069725431501865]
    Last 10:  [0.018208784982562065, 0.37751805782318115, -0.14974410831928253, 0.06414318084716797, -0.020638400688767433, -0.019784273579716682, -0.34729066491127014, -0.09251336753368378, 0.18292489647865295, -0.4623536169528961]
  OUT[0]: [1, 4, 640] | μ=0.002130 σ=0.082821
    First 10: [0.00042184742051176727, -0.0013882522471249104, 0.001752090873196721, -0.0008490878390148282, 0.0007450255798175931, 0.001499584992416203, -0.00012240168871358037, 0.003291474189609289, -0.0021785059943795204, -0.00023373335716314614]
    Last 10:  [0.05586051568388939, -0.03975488245487213, 0.020572563633322716, -0.034600820392370224, -0.054469577968120575, -0.012684420682489872, 0.011202637106180191, 0.03233649581670761, 0.020729579031467438, -0.06031930446624756]

272_model.layers.16.post_feedforward_layernorm: model.layers.16.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.002130 σ=0.082821
    First 10: [0.00042184742051176727, -0.0013882522471249104, 0.001752090873196721, -0.0008490878390148282, 0.0007450255798175931, 0.001499584992416203, -0.00012240168871358037, 0.003291474189609289, -0.0021785059943795204, -0.00023373335716314614]
    Last 10:  [0.05586051568388939, -0.03975488245487213, 0.020572563633322716, -0.034600820392370224, -0.054469577968120575, -0.012684420682489872, 0.011202637106180191, 0.03233649581670761, 0.020729579031467438, -0.06031930446624756]
  OUT[0]: [1, 4, 640] | μ=-5.391714 σ=466.573944
    First 10: [71.27413940429688, -175.76939392089844, 33.201168060302734, -11.595407485961914, 3.213974952697754, 227.96554565429688, -22.131912231445312, 22.126253128051758, -62.2681884765625, -14.153388023376465]
    Last 10:  [10.495523452758789, -207.1880340576172, 62.65562057495117, -278.4393005371094, -140.86526489257812, -62.110225677490234, 53.67805480957031, 190.6000213623047, 110.21207427978516, -75.22523498535156]
  WEIGHT: [640] | μ=283.390228

273_model.layers.17.input_layernorm: model.layers.17.input_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=65.640030 σ=1912.738525
    First 10: [94.69377899169922, -430.82354736328125, 28.39954948425293, -72.34820556640625, 0.9080243110656738, 277.3321533203125, 10.126014709472656, 27.150798797607422, -78.590087890625, 26.690216064453125]
    Last 10:  [11.939580917358398, 464.7376708984375, -141.5181427001953, -139.0079345703125, -158.6644744873047, -93.81260681152344, -511.35693359375, -272.31195068359375, 432.5460510253906, -230.82818603515625]
  OUT[0]: [1, 4, 640] | μ=0.044712 σ=2.101685
    First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
    Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
  WEIGHT: [640] | μ=10.482544

274_model.layers.17.self_attn.q_proj: model.layers.17.self_attn.q_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.044712 σ=2.101685
    First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
    Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
  OUT[0]: [1, 4, 1024] | μ=-0.001358 σ=1.868693
    First 10: [-0.22195491194725037, -3.241860866546631, 0.01906605064868927, -3.830760955810547, -1.901699423789978, 0.18472124636173248, 1.5615875720977783, -0.07261425256729126, -0.4270014762878418, 3.1288466453552246]
    Last 10:  [1.402298927307129, -0.39026933908462524, 0.6423313617706299, 1.2350013256072998, -0.25222915410995483, -2.0360782146453857, -1.9637935161590576, 1.0525981187820435, -1.6070647239685059, 2.18375563621521]
  WEIGHT: [1024, 640] | μ=-0.000057

275_model.layers.17.self_attn.k_proj: model.layers.17.self_attn.k_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.044712 σ=2.101685
    First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
    Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
  OUT[0]: [1, 4, 256] | μ=-0.091316 σ=1.733748
    First 10: [-0.025150056928396225, 0.3404127359390259, -0.005099445581436157, 3.1741743087768555, -0.018208907917141914, 0.016297534108161926, -0.5040911436080933, 0.0033912211656570435, 0.07137191295623779, -0.0493408739566803]
    Last 10:  [3.5663981437683105, -0.7459906339645386, 0.1854495406150818, 2.554460287094116, -2.814157724380493, 0.4416767358779907, -0.9082603454589844, 3.697408437728882, -2.477283477783203, 4.522344589233398]
  WEIGHT: [256, 640] | μ=-0.000032

276_model.layers.17.self_attn.v_proj: model.layers.17.self_attn.v_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=0.044712 σ=2.101685
    First 10: [0.05447268486022949, -0.2963760197162628, 0.2802530825138092, -0.7029643654823303, 0.02315966784954071, 0.2098633050918579, 0.006461545825004578, 0.5173112750053406, -0.5458639860153198, 0.040520768612623215]
    Last 10:  [0.5500515103340149, 0.8764277696609497, -0.7105645537376404, -0.2125307023525238, -0.6758365631103516, -0.20370562374591827, -1.0981972217559814, -0.3353404998779297, 0.9006361365318298, -3.4055674076080322]
  OUT[0]: [1, 4, 256] | μ=0.053376 σ=0.894670
    First 10: [0.023865699768066406, -0.06637456268072128, -0.04135437309741974, -0.01569271832704544, 0.0027111470699310303, 0.025548547506332397, -0.15465165674686432, 0.10315865278244019, -0.03206530213356018, -0.08218654990196228]
    Last 10:  [-0.9262914061546326, 0.13054382801055908, 0.9405217170715332, -0.6541187167167664, 0.2109556794166565, 0.5401588082313538, -0.06547743082046509, -0.03769075870513916, 0.5222419500350952, -0.4424525499343872]
  WEIGHT: [256, 640] | μ=-0.000193

277_model.layers.17.self_attn.q_norm: model.layers.17.self_attn.q_norm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 4, 256] | μ=-0.001358 σ=1.868693
    First 10: [-0.22195491194725037, -3.241860866546631, 0.01906605064868927, -3.830760955810547, -1.901699423789978, 0.18472124636173248, 1.5615875720977783, -0.07261425256729126, -0.4270014762878418, 3.1288466453552246]
    Last 10:  [1.402298927307129, -0.39026933908462524, 0.6423313617706299, 1.2350013256072998, -0.25222915410995483, -2.0360782146453857, -1.9637935161590576, 1.0525981187820435, -1.6070647239685059, 2.18375563621521]
  OUT[0]: [1, 4, 4, 256] | μ=-0.029777 σ=1.425665
    First 10: [-0.2635241448879242, -0.0, 0.03937313333153725, -0.0, -0.0214015394449234, 0.40953049063682556, 0.0, -0.17814822494983673, -0.003604074940085411, -0.03521173447370529]
    Last 10:  [1.8586121797561646, -0.6090865135192871, 1.4457809925079346, 2.169586420059204, -0.4945346713066101, -6.195661544799805, -6.560952663421631, 1.9729766845703125, -3.9449288845062256, 2.8001627922058105]
  WEIGHT: [256] | μ=0.809194

278_model.layers.17.self_attn.k_norm: model.layers.17.self_attn.k_norm (Gemma3RMSNorm)
  IN[0]:  [1, 1, 4, 256] | μ=-0.091316 σ=1.733748
    First 10: [-0.025150056928396225, 0.3404127359390259, -0.005099445581436157, 3.1741743087768555, -0.018208907917141914, 0.016297534108161926, -0.5040911436080933, 0.0033912211656570435, 0.07137191295623779, -0.0493408739566803]
    Last 10:  [3.5663981437683105, -0.7459906339645386, 0.1854495406150818, 2.554460287094116, -2.814157724380493, 0.4416767358779907, -0.9082603454589844, 3.697408437728882, -2.477283477783203, 4.522344589233398]
  OUT[0]: [1, 1, 4, 256] | μ=-0.168407 σ=2.112510
    First 10: [-0.08488571643829346, 0.0, -0.006594241596758366, 0.0, 0.03997278958559036, 0.02185743674635887, -0.0, 0.004873833619058132, -0.0009792371420189738, 0.002369383815675974]
    Last 10:  [3.3527562618255615, -0.49549075961112976, 0.12252038717269897, 2.7101473808288574, -1.9027397632598877, 0.7079503536224365, -2.9301326274871826, 4.092082500457764, -1.7781769037246704, 4.257190227508545]
  WEIGHT: [256] | μ=1.574963

279_model.layers.17.self_attn.o_proj: model.layers.17.self_attn.o_proj (Linear)
  IN[0]:  [1, 4, 1024] | μ=0.002630 σ=0.357873
    First 10: [0.023865699768066406, -0.06637456268072128, -0.04135437309741974, -0.01569271832704544, 0.0027111470699310303, 0.025548547506332397, -0.15465165674686432, 0.10315865278244019, -0.03206530213356018, -0.08218654990196228]
    Last 10:  [-0.09523449093103409, 0.022913454100489616, 0.03377542272210121, -0.06238217651844025, -0.018154432997107506, 0.020654987543821335, -0.022718222811818123, 0.05276947095990181, 0.03426535055041313, -0.08765309303998947]
  OUT[0]: [1, 4, 640] | μ=-0.010114 σ=0.594780
    First 10: [-0.11932455003261566, -0.21940511465072632, -0.23479405045509338, -0.03199487924575806, 0.13006635010242462, 0.09975278377532959, -0.16305924952030182, 0.049375712871551514, 0.07185353338718414, -0.005557570606470108]
    Last 10:  [-0.12229359149932861, -0.03926490992307663, 0.07144839316606522, 0.01862509176135063, -0.003162730485200882, -0.3158438801765442, -0.10722315311431885, 0.07629942893981934, -0.11908232420682907, 0.20067761838436127]
  WEIGHT: [640, 1024] | μ=-0.000040

280_model.layers.17.self_attn: model.layers.17.self_attn (Gemma3Attention)
  OUT[0]: [1, 4, 640] | μ=-0.010114 σ=0.594780
    First 10: [-0.11932455003261566, -0.21940511465072632, -0.23479405045509338, -0.03199487924575806, 0.13006635010242462, 0.09975278377532959, -0.16305924952030182, 0.049375712871551514, 0.07185353338718414, -0.005557570606470108]
    Last 10:  [-0.12229359149932861, -0.03926490992307663, 0.07144839316606522, 0.01862509176135063, -0.003162730485200882, -0.3158438801765442, -0.10722315311431885, 0.07629942893981934, -0.11908232420682907, 0.20067761838436127]

281_model.layers.17.post_attention_layernorm: model.layers.17.post_attention_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=-0.010114 σ=0.594780
    First 10: [-0.11932455003261566, -0.21940511465072632, -0.23479405045509338, -0.03199487924575806, 0.13006635010242462, 0.09975278377532959, -0.16305924952030182, 0.049375712871551514, 0.07185353338718414, -0.005557570606470108]
    Last 10:  [-0.12229359149932861, -0.03926490992307663, 0.07144839316606522, 0.01862509176135063, -0.003162730485200882, -0.3158438801765442, -0.10722315311431885, 0.07629942893981934, -0.11908232420682907, 0.20067761838436127]
  OUT[0]: [1, 4, 640] | μ=-3.988911 σ=56.292683
    First 10: [-37.41681671142578, -48.95947265625, -9.716760635375977, -0.869111180305481, 1.3160321712493896, 23.132402420043945, -41.1424446105957, 0.7156308889389038, 3.7726731300354004, -0.5917065739631653]
    Last 10:  [-2.146132230758667, -14.262785911560059, 25.505809783935547, 9.75938892364502, -0.5975321531295776, -112.75066375732422, -37.60523986816406, 34.72388458251953, -44.747657775878906, 23.56524658203125]
  WEIGHT: [640] | μ=134.691193

282_model.layers.17.pre_feedforward_layernorm: model.layers.17.pre_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=61.651123 σ=1925.038818
    First 10: [57.27696228027344, -479.78302001953125, 18.682788848876953, -73.21731567382812, 2.2240564823150635, 300.4645690917969, -31.016429901123047, 27.866430282592773, -74.81741333007812, 26.098508834838867]
    Last 10:  [9.793448448181152, 450.4748840332031, -116.0123291015625, -129.24855041503906, -159.2620086669922, -206.56326293945312, -548.962158203125, -237.58807373046875, 387.79840087890625, -207.262939453125]
  OUT[0]: [1, 4, 640] | μ=-0.014325 σ=0.293608
    First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
    Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
  WEIGHT: [640] | μ=1.167118

283_model.layers.17.mlp.gate_proj: model.layers.17.mlp.gate_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.014325 σ=0.293608
    First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
    Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
  OUT[0]: [1, 4, 2048] | μ=-0.073597 σ=0.339770
    First 10: [0.05365362763404846, 0.055759791284799576, 0.04292953759431839, -0.1291077882051468, -0.13759341835975647, -0.004945250228047371, -0.13585807383060455, 0.03457877039909363, 0.06509476900100708, -0.018617281690239906]
    Last 10:  [-0.12517747282981873, 0.022637367248535156, -0.37455517053604126, -0.13395927846431732, -0.04859810695052147, -0.4463132619857788, -0.29756954312324524, -0.1041971817612648, -0.5414638519287109, -0.21968626976013184]
  WEIGHT: [2048, 640] | μ=-0.000132

284_model.layers.17.mlp.act_fn: model.layers.17.mlp.act_fn (PytorchGELUTanh)
  IN[0]:  [1, 4, 2048] | μ=-0.073597 σ=0.339770
    First 10: [0.05365362763404846, 0.055759791284799576, 0.04292953759431839, -0.1291077882051468, -0.13759341835975647, -0.004945250228047371, -0.13585807383060455, 0.03457877039909363, 0.06509476900100708, -0.018617281690239906]
    Last 10:  [-0.12517747282981873, 0.022637367248535156, -0.37455517053604126, -0.13395927846431732, -0.04859810695052147, -0.4463132619857788, -0.29756954312324524, -0.1041971817612648, -0.5414638519287109, -0.21968626976013184]
  OUT[0]: [1, 4, 2048] | μ=0.003373 σ=0.202564
    First 10: [0.027974698692560196, 0.029119623824954033, 0.02219977043569088, -0.05792251601815224, -0.06126783415675163, -0.0024628688115626574, -0.0605882927775383, 0.017766300588846207, 0.03423663228750229, -0.009170373901724815]
    Last 10:  [-0.056353915482759476, 0.011523104272782803, -0.13259677588939667, -0.05984204262495041, -0.02335721254348755, -0.14626172184944153, -0.11397627741098404, -0.04777511581778526, -0.15926417708396912, -0.09074385464191437]

285_model.layers.17.mlp.up_proj: model.layers.17.mlp.up_proj (Linear)
  IN[0]:  [1, 4, 640] | μ=-0.014325 σ=0.293608
    First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
    Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
  OUT[0]: [1, 4, 2048] | μ=0.011847 σ=0.300520
    First 10: [0.04402327165007591, 0.08149217069149017, 0.007446791976690292, -0.07746966928243637, -0.014085261151194572, -0.06364326924085617, 0.08462493121623993, -0.08976950496435165, -0.05302724614739418, -0.020938824862241745]
    Last 10:  [0.5140440464019775, 0.6376049518585205, -0.11391889303922653, 0.1855631321668625, -0.24643316864967346, -0.25170210003852844, 0.5061348676681519, 0.2164064645767212, -0.032487645745277405, 0.019206563010811806]
  WEIGHT: [2048, 640] | μ=0.000012

286_model.layers.17.mlp.down_proj: model.layers.17.mlp.down_proj (Linear)
  IN[0]:  [1, 4, 2048] | μ=-0.000164 σ=0.098172
    First 10: [0.001231537782587111, 0.0023730213288217783, 0.0001653170766076073, 0.004487238358706236, 0.0008629734511487186, 0.00015674502355977893, -0.00512728001922369, -0.0015948719810694456, -0.001815474359318614, 0.00019201685790903866]
    Last 10:  [-0.02896839566528797, 0.007347188424319029, 0.015105278231203556, -0.011104476638138294, 0.00575599167495966, 0.036814384162425995, -0.057687368243932724, -0.010338843800127506, 0.005174118094146252, -0.0017428775317966938]
  OUT[0]: [1, 4, 640] | μ=0.002273 σ=0.064479
    First 10: [-0.0004225907614454627, -0.0025450338143855333, -0.001179103972390294, -0.002013721037656069, 0.000198440597159788, -8.550414349883795e-05, 0.00027018238324671984, 0.0020912857726216316, -0.002258156193420291, 0.00017601378203835338]
    Last 10:  [0.03127926588058472, -0.012165391817688942, 0.03675460070371628, 0.08866649866104126, -0.006740258075296879, -0.00034081749618053436, -0.0004915045574307442, -0.003268558531999588, 0.01683509722352028, -0.0456930473446846]
  WEIGHT: [640, 2048] | μ=-0.000000

287_model.layers.17.mlp: model.layers.17.mlp (Gemma3MLP)
  IN[0]:  [1, 4, 640] | μ=-0.014325 σ=0.293608
    First 10: [0.0063055772334337234, -0.07667267322540283, 0.023177413269877434, -0.3383651077747345, 0.011963164433836937, 0.054596349596977234, -0.005415590014308691, 0.12825359404087067, -0.08218879252672195, 0.010766644962131977]
    Last 10:  [0.09692902863025665, 0.18533271551132202, -0.057207535952329636, -0.045632410794496536, -0.14057236909866333, -0.09975013136863708, -0.25548505783081055, -0.03951498866081238, 0.16294120252132416, -0.6362089514732361]
  OUT[0]: [1, 4, 640] | μ=0.002273 σ=0.064479
    First 10: [-0.0004225907614454627, -0.0025450338143855333, -0.001179103972390294, -0.002013721037656069, 0.000198440597159788, -8.550414349883795e-05, 0.00027018238324671984, 0.0020912857726216316, -0.002258156193420291, 0.00017601378203835338]
    Last 10:  [0.03127926588058472, -0.012165391817688942, 0.03675460070371628, 0.08866649866104126, -0.006740258075296879, -0.00034081749618053436, -0.0004915045574307442, -0.003268558531999588, 0.01683509722352028, -0.0456930473446846]

288_model.layers.17.post_feedforward_layernorm: model.layers.17.post_feedforward_layernorm (Gemma3RMSNorm)
  IN[0]:  [1, 4, 640] | μ=0.002273 σ=0.064479
    First 10: [-0.0004225907614454627, -0.0025450338143855333, -0.001179103972390294, -0.002013721037656069, 0.000198440597159788, -8.550414349883795e-05, 0.00027018238324671984, 0.0020912857726216316, -0.002258156193420291, 0.00017601378203835338]
    Last 10:  [0.03127926588058472, -0.012165391817688942, 0.03675460070371628, 0.08866649866104126, -0.006740258075296879, -0.00034081749618053436, -0.0004915045574307442, -0.003268558531999588, 0.01683509722352028, -0.0456930473446846]
  OUT[0]: [1, 4, 640] | μ=-3.856964 σ=413.946991
    First 10: [-72.83763122558594, -394.4461669921875, -36.11780548095703, -50.635658264160156, 1.1397275924682617, -15.206567764282227, 55.21524429321289, 20.07830810546875, -97.56190490722656, 14.967665672302246]
    Last 10:  [8.63332748413086, -79.8128890991211, 207.27023315429688, 914.1156005859375, -24.198131561279297, -2.2901246547698975, -3.39635968208313, -22.482315063476562, 120.6115951538086, -137.18568420410156]
  WEIGHT: [640] | μ=323.013672
Debug data exported to hf_model_debug_operations.json and hf_model_debug_tensors.pkl
