{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58cd21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1024]) torch.Size([1024, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2921,  1.2883,  1.2845,  ..., -2.5796, -2.5834, -2.5872],\n",
       "         [ 1.2747,  1.2710,  1.2672,  ..., -2.5450, -2.5487, -2.5524],\n",
       "         [ 1.2556,  1.2520,  1.2483,  ..., -2.5069, -2.5106, -2.5142],\n",
       "         ...,\n",
       "         [-0.1097, -0.1094, -0.1090,  ...,  0.2172,  0.2175,  0.2178],\n",
       "         [-0.1241, -0.1237, -0.1234,  ...,  0.2459,  0.2463,  0.2467],\n",
       "         [-0.1383, -0.1379, -0.1375,  ...,  0.2743,  0.2748,  0.2752]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from flashml.modules import GQA\n",
    "expansion_factor = 2\n",
    "x = torch.linspace(-0.01, 0.01, steps=81_920).view(1, 80, 1024)\n",
    "\n",
    "mha = GQA(1024, 16, 8,\n",
    "          expansion_factor=expansion_factor, \n",
    "          is_causal=True, \n",
    "          use_rope=True, \n",
    "          dropout=0.0, \n",
    "          qk_norm=True,\n",
    "          rope_theta=1_000_000,\n",
    "          rope_max_seq_len=32768)\n",
    "print(mha.w_qkv.weight.shape, mha.w_o.weight.shape)\n",
    "\n",
    "mha.w_qkv.weight.data = torch.linspace(-0.02, 0.01, 2048 * 1024 * expansion_factor).reshape(2048 * expansion_factor, 1024)\n",
    "mha.w_o.weight.data = torch.linspace(-0.01, 0.02, 1024 * 1024 *expansion_factor).reshape(1024, 1024 * expansion_factor)\n",
    "y=mha(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "\n",
    "\n",
    "rope = RotaryPositionalEmbeddings(8, max_seq_len=128)\n",
    "\n",
    "x = torch.linspace(-0.95, 0.95, steps=640).view(1, 10, 8, 8)\n",
    "# rope gets input (B, L, heads_num, head_dim)\n",
    "rope(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.zeros(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
